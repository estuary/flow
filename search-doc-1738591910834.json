{"searchDocs":[{"title":"Journals","type":0,"sectionRef":"#","url":"/concepts/advanced/journals/","content":"","keywords":"","version":"Next"},{"title":"Specification​","type":1,"pageTitle":"Journals","url":"/concepts/advanced/journals/#specification","content":" Flow collections can control some aspects of how their contents are mapped into journals through the journals stanza:  collections: acmeCo/orders: schema: orders.schema.yaml key: [/id] journals: # Configuration for journal fragments. # Required, type: object. fragments: # Codec used to compress fragment files. # One of ZSTANDARD, SNAPPY, GZIP, or NONE. # Optional. Default is GZIP. compressionCodec: GZIP # Maximum flush delay before in-progress fragment buffers are closed # and persisted. Default uses no flush interval. # Optional. Given as a time duration. flushInterval: 15m # Desired content length of each fragment, in megabytes before compression. # Default is 512MB. # Optional, type: integer. length: 512 # Duration for which historical files of the collection should be kept. # Default is forever. # Optional. Given as a time duration. retention: 720h   Your storage mappings determine which of your cloud storage buckets is used for storage of collection fragment files.  ","version":"Next","tagName":"h2"},{"title":"Physical partitions​","type":1,"pageTitle":"Journals","url":"/concepts/advanced/journals/#physical-partitions","content":" Every logical partition of a Flow collection is created with a single physical partition. Later and as required, new physical partitions are added in order to increase the write throughput of the collection.  Each physical partition is responsible for all new writes covering a range of keys occurring in collection documents. Conceptually, if keys range from [A-Z] then one partition might cover [A-F] while another covers [G-Z]. The pivot of a partition reflects the first key in its covered range. One physical partition is turned into more partitions by subdividing its range of key ownership. For instance, a partition covering [A-F] is split into partitions [A-C] and [D-F].  Physical partitions are journals. The relationship between the journal and its specific collection and logical partition are encoded withinits journal specification.  ","version":"Next","tagName":"h2"},{"title":"Fragment files​","type":1,"pageTitle":"Journals","url":"/concepts/advanced/journals/#fragment-files","content":" Journal fragment files each hold a slice of your collection's content, stored as a compressed file of newline-delimited JSON documents in your cloud storage bucket.  Files are flushed to cloud storage periodically, typically after they reach a desired size threshold. They use a content-addressed naming scheme which allows Flow to understand how each file stitches into the overall journal. Consider a fragment file path like:  s3://acmeCo-bucket/acmeCo/orders/category=Anvils/pivot=00/utc_date=2022-01-07/utc_hour=19/0000000000000000-00000000201a3f27-1ec69e2de187b7720fb864a8cd6d50bb69cc7f26.gz  This path has the following components:  Component\tExampleStorage prefix of physical partition\ts3://acmeCo-bucket/acmeCo/orders/category=Anvils/pivot=00/ Supplemental time pseudo-partitions\tutc_date=2022-01-07/utc_hour=19/ Beginning content offset\t0000000000000000 Ending content offset\t00000000201a3f27 SHA content checksum\t1ec69e2de187b7720fb864a8cd6d50bb69cc7f26 Compression codec\t.gz  The supplemental time pseudo-partitions are not logical partitions, but are added to each fragment file path to facilitate integration with external tools that understand Hive layouts.  ","version":"Next","tagName":"h2"},{"title":"Hive layouts​","type":1,"pageTitle":"Journals","url":"/concepts/advanced/journals/#hive-layouts","content":" As we've seen, collection fragment files are written to cloud storage with path components like/category=Anvils/pivot=00/utc_date=2022-01-07/utc_hour=19/.  If you've used tools within the Apache Hive ecosystem, this layout should feel familiar. Flow organizes files in this way to make them directly usable by tools that understand Hive partitioning, like Spark and Hive itself. Collections can also be integrated as Hive-compatible external tables in tools likeSnowflakeandBigQueryfor ad-hoc analysis.  SQL queries against these tables can even utilize predicate push-down, taking query predicates over category, utc_date, and utc_hourand pushing them down into the selection of files that must be read to answer the query — often offering much faster and more efficient query execution because far less data must be read. ","version":"Next","tagName":"h2"},{"title":"Schema evolution","type":0,"sectionRef":"#","url":"/concepts/advanced/evolutions/","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Schema evolution","url":"/concepts/advanced/evolutions/#introduction","content":" Flow stores your real-times datasets as collections, groups of continually updating JSON documents.Captures write data to collections, and materializations read data from collections. Together, these three components form a complete Data Flow.  Note Derivations can also read data from and write data to collections. To keep things simple in this article, we'll be referring only to captures and materializations.    graph LR; Source[Source System]--&gt;Capture; Capture--&gt;Collection; Collection--&gt;Materialization; Materialization--&gt;Dest[Destination System];  Each collection and its data are defined by a collection specification, or spec. The spec serves as a formal contract between the capture and the materialization, ensuring that data is correctly shaped and moves through the Data Flow without error.  The spec includes the collection's key, its schema, and logical partitions of the collection, if any.  When any of these parts change, any capture or materialization writing to or reading from the collection must be updated to approve of the change, otherwise, the Data Flow will fail with an error.  You can use Flow's schema evolutions feature to quickly and simultaneously update other parts of a Data Flow so you're able to re-start it without error when you introduce a collection change.  Collection specs may change for a variety of reasons, such as:  The source system is a database, and someone ran an ALTER TABLE statement on a captured table, so you need to update the collection schema (through AutoDiscover or manually).The source system contains unstructured data, and some data with a different shape was just captured so you need to update the collection schema (through AutoDiscover or manually).Someone manually changed the collection's logical partitions.  Regardless of why or how a spec change is introduced, the effect is the same. Flow will never permit you to publish changes that break this contract between captures and materializations, so you'll need to update the contract.  ","version":"Next","tagName":"h2"},{"title":"Using evolutions​","type":1,"pageTitle":"Schema evolution","url":"/concepts/advanced/evolutions/#using-evolutions","content":" When you attempt to publish a breaking change to a collection in the Flow web app, you get an error message that looks similar to this one:    Click the Apply button to trigger an evolution and update all necessary specification to keep your Data Flow functioning. Then, review and publish your draft.  If you enabled AutoDiscover on a capture, any breaking changes that it introduces will trigger an automatic schema evolution, so long as you selected the Breaking change re-versions collections option (evolveIncompatibleCollections).  ","version":"Next","tagName":"h2"},{"title":"What do schema evolutions do?​","type":1,"pageTitle":"Schema evolution","url":"/concepts/advanced/evolutions/#what-do-schema-evolutions-do","content":" The schema evolution feature is available in the Flow web app when you're editing pre-existing Flow entities. It notices when one of your edits would cause other components of the Data Flow to fail, alerts you, and gives you the option to automatically update the specs of these components to prevent failure.  In other words, evolutions happen in the draft state. Whenever you edit, you create a draft. Evolutions add to the draft so that when it is published and updates the active data flow, operations can continue seamlessly.  Alternatively, you could manually update all the specs to agree to your edit, but this becomes time-consuming and repetitive.  Evolutions can prevent errors resulting from mismatched specs in two ways:  Materialize data to a new resource in the endpoint system: The evolution updates the affected materialization bindings to increment their backfill counter, which causes the materialization to re-create the resource (database table, for example) and backfill it from the beginning. This is a simpler change, and how evolutions work in most cases. Re-create the Flow collection with a new name: The evolution creates a completely new collection with numerical suffix, such as _v2. This collection starts out empty and backfills from the source. The evolution also updates all captures and materializations that reference the old collection to instead reference the new collection, and increments their backfill counters. This is a more complicated change, and evolutions only work this way when necessary: when the collection key or logical partitioning changes.  In either case, the names of the destination resources will remain the same. For example, a materialization to Postgres would drop and re-create the affected tables with the same names they had previously.  Also in either case, only the specific bindings that had incompatible changes will be affected. Other bindings will remain untouched, and will not re-backfill.  The onIncompatibleSchemaChange field in materialization specs provides granular control over responses to incompatible schema changes. This field can be set at the top level of a materialization spec or within each binding. If not specified at the binding level, the top-level setting applies by default. The onIncompatibleSchemaChange field offers four options:  backfill (default if unspecified): Increments the backfill counter for affected bindings, recreating the destination resources to fit the new schema and backfilling them.disableBinding: Disables the affected bindings, requiring manual intervention to re-enable and resolve the incompatible fields.disableTask: Disables the entire materialization, necessitating human action to re-enable and address the incompatible fields.abort: Halts any automated action, leaving the resolution decision to a human.  These behaviors are triggered only when an automated action detects an incompatible schema change. Manual changes via the UI will ignore onIncompatibleSchemaChange. This feature can be configured using flowctl or the &quot;Advanced specification editor&quot;.  ","version":"Next","tagName":"h2"},{"title":"What causes breaking schema changes?​","type":1,"pageTitle":"Schema evolution","url":"/concepts/advanced/evolutions/#what-causes-breaking-schema-changes","content":" Though changes to the collection key or logical partition can happen, the most common cause of a breaking change is a change to the collection schema.  Generally materializations, not captures, require updates following breaking schema changes. This is because the new collection specs are usually discovered from the source, so the capture is edited at the same time as the collection.  Consider a collection schema that looks like this:  schema: type: object properties: id: { type: integer } foo: { type: string, format: date-time } required: [id] key: [/id]   If you materialized that collection into a relational database table, the table would look something like my_table (id integer primary key, foo timestamptz).  Now, say you edit the collection spec to remove format: date-time from bar. You'd expect the materialized database table to then look like (id integer primary key, foo text). But since the column type of foo has changed, this will fail. An easy solution in this case would be to change the name of the table that the collection is materialized into. Evolutions do this by appending a suffix to the original table name. In this case, you'd end up with my_table_v2 (id integer primary key, foo text). ","version":"Next","tagName":"h2"},{"title":"Logs and statistics","type":0,"sectionRef":"#","url":"/concepts/advanced/logs-stats/","content":"","keywords":"","version":"Next"},{"title":"Logs​","type":1,"pageTitle":"Logs and statistics","url":"/concepts/advanced/logs-stats/#logs","content":" Each organization that uses Flow has a logs collection under the global ops prefix. For the organization Acme Co, it would have the name ops/acmeCo/logs.  These can be thought of as standard application logs: they store information about events that occur at runtime. They’re distinct from recovery logs, which track the state of various task shards.  Regardless of how many Data Flows your organization has, all logs are stored in the same collection, which is read-only and logically partitioned on tasks. Logs are collected from events that occur within the Flow runtime, as well as the capture and materialization connectors your Data Flow is using.  ","version":"Next","tagName":"h2"},{"title":"Log level​","type":1,"pageTitle":"Logs and statistics","url":"/concepts/advanced/logs-stats/#log-level","content":" You can set the log level for each catalog task to control the level of detail at which logs are collected for that task. The available levels, listed from least to most detailed, are:  error: Non-recoverable errors from the Flow runtime or connector that are critical to know aboutwarn: Errors that can be re-tried, but likely require investigationinfo: Task lifecycle events, or information you might want to collect on an ongoing basisdebug: Details that will help debug an issue with a tasktrace: Maximum level of detail that may yield gigabytes of logs  The default log level is info. You can change a task’s log level by adding the shards keyword to its definition in the catalog spec:  materializations: acmeCo/debugMaterialization: shards: logLevel: debug endpoint: {}   ","version":"Next","tagName":"h3"},{"title":"Statistics​","type":1,"pageTitle":"Logs and statistics","url":"/concepts/advanced/logs-stats/#statistics","content":" Each organization that uses Flow has a stats collection under the global ops prefix. For the organization Acme Co, it would have the name ops/acmeCo/stats.  Regardless of how many Data Flows your organization has, all stats are stored in the same collection, which is read-only and logically partitioned on tasks.  A new document is published to the stats collection for each task transaction. Each document includes information about the time and quantity of data inputs and outputs. Statistics vary by task type (capture, materialization, or derivation).  Use stats to:  Evaluate the data throughput of a task; for example, a derivation.Compare a data throughput of a task between platforms; for example, compare reported data capture by Flow to detected change rate in a source system.Access the same information used by Estuary for billing.Optimize your tasks for increased efficiency.  See a detailed table of the properties included in stats documents.  ","version":"Next","tagName":"h2"},{"title":"Working with logs and statistics​","type":1,"pageTitle":"Logs and statistics","url":"/concepts/advanced/logs-stats/#working-with-logs-and-statistics","content":" Learn more about working with logs and statistics ","version":"Next","tagName":"h2"},{"title":"Concepts","type":0,"sectionRef":"#","url":"/concepts/","content":"","keywords":"","version":"Next"},{"title":"Working with Flow​","type":1,"pageTitle":"Concepts","url":"/concepts/#working-with-flow","content":" There are two main environments in which you can work with Flow: the web application or your preferred local environment using the flowctl command-line tool.  ","version":"Next","tagName":"h2"},{"title":"Web application​","type":1,"pageTitle":"Concepts","url":"/concepts/#web-application","content":" The Flow web application is where you'll most likely create, monitor, and update your Data Flows. You can find it at dashboard.estuary.dev. The app is backed by secure, cloud-hosted infrastructure that Estuary manages.  Take a tour of the web app.  Create your first Data Flow with the web app.  ","version":"Next","tagName":"h3"},{"title":"flowctl​","type":1,"pageTitle":"Concepts","url":"/concepts/#flowctl","content":" flowctl is a command-line interface for working with Flow's public API. Using flowctl, developers can inspect, edit, test, and publish Data Flows — just as with the web application. For example, you can create a Data Flow with the web app, and then use flowctl to fetch it into files that you manage within a Git repo.  Learn more about flowctl  ","version":"Next","tagName":"h3"},{"title":"Essential concepts​","type":1,"pageTitle":"Concepts","url":"/concepts/#essential-concepts","content":" In this section, you'll find the most important Flow terms and concepts. If you're new to Flow, start here.  A complete end-to-end Data Flow between two systems has three components:  Capture: Flow ingests data from an outside source.Collections: Flow maintains the captured data in cloud storage.Materialization: Flow pushes data to an outside destination.    graph LR; Capture--&gt;Collection; Collection--&gt;Materialization;  It may also include:  Derivations: You apply a transformation to data in a collection, resulting in a new collection.  All of these entities are described in the catalog.  ","version":"Next","tagName":"h2"},{"title":"Catalog​","type":1,"pageTitle":"Concepts","url":"/concepts/#catalog","content":" The catalog is the set of active entities that comprise all Data Flows: captures, materializations, derivations, collections, schemas, tests, and more.  All catalog entities are defined in Flow specification files — either manually, by you, or generated by the Flow web app. You create the specifications as drafts, and publish them to add them to the catalog.  You can mix and match catalog entities to create a variety of Data Flows.  Learn more about the catalog    ","version":"Next","tagName":"h3"},{"title":"Collections​","type":1,"pageTitle":"Concepts","url":"/concepts/#collections","content":" Collections represent datasets within Flow. All captured documents are written to a collection, and all materialized documents are read from a collection.  Collections are a real-time data lake. Like a traditional data lake, the documents that make up a collection are stored as plain JSON in your cloud storage bucket. Unlike a traditional data lake, updates to the collection are reflected downstream in the data flow within milliseconds.  Documents in collections are stored indefinitely in your cloud storage bucket (or may be managed with your regular bucket lifecycle policies). This means that the full historical content of a collection is available to support future data operations and perform backfills without going back to the source.  Each collection has a keyed schema against which incoming documents are validated. This ensures that data is always clean and organized.  Learn more about collections    ","version":"Next","tagName":"h3"},{"title":"Captures​","type":1,"pageTitle":"Concepts","url":"/concepts/#captures","content":" A capture is a Flow task that ingests data from an external source into one or more Flow collections. Documents continuously move from the source into Flow; as new documents become available at the source, Flow validates their schema and adds them to their corresponding collection. Captures interface with source systems using connectors.  Learn more about captures    ","version":"Next","tagName":"h3"},{"title":"Materializations​","type":1,"pageTitle":"Concepts","url":"/concepts/#materializations","content":" A materialization is a Flow task that pushes data from one or more collections to an external destination. Documents continuously moves from each Flow collection to the destination. Materializations are the conceptual inverse of captures.  As new documents become available within bound collections, the materialization keeps the destination up to date within milliseconds, or as fast as that system allows. Materializations interface with destinations using connectors.  Learn more about materializations    ","version":"Next","tagName":"h3"},{"title":"Endpoints​","type":1,"pageTitle":"Concepts","url":"/concepts/#endpoints","content":" Endpoints are the source systems from which Flow captures data and the destination systems to which Flow materializes data. All kinds of data systems can be endpoints, including databases, key/value stores, streaming pub/sub systems, SaaS products, and cloud storage locations.  Flow connects to this wide variety of endpoints using connectors.    ","version":"Next","tagName":"h3"},{"title":"Connectors​","type":1,"pageTitle":"Concepts","url":"/concepts/#connectors","content":" Connectors are plugin components that allow Flow to interface with endpoint data systems. They power captures and materializations.  Flow uses an open-source connector model. Many connectors are made by Estuary, and others are made by third parties. Because connectors are open-source and kept separate from Flow itself, new integrations can be added and updated quickly. This is important, as the landscape of data systems and platforms is constantly evolving.  All currently supported connectors are ready to use in the Flow web application. They're also available as Docker images, each encapsulating the details of working with a particular source or destination system.  Learn more about connectors    ","version":"Next","tagName":"h3"},{"title":"Intermediate concepts​","type":1,"pageTitle":"Concepts","url":"/concepts/#intermediate-concepts","content":" In this section, you'll find important concepts that are optional for basic usage. Read this to unlock more powerful workflows.  ","version":"Next","tagName":"h2"},{"title":"Derivations​","type":1,"pageTitle":"Concepts","url":"/concepts/#derivations","content":" A derivation is a collection that results from the transformation of one or more other collections, which is continuously updated in sync with its source collection(s).  You can use derivations to map, reshape, and filter documents. They can also be used to tackle complex stateful streaming workflows, including joins and aggregations, without windowing and scaling limitations.  Learn more about derivations    ","version":"Next","tagName":"h3"},{"title":"Schemas​","type":1,"pageTitle":"Concepts","url":"/concepts/#schemas","content":" All collections in Flow have an associatedJSON schemaagainst which documents are validated every time they're written or read. Schemas are critical to how Flow ensures the integrity of your data. Flow validates your documents to ensure that bad data doesn't make it into your collections — or worse, into downstream data products!  JSON schema is a flexible standard for representing structure, invariants, and other constraints over your documents. Schemas can be very permissive, highly exacting, or somewhere in between.  Flow pauses catalog tasks when documents don't match the collection schema, alerting you to the mismatch and allowing you to fix it before it creates a bigger problem.  Learn more about schemas    ","version":"Next","tagName":"h3"},{"title":"Reductions​","type":1,"pageTitle":"Concepts","url":"/concepts/#reductions","content":" Every Flow collection schema includes a key. The key is used to identify collection documents and determine how they are grouped. When a collection is materialized into a database table, for example, its key becomes the SQL primary key of the materialized table.  Flow also uses the key to reduce documents in collections, making storage and materializations more efficient. If multiple documents of a given key are added to a collection, by default, the most recent document supersedes all previous documents of that key.  You can exert more control over your data by changing the default reduction strategy. By doing so, you can deeply merge documents, maintain running counts, and achieve other complex aggregation behaviors.  Learn more about reductions    ","version":"Next","tagName":"h3"},{"title":"Tests​","type":1,"pageTitle":"Concepts","url":"/concepts/#tests","content":" Tests become an important part of your Data Flows when you add derivations and customized reduction behavior. You use tests to verify the end-to-end behavior of your collections and derivations.  A test is a sequence of ingestion or verification steps. Ingestion steps ingest one or more document fixtures into a collection, and verification steps assert that the contents of another derived collection match a test expectation.  Learn more about tests    ","version":"Next","tagName":"h3"},{"title":"Tasks​","type":1,"pageTitle":"Concepts","url":"/concepts/#tasks","content":" Captures, derivations, and materializations are collectively referred to as catalog tasks. They are the &quot;active&quot; components of a Data Flow, each running continuously and reacting to documents as they become available.  Collections, by way of comparison, are inert. They reflect data at rest, and are acted upon by catalog tasks:  A capture adds documents to a collection pulled from a source endpoint.A derivation updates a collection by applying transformations to other collections.A materialization reacts to changes of a collection to update a destination endpoint.    ","version":"Next","tagName":"h3"},{"title":"Resources and bindings​","type":1,"pageTitle":"Concepts","url":"/concepts/#resources-and-bindings","content":" A resource is an addressable collection of data within a source or destination system. The exact meaning of a resource is up to the endpoint and its connector. For example:  Resources of a database endpoint might be its individual tables.Resources of a Kafka cluster might be its topics.Resources of a SaaS connector might be its various API feeds.  When you create capture or materialization, it connects a collection to a resource through a binding. A given capture or materialization may have multiple bindings, which connect multiple collections to different resources.    ","version":"Next","tagName":"h3"},{"title":"Storage mappings​","type":1,"pageTitle":"Concepts","url":"/concepts/#storage-mappings","content":" Flow collections use cloud storage buckets for the durable storage of data. Storage mappings define how Flow maps your various collections into your storage buckets and prefixes.  Learn more about storage mappings  ","version":"Next","tagName":"h3"},{"title":"Advanced concepts​","type":1,"pageTitle":"Concepts","url":"/concepts/#advanced-concepts","content":" This section discusses advanced Flow concepts. The information here unlocks a more technical understanding of how Flow works, and may be helpful in advanced use cases.  ","version":"Next","tagName":"h2"},{"title":"Journals​","type":1,"pageTitle":"Concepts","url":"/concepts/#journals","content":" Journals provide the low-level storage for Flow collections. Each logical and physical partition of a collection is backed by a journal.  Task shards also use journals to provide for their durability and fault tolerance. Each shard has an associated recovery log, which is a journal into which internal checkpoint states are written.  Learn more about journals  ","version":"Next","tagName":"h3"},{"title":"Task shards​","type":1,"pageTitle":"Concepts","url":"/concepts/#task-shards","content":" Task shards are the unit of execution for a catalog task. A single task can have many shards, which allow the task to scale across many machines to achieve more throughput and parallelism.  Shards are created and managed by the Flow runtime. Each shard represents a slice of the overall work of the catalog task, including its processing status and associated internal checkpoints. Catalog tasks are created with a single shard, which can be repeatedly subdivided at any time — with no downtime — to increase the processing capacity of the task.  Learn more about shards  ","version":"Next","tagName":"h3"},{"title":"Projections​","type":1,"pageTitle":"Concepts","url":"/concepts/#projections","content":" Flow leverages your JSON schemas to produce other types of schemas as needed, such as TypeScript types and SQL CREATE TABLE statements.  In many cases these projections provide comprehensive end-to-end type safety of Data Flows and their TypeScript transformations, all statically verified at build time.  Learn more about projections ","version":"Next","tagName":"h3"},{"title":"Projections","type":0,"sectionRef":"#","url":"/concepts/advanced/projections/","content":"","keywords":"","version":"Next"},{"title":"Logical partitions​","type":1,"pageTitle":"Projections","url":"/concepts/advanced/projections/#logical-partitions","content":" Projections can also be used to logically partition a collection, specified as a longer-form variant of a projection definition:  collections: acmeCo/user-sessions: schema: session.schema.yaml key: [/user/id, /timestamp] projections: country: location: /country partition: true device: location: /agent/type partition: true network: location: /agent/network partition: true   Logical partitions isolate the storage of documents by their differing values for partitioned fields. Flow extracts partitioned fields from each document, and every unique combination of partitioned fields is a separate logical partition.  Every logical partition has one or more physical partitionsinto which their documents are written, which in turn controls how files are arranged within cloud storage.  For example, a document of &quot;acmeCo/user-sessions&quot; like:  {&quot;country&quot;: &quot;CA&quot;, &quot;agent&quot;: {&quot;type&quot;: &quot;iPhone&quot;, &quot;network&quot;: &quot;LTE&quot;}, ...}   Might produce files in cloud storage like:  s3://bucket/example/sessions/country=CA/device=iPhone/network=LTE/pivot=00/utc_date=2020-11-04/utc_hour=16/&lt;name&gt;.gz   info country, device, and network together identify a logical partition, while pivot identifies a physical partition.utc_date and utc_hour is the time at which the journal fragment was created.  Learn more about physical partitions.  ","version":"Next","tagName":"h2"},{"title":"Partition selectors​","type":1,"pageTitle":"Projections","url":"/concepts/advanced/projections/#partition-selectors","content":" When reading from a collection, Flow catalog entities like derivations, materializations, and tests can provide a partition selector, which identifies the subset of partitions that should be read from a source collection:  # Partition selectors are included as part of a larger entity, # such as a derivation or materialization. partitions: # `include` selects partitioned fields and corresponding values that # must be matched in order for a partition to be processed. # All of the included fields must be matched. # Default: All partitions are included. type: object include: # Include partitions from North America. country: [US, CA] # AND where the device is a mobile phone. device: [iPhone, Android] # `exclude` selects partitioned fields and corresponding values which, # if matched, exclude the partition from being processed. # A match of any of the excluded fields will exclude the partition. # Default: No partitions are excluded. type: object exclude: # Skip sessions which were over a 3G network. network: [&quot;3G&quot;]   Partition selectors are efficient as they allow Flow to altogether avoid reading documents that aren’t needed. ","version":"Next","tagName":"h3"},{"title":"Task shards","type":0,"sectionRef":"#","url":"/concepts/advanced/shards/","content":"","keywords":"","version":"Next"},{"title":"Shard splits​","type":1,"pageTitle":"Task shards","url":"/concepts/advanced/shards/#shard-splits","content":" When a task is first created, it is initialized with a single shard. Later and as required, shards may be split into two shards. This is done by the service operator on your behalf, depending on the size of your task. Shard splitting doesn't require downtime; your task will continue to run as normal on the old shard until the split occurs and then shift seamlessly to the new, split shards.  This process can be repeated as needed until your required throughput is achieved. If you have questions about how shards are split for your tasks, contact your Estuary account representative.  ","version":"Next","tagName":"h2"},{"title":"Transactions​","type":1,"pageTitle":"Task shards","url":"/concepts/advanced/shards/#transactions","content":" Shards process messages in dynamic transactions.  Whenever a message is ready to be processed by the task (when new documents appear at the source endpoint or collection), a new transaction is initiated. The transaction will continue so long as further messages are available for processing. When no more messages are immediately available, the transaction closes. A new transaction is started whenever the next message is available.  In general, shorter transaction durations decrease latency, while longer transaction durations increase efficiency. Flow automatically balances these two extremes to optimize each task, but it may be useful in some cases to control transaction duration. For example, materializations to large analytical warehouses may benefit from longer transactions, which can reduce cost by performing more data reduction before landing data in the warehouse. Some endpoint systems, like BigQuery, limit the number of table operations you can perform. Longer transaction durations ensure that you don't exceed these limits.  You can set the minimum and maximum transaction duration in a task's shards configuration.  ","version":"Next","tagName":"h2"},{"title":"Recovery logs​","type":1,"pageTitle":"Task shards","url":"/concepts/advanced/shards/#recovery-logs","content":" All task shards have associated state, which is managed in the shard's store.  Capture tasks must track incremental checkpoints of their endpoint connectors.Derivation tasks manage a potentially very large index of registers, as well as read checkpoints of sourced collection journals.Materialization tasks track incremental checkpoints of their endpoint connectors, as well as read checkpoints of sourced collection journals.  Shard stores userecovery logsto replicate updates and implement transaction semantics.  Recovery logs are regular journals, but hold binary data and are not intended for direct use. However, they can hold your user data. Recovery logs of derivations hold your derivation register values.  Recovery logs are stored in your cloud storage bucket, and must have a configured storage mapping. ","version":"Next","tagName":"h2"},{"title":"Catalog","type":0,"sectionRef":"#","url":"/concepts/catalogs/","content":"","keywords":"","version":"Next"},{"title":"Data Flows​","type":1,"pageTitle":"Catalog","url":"/concepts/catalogs/#data-flows","content":" You can mix and match catalog entities to create a variety of Data Flows.  The simplest Data Flow has just three entities.    graph LR; Capture--&gt;Collection; Collection--&gt;Materialization;  It may also be more complex, combining multiple entities of each type.  graph LR; capture/two--&gt;collection/D; capture/one--&gt;collection/C; capture/one--&gt;collection/A; collection/A--&gt;derivation/B; collection/D--&gt;derivation/E; collection/C--&gt;derivation/E; derivation/B--&gt;derivation/E; collection/D--&gt;materialization/one; derivation/E--&gt;materialization/two;  ","version":"Next","tagName":"h2"},{"title":"Flow specification files​","type":1,"pageTitle":"Catalog","url":"/concepts/catalogs/#flow-specification-files","content":" Catalog entities are defined and described in Flow specification files.These YAML files contain the configuration details that each entity requires.  You work on specification files as drafts before you publish them to a catalog.  There are two ways to create and work with specification files.  ","version":"Next","tagName":"h2"},{"title":"In the Flow web app​","type":1,"pageTitle":"Catalog","url":"/concepts/catalogs/#in-the-flow-web-app","content":" You don't need to write or edit the specification files directly — the web app is designed to generate them for you. You do have the option to review and edit the generated specification as you create captures and materializations using the Catalog Editor.  ","version":"Next","tagName":"h3"},{"title":"With flowctl​","type":1,"pageTitle":"Catalog","url":"/concepts/catalogs/#with-flowctl","content":" If you prefer a developer workflow, you can also work with specification files directly in your local environment using flowctl. You then publish them back to the catalog.  A given Data Flow may be described by one specification file, or by many, so long as a top-level file imports all the others.  The files use the extension *.flow.yaml or are simply named flow.yaml by convention. Using this extension activates Flow's VS Code integration and auto-complete. Flow integrates with VS Code for development environment support, like auto-complete, tooltips, and inline documentation.  Depending on your Data Flow, you may also have TypeScript modules, JSON schemas, or test fixtures.  ","version":"Next","tagName":"h3"},{"title":"Namespace​","type":1,"pageTitle":"Catalog","url":"/concepts/catalogs/#namespace","content":" All catalog entities (captures, materializations, and collections) are identified by a namesuch as acmeCo/teams/manufacturing/anvils. Names have directory-like prefixes and every name within Flow is globally unique.  If you've ever used database schemas to organize your tables and authorize access, you can think of name prefixes as being akin to database schemas with arbitrary nesting.  All catalog entities exist together in a single namespace. As a Flow customer, you're provisioned one or more high-level prefixes for your organization. Further division of the namespace into prefixes is up to you.  Prefixes of the namespace, like acmeCo/teams/manufacturing/, are the foundation for Flow's authorization model. ","version":"Next","tagName":"h2"},{"title":"Connectors","type":0,"sectionRef":"#","url":"/concepts/connectors/","content":"","keywords":"","version":"Next"},{"title":"Using connectors​","type":1,"pageTitle":"Connectors","url":"/concepts/connectors/#using-connectors","content":" Most — if not all — of your Data Flows will use at least one connector. You configure connectors within capture or materialization specifications. When you publish one of these entities, you're also deploying all the connectors it uses.  You can interact with connectors using either the Flow web application or the flowctl CLI.  ","version":"Next","tagName":"h2"},{"title":"Flow web application​","type":1,"pageTitle":"Connectors","url":"/concepts/connectors/#flow-web-application","content":" The Flow web application is designed to assist you with connector configuration and deployment. It's a completely no-code experience, but it's compatible with Flow's command line tools, discussed below.  When you add a capture or materialization in the Flow web app, choose the desired data system from the Connector drop-down menu.  The required fields for the connector appear below the drop-down. When you fill in the fields and click Discover Endpoint, Flow automatically &quot;discovers&quot; the data streams or tables — known as resources — associated with the endpoint system. From there, you can refine the configuration, save, and publish the resulting Flow specification.  ","version":"Next","tagName":"h3"},{"title":"GitOps and flowctl​","type":1,"pageTitle":"Connectors","url":"/concepts/connectors/#gitops-and-flowctl","content":" Connectors are packaged as Open Container (Docker) images, and can be tagged, and pulled usingDocker Hub,GitHub Container registry, or any other public image registry provider.  To interface with a connector, the Flow runtime needs to know:  The specific image to use, through an image name such as ghcr.io/estuary/source-postgres:dev. Notice that the image name also conveys the specific image registry and version tag to use. Endpoint configuration such as a database address and account, with meaning that is specific to the connector. Resource configuration such as a specific database table to capture, which is also specific to the connector.  To integrate a connector into your dataflow, you must define all three components within your Flow specification.  The web application is intended to help you generate the Flow specification. From there, you can use flowctl to refine it in your local environment. It's also possible to manually write your Flow specification files, but this isn't the recommended workflow.  materializations: acmeCo/postgres-views: endpoint: connector: # 1: Provide the image that implements your endpoint connector. # The `dev` tag uses the most recent version (the web app chooses this tag automatically) image: ghcr.io/estuary/materialize-postgres:dev # 2: Provide endpoint configuration that the connector requires. config: address: localhost:5432 password: password database: postgres user: postgres bindings: - source: acmeCo/products/anvils # 3: Provide resource configuration for the binding between the Flow # collection and the endpoint resource. This connector interfaces # with a SQL database and its resources are database tables. Here, # we provide a table to create and materialize which is bound to the # `acmeCo/products/anvils` source collection. resource: table: anvil_products # Multiple resources can be configured through a single connector. # Bind additional collections to tables as part of this connector instance: - source: acmeCo/products/TNT resource: table: tnt_products - source: acmeCo/customers resource: table: customers   Configuration​  Because connectors interface with external systems, each requires a slightly different endpoint configuration. Here you specify information such as a database hostname or account credentials — whatever that specific connector needs to function.  If you're working directly with Flow specification files, you have the option of including the configuration inline or storing it in separate files:  InlineReferenced file my.flow.yaml materializations: acmeCo/postgres-views: endpoint: connector: image: ghcr.io/estuary/materialize-postgres:dev config: address: localhost:5432 password: password database: postgres user: postgres bindings: []   Storing configuration in separate files serves two important purposes:  Re-use of configuration across multiple captures or materializationsThe ability to protect sensitive credentials  ","version":"Next","tagName":"h3"},{"title":"Connecting to endpoints on secure networks​","type":1,"pageTitle":"Connectors","url":"/concepts/connectors/#connecting-to-endpoints-on-secure-networks","content":" In some cases, your source or destination endpoint may be within a secure network, and you may not be able to allow direct access to its port due to your organization's security policy.  tip If permitted by your organization, a quicker solution is to allowlist the Estuary IP addresses For help completing this task on different cloud hosting platforms, see the documentation for the connector you're using.  SHH tunneling, or port forwarding, provides a means for Flow to access the port indirectly through an SSH server. SSH tunneling is available in Estuary connectors for endpoints that use a network address for connection.  To set up and configure the SSH server, see the guide. Then, add the appropriate properties when you define the capture or materialization in the Flow web app, or add the networkTunnel stanza directly to the YAML, as shown below.  Sample​  source-postgres-ssh-tunnel.flow.yaml captures: acmeCo/postgres-capture-ssh: endpoint: connector: image: ghcr.io/estuary/source-postgres:dev config: address: 127.0.0.1:5432 database: flow user: flow_user password: secret networkTunnel: sshForwarding: # Location of the remote SSH server that supports tunneling. # Formatted as ssh://user@hostname[:port]. sshEndpoint: ssh://sshUser@198.21.98.1:22 # Private key to connect to the SSH server, formatted as multiline plaintext. # Use the YAML literal block style with the indentation indicator. # See https://yaml-multiline.info/ for details. privateKey: |2 -----BEGIN RSA PRIVATE KEY----- MIICXAIBAAKBgQCJO7G6R+kv2MMS8Suw21sk2twHg8Vog0fjimEWJEwyAfFM/Toi EJ6r5RTaSvN++/+MPWUll7sUdOOBZr6ErLKLHEt7uXxusAzOjMxFKZpEARMcjwHY v/tN1A2OYU0qay1DOwknEE0i+/Bvf8lMS7VDjHmwRaBtRed/+iAQHf128QIDAQAB AoGAGoOUBP+byAjDN8esv1DCPU6jsDf/Tf//RbEYrOR6bDb/3fYW4zn+zgtGih5t CR268+dwwWCdXohu5DNrn8qV/Awk7hWp18mlcNyO0skT84zvippe+juQMK4hDQNi ywp8mDvKQwpOuzw6wNEitcGDuACx5U/1JEGGmuIRGx2ST5kCQQDsstfWDcYqbdhr 5KemOPpu80OtBYzlgpN0iVP/6XW1e5FCRp2ofQKZYXVwu5txKIakjYRruUiiZTza QeXRPbp3AkEAlGx6wMe1l9UtAAlkgCFYbuxM+eRD4Gg5qLYFpKNsoINXTnlfDry5 +1NkuyiQDjzOSPiLZ4Abpf+a+myjOuNL1wJBAOwkdM6aCVT1J9BkW5mrCLY+PgtV GT80KTY/d6091fBMKhxL5SheJ4SsRYVFtguL2eA7S5xJSpyxkadRzR0Wj3sCQAvA bxO2fE1SRqbbF4cBnOPjd9DNXwZ0miQejWHUwrQO0inXeExNaxhYKQCcnJNUAy1J 6JfAT/AbxeSQF3iBKK8CQAt5r/LLEM1/8ekGOvBh8MAQpWBW771QzHUN84SiUd/q xR9mfItngPwYJ9d/pTO7u9ZUPHEoat8Ave4waB08DsI= -----END RSA PRIVATE KEY----- bindings: []   ","version":"Next","tagName":"h3"},{"title":"Why an open connector architecture?​","type":1,"pageTitle":"Connectors","url":"/concepts/connectors/#why-an-open-connector-architecture","content":" Historically, data platforms have directly implemented integrations to external systems with which they interact. Today, there are simply so many systems and APIs that companies use, that it’s not feasible for a company to provide all possible integrations. Users are forced to wait indefinitely while the platform works through their prioritized integration list.  An open connector architecture removes Estuary — or any company — as a bottleneck in the development of integrations. Estuary contributes open-source connectors to the ecosystem, and in turn is able to leverage connectors implemented by others. Users are empowered to write their own connectors for esoteric systems not already covered by the ecosystem.  Furthermore, implementing a Docker-based community specification brings other important qualities to Estuary connectors:  Cross-platform interoperability between Flow, Airbyte, and any other platform that supports the protocolThe abilities to write connectors in any language and run them on any machineBuilt-in solutions for version management (through image tags) and distributionThe ability to integrate connectors from different sources at will, without the centralized control of a single company, thanks to container image registries  info In order to be reflected in the Flow web app and used on the managed Flow platform, connectors must be reviewed and added by the Estuary team. Have a connector you'd like to add?Contact us.  ","version":"Next","tagName":"h2"},{"title":"Available connectors​","type":1,"pageTitle":"Connectors","url":"/concepts/connectors/#available-connectors","content":" Learn about available connectors in the reference section ","version":"Next","tagName":"h2"},{"title":"Captures","type":0,"sectionRef":"#","url":"/concepts/captures/","content":"","keywords":"","version":"Next"},{"title":"Connectors​","type":1,"pageTitle":"Captures","url":"/concepts/captures/#connectors","content":" Captures extract data from an endpoint using a connector. Estuary builds and maintains many real-time connectors for various technology systems, such as database change data capture (CDC) connectors.  See the source connector reference documentation.  ","version":"Next","tagName":"h2"},{"title":"Batch sources​","type":1,"pageTitle":"Captures","url":"/concepts/captures/#batch-sources","content":" Flow supports running both first and third party connectors to batch sources as well as natively-written Estuary connectors. These connectors tend to focus on SaaS APIs, and do not offer real-time streaming integrations. Flow runs the connector at regular intervals to capture updated documents.  Third-party source connectors are independently reviewed and sometimes updated for compatibility with Flow. Estuary's source connectors documentation includes all actively supported connectors. If you see a connector you'd like to prioritize for access in the Flow web app, contact us.  ","version":"Next","tagName":"h3"},{"title":"Discovery​","type":1,"pageTitle":"Captures","url":"/concepts/captures/#discovery","content":" To help you configure new pull captures, Flow offers the guided discovery workflow in the Flow web application.  To begin discovery, you tell Flow the connector you'd like to use and basic information about the endpoint. Flow automatically generates a capture configuration for you. It identifies one or moreresources — tables, data streams, or the equivalent — and generates bindings so that each will be mapped to a data collection in Flow.  You may then modify the generated configuration as needed before publishing the capture.  info Discovers can also be run when editing an existing capture. This is commonly done in order to add new bindings, or update the collection specs and schemas associated with existing bindings.  ","version":"Next","tagName":"h2"},{"title":"Automatically update captures​","type":1,"pageTitle":"Captures","url":"/concepts/captures/#automatically-update-captures","content":" You can choose to run periodic discovers in the background by adding the autoDiscover property to the capture. Flow will periodically check for changes to the source and re-publish the capture to reflect those changes.  There are several options for controlling the behavior of autoDiscover:  The addNewBindings option determines whether to add newly discovered resources, such as database tables, to the capture as bindings. If set to false, autoCapture will only update the collection specs for existing bindings. The evolveIncompatibleCollections option determines how to respond when the discovered updates would cause a breaking change to the collection. If true, it will trigger an evolution of the incompatible collection(s) to prevent failures.  In the Flow web app, you can set these properties when you create or edit a capture.    The toggles in the web app correspond directly to the properties above:  &quot;Automatically keep schemas up to date&quot; enables autoDiscover&quot;Automatically add new collections&quot; corresponds to addNewBindings&quot;Breaking changes re-versions collections&quot; corresponds to evolveIncompatibleCollections  ","version":"Next","tagName":"h2"},{"title":"Specification​","type":1,"pageTitle":"Captures","url":"/concepts/captures/#specification","content":" Captures are defined in Flow specification files per the following format:  # A set of captures to include in the catalog. # Optional, type: object captures: # The name of the capture. acmeCo/example/source-s3: # Automatically performs periodic discover operations, which updates the bindings # to reflect what's in the source, and also updates collection schemas. # To disable autoDiscover, either omit this property or set it to `null`. autoDiscover: # Also add any newly discovered bindings automatically addNewBindings: true # How to handle breaking changes to discovered collections. If true, then existing # materialization bindings will be re-created with new names, as necessary. Or if # collection keys have changed, then new Flow collections will be created. If false, # then incompatible changes will simply result in failed publications, and will # effectively be ignored. evolveIncompatibleCollections: true # Endpoint defines how to connect to the source of the capture. # Required, type: object endpoint: # This endpoint uses a connector provided as a Docker image. connector: # Docker image that implements the capture connector. image: ghcr.io/estuary/source-s3:dev # File that provides the connector's required configuration. # Configuration may also be presented inline. config: path/to/connector-config.yaml # Bindings define how collections are populated from the data source. # A capture may bind multiple resources to different collections. # Required, type: array bindings: - # The target collection to capture into. # This may be defined in a separate, imported specification file. # Required, type: string target: acmeCo/example/collection # The resource is additional configuration required by the endpoint # connector to identify and capture a specific endpoint resource. # The structure and meaning of this configuration is defined by # the specific connector. # Required, type: object resource: stream: a-bucket/and-prefix # syncMode should be set to incremental for all Estuary connectors syncMode: incremental - target: acmeCo/example/another-collection resource: stream: a-bucket/another-prefix syncMode: incremental # Interval of time between invocations of non-streaming connectors. # If a connector runs to completion and then exits, the capture task will # restart the connector after this interval of time has elapsed. # # Intervals are relative to the start of an invocation and not its completion. # For example, if the interval is five minutes, and an invocation of the # capture finishes after two minutes, then the next invocation will be started # after three additional minutes. # # Optional. Default: Five minutes. interval: 5m  ","version":"Next","tagName":"h2"},{"title":"Imports","type":0,"sectionRef":"#","url":"/concepts/import/","content":"","keywords":"","version":"Next"},{"title":"Specification​","type":1,"pageTitle":"Imports","url":"/concepts/import/#specification","content":" The import section is structured as a list of partial or absolute URIs, which Flow always evaluates relative to the base directory of the current source file. For example, these are possible imports within a collection:  # Suppose we're in file &quot;/path/dir/flow.yaml&quot; import: - sub/directory/flow.yaml # Resolves to &quot;file:///path/dir/sub/directory/flow.yaml&quot;. - ../sibling/directory/flow.yaml # Resolves to &quot;file:///path/sibling/directory/flow.yaml&quot;. - https://example/path/flow.yaml # Uses the absolute url.   The import rule is flexible; a collection doesn’t have to do anything special to be imported by another, and flowctl can even directly build remote sources:  # Test an example from a GitHub repository. $ flowctl draft test --source https://raw.githubusercontent.com/estuary/flow-template/main/word-counts.flow.yaml   ","version":"Next","tagName":"h2"},{"title":"Fetch behavior​","type":1,"pageTitle":"Imports","url":"/concepts/import/#fetch-behavior","content":" Flow resolves, fetches, and validates all imports in your local environment during the catalog build process, and then includes their fetched contents within the published catalog on the Estuary servers. The resulting catalog entities are thus self-contained snapshots of all resourcesas they were at the time of publication.  This means it's both safe and recommended to directly reference an authoritative source of a resource, such as a third-party JSON schema, as well as resources within your private network. It will be fetched and verified locally at build time, and thereafter that fetched version will be used for execution, regardless of whether the authority URL itself later changes or errors.  ","version":"Next","tagName":"h2"},{"title":"Import types​","type":1,"pageTitle":"Imports","url":"/concepts/import/#import-types","content":" Almost always, the import stanza is used to import other Flow specification files. This is the default when given a string path:  import: - path/to/source/catalog.flow.yaml   A long-form variant also accepts a content type of the imported resource:  import: - url: path/to/source/catalog.flow.yaml contentType: CATALOG   Other permitted content types include JSON_SCHEMA, but these are not typically used and are needed only for advanced use cases.  ","version":"Next","tagName":"h2"},{"title":"JSON Schema $ref​","type":1,"pageTitle":"Imports","url":"/concepts/import/#json-schema-ref","content":" Certain catalog entities, like collections, commonly reference JSON schemas. It's not necessary to explicitly add these to the import section; they are automatically resolved and treated as an import. You can think of this as an analog to the JSON Schema $ref keyword, which is used to reference a schema that may be contained in another file.  The one exception is schemas that use the $id keyword at their root to define an alternative canonical URL. In this case, the schema must be referenced through its canonical URL, and then explicitly added to the import section with JSON_SCHEMA content type.  ","version":"Next","tagName":"h2"},{"title":"Importing derivation resources​","type":1,"pageTitle":"Imports","url":"/concepts/import/#importing-derivation-resources","content":" In many cases, derivations in your catalog will need to import resources. Usually, these are TypeScript modules that define the lambda functions of a transformation, and, in certain cases, the NPM dependencies of that TypeScript module.  These imports are specified in the derivation specification, not in the import section of the specification file.  For more information, see Derivation specification and creating TypeScript modules.  ","version":"Next","tagName":"h2"},{"title":"Import paths​","type":1,"pageTitle":"Imports","url":"/concepts/import/#import-paths","content":"   If a catalog source file foo.flow.yaml references a collection in bar.flow.yaml, for example as a target of a capture, there must be an import path where either foo.flow.yamlimports bar.flow.yaml or vice versa.  When you omit the import section, Flow chooses an import path for you. When you explicitly include the import section, you have more control over the import path.  Import paths can be direct:  graph LR; foo.flow.yaml--&gt;bar.flow.yaml;  Or they can be indirect:  graph LR; bar.flow.yaml--&gt;other.flow.yaml; other.flow.yaml--&gt;foo.flow.yaml;  The sources must still have an import path even if referenced from a common parent. The following would not work:  graph LR; parent.flow.yaml--&gt;foo.flow.yaml; parent.flow.yaml--&gt;bar.flow.yaml;  These rules make your catalog sources more self-contained and less brittle to refactoring and reorganization. Consider what might otherwise happen if foo.flow.yamlwere imported in another project without bar.flow.yaml. ","version":"Next","tagName":"h2"},{"title":"Collections","type":0,"sectionRef":"#","url":"/concepts/collections/","content":"","keywords":"","version":"Next"},{"title":"Documents​","type":1,"pageTitle":"Collections","url":"/concepts/collections/#documents","content":" Flow processes and stores data in terms of documents: JSON files that consist of multiple key-value pair objects. Collections are comprised of documents; Flow tasks (captures, materializations, and derivations) process data in terms of documents.  A Flow document corresponds to different units of data in different types of endpoint systems. For example, it might map to a table row, a pub/sub message, or an API response. The structure of a given collection’s documents is determined by that collection’s schema and the way in which tasks handle documents is determined by the collection key.  The size of a document depends on the complexity of the source data. Flow allows documents up to 16 MB in size, but it's rare for documents to approach this limit.  An example document for a collection with two fields, name and count is shown below.  { &quot;_meta&quot;: { &quot;uuid&quot;: &quot;xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx&quot; }, &quot;count&quot;: 5954, &quot;message&quot;: &quot;Hello #5954&quot; }   ","version":"Next","tagName":"h2"},{"title":"System Fields and Metadata​","type":1,"pageTitle":"Collections","url":"/concepts/collections/#system-fields-and-metadata","content":" The _meta object is present in all Flow documents, and contains metadata added by Flow. Minimally, every document _meta always has a uuid, which is a globally unique id for each document. Some capture connectors may add additional _meta properties to tie each document to a specific record within the source system. Documents that were captured from cloud storage connectors, for example, will contain /_meta/file and /_meta/offset properties that tell you where the document came from within your cloud storage bucket.  _meta/uuid​  The _meta/uuid field is a system-generated globally unique identifier for each document within Estuary Flow.  flow_published_at​  The flow_published_at field is a system-generated timestamp within Estuary Flow, derived from the runtime environment. It captures the exact moment a document is published to a collection, offering a reliable proxy for when the document was last modified or inserted.  Source: The flow_published_at field is generated by the runtime environment of Estuary Flow.Definition: This field represents the timestamp when a document is captured and subsequently published to a collection. Essentially, it is a projection of the _meta/uuid field, where the UUID contains an encoded timestamp component.Availability: The flow_published_at field is available in every collection, as it is a derived projection from the _meta/uuid field.  For a given document identified by a unique key, the flow_published_at field can be used as a proxy for the last time the document was modified. This is particularly useful when performing incremental updates or transformations, such as in a data warehouse environment.  When dealing with materializations that are not delta updates:  A document in Estuary Flow is any JSON object emitted by a capture connector. The flow_published_at field provides the timestamp for when this JSON object was captured and inserted into the collection.If the collection is reduced with a strategy like lastWriteWins or merge on the capture side, flow_published_at becomes the timestamp for the last event that updated the document.  ","version":"Next","tagName":"h3"},{"title":"Viewing collection documents​","type":1,"pageTitle":"Collections","url":"/concepts/collections/#viewing-collection-documents","content":" In many cases, it's not necessary to view your collection data — you're able to materialize it directly to a destination in the correct shape using a connector.  However, it can be helpful to view collection documents to confirm the source data was captured as expected, or verify a schema change.  In the web application​  Sign into the Flow web application and click the Collections tab. The collections to which you have access are listed. Click the Details drop down to show a sample of collection documents as well as the collection specification.  The collection documents are displayed by key. Click the desired key to preview it in its native JSON format.  Using the flowctl CLI​  In your authenticated flowctl session, issue the command flowctl collections read --collection &lt;full/collection-name&gt; --uncommitted. For example, flowctl collections read --collection acmeCo/inventory/anvils --uncommitted.  Options are available to read a subset of data from collections. For example, --since allows you to specify an approximate start time from which to read data, and--include-partition allows you to read only data from a specified logical partition. Use flowctl collections read --help to see documentation for all options.  Beta While in beta, this command currently has the following limitations. They will be removed in a later release: The --uncommitted flag is required. This means that all collection documents are read, regardless of whether they were successfully committed or not. In the future, reads of committed documents will be the default. Only reads of a single partition are supported. If you need to read from a partitioned collection, use --include-partition or --exclude-partition to narrow down to a single partition. The --output flag is not usable for this command. Only JSON data can be read from collections.  ","version":"Next","tagName":"h2"},{"title":"Specification​","type":1,"pageTitle":"Collections","url":"/concepts/collections/#specification","content":" Collections are defined in Flow specification files per the following format:  # A set of collections to include in the catalog. # Optional, type: object collections: # The unique name of the collection. acmeCo/products/anvils: # The schema of the collection, against which collection documents # are validated. This may be an inline definition or a relative URI # reference. # Required, type: string (relative URI form) or object (inline form) schema: anvils.schema.yaml # The key of the collection, specified as JSON pointers of one or more # locations within collection documents. If multiple fields are given, # they act as a composite key, equivalent to a SQL table PRIMARY KEY # with multiple table columns. # Required, type: array key: [/product/id] # Projections and logical partitions for this collection. # Optional, type: object projections: # Derivation that builds this collection from others through transformations. # See the &quot;Derivations&quot; concept page to learn more. # Optional, type: object derive: ~   ","version":"Next","tagName":"h2"},{"title":"Schemas​","type":1,"pageTitle":"Collections","url":"/concepts/collections/#schemas","content":" Every Flow collection must declare a schema, and will never accept documents that do not validate against the schema. This helps ensure the quality of your data products and the reliability of your derivations and materializations. Schema specifications are flexible: yours could be exactingly strict, extremely permissive, or somewhere in between. For many source types, Flow is able to generate a basic schema during discovery.  Schemas may either be declared inline, or provided as a reference to a file. References can also include JSON pointers as a URL fragment to name a specific schema of a larger schema document:    InlineFile referenceReference with pointer collections: acmeCo/collection: schema: type: object required: [id] properties: id: string key: [/id]   Learn more about schemas  ","version":"Next","tagName":"h2"},{"title":"Keys​","type":1,"pageTitle":"Collections","url":"/concepts/collections/#keys","content":" Every Flow collection must declare a key which is used to group its documents. Keys are specified as an array of JSON pointers to document locations. For example:  flow.yamlschema.yaml collections: acmeCo/users: schema: schema.yaml key: [/userId]   Suppose the following JSON documents are captured into acmeCo/users:  {&quot;userId&quot;: 1, &quot;name&quot;: &quot;Will&quot;} {&quot;userId&quot;: 1, &quot;name&quot;: &quot;William&quot;} {&quot;userId&quot;: 1, &quot;name&quot;: &quot;Will&quot;}   As its key is [/userId], a materialization of the collection into a database table will reduce to a single row:  userId | name 1 | Will   If its key were instead [/name], there would be two rows in the table:  userId | name 1 | Will 1 | William   ","version":"Next","tagName":"h2"},{"title":"Schema restrictions​","type":1,"pageTitle":"Collections","url":"/concepts/collections/#schema-restrictions","content":" Keyed document locations may be of a limited set of allowed types:  booleanintegerstring  Excluded types are:  arraynullobjectFractional number  Keyed fields also must always exist in collection documents. Flow performs static inference of the collection schema to verify the existence and types of all keyed document locations, and will report an error if the location could not exist, or could exist with the wrong type.  Flow itself doesn't mind if a keyed location has multiple types, so long as they're each of the allowed types: an integer or string for example. Some materialization connectors, however, may impose further type restrictions as required by the endpoint. For example, SQL databases do not support multiple types for a primary key.  ","version":"Next","tagName":"h3"},{"title":"Composite Keys​","type":1,"pageTitle":"Collections","url":"/concepts/collections/#composite-keys","content":" A collection may have multiple locations which collectively form a composite key. This can include locations within nested objects and arrays:  flow.yamlschema.yaml collections: acmeCo/compound-key: schema: schema.yaml key: [/foo/a, /foo/b, /foo/c/0, /foo/c/1]   ","version":"Next","tagName":"h3"},{"title":"Key behaviors​","type":1,"pageTitle":"Collections","url":"/concepts/collections/#key-behaviors","content":" A collection key instructs Flow how documents of a collection are to be reduced, such as while being materialized to an endpoint. Flow also performs opportunistic local reductions over windows of documents to improve its performance and reduce the volumes of data at each processing stage.  An important subtlety is that the underlying storage of a collection will potentially retain many documents of a given key.  In the acmeCo/users example, each of the &quot;Will&quot; or &quot;William&quot; variants is likely represented in the collection's storage — so long as they didn't arrive so closely together that they were locally combined by Flow. If desired, a derivation could re-key the collection on [/userId, /name] to materialize the various /names seen for a /userId.  This property makes keys less lossy than they might otherwise appear, and it is generally good practice to choose a key that reflects how you wish to query a collection, rather than an exhaustive key that's certain to be unique for every document.  ","version":"Next","tagName":"h3"},{"title":"Empty keys​","type":1,"pageTitle":"Collections","url":"/concepts/collections/#empty-keys","content":" When a specification is automatically generated, there may not be an unambiguously correct key for all collections. This could occur, for example, when a SQL database doesn't have a primary key defined for some table.  In cases like this, the generated specification will contain an empty collection key. However, every collection must have a non-empty key, so you'll need to manually edit the generated specification and specify keys for those collections before publishing to the catalog.  ","version":"Next","tagName":"h3"},{"title":"Projections​","type":1,"pageTitle":"Collections","url":"/concepts/collections/#projections","content":" Projections are named locations within a collection document that may be used for logical partitioning or directly exposed to databases into which collections are materialized.  Many projections are automatically inferred from the collection schema. The projections stanza can be used to provide additional projections, and to declare logical partitions:  collections: acmeCo/products/anvils: schema: anvils.schema.yaml key: [/product/id] # Projections and logical partitions for this collection. # Keys name the unique projection field, and values are its JSON Pointer # location within the document and configure logical partitioning. # Optional, type: object projections: # Short form: define a field &quot;product_id&quot; with document pointer /product/id. product_id: &quot;/product/id&quot; # Long form: define a field &quot;metal&quot; with document pointer /metal_type # which is a logical partition of the collection. metal: location: &quot;/metal_type&quot; partition: true   Learn more about projections.  ","version":"Next","tagName":"h2"},{"title":"Storage​","type":1,"pageTitle":"Collections","url":"/concepts/collections/#storage","content":" Collections are real-time data lakes. Historical documents of the collection are stored as an organized layout of regular JSON files in your cloud storage bucket. Reads of that history are served by directly reading files from your bucket.  Your storage mappingsdetermine how Flow collections are mapped into your cloud storage buckets.  Unlike a traditional data lake, however, it's very efficient to read collection documents as they are written. Derivations and materializations that source from a collection are notified of its new documents within milliseconds of their being published.  Learn more about journals, which provide storage for collections ","version":"Next","tagName":"h2"},{"title":"flowctl","type":0,"sectionRef":"#","url":"/concepts/flowctl/","content":"","keywords":"","version":"Next"},{"title":"Installation and setup​","type":1,"pageTitle":"flowctl","url":"/concepts/flowctl/#installation-and-setup","content":" flowctl binaries for MacOS and Linux are available. Fow Windows, install Windows Subsystem for Linux (WSL) to run Linux on Windows, or use a remote development environment.  Copy and paste the appropriate script below into your terminal. This will download flowctl, make it executable, and add it to your PATH. For Linux: sudo curl -o /usr/local/bin/flowctl -L 'https://github.com/estuary/flow/releases/latest/download/flowctl-x86_64-linux' &amp;&amp; sudo chmod +x /usr/local/bin/flowctl For Mac: sudo curl -o /usr/local/bin/flowctl -L 'https://github.com/estuary/flow/releases/latest/download/flowctl-multiarch-macos' &amp;&amp; sudo chmod +x /usr/local/bin/flowctl Alternatively, Mac users can install with Homebrew: brew tap estuary/flowctl brew install flowctl You can also find the source files on GitHub here. To connect to your Flow account and start a session, use an authentication token from the web app.  ","version":"Next","tagName":"h2"},{"title":"User guides​","type":1,"pageTitle":"flowctl","url":"/concepts/flowctl/#user-guides","content":" View guides for common flowctl workflows.  ","version":"Next","tagName":"h2"},{"title":"flowctl subcommands​","type":1,"pageTitle":"flowctl","url":"/concepts/flowctl/#flowctl-subcommands","content":" flowctl includes several top-level subcommands representing different functional areas. Each of these include multiple nested subcommands. Important top-level flowctl subcommands are described below.  auth allows you to authenticate your development session in your local development environment. It's also how you provision Flow roles and users. Learn more about authentication. catalog allows you to work with your organization's current active catalog entities. You can investigate the current Data Flows, pull specifications for local editing, test and publish specifications that you wrote or edited locally, and delete entities from the catalog. collections allows you to work with your Flow collections. You can read the data from the collection and output it to stdout, or list the journals or journal fragments that comprise the collection. Learn more about reading collections with flowctl. draft provides an alternative method for many of the actions you'd normally perform with catalog, but common workflows have more steps.  You can access full documentation of all flowctl subcommands from the command line by passing the --help or -h flag, for example:  flowctl --help lists top-level flowctl subcommands. flowctl catalog --help lists subcommands of catalog.  ","version":"Next","tagName":"h2"},{"title":"Editing Data Flows with flowctl​","type":1,"pageTitle":"flowctl","url":"/concepts/flowctl/#editing-data-flows-with-flowctl","content":" flowctl allows you to work locally on the specification files that define your Data Flows. You'll often need to move these specifications back and forth between your local environment and the catalogof published entities.  The basic steps of this workflow are listed below, along with a diagram of the subcommands you'd use to accomplish them. Keep in mind that there's no single, correct way to work with flowctl, but we recommend this method to get started.  List all the active specifications in the catalog, which you can then pull into your local environment. You can filter the output by prefix or entity type. For example, flowctl catalog list --prefix acmeCo/sales/ --collections only lists collections under theacmeCo/sales/ prefix. Pull a group of active specifications directly, resulting in local source files. You can refine results by prefix or entity type as described above (1). Note that if there are already files in your working directory, flowctl must reconcile them with the newly pulled specification.Learn more about your options. Make edits locally. Test local specifications (2). Publish local specifications to the catalog (3).  graph LR; d[Local environment]; c[Active catalog]; d-- 2: flowctl catalog test --&gt;d; d-- 3: flowctl catalog publish --&gt;c; c-- 1: flowctl catalog pull-specs --&gt;d;  View the step-by-step guide.  ","version":"Next","tagName":"h2"},{"title":"Reconciling specifications in local drafts​","type":1,"pageTitle":"flowctl","url":"/concepts/flowctl/#reconciling-specifications-in-local-drafts","content":" When you pull specifications to your working directory directly using flowctl catalog pull-specs, there may be conflicts between the existing files in that directory and the specifications you pull.  By default, flowctl catalog pull-specs will abort if it detects an existing file with the same name as a specification it is attempting to pull. You can change this behavior with the --existing flag:  --existing=overwrite pulls the new versions of conflicting files in place of the old versions. --existing=keep keeps the old versions of conflicting files. --existing=merge-specs performs a simple merge of new and old versions of conflicting files. For example, if an existing flow.yaml file references collections a and b, and the new version of flow.yaml references collections a and c, the merged version will reference collections a, b, and c.  ","version":"Next","tagName":"h3"},{"title":"Development directories​","type":1,"pageTitle":"flowctl","url":"/concepts/flowctl/#development-directories","content":" Flow specifications and other files are written to your working directory when you run flowctl draft develop or flowctl catalog pull-specs.  They typically include:  flow.yaml: The main specification file that imports all other Flow specification files created in a single operation. As part of local development, you may add new specifications that you create as imports. flow_generated/: Directory of generated files, including TypeScript classes and interfaces. See TypeScript code generation. &lt;prefix-name&gt;/: Directory of specifications that you pulled. Its name corresponds to your catalog prefix. Its contents will vary, but it may contain various YAML files and subdirectories. package.json and package-lock.json: Files used by npm to manage dependencies and your Data Flow's associated JavaScript project. You may customize package.json, but its dependencies stanza will be overwritten by thenpmDependenciesof your Flow specification source files, if any exist.  When you run commands like flowctl catalog publish or flowctl draft author, you can use the --source-dir flag to push specifications from a directory other than your current working directory, for example, flowctl draft author --source-dir ../AcmeCoNew/marketing.  ","version":"Next","tagName":"h2"},{"title":"TypeScript code generation​","type":1,"pageTitle":"flowctl","url":"/concepts/flowctl/#typescript-code-generation","content":" TypeScript files are used in the Flow catalog both as part of the automatic build process, and to define lambdas functions for derivations, which requires your input.  As part of the Data Flow build process, Flow translates yourschemasinto equivalent TypeScript types on your behalf. These definitions live within flow_generated/ in your Data Flow's build directory , and are frequently over-written by invocations of flowctl. Files in this subdirectory are human-readable and stable. You may want to commit them as part of a GitOps-managed project, but this isn't required.  Whenever you define a derivation that uses a lambda, you must define the lambda in an accompanying TypeScript module, and reference that module in the derivation's definition. To facilitate this, you can generate a stub of the module using flowctl generateand simply write the function bodies.Learn more about this workflow.  If a TypeScript module exists, flowctl will never overwrite it, even if you update or expand your specifications such that the required interfaces have changed.  ","version":"Next","tagName":"h3"},{"title":"Protecting secrets​","type":1,"pageTitle":"flowctl","url":"/concepts/flowctl/#protecting-secrets","content":" Most endpoint systems require credentials of some kind, such as a username or password.  Sensitive credentials should be protected while not in use. The only time a credential needs to be directly accessed is when Flow initiates the connector.  Flow integrates with Mozilla’s sops tool, which can encrypt and protect credentials. It stores a sops-protected configuration in its encrypted form, and decrypts it only when invoking a connector on the your behalf.  sops, short for “Secrets Operations,” is a tool that encrypts the values of a JSON or YAML document against a key management system (KMS) such as Google Cloud Platform KMS, Azure Key Vault, or Hashicorp Vault. Encryption or decryption of a credential with sops is an active process: it requires that the user (or the Flow runtime identity) have a current authorization to the required KMS, and creates a request trace which can be logged and audited. It's also possible to revoke access to the KMS, which immediately and permanently removes access to the protected credential.  When you use the Flow web application, Flow automatically adds sops protection to sensitive fields on your behalf. You can also implement sops manually if you are writing a Flow specification locally. The examples below provide a useful reference.  ","version":"Next","tagName":"h2"},{"title":"Example: Protect a configuration​","type":1,"pageTitle":"flowctl","url":"/concepts/flowctl/#example-protect-a-configuration","content":" Suppose you're given a connector configuration:  config.yaml host: my.hostname password: &quot;this is sensitive!&quot; user: my-user   You can protect it using a Google KMS key that you own:  # Login to Google Cloud and initialize application default credentials used by `sops`. $ gcloud auth application-default login # Use `sops` to re-write the configuration document in place, protecting its values. $ sops --encrypt --in-place --gcp-kms projects/your-project-id/locations/us-central1/keyRings/your-ring/cryptoKeys/your-key-name config.yaml   sops re-writes the file, wrapping each value in an encrypted envelope and adding a sops metadata section:  config.yaml host: ENC[AES256_GCM,data:K/clly65pThTg2U=,iv:1bNmY8wjtjHFBcXLR1KFcsNMGVXRl5LGTdREUZIgcEU=,tag:5GKcguVPihXXDIM7HHuNnA==,type:str] password: ENC[AES256_GCM,data:IDDY+fl0/gAcsH+6tjRdww+G,iv:Ye8st7zJ9wsMRMs6BoAyWlaJeNc9qeNjkkjo6BPp/tE=,tag:EPS9Unkdg4eAFICGujlTfQ==,type:str] user: ENC[AES256_GCM,data:w+F7MMwQhw==,iv:amHhNCJWAJnJaGujZgjhzVzUZAeSchEpUpBau7RVeCg=,tag:62HguhnnSDqJdKdwYnj7mQ==,type:str] sops: # Some items omitted for brevity: gcp_kms: - resource_id: projects/your-project-id/locations/us-central1/keyRings/your-ring/cryptoKeys/your-key-name created_at: &quot;2022-01-05T15:49:45Z&quot; enc: CiQAW8BC2GDYWrJTp3ikVGkTI2XaZc6F4p/d/PCBlczCz8BZiUISSQCnySJKIptagFkIl01uiBQp056c lastmodified: &quot;2022-01-05T15:49:45Z&quot; version: 3.7.1   You then use this config.yaml within your Flow specification. The Flow runtime knows that this document is protected by sopswill continue to store it in its protected form, and will attempt a decryption only when invoking a connector on your behalf.  If you need to make further changes to your configuration, edit it using sops config.yaml. It's not required to provide the KMS key to use again, as sops finds it within its metadata section.  important When deploying catalogs onto the managed Flow runtime, you must grant access to decrypt your GCP KMS key to the Flow runtime service agent, which is: flow-258@helpful-kingdom-273219.iam.gserviceaccount.com   ","version":"Next","tagName":"h3"},{"title":"Example: Protect portions of a configuration​","type":1,"pageTitle":"flowctl","url":"/concepts/flowctl/#example-protect-portions-of-a-configuration","content":" Endpoint configurations are typically a mix of sensitive and non-sensitive values. It can be cumbersome when sops protects an entire configuration document as you lose visibility into non-sensitive values, which you might prefer to store as cleartext for ease of use.  You can use the encrypted-suffix feature of sops to selectively protect credentials:  config.yaml host: my.hostname password_sops: &quot;this is sensitive!&quot; user: my-user   Notice that password in this configuration has an added _sops suffix. Next, encrypt only values which have that suffix:  $ sops --encrypt --in-place --encrypted-suffix &quot;_sops&quot; --gcp-kms projects/your-project-id/locations/us-central1/keyRings/your-ring/cryptoKeys/your-key-name config.yaml   sops re-writes the file, wrapping only values having a &quot;_sops&quot; suffix and adding its sops metadata section:  config.yaml host: my.hostname password_sops: ENC[AES256_GCM,data:dlfidMrHfDxN//nWQTPCsjoG,iv:DHQ5dXhyOOSKI6ZIzcUM67R6DD/2MSE4LENRgOt6GPY=,tag:FNs2pTlzYlagvz7vP/YcIQ==,type:str] user: my-user sops: # Some items omitted for brevity: encrypted_suffix: _sops gcp_kms: - resource_id: projects/your-project-id/locations/us-central1/keyRings/your-ring/cryptoKeys/your-key-name created_at: &quot;2022-01-05T16:06:36Z&quot; enc: CiQAW8BC2Au779CGdMFUjWPhNleCTAj9rL949sBvPQ6eyAC3EdESSQCnySJKD3eWX8XrtrgHqx327 lastmodified: &quot;2022-01-05T16:06:37Z&quot; version: 3.7.1   You then use this config.yaml within your Flow specification. Flow looks for and understands the encrypted_suffix, and will remove this suffix from configuration keys before passing them to the connector.  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"flowctl","url":"/concepts/flowctl/#troubleshooting","content":" If you're developing locally with flowctl, watch out for these errors:  Failed to locate sops: sops may not be installed correctly. See these installation instructions and ensure sops is on your PATH. For details on working with sops, see Protecting secrets above. Decrypting sops document failed: ensure you have correctly applied a KMS key using sops to your configuration file. See above for examples. Note that you will not be able to decrypt credentials entered via the Flow web app.  Since updates are released regularly, make sure you're using the latest version of flowctl. You can see the latest versions and changelogs on the Flow releases page.  To check your current version, run: flowctl --version  If you installed flowctl with Homebrew, you can upgrade with: brew update &amp;&amp; brew upgrade flowctl ","version":"Next","tagName":"h2"},{"title":"Storage mappings","type":0,"sectionRef":"#","url":"/concepts/storage-mappings/","content":"","keywords":"","version":"Next"},{"title":"Recovery logs​","type":1,"pageTitle":"Storage mappings","url":"/concepts/storage-mappings/#recovery-logs","content":" In addition to collection data, Flow uses your storage mapping to temporarily store recovery logs.  Flow tasks — captures, derivations, and materializations — use recovery logs to durably store their processing context as a backup. Recovery logs are an opaque binary log, but may contain user data.  The recovery logs of a task are always prefixed by recovery/, so a task named acmeCo/produce-TNT would have a recovery log called recovery/acmeCo/produce-TNT  Flow prunes data from recovery logs once it is no longer required.  warning Deleting data from recovery logs while it is still in use can cause Flow processing tasks to fail permanently. ","version":"Next","tagName":"h2"},{"title":"Derivations","type":0,"sectionRef":"#","url":"/concepts/derivations/","content":"","keywords":"","version":"Next"},{"title":"Specification​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#specification","content":" A derivation is specified as a regular collectionwith an additional derive stanza:  collections: # The unique name of the derivation. acmeCo/my/derivation: schema: my-schema.yaml key: [/key] # Presence of a `derive` stanza makes this collection a derivation. # Type: object derive: # Connector which this derivation uses. # One of `typescript` or `sqlite`. using: # Derivation is using the SQLite connector. # Optional, type: object sqlite: # SQL migrations to apply as inline SQL or file references. # If a referenced file does not exist # a stub can be generated using `flowctl generate`. # Optional, type: array of strings migrations: - CREATE TABLE foobar (id INTEGER PRIMARY KEY NOT NULL); - ../path/to/other/migration.sql # Derivation is using the TypeScript connector. # Optional, type: object typescript: # TypeScript module implementing this derivation, # as inline TypeScript or a relative file reference. # If a referenced file does not exist # a stub can be generated using `flowctl generate`. module: acmeModule.ts # The array of transformations that build this derived collection. transform: # Unique name of the transformation, containing only Unicode # Letters, Numbers, `-`, or `_` (no spaces or other punctuation). - name: myTransformName # Source collection read by this transformation. # Required, type: object or string. source: # Name of the collection to be read. # Required. name: acmeCo/my/source/collection # Partition selector of the source collection. # Optional. Default is to read all partitions. partitions: {} # Lower bound date-time for documents which should be processed. # Source collection documents published before this date-time are filtered. # `notBefore` is *only* a filter. Updating its value will not cause Flow # to re-process documents that have already been read. # Optional. Default is to process all documents. notBefore: 2023-01-23T01:00:00Z # Upper bound date-time for documents which should be processed. # Source collection documents published after this date-time are filtered. # Like `notBefore`, `notAfter` is *only* a filter. Updating its value will # not cause Flow to re-process documents that have already been read. # Optional. Default is to process all documents. notAfter: 2023-01-23T02:00:00Z # Lambda of this transform, with a meaning which depends # on the derivation connector: # * SQLite derivation lambdas are blocks of SQL code. # * TypeScript does not use `lambda`, as implementations # are provided by the derivation's TypeScript module. # Lambdas can be either inline or a relative file reference. lambda: SELECT $foo, $bar; # Delay applied to sourced documents before being processed # by this transformation. # Default: No delay, pattern: ^\\\\d+(s|m|h)$ readDelay: &quot;48h&quot; # Key by which source documents are shuffled to task shards. # Optional, type: object. # If not set, the source collection key is used. shuffle: # Composite key of JSON pointers which are extracted from # source documents. key: [/shuffle/key/one, /shuffle/key/two] # Priority applied to documents of this transformation # relative to other transformations of the derivation. # Default: 0, integer &gt;= 0 priority: 0   ","version":"Next","tagName":"h2"},{"title":"Supported Languages​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#supported-languages","content":" As with captures and materializations, Flow derivations are built around a plug-able connectors architecture. Derivation connectors encapsulate the details of how documents are transformed, and integrate with Flow's runtime through a common protocol.  At present, Flow supports transformations in SQL using SQLite, and TypeScript.  ","version":"Next","tagName":"h2"},{"title":"SQLite​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#sqlite","content":" Flow's SQLite connector lets you write plain SQL which is evaluated with each source collection document:  derive: using: sqlite: {} transforms: - name: fromOrders source: acmeCo/orders shuffle: any lambda: SELECT $customer, DATE($timestamp) AS date, PRINTF('$%.2f', $item_price + $sales_tax) AS cost;   Given an input document:  { &quot;customer&quot;: &quot;Wile E. Coyote&quot;, &quot;timestamp&quot;: &quot;2023-04-17T16:45:31Z&quot;, &quot;item_price&quot;: 11.5, &quot;sales_tax&quot;: 0.8 }   The derivation will produce an output document like:  { &quot;customer&quot;: &quot;Wile E. Coyote&quot;, &quot;date&quot;: &quot;2023-04-17&quot;, &quot;cost&quot;: &quot;$12.30&quot; }   SQLite derivations run within the context of a persistent, managed SQLite database. Most anything you can do within SQLite, you can do within a SQLite derivation.  ","version":"Next","tagName":"h2"},{"title":"SQL Lambdas​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#sql-lambdas","content":" Lambdas are blocks of one or more SQL statements. They can be defined inline within a Flow specification, or they can be provided as a relative file reference to a file of SQL.  Your SQL lambda code can include any number of statements, and your statements are evaluated in the context of your applied database migrations. Use regular INSERT, UPDATE, and DELETE statements in your SQL blocks to manipulate your internal tables as required.  Any rows which are returned by SQL statements, such as SELECT and also variations like INSERT ... RETURNING, are mapped into documents that are published into your derived collection. Published documents must conform to your collection schema or your derivation task will stop due to the schema violation.  tip The SQLite connector wraps your lambdas in an enclosing transaction. Do not include BEGIN or COMMIT statements in your lambdas. You may use a SAVEPOINT or ROLLBACK TO.  ","version":"Next","tagName":"h3"},{"title":"Document Mapping​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#document-mapping","content":" In most cases, each named output column of your query becomes a top-level property of its corresponding document.  When you directly select a $parameter its corresponding field name is used. For example, a projection with field name my-field would be queried asSELECT $my_field; and map into a document like {&quot;my-field&quot;:&quot;value&quot;}.  A named column such as SELECT $x * 100 AS foo;, maps to a property using the provided name: {&quot;foo&quot;: 200}.  Your selected columns may included nested JSON documents, such as SELECT 'hello' AS greeting, JSON_ARRAY(1, 'two', 3) AS items;. The connector looks for SQLite TEXT values which can be parsed into JSON arrays or objects and embeds them into the mapped document:{&quot;greeting&quot;: &quot;hello&quot;, &quot;items&quot;: [1, &quot;two&quot;, 3]}. If parsing fails, the raw string is used instead.  If you would like to select all columns of the input collection, rather than select *, use select JSON($flow_document), e.g.select JSON($flow_document where $status = open;.  As a special case if your query selects a single column having a name that begins with json or JSON, as is common when working with SQLite's JSON functions, then that column will become the output document. For example SELECT JSON_OBJECT('a', 1, 'b', JSON('true')); maps into document {&quot;a&quot;: 1, &quot;b&quot;: true}. This can be used to build documents with dynamic top-level properties.  ","version":"Next","tagName":"h3"},{"title":"Parameters​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#parameters","content":" Your SQL lambda will execute with every source document of the collection it transforms. To access locations within the document, you utilize $parameter placeholders in your SQL code, which bind to projections of the source document. You can use both your defined projections as well as projections which are statically inferred from your source collection's schema.  You can access projected fields that are top-level as well as those which are nested within a source document. Consider the following schematized document:  {&quot;top-level&quot;: true, &quot;object&quot;: {&quot;foo&quot;: 42}, &quot;arr&quot;: [&quot;bar&quot;]}   In your SQL code, you can use parameters like $top_level, $object$foo, or $arr$0. If you're unsure of what parameter to use for a given field, try typing something approximate and Flow will suggest the appropriate $parameter.  ","version":"Next","tagName":"h3"},{"title":"Migrations​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#migrations","content":" The SQLite connector offers a managed, persistent SQLite database that can accommodate any number of tables, indices, views, triggers, and other schema, as defined by your database migrations. To add a migration, simply append it to the migrations array, either as a block of inline SQL statements or as a relative path to a file of SQL statements:  derive: using: sqlite: migrations: - CREATE TABLE foo (thing INTEGER NOT NULL); CREATE INDEX idx_foo_thing foo (thing); - ../path/to/another/migration.sql - ALTER TABLE foo ADD COLUMN other_thing TEXT NOT NULL; - https://example.com/yet/another/migration.sql   caution You cannot change an existing migration once it has been published. Instead, add a new migration which applies your desired schema.  The tables and other schema you create through database migrations are the internal state of your derivation. They don't directly cause any documents to be published into your derived collection, but changes made to tables in one SQL lambda execution are immediately visible to others. Changes are also durable and transactional: a Flow derivation transaction commits new documents to the derived collection in lockstep with committing changes made to your task tables.  Flow is responsible for the persistence and replication of your SQLite database, and the SQLite connector tracks and will apply your migrations as needed.  ","version":"Next","tagName":"h3"},{"title":"Performance​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#performance","content":" Your observed performance will of course depend on the specifics of your use case, including the size of your task states and the complexity of your source documents and transformations.  Generally speaking, SQLite is very performant and Flow's SQLite connector strives to drive it as efficiently as possible. Real-world use cases are observed to process many tens of thousands of documents per second on a single core.  Flow can also scale your task without downtime by creating point-in-time clones of the database that subdivide the overall workload and storage of the task. Once created, these subdivisions process in parallel across multiple physical machines to enhance performance.  ","version":"Next","tagName":"h3"},{"title":"TypeScript​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#typescript","content":" Flow's TypeScript derivation connector transforms your source documents by executing methods of a TypeScript class which you implement. TypeScript derivations are executed using Denoand let you take advantage of the broad ecosystem of available third-party JavaScript and TypeScript libraries, as well as native code compiled to WASM.  TypeScript derivations are strongly typed: Flow maps the JSON schemas of your source and output collections into corresponding TypeScript types, which are type-checked as you develop and test your derivation. This helps catch a wide variety of potential bugs and avoid accidental violations of your collection data contracts.  ","version":"Next","tagName":"h2"},{"title":"Modules​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#modules","content":" The bulk of a TypeScript derivation lives in its associated module, which is a TypeScript source file that exports the class that implements your derivation.  Each derivation also has an accompanying, generated interfaces module. Interface modules are managed by Flow and are purely advisory: they're generated to improve your development experience, but any changes you make are ignored.  The flowctl generate --source path/to/my/derivation.flow.yaml CLI command will generate interface modules under paths likeflow_generated/typescript/acmeCo/my-derivation.ts, under the top-level directory under --source having a flow.yaml or flow.json file.  It will also generate a deno.json file in your top-level directory, which is designed to work with developer tooling likeVSCode's Deno extension.  See the Current Account Balances tutorial for a concrete example of modules.  ","version":"Next","tagName":"h3"},{"title":"State​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#state","content":" The abstract IDerivation class generated within the interfaces module includes additional, experimental methods which can be used for persisting and recovering internal state of the connector.  Consult the generated implementation and feel free to reach out to support if you'd like more information on building stateful TypeScript derivations.  ","version":"Next","tagName":"h3"},{"title":"Transformations​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#transformations","content":" A transformation binds a source collection to a derivation, causing its documents to be read and processed by the derivation connector.  Read source documents are first shuffled on a shuffle keyto co-locate the processing of documents that have equal shuffle keys. The transformation then processes documents by invoking lambdas: user-defined functions that accept documents as arguments, return documents in response, and potentially update internal task state.  A derivation may have many transformations, and each transformation has a long-lived and stable name. Each transformation independently reads documents from its source collection and tracks its own read progress. More than one transformation can read from the same source collection, and transformations may also source from their own derivation, enabling cyclic data-flows and graph algorithms.  Transformations may be added to or removed from a derivation at any time. This makes it possible to, for example, add a new collection into an existing multi-way join, or gracefully migrate to a new source collection without incurring downtime. However, renaming a running transformation is not possible. If attempted, the old transformation is dropped and a new transformation under the new name is created, which begins reading its source collection all over again.  graph LR; d[Derivation]; t[Transformation]; s[Internal State]; l[Lambda]; c[Sourced Collection]; o[Derived Collection]; d-- has many --&gt;t; d-- has one --&gt;s; d-- has one --&gt;o; c-- reads from --&gt;t; t-- invokes --&gt;l; l-- updates --&gt;s; s-- queries --&gt;l; l-- publishes to --&gt;o;  ","version":"Next","tagName":"h2"},{"title":"Sources​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#sources","content":" The source of a transformation is a collection. As documents are published into the source collection, they are continuously read and processed by the transformation.  A partition selector may be provided to process only a subset of the source collection's logical partitions. Selectors are efficient: only partitions that match the selector are read, and Flow can cheaply skip over partitions that don't.  Derivations re-validate their source documents against the source collection's schema as they are read. This is because collection schemas may evolve over time, and could have inadvertently become incompatible with historical documents of the source collection. Upon a schema error, the derivation will pause and give you an opportunity to correct the problem.  ","version":"Next","tagName":"h3"},{"title":"Shuffles​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#shuffles","content":" As each source document is read, it's shuffled — or equivalently, mapped — on an extracted key.  If you're familiar with data shuffles in tools like MapReduce, Apache Spark, or Flink, the concept is very similar. Flow catalog tasks scale out into multiple shards, each running in parallel on different physical machines, where each shard processes a subset of source documents.  Shuffles let Flow identify the shard that should process a particular source document, in order to co-locate that processing with other documents it may need to know about.  For example, transforms of theApproving Transfers exampleshuffle on either /sender or /recipient in order to process documents that debit or credit accounts on the specific shard that is uniquely responsible for maintaining the balance of a given account.  graph LR; subgraph s1 [Source Partitions] p1&gt;acmeBank/transfers/part-1]; p2&gt;acmeBank/transfers/part-2]; end subgraph s2 [Derivation Task Shards] t1([derivation/shard-1]); t2([derivation/shard-2]); end p1-- sender: alice --&gt;t1; p1-- recipient: bob --&gt;t2; p2-- recipient: alice --&gt;t1; p2-- sender: bob --&gt;t2;  Flow offers three modes for configuring document shuffles: key, any, and lambda.  shuffle: key​  Shuffle keys are defined as an array of JSON pointers to locations that should be extracted from your source documents. This array forms the composite key over which your documents are shuffled:  transforms: - name: fromOrders source: acmeCo/orders shuffle: key: [/item/product_id, /customer_id] # Flow guarantees that the same shard will process the user's lambda # for all instances of a specific (product ID, customer ID) tuple. lambda: ...   If a derivation has more than one transformation, the shuffle keys of all transformations must align with one another in terms of the extracted key types (string vs integer) as well as the number of components in a composite key.  For example, one transformation couldn't shuffle transfers on [/id]while another shuffles on [/sender], because sender is a string andid an integer.  Similarly mixing a shuffle of [/sender] alongside [/sender, /recipient]is prohibited because the keys have different numbers of components.  shuffle: any​  If your lambda doesn't rely on any task state then it may not matter which task shard processes a given source document. In these instances you can use shuffle: any, which allows source documents to be processed by any available task shard.  This is common for transformation lambdas which perform basic filtering or mapping of source documents and which don't require any joined task state.  transforms: - name: fromOrders source: acmeCo/orders shuffle: any # The user's lambda is a pure function and can be evaluated by any available shard. lambda: SELECT $customer_id, $item_price WHERE $item_price &gt; 100;   shuffle: lambda​  Warning Computed shuffles are in active development and are not yet functional.  Your source documents may not always contain an appropriate value to shuffle upon. For instance, you might want to shuffle on product ID and order date, but your source documents contain only an order timestamp field.  You can use shuffle: lambda to define a function that maps your source document into the appropriate shuffle key:  transforms: - name: fromOrders source: acmeCo/orders shuffle: lambda: SELECT $product_id, DATE($order_timestamp); # Flow guarantees that the same shard will process the user's lambda # for all instances of a specific (product ID, date) tuple. lambda: ...   Your shuffle lambda must return exactly one row, and its columns and types must align with the other shuffles of your derivation transformations.  Flow must know the types of your composite shuffle key. In most cases it will infer these types from the shuffle: key of another transformation. If you have no shuffle: key transformations, Flow will ask that you explicitly tell it your shuffle types:  derive: using: sqlite: {} shuffleKeyTypes: [integer, string] transforms: - name: fromOrders source: acmeCo/orders shuffle: lambda: SELECT $product_id, DATE($order_timestamp); lambda: ...   ","version":"Next","tagName":"h3"},{"title":"Lambdas​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#lambdas","content":" Lambdas are user-defined functions that are invoked by transformations. They accept documents as arguments and return transformed documents in response. Lambdas can update internal task state, publish documents into the derived collection, or both.  Lambdas are &quot;serverless&quot;: Flow manages the execution and scaling of your transformation lambdas on your behalf.  ","version":"Next","tagName":"h3"},{"title":"Processing order​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#processing-order","content":" Transformations may simultaneously read from many source collections, or even read from the same source collection multiple times.  Roughly speaking, the derivation will globally process transformations and their source documents in the time-based order in which the source documents were originally written to their source collections. This means that a derivation started a month ago and a new copy of the derivation started today, will process documents in the same order and arrive at the same result. Derivations are repeatable.  More precisely, processing order is stable for each individual shuffle key, though different shuffle keys may process in different orders if more than one task shard is used.  Processing order can be attenuated through a read delayor differentiated read priority.  ","version":"Next","tagName":"h3"},{"title":"Read delay​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#read-delay","content":" A transformation can define a read delay, which will hold back the processing of its source documents until the time delay condition is met. For example, a read delay of 15 minutes would mean that a source document cannot be processed until it was published at least 15 minutes ago. If the derivation is working through a historical backlog of source documents, than a delayed transformation will respect its ordering delay relative to the publishing times of other historical documents also being read.  Event-driven workflows are a great fit for reacting to events as they occur, but aren’t terribly good at taking action when something hasn’t happened:  A user adds a product to their cart, but then doesn’t complete a purchase.A temperature sensor stops producing its expected, periodic measurements.  A common pattern for tackling these workflows in Flow is to read a source collection without a delay and update an internal state. Then, read a collection with a read delay and determine whether the desired action has happened or not. For example, source from a collection of sensor readings and index the last timestamp of each sensor. Then, source the same collection again with a read delay: if the register timestamp isn't more recent than the delayed source reading, the sensor failed to produce a measurement.  Flow read delays are very efficient and scale better than managing very large numbers of fine-grain timers.  See Grouped Windows of Transfers for an example using a read delay  Learn more from the Citi Bike &quot;idle bikes&quot; example  ","version":"Next","tagName":"h3"},{"title":"Read priority​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#read-priority","content":" Sometimes it's necessary for all documents of a source collection to be processed by a transformation before any documents of some other source collection are processed, regardless of their relative publishing time. For example, a collection may have corrections that should be applied before the historical data of another collection is re-processed.  Transformation priorities allow you to express the relative processing priority of a derivation's various transformations. When priorities are not equal, all available source documents of a higher-priority transformation are processed before any source documents of a lower-priority transformation.  ","version":"Next","tagName":"h3"},{"title":"Internal State​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#internal-state","content":" Derivation tasks often require an internal state, perhaps to hold a partial aggregation or join result. Internal state is not a direct part of the output of a derivation. Instead, transformation lambdas query and update internal state as they process source documents and return derived documents.  For SQLite derivations, the entire SQLite database is the internal state of the task. TypeScript derivations can use in-memory states with a recovery and checkpoint mechanism. Estuary intends to offer additional mechanisms for automatic internal state snapshot and recovery in the future.  The exact nature of internal task states vary, but under the hood they're backed by a replicated embedded RocksDB instance which is co-located with the task shard execution contexts that Flow manages. As contexts are assigned and re-assigned, their state databases travel with them.  If a task shard needs to be scaled out, Flow is able to perform an online split, which cheaply clones its state database into two new databases — and paired shards — which are re-assigned to other machines.  ","version":"Next","tagName":"h2"},{"title":"Where to accumulate?​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#where-to-accumulate","content":" Derivation collection schemas may havereduction annotations, and lambdas can be combined with reductions in interesting ways. You may be familiar with map and reduce functions built into languages likePython,JavaScript, and many others or have used tools like MapReduce or Spark. In functional terms, lambdas you write within Flow are &quot;mappers,&quot; and reductions are always done by the Flow runtime using your schema annotations.  This means that, when you implement a derivation, you get to choose where accumulation will happen:  Your lambdas can update and query aggregates stored in internal task state.Approving Transfers is an example that maintains account balances in a SQLite table.Or, your lambdas can compute changes of an aggregate, which are then reduced by Flow using reduction annotations.Current Account Balances is an example that combines a lambda with a reduce annotation.  These two approaches can produce equivalent results, but they do so in very different ways.  ","version":"Next","tagName":"h2"},{"title":"Accumulate in Internal Task State​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#accumulate-in-internal-task-state","content":" You can accumulate using the internal state of your derivation: for instance, by using an internal table within your SQLite derivation. You then write lambdas which update that state, or query it to publish derived documents.  For example, consider a collection that’s summing a value:  Time\tState\tLambdas\tDerived DocumentT0\t0\tUPDATE val = val + 5; SELECT val;\t5 T1\t5\tUPDATE val = val - 1; SELECT val;\t4 T2\t4\tUPDATE val = val + 2; SELECT val;\t6 T3\t6   Using a derivation's internal state is a great solution if you expect to materialize the derived collection into a non-transactional store. That's because its documents are complete statements of the current answer, and can be correctly applied to systems that support only at-least-once semantics.  They’re also well-suited for materializations into endpoints that aren't stateful, such as Pub/Sub systems or Webhooks.  ","version":"Next","tagName":"h3"},{"title":"Accumulate in a Database​","type":1,"pageTitle":"Derivations","url":"/concepts/derivations/#accumulate-in-a-database","content":" To accumulate in your materialization endpoint, such as a database, you define a derivation with a reducible schema and implement lambdas which publish the changes to a current answer. The Flow runtime then uses your reduction annotations to combine the documents published into your derived collection.  Later, when the collection is materialized, your reduction annotations are applied again to reduce each collection document into a final, fully-reduced value for each collection key that's kept up to date in the materialized table.  A key insight is that the database is the only stateful system in this scenario. The derivation itself is stateless, with lambdas that are pure functions, which is typically extremely performant.  Returning to our summing example:  Time\tDB\tLambdas\tDerived DocumentT0\t0\tSELECT 5;\t5 T1\t5\tSELECT -1;\t-1 T2\t4\tSELECT 2;\t2 T3\t6   This works especially well when materializing into a transactional database. Flow couples its processing transactions with corresponding database transactions, ensuring end-to-end “exactly once” semantics.  When materializing into a non-transactional store, Flow is only able to provide weaker “at least once” semantics; it’s possible that a document may be combined into a database value more than once. Whether that’s a concern depends a bit on the task at hand. Some reductions like merge can be applied repeatedly without changing the result, while in other use cases approximations are acceptable. For the summing example above, &quot;at-least-once&quot; semantics could give an incorrect result.  Learn more in the derivation pattern examples of Flow's repository ","version":"Next","tagName":"h3"},{"title":"Tests","type":0,"sectionRef":"#","url":"/concepts/tests/","content":"","keywords":"","version":"Next"},{"title":"Ingest​","type":1,"pageTitle":"Tests","url":"/concepts/tests/#ingest","content":" ingest steps add documents to a named collection. All documents must validate against the collection'sschema, or a catalog build error will be reported.  All documents from a single ingest step are added in one transaction. This means that multiple documents with a common key will be combined priorto their being appended to the collection. Suppose acmeCo/people had key [/id]:  tests: acmeCo/tests/greetings: - ingest: description: Zeldas are combined to one added document. collection: acmeCo/people documents: - { userId: 1, name: &quot;Zelda One&quot; } - { userId: 1, name: &quot;Zelda Two&quot; } - verify: description: Only one Zelda is greeted. collection: acmeCo/greetings documents: - { userId: 1, greeting: &quot;Hello Zelda Two&quot; }   ","version":"Next","tagName":"h2"},{"title":"Verify​","type":1,"pageTitle":"Tests","url":"/concepts/tests/#verify","content":" verify steps assert that the current contents of a collection match the provided document fixtures. Verified documents are fully reduced, with one document for each unique key, ordered under the key's natural order.  You can verify the contents of both derivations and captured collections. Documents given in verify steps do not need to be comprehensive. It is not an error if the actual document has additional locations not present in the document to verify, so long as all matched document locations are equal. Verified documents also do not need to validate against the collection's schema. They do, however, need to include all fields that are part of the collection's key.  tests: acmeCo/tests/greetings: - ingest: collection: acmeCo/people documents: - { userId: 1, name: &quot;Zelda&quot; } - { userId: 2, name: &quot;Link&quot; } - ingest: collection: acmeCo/people documents: - { userId: 1, name: &quot;Zelda Again&quot; } - { userId: 3, name: &quot;Pikachu&quot; } - verify: collection: acmeCo/greetings documents: # greetings are keyed on /userId, and the second greeting is kept. - { userId: 1, greeting: &quot;Hello Zelda Again&quot; } # `greeting` is &quot;Hello Link&quot;, but is not asserted here. - { userId: 2 } - { userId: 3, greeting: &quot;Hello Pikachu&quot; }   ","version":"Next","tagName":"h2"},{"title":"Partition selectors​","type":1,"pageTitle":"Tests","url":"/concepts/tests/#partition-selectors","content":" Verify steps may include a partition selector to verify only documents of a specific partition:  tests: acmeCo/tests/greetings: - verify: collection: acmeCo/greetings description: Verify only documents which greet Nintendo characters. documents: - { userId: 1, greeting: &quot;Hello Zelda&quot; } - { userId: 3, greeting: &quot;Hello Pikachu&quot; } partitions: include: platform: [Nintendo]   Learn more about partition selectors.  ","version":"Next","tagName":"h3"},{"title":"Tips​","type":1,"pageTitle":"Tests","url":"/concepts/tests/#tips","content":" The following tips can aid in testing large or complex derivations.  ","version":"Next","tagName":"h2"},{"title":"Testing reductions​","type":1,"pageTitle":"Tests","url":"/concepts/tests/#testing-reductions","content":" Reduction annotations are expressive and powerful, and their use should thus be tested thoroughly. An easy way to test reduction annotations on captured collections is to write a two-step test that ingests multiple documents with the same key and then verifies the result. For example, the following test might be used to verify the behavior of a simple sum reduction:  tests: acmeCo/tests/sum-reductions: - ingest: description: Ingest documents to be summed. collection: acmeCo/collection documents: - {id: 1, value: 5} - {id: 1, value: 4} - {id: 1, value: -3} - verify: description: Verify value was correctly summed. collection: acmeCo/collection documents: - {id: 1, value: 6}   ","version":"Next","tagName":"h3"},{"title":"Reusing common fixtures​","type":1,"pageTitle":"Tests","url":"/concepts/tests/#reusing-common-fixtures","content":" When you write a lot of tests, it can be tedious to repeat documents that are used multiple times. YAML supports anchors and references, which you can implement to re-use common documents throughout your tests. One nice pattern is to define anchors for common ingest steps in the first test, which can be re-used by subsequent tests. For example:  tests: acmeCo/tests/one: - ingest: &amp;mySetup collection: acmeCo/collection documents: - {id: 1, ...} - {id: 2, ...} ... - verify: ... acmeCo/tests/two: - ingest: *mySetup - verify: ...   This allows all the subsequent tests to re-use the documents from the first ingest step without having to duplicate them. ","version":"Next","tagName":"h3"},{"title":"Materializations","type":0,"sectionRef":"#","url":"/concepts/materialization/","content":"","keywords":"","version":"Next"},{"title":"Discovery​","type":1,"pageTitle":"Materializations","url":"/concepts/materialization/#discovery","content":" Materializations use real-time connectors to connect to many endpoint types.  When you use a materialization connector in the Flow web app, Flow helps you configure it through the discovery workflow.  To begin discovery, you tell Flow the connector you'd like to use, basic information about the endpoint, and the collection(s) you'd like to materialize there. Flow maps the collection(s) to one or more resources — tables, data streams, or the equivalent — through one or more bindings.  You may then modify the generated configuration as needed before publishing the materialization.  ","version":"Next","tagName":"h2"},{"title":"Specification​","type":1,"pageTitle":"Materializations","url":"/concepts/materialization/#specification","content":" Materializations are defined in Flow specification files per the following format:  # A set of materializations to include in the catalog. # Optional, type: object materializations: # The name of the materialization. acmeCo/example/database-views: # Endpoint defines how to connect to the destination of the materialization. # Required, type: object endpoint: # This endpoint uses a connector provided as a Docker image. connector: # Docker image that implements the materialization connector. image: ghcr.io/estuary/materialize-postgres:dev # File that provides the connector's required configuration. # Configuration may also be presented inline. config: path/to/connector-config.yaml # Bindings define how one or more collections map to materialized endpoint resources. # A single materialization may include many collections and endpoint resources, # each defined as a separate binding. # Required, type: object bindings: - # Source collection read by this binding. # Required, type: object or string source: # Name of the collection to be read. # Required. name: acmeCo/example/collection # Lower bound date-time for documents which should be processed. # Source collection documents published before this date-time are filtered. # `notBefore` is *only* a filter. Updating its value will not cause Flow # to re-process documents that have already been read. # Optional. Default is to process all documents. notBefore: 2023-01-23T01:00:00Z # Upper bound date-time for documents which should be processed. # Source collection documents published after this date-time are filtered. # Like `notBefore`, `notAfter` is *only* a filter. Updating its value will # not cause Flow to re-process documents that have already been read. # Optional. Default is to process all documents. notAfter: 2023-01-23T02:00:00Z # The resource is additional configuration required by the endpoint # connector to identify and materialize a specific endpoint resource. # The structure and meaning of this configuration is defined by # the specific connector. # Required, type: object resource: # The materialize-postgres connector expects a `table` key # which names a table to materialize into. table: example_table # Priority applied to documents processed by this binding. # When all bindings are of equal priority, documents are processed # in order of their associated publishing time. # # However, when one binding has a higher priority than others, # then *all* ready documents are processed through the binding # before *any* documents of other bindings are processed. # # Optional. Default: 0, integer &gt;= 0 priority: 0 # A sourceCapture allows bindings to be managed automatically based on the # bindings of the given capture. As new bindings are added to the capture, # they will automatically be added to the materialization. This property # is optional. sourceCapture: acmeCo/example/a-capture   ","version":"Next","tagName":"h2"},{"title":"How continuous materialization works​","type":1,"pageTitle":"Materializations","url":"/concepts/materialization/#how-continuous-materialization-works","content":" Flow materializations are continuous materialized views. They maintain a representation of the collection within the endpoint system that is updated in near real-time. It's indexed on thecollection key. As the materialization runs, it ensures that all collection documents and their accumulated reductions are reflected in this managed endpoint resource.  When you first publish a materialization, Flow back-fills the endpoint resource with the historical documents of the collection. Once caught up, Flow applies new collection documents using incremental and low-latency updates.  As collection documents arrive, Flow:  Reads previously materialized documents from the endpoint for the relevant keysReduces new documents into these read documentsWrites updated documents back into the endpoint resource, indexed by their keys  For example, consider a collection and its materialization:   collections: acmeCo/colors: key: [/color] schema: type: object required: [color, total] reduce: {strategy: merge} properties: color: {enum: [red, blue, purple]} total: type: integer reduce: {strategy: sum} materializations: acmeCo/example/database-views: endpoint: ... bindings: - source: acmeCo/colors resource: { table: colors }   Suppose documents are periodically added to the collection:  {&quot;color&quot;: &quot;red&quot;, &quot;total&quot;: 1} {&quot;color&quot;: &quot;blue&quot;, &quot;total&quot;: 2} {&quot;color&quot;: &quot;blue&quot;, &quot;total&quot;: 3}   Its materialization into a database table will have a single row for each unique color. As documents arrive in the collection, the row total is updated within the materialized table so that it reflects the overall count:    Flow does not keep separate internal copies of collection or reduction states, as some other systems do. The endpoint resource is the one and only place where state &quot;lives&quot; within a materialization. This makes materializations very efficient and scalable to operate. They are able to maintain very large tables stored in highly scaled storage systems like OLAP data warehouses.  ","version":"Next","tagName":"h2"},{"title":"Projected fields​","type":1,"pageTitle":"Materializations","url":"/concepts/materialization/#projected-fields","content":" Many endpoint systems are document-oriented and can directly work with collections of JSON documents. Others are table-oriented and require an up-front declaration of columns and types to be most useful, such as a SQL CREATE TABLE definition.  Flow uses collection projections to relate locations within a hierarchical JSON document to equivalent named fields. A materialization can in turn select a subset of available projected fields where, for example, each field becomes a column in a SQL table created by the connector.  It would be tedious to explicitly list projections for every materialization, though you certainly can if desired. Instead, Flow and the materialization connector negotiate a recommended field selection on your behalf, which can be fine-tuned. For example, a SQL database connector will typically require that fields comprising the primary key be included, and will recommend that scalar values be included, but will by default exclude document locations that don't have native SQL representations, such as locations which can have multiple JSON types or are arrays or maps.  materializations: acmeCo/example/database-views: endpoint: ... bindings: - source: acmeCo/example/collection resource: { table: example_table } # Select (or exclude) projections of the collection for materialization as fields. # If not provided, the recommend fields of the endpoint connector are used. # Optional, type: object fields: # Whether to include fields that are recommended by the endpoint connector. # If false, then fields can still be added using `include`. # Required, type: boolean recommended: true # Fields to exclude. This is useful for deselecting a subset of recommended fields. # Default: [], type: array exclude: [myField, otherField] # Fields to include. This can supplement recommended fields, or can # designate explicit fields to use if recommended fields are disabled. # # Values of this map are used to customize connector behavior on a per-field basis. # They are passed directly to the connector and are not interpreted by Flow. # Consult your connector's documentation for details of what customizations are available. # This is an advanced feature and is not commonly used. # # default: {}, type: object include: {goodField: {}, greatField: {}}   ","version":"Next","tagName":"h2"},{"title":"Partition selectors​","type":1,"pageTitle":"Materializations","url":"/concepts/materialization/#partition-selectors","content":" Partition selectors let you materialize only a subset of a collection that haslogical partitions. For example, you might have a large collection that is logically partitioned on each of your customers:  collections: acmeCo/anvil/orders: key: [/id] schema: orders.schema.yaml projections: customer: location: /order/customer partition: true   A large customer asks if you can provide an up-to-date accounting of their orders. This can be accomplished with a partition selector:  materializations: acmeCo/example/database-views: endpoint: ... bindings: # The source can be specified as an object, which allows setting a partition selector. - source: name: acmeCo/anvil/orders # Process partitions where &quot;Coyote&quot; is the customer. partitions: include: customer: [Coyote] resource: { table: coyote_orders }   Learn more about partition selectors.  ","version":"Next","tagName":"h2"},{"title":"Destination-specific performance​","type":1,"pageTitle":"Materializations","url":"/concepts/materialization/#destination-specific-performance","content":" Flow processes updates in transactions, as quickly as the destination endpoint can handle them. This might be milliseconds in the case of a fast key/value store, or many minutes in the case of an OLAP warehouse.  If the endpoint is also transactional, Flow integrates its internal transactions with those of the endpoint for integrated end-to-end “exactly once” semantics.  The materialization is sensitive to back pressure from the endpoint. As a database gets busy, Flow adaptively batches and combines documents to consolidate updates:  In a given transaction, Flow reduces all incoming documents on the collection key. Multiple documents combine and result in a single endpoint read and write during the transaction.As a target database becomes busier or slower, transactions become larger. Flow does more reduction work within each transaction, and each endpoint read or write accounts for an increasing volume of collection documents.  This allows you to safely materialize a collection with a high rate of changes into a small database, so long as the cardinality of the materialization is of reasonable size.  ","version":"Next","tagName":"h2"},{"title":"Delta updates​","type":1,"pageTitle":"Materializations","url":"/concepts/materialization/#delta-updates","content":" As described above, Flow's standard materialization mechanism involves querying the target system for data state before reducing new documents directly into it.  For these standard updates to work, the endpoint must be a stateful system, like a relational database. However, other systems — like Webhooks and Pub/Sub — may also be endpoints. None of these typically provide a state representation that Flow can query. They are write-only in nature, so Flow cannot use their endpoint state to help it fully reduce collection documents on their keys. Even some stateful systems are incompatible with Flow's standard updates due to their unique design and architecture.  For all of these endpoints, Flow offers a delta-updates mode. When using delta updates, Flow does not attempt to maintain full reductions of each unique collection key. Instead, Flow locally reduces documents within each transaction (this is often called a &quot;combine&quot;), and then materializes onedelta document per key to the endpoint.  In other words, when delta updates are used, Flow sends information about data changes by key, and further reduction is left up to the endpoint system. Some systems may reduce documents similar to Flow; others use a different mechanism; still others may not perform reductions at all.  A given endpoint may support standard updates, delta updates, or both. This depends on the materialization connector. Expect that a connector will use standard updates only unless otherwise noted in its documentation.  ","version":"Next","tagName":"h2"},{"title":"Delta updates for every binding in a Materialization​","type":1,"pageTitle":"Materializations","url":"/concepts/materialization/#delta-updates-for-every-binding-in-a-materialization","content":" When configuring a materialization, you can set delta updates as the materialization mechanism for every binding. Afterwards, you are able to modify this setting individually for each binding for further customization.  note This functionality is only supported for Materialization connectors that have the x-delta-updatesfield implemented. Consult the individual connector documentation for details.  ","version":"Next","tagName":"h3"},{"title":"Using sourceCapture to synchronize capture and materialization bindings​","type":1,"pageTitle":"Materializations","url":"/concepts/materialization/#using-sourcecapture-to-synchronize-capture-and-materialization-bindings","content":" In some cases, you just want a destination system to always reflects the source system as closely as possible, even as the source system changes over time. ThesourceCapture property of a materialization allows you to do exactly that. If you set a sourceCapture on your materialization, then the bindings of the materialization will be automatically kept in sync with the bindings of the capture. As bindings are added to the capture, they will be automatically added to the materialization. This works regardless of whether the bindings were added to the capture manually or automatically. Bindings that are removed from the capture are not removed from the materialization.  ","version":"Next","tagName":"h2"},{"title":"Default Schema Names in Destinations​","type":1,"pageTitle":"Materializations","url":"/concepts/materialization/#default-schema-names-in-destinations","content":" Estuary Flow supports configuring the default destination schema on the materialization level.  When a capture provides a schema or namespace, it will automatically be used as the default schema value for all bindings in the materialization.  While schema names are automatically assigned based on the capture, you can still manually set the schema name for individual bindings if needed.  You can also set a default schema name at the materialization level. This applies to all bindings within that materialization, ensuring a consistent schema naming convention throughout the data pipeline.  note This functionality is only supported for Materialization connectors that have the x-schema-namefield implemented. Consult the individual connector documentation for details.  ","version":"Next","tagName":"h2"},{"title":"How It Works​","type":1,"pageTitle":"Materializations","url":"/concepts/materialization/#how-it-works","content":" Source Capture Level: If the source capture provides a schema or namespace, it will be used as the default schema for all bindings in the materialization. Manual Overrides: You can still manually configure schema names for each binding, overriding the default schema if needed. Materialization-Level Configuration: The default schema name can be set at the materialization level, ensuring that all new captures within that materialization automatically inherit the default schema name.  ","version":"Next","tagName":"h3"},{"title":"Configuration Steps​","type":1,"pageTitle":"Materializations","url":"/concepts/materialization/#configuration-steps","content":" Set Default Schema at Source Capture Level: When defining your source capture, specify the schema or namespace. If no schema is provided, Estuary Flow will automatically assign a default schema. Override Schema at Binding Level: For any binding, you can manually override the default schema by specifying a different schema name. Set Default Schema at Materialization Level: During the materialization configuration, set a default schema name for all captures within the materialization. ","version":"Next","tagName":"h3"},{"title":"Schemas","type":0,"sectionRef":"#","url":"/concepts/schemas/","content":"","keywords":"","version":"Next"},{"title":"JSON Schema​","type":1,"pageTitle":"Schemas","url":"/concepts/schemas/#json-schema","content":" JSON Schemais an expressive open standard for defining the schema and structure of documents. Flow uses it for all schemas defined in Flow specifications.  JSON Schema goes well beyond basic type information and can modeltagged unions, recursion, and other complex, real-world composite types. Schemas can also define rich data validations like minimum and maximum values, regular expressions, dates, timestamps, email addresses, and other formats.  Together, these features let schemas represent structure as well asexpectations and constraints that are evaluated and must hold true for every collection document before it’s added to the collection. They’re a powerful tool for ensuring end-to-end data quality: for catching data errors and mistakes early, before they can impact your production data products.  ","version":"Next","tagName":"h2"},{"title":"Generation​","type":1,"pageTitle":"Schemas","url":"/concepts/schemas/#generation","content":" When capturing data from an external system, Flow can usually generate suitable JSON schemas on your behalf.  Learn more about using connectors  For systems like relational databases, Flow will typically generate a complete JSON schema by introspecting the table definition.  For systems that store unstructured data, Flow will typically generate a very minimal schema, and will rely on schema inference to fill in the details. See continuous schema inference for more information.  ","version":"Next","tagName":"h3"},{"title":"Translations​","type":1,"pageTitle":"Schemas","url":"/concepts/schemas/#translations","content":" You must only provide Flow a model of a given dataset one time, as a JSON schema. Having done that, Flow leverages static inference over your schemas to perform many build-time validations of your catalog entities, helping you catch potential problems early.  Schema inference is also used to provide translations into other schema flavors:  Most projections of a collection are automatically inferred from its schema. Materializations use your projections to create appropriate representations in your endpoint system. A SQL connector will create table definitions with appropriate columns, types, and constraints.Flow generates TypeScript definitions from schemas to provide compile-time type checks of user lambda functions. These checks are immensely helpful for surfacing mismatched expectations around, for example, whether a field could ever be null or is misspelt — which, if not caught, might otherwise fail at runtime.  ","version":"Next","tagName":"h3"},{"title":"Annotations​","type":1,"pageTitle":"Schemas","url":"/concepts/schemas/#annotations","content":" The JSON Schema standard introduces the concept ofannotations, which are keywords that attach metadata to a location within a validated JSON document. For example, title and description can be used to annotate a schema with its meaning:  properties: myField: title: My Field description: A description of myField   Flow extends JSON Schema with additional annotation keywords, which provide Flow with further instruction for how documents should be processed. In particular, the reduce and default keywords help you define merge behaviors and avoid null values at your destination systems, respectively.  What’s especially powerful about annotations is that they respond toconditionals within the schema. Consider a schema validating a positive or negative number:  type: number oneOf: - exclusiveMinimum: 0 description: A positive number. - exclusiveMaximum: 0 description: A negative number. - const: 0 description: Zero.   Here, the activated description of this schema location depends on whether the integer is positive, negative, or zero.  ","version":"Next","tagName":"h3"},{"title":"Writing schemas​","type":1,"pageTitle":"Schemas","url":"/concepts/schemas/#writing-schemas","content":" Your schema can be quite permissive or as strict as you wish. There are a few things to know, however.  The top-level type must be object. Flow adds a bit of metadata to each of your documents under the _meta property, which can only be done with a top-level object. Any fields that are part of the collection's key must provably exist in any document that validates against the schema. Put another way, every document within a collection must include all of the fields of the collection's key, and the schema must guarantee that.  For example, the following collection schema would be invalid because the id field, which is used as its key, is not required, so it might not actually exist in all documents:  collections: acmeCo/whoops: schema: type: object required: [value] properties: id: {type: integer} value: {type: string} key: [/id]   To fix the above schema, change required to [id, value].  Learn more of how schemas can be expressed within collections.  ","version":"Next","tagName":"h2"},{"title":"Organization​","type":1,"pageTitle":"Schemas","url":"/concepts/schemas/#organization","content":" JSON schema has a $ref keyword which is used to reference a schema stored elsewhere. Flow resolves $ref as a relative URL of the current file, and also supportsJSON fragment pointersfor referencing a specific schema within a larger schema document, such as ../my/widget.schema.yaml#/path/to/schema. It's recommended to use references in order to organize your schemas for reuse.  $ref can also be used in combination with other schema keywords to further refine a base schema. Here's an example that uses references to organize and further tighten the constraints of a reused base schema:    flow.yamlschemas.yaml collections: acmeCo/coordinates: key: [/id] schema: schemas.yaml#/definitions/coordinate acmeCo/integer-coordinates: key: [/id] schema: schemas.yaml#/definitions/integer-coordinate acmeCo/positive-coordinates: key: [/id] schema: # Compose a restriction that `x` &amp; `y` must be positive. $ref: schemas.yaml#/definitions/coordinate properties: x: {exclusiveMinimum: 0} y: {exclusiveMinimum: 0}   tip You can write your JSON schemas as either YAML or JSON across any number of files, all referenced from Flow catalog files or other schemas. Schema references are always resolved as URLs relative to the current file, but you can also use absolute URLs to a third-party schema likeschemastore.org.  ","version":"Next","tagName":"h3"},{"title":"Write and read schemas​","type":1,"pageTitle":"Schemas","url":"/concepts/schemas/#write-and-read-schemas","content":" In some cases, you may want to impose different constraints to data that is being added (written) to the collection and data that is exiting (read from) the collection.  For example, you may need to start capturing data now from a source system; say, a pub-sub system with short-lived historical data support or an HTTP endpoint, but don't know or don't control the endpoint's schema. You can capture the data with a permissive write schema, and impose a stricter read schema on the data as you need to perform a derivation or materialization. You can safely experiment with the read schema at your convenience, knowing the data has already been captured.  To achieve this, edit the collection, re-naming the standard schema to writeSchema and adding a readSchema. Make sure that the field used as the collection key is defined in both schemas.  You can either perform this manually, or use Flow's Schema Inference tool to infer a read schema. Schema Inference is available in the web app when you edit a capture or materialization and create a materialization.  Before separating your write and read schemas, have the following in mind:  The write schema comes from the capture connector that produced the collection and shouldn't be modified. Always apply your schema changes to the read schema. Separate read and write schemas are typically useful for collections that come from a source system with a flat or loosely defined data structure, such as cloud storage or pub-sub systems. Collections sourced from databases and most SaaS systems come with an explicitly defined data structure and shouldn't need a different read schema. If you're using standard projections, you must only define them in the read schema. However, if your projections are logical partitions, you must define them in both schemas.  Here's a simple example in which you don't know how purchase prices are formatted when capturing them, but find out later that number is the appropriate data type:  collections: purchases: writeSchema: type: object title: Store price as strings description: Not sure if prices are formatted as numbers or strings. properties: id: { type: integer} price: {type: [string, number]} readSchema: type: object title: Prices as numbers properties: id: { type: integer} price: {type: number} key: [/id]   ","version":"Next","tagName":"h2"},{"title":"Reductions​","type":1,"pageTitle":"Schemas","url":"/concepts/schemas/#reductions","content":" Flow collections have keys, and multiple documents may be added to collections that share a common key. When this happens, Flow will opportunistically merge all such documents into a single representative document for that key through a process known as reduction.  Flow's default is simply to retain the most recent document of a given key, which is often the behavior that you're after. Schema reduce annotations allow for far more powerful behaviors.  The Flow runtime performs reductions frequently and continuously to reduce the overall movement and cost of data transfer and storage. A torrent of input collection documents can often become a trickle of reduced updates that must be stored or materialized into your endpoints.  info Flow never delays processing in order to batch or combine more documents, as some systems do (commonly known as micro-batches, or time-based polling). Every document is processed as quickly as possible, from end to end. Instead Flow uses optimistic transaction pipelining to do as much useful work as possible, while it awaits the commit of a previous transaction. This natural back-pressure affords plenty of opportunity for data reductions while minimizing latency.  ","version":"Next","tagName":"h2"},{"title":"reduce annotations​","type":1,"pageTitle":"Schemas","url":"/concepts/schemas/#reduce-annotations","content":" Reduction behaviors are defined by reduceJSON schema annotationswithin your document schemas. These annotations provide Flow with the specific reduction strategies to use at your various document locations.  If you're familiar with the map and reduce primitives present in Python, Javascript, and many other languages, this should feel familiar. When multiple documents map into a collection with a common key, Flow reduces them on your behalf by using your reduce annotations.  Here's an example that sums an integer:  type: integer reduce: { strategy: sum } # 1, 2, -1 =&gt; 2   Or deeply merges a map:  type: object reduce: { strategy: merge } # {&quot;a&quot;: &quot;b&quot;}, {&quot;c&quot;: &quot;d&quot;} =&gt; {&quot;a&quot;: &quot;b&quot;, &quot;c&quot;: &quot;d&quot;}   Learn more in thereduction strategiesreference documentation.  Reductions and collection keys​  Reduction annotations change the common patterns for how you think about collection keys.  Suppose you are building a reporting fact table over events of your business. Today you would commonly consider a unique event ID to be its natural key. You would load all events into your warehouse and perform query-time aggregation. When that becomes too slow, you periodically refresh materialized views for fast-but-stale queries.  With Flow, you instead use a collection key of your fact table dimensions, and use reduce annotations to define your metric aggregations. A materialization of the collection then maintains a database table which is keyed on your dimensions, so that queries are both fast and up to date.  Composition with conditionals​  Like any other JSON Schema annotation,reduce annotations respond to schema conditionals. Here we compose append and lastWriteWins strategies to reduce an appended array which can also be cleared:  type: array oneOf: # If the array is non-empty, reduce by appending its items. - minItems: 1 reduce: { strategy: append } # Otherwise, if the array is empty, reset the reduced array to be empty. - maxItems: 0 reduce: { strategy: lastWriteWins } # [1, 2], [3, 4, 5] =&gt; [1, 2, 3, 4, 5] # [1, 2], [], [3, 4, 5] =&gt; [3, 4, 5] # [1, 2], [3, 4, 5], [] =&gt; []   You can combine schema conditionals with annotations to buildrich behaviors.  ","version":"Next","tagName":"h3"},{"title":"Continuous schema inference​","type":1,"pageTitle":"Schemas","url":"/concepts/schemas/#continuous-schema-inference","content":" Flow automatically infers a JSON schema for every captured collection. This schema is updated automatically as data is captured.  For some systems, like relational databases, Flow is able to determine a complete JSON schema for each collection up front, before even starting the capture. But many other systems are not able to provide detailed and accurate information about the data before it's captured. Often, this is because the source system data is unstructured or loosely structured. For these systems, the schema can only be known after the data is captured. Continuous schema inference is most useful in these scenarios.  For example, say you're capturing from MongoDB. MongoDB documents must all have an _id field, but that is essentially the only requirement. You can't know what other fields may exist on MongoDB documents until you've read them. When you set up a capture from MongoDB using the Flow web app, the collection specifications will look something like this:  key: [ /_id ] writeSchema: type: object properties: _id: { type: string } required: [ _id ] readSchema: allOf: - $ref: flow://write-schema - $ref: flow://inferred-schema   Note that this spec uses separate read and write schemas. The writeSchema is extremely permissive, and only requires an _id property with a string value. The readSchema references flow://inferred-schema, which expands to the current inferred schema when the collection is published.  info Note that $ref: flow://write-schema expands to the current writeSchema. Whenever you use $ref: flow://inferred-schema, you should always include the flow://write-schema as well, so that you don't need to repeat any fields that are defined in the writeSchema or wait for those fields to be observed by schema inference.  When you first publish a collection using the inferred schema, flow://inferred-schema expands to a special placeholder schema that rejects all documents. This is to ensure that a non-placeholder inferred schema has been published before allowing any documents to be materialized. Once data is captured to the collection, the inferred schema immediately updates to strictly and minimally describe the captured.  Because the effective readSchema is only ever updated when the collection is published, the best option is usually to use the inferred schema in conjunction with autoDiscover.  ","version":"Next","tagName":"h2"},{"title":"default annotations​","type":1,"pageTitle":"Schemas","url":"/concepts/schemas/#default-annotations","content":" You can use default annotations to prevent null values from being materialized to your endpoint system.  When this annotation is absent for a non-required field, missing values in that field are materialized as null. When the annotation is present, missing values are materialized with the field's default value:  collections: acmeCo/coyotes: schema: type: object required: [id] properties: id: {type: integer} anvils_dropped: {type: integer} reduce: {strategy: sum } default: 0 key: [/id]   default annotations are only used for materializations; they're ignored by captures and derivations. If your collection has both a write and read schema, make sure you add this annotation to the read schema. ","version":"Next","tagName":"h2"},{"title":"Web application","type":0,"sectionRef":"#","url":"/concepts/web-app/","content":"","keywords":"","version":"Next"},{"title":"When to use the web app​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#when-to-use-the-web-app","content":" The web app and flowctl are designed to work together as a complete platform. You can use either, or both, to work on your Data Flows, depending on your preference.  With the Flow web app, you can perform most common workflows, including:  Creating end-to-end Data Flows: capturing data from source systems and materializing it to destinations.Creating, viewing, and editing individual captures and materializations.Monitor the amount of data being processed by the system.Viewing data collections.Viewing users and permissions.Granting permissions to other users.Authenticating with the flowctl CLI.Managing billing details.  Some advanced workflows, like transforming data with derivations, aren't fully available in the web app.  Even if you prefer the command line or plan to perform a task that's only available through flowctl, we recommend you begin your work in the web app; it provides a quicker and easier path to create captures and materializations. You can then switch to flowctl to continue working.  ","version":"Next","tagName":"h2"},{"title":"Signing in​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#signing-in","content":" You use a Google, Microsoft, or GitHub account to sign into Flow. Alternatively, contact us about Single Sign-On (SSO) options.    If you've never used Flow before, you'll be prompted to register before being issued a trial account. If you want to use Flow for production workflows or collaborate with team members, you'll need an organizational account.Contact Estuary to create a new organizational account or join an existing organization.  ","version":"Next","tagName":"h2"},{"title":"Navigating the web app​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#navigating-the-web-app","content":" When you log into the web app, you land on the Welcome page. There are four additional pages visible as tabs in the side navigation: Sources (captures), Collections, Destinations (materializations), and Admin.  The order of the tabs mirrors the order of a basic Data Flow:    graph LR; Capture--&gt;Collection; Collection--&gt;Materialization;  While you may choose to use the tabs in this sequence, it's not necessary. All Flow entities exist individually, outside of the context of a complete Data Flow. You can use the different pages in the web app to monitor and manage your items in a number of other ways, as described below.  ","version":"Next","tagName":"h2"},{"title":"Captures page​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#captures-page","content":" The Captures page shows you a table of existing Flow captures to which you have access. The New Capture button is also visible. You can use the table to monitor your captures.    1: Select all or deselect all.  2: Enable, Disable, and Delete buttons. These actions will be applied to the selected table rows. Choose Disable to temporarily pause the flow of data, Enable to resume, and Delete to permanently remove the capture(s).  3: Materialize button. When you click this button, you're directed to the Create Materializations page. All the collections of the selected capture(s) will be added to the materialization.  4: Filter captures. Type a catalog prefix, unique capture name, or connector name to return captures that match your query.  Capture names follow the pattern prefix/unique-identifier/connector-name, with prefix supporting multiple layers of nesting. You can search for any part of this full capture name. You can also use the * wildcard. For example, if you have a capture called acmeCo/logistics/anvil-locations/source-postgres, you can find it by filtering for acmeCo*source-postgres.  5: Status indicator. Shows the status of the primary task shard that backs this capture.  Primary (Green): Data is actively flowing through the capture.Pending (Yellow): The capture is attempting to re-connect. Often, you'll see this after you re-enable the capture as Flow backfills historical data.Failed (Red): The capture has failed with an unrecoverable error.Disabled (Hollow circle): The capture is disabled.Unknown (Black when app is in light mode; white when app is in dark mode): The web app is unable to determine shard status. Usually, this is due to a temporary connection error.  6: Capture name. The full name is shown, including all prefixes. It is also a link to the details page of the capture.  7: Capture type. The icon shows the type of source system data is captured from.  8: Capture statistics. The Data Written column shows the total amount of data, in bytes and in documents, that the capture has written to its associated collections within a configurable time interval. Click the time interval in the header to select from Today, Yesterday, This Week, Last Week, This Month, Last Month, or All Time.  9: Associated collections. The Writes to column shows all the collections to which the capture writes data. For captures with a large number of collections, there is a chip stating how many collections are hidden. Clicking on this will allow you to hover over this column and scroll to view the full list. These also link to the details page of the collection.  10: Publish time. Hover over this value to see the exact time the capture was last published.  11: Options. Click to open the menu to Edit Specification.  ","version":"Next","tagName":"h2"},{"title":"Editing captures and collections​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#editing-captures-and-collections","content":" When you click Edit specification for a capture, you're taken to the Edit Capture page.  This page is similar to the Create Capture page as it was filled out just before the capture was published.  For detailed steps to edit a capture, see the guide.  ","version":"Next","tagName":"h3"},{"title":"Creating a capture​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#creating-a-capture","content":" When you click Create Capture, you're taken to the Create Capture page. In the first view, all available capture connectors are displayed.  Select the tile of the system from which you want to capture data to show the full capture form. The form details are specific to the connector you chose.  For detailed steps to create a capture, see the guide.  After you successfully publish a capture, you're given the option to materialize the collections you just captured. You can proceed to the materialization, or opt to exit to a different page of the web app.  ","version":"Next","tagName":"h3"},{"title":"Collections page​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#collections-page","content":" The Collections page shows a table of collections to which you have access. There is also a button to begin a new derivation, or transformation.  The table has nearly all of the same features as the Captures table, with several important distinctions that are called out in the image below.  You can use the table to view each collection's specification and see a sample of its data. This can help you verify that collection data was captured as expected and that you'll be able to materialize it how you want, and troubleshoot if necessary.    1: Select all or deselect all.  2: Enable, Disable, and Delete buttons. These actions will be applied to the selected table rows. Choose Disable to temporarily pause the flow of data, Enable to resume, and Delete to permanently remove the collection(s).  3: Filter collections by name. Type a catalog prefix, unique collection name to return collections that match your query.  4: Status indicator. If the collection does not contain a derivation, the indicator should always show green, and hover text will say &quot;Collection.&quot; In the event that the server cannot be reached, the indicator will show &quot;Unknown&quot; status (black in light mode and white in dark mode).  If the collection contains a derivation, the status of the derivation's primary task shard will be indicated:  Primary (Green): Data is actively flowing through the derivation.Pending (Yellow): The derivation is attempting to re-connect.Failed (Red): The derivation has failed with an unrecoverable error.Disabled (Hollow circle): The derivation is disabled.Unknown (Black when app is in light mode; white when app is in dark mode): The web app is unable to determine shard status. Usually, this is due to a temporary connection error.  5: Collection name. The full name is shown, including all prefixes. It is also a link to the details page of the collection.  6: Collection statistics. The Data Written column shows the total amount of data, in bytes and in documents, that has been written to each collection from its associated capture or derivation within a configurable time interval. Click the time interval in the header to select from Today, Yesterday, This Week, Last Week, This Month, Last Month, or All Time.  7: Publish time. Hover over this value to see the exact time the collection was last published.  ","version":"Next","tagName":"h2"},{"title":"Materializations page​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#materializations-page","content":" The Materializations page shows you a table of existing Flow materializations to which you have access. The New Materialization button is also visible.  You can use the table to monitor your materializations. It's nearly identical to the table on the Captures page, with a few exceptions.    1: Select all or deselect all.  2: Enable, Disable, and Delete buttons. These actions will be applied to the selected table rows. Choose Disable to temporarily pause the flow of data, Enable to resume, and Delete to permanently remove the materialization(s).  3: Filter materializations by name. Type a catalog prefix, unique materialization name, or connector name to return materializations that match your query.  Materialization names follow the pattern prefix/unique-identifier/connector-name, with prefix supporting multiple layers of nesting. You can search for any part of this full materialization name. You can also use the * wildcard. For example, if you have a materialization called acmeCo/anvil-locations/materialize-mysql, you can find it by filtering for acmeCo*mysql.  4: Status indicator. Shows the status of the primary task shard that backs this materialization.  Primary (Green): Data is actively flowing through the materialization.Pending (Yellow): The materialization is attempting to re-connect. Often, you'll see this after you re-enable the materialization as Flow backfills historical data.Failed (Red): The materialization has failed with an unrecoverable error.Disabled (Hollow circle): The materialization is disabled.Unknown (Black when app is in light mode; white when app is in dark mode): The web app is unable to determine shard status. Usually, this is due to a temporary connection error.  5: Materialization name. The full name is shown, including all prefixes. It is also a link to the details page of the materialization.  6: Materialization type. The icon shows the type of destination system data is materialized to.  7: Materialization statistics. The Data Read column shows the total amount of data, in bytes and in documents, that the materialization has read from its associated collections within a configurable time interval. Click the time interval in the header to select from Today, Yesterday, This Week, Last Week, This Month, Last Month, or All Time.  8: Associated collections. The Reads from column shows all the collections from which the materialization reads data. For materializations with a large number of collections, there is a chip stating how many collections are hidden. Clicking on this will allow you to hover over this column and scroll to view the full list. These also link to the details page of the collection.  9: Publish time. Hover over this value to see the exact time the materialization was last published.  10: Options. Click to open the menu to Edit Specification.  ","version":"Next","tagName":"h2"},{"title":"Editing materializations​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#editing-materializations","content":" When you click Edit specification for a materialization, you're taken to the Edit Materialization page.  This page is similar to the Create Materialization page as it was filled out just before the materialization was published.  For detailed steps to edit a materialization, see the guide.  ","version":"Next","tagName":"h3"},{"title":"Creating a materialization​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#creating-a-materialization","content":" There are three ways to begin creating a materialization:  Clicking New Materialization on the Materializations page.Selecting one or more captures from the Captures page and clicking Materialize.Clicking Materialize Collections immediately after publishing a capture.  When you initiate the workflow in any of these ways, all available materialization connectors are displayed. Select a connector to reveal the full form with configuration options specific to your desired destination.  Fill out the Endpoint Config form and in the Source Collections section use the Collection Selector to map Flow collections to resources in the destination system.  Note that if you entered the workflow from the Captures page or after publishing a capture, collections will be pre-populated for you.  For detailed steps to create a materialization, see the guide.  ","version":"Next","tagName":"h3"},{"title":"Capture Details page​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#capture-details-page","content":" When you click on the name of a capture on the captures page you will be taken to the capture details page to view data stats, sharding information, and general details.  ","version":"Next","tagName":"h2"},{"title":"Overview Tab​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#overview-tab","content":"   1: The full name of the capture.  2: Capture statistics. The Usage section displays the total amount of data, in bytes and in documents written by the capture, per hour. The number of hours being displayed in the chart can be changed by clicking the time interval in the header to select from 6 hours, 12 hours, 24 hours, 48 hours, or 30 days.  3: The Details section shows information about the capture: when it was last updated, when it was created, the connector being used, and the collections to which the capture writes data.  4: Detailed tooltip. You can hover over a section in the graph to see the specific data of that hour.  5: The most recent hour. This will automatically update every 15 seconds with the most recent data and docs.  6: Associated collections. Shows all the collections to which the capture writes data and when clicked will take you to the collection's detail page.  7: The Shard Information section shows the full identifier of the shard(s) that back your capture. If there's an error, you'll see an alert identifying the failing shard(s). Use the drop-down to open an expanded view of the failed shard's logs.  ","version":"Next","tagName":"h3"},{"title":"Spec Tab​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#spec-tab","content":"   In the Spec tab, you can view the specification of the capture itself.  ","version":"Next","tagName":"h3"},{"title":"Collection Details page​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#collection-details-page","content":" When you click on the name of a collection on the collections page you will be taken to the collection details page to view data stats, sharding information, preview of data, and general details.  ","version":"Next","tagName":"h2"},{"title":"Overview Tab​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#overview-tab-1","content":"   1: The full name of the collection.  2: Collection statistics. The Usage section shows the total amount of data, in bytes and in documents passing through a collection, per hour. The number of hours being displayed in the chart can be changed by clicking the time interval in the header to select from 6 hours, 12 hours, 24 hours, 48 hours, or 30 days.  3: The Details section shows information about the collection: when it was last updated, when it was created, and the associated collections (if any).  4: Detailed tooltip. You can hover over a section in the graph to see the specific data of that hour.  5: The most recent hour. This will automatically update every 15 seconds with the most recent data and docs.  6: Associated collections. Shows source collections that this collection reads from. Click to go to the collection's detail page.  7: The Shard Information section (for derivations) shows the full identifier of the shard(s) that back your derivation. If there's an error, you'll see an alert identifying the failing shard(s). Use the drop-down to open an expanded view of the failed shard's logs.  8: The Data Preview section shows a sample of collection documents: the individual JSON files that comprise the collection. Documents are organized by their collection key value. Click a key from the list to view its document.  ","version":"Next","tagName":"h3"},{"title":"Spec Tab​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#spec-tab-1","content":"   1: The collection's key of the collection.  2: The collection's schema displayed in a read only table. The table columns can be sorted to more easily find what you need.  tip If you need to modify a collection, edit the capture or derivation that provides its data.  ","version":"Next","tagName":"h3"},{"title":"Materialization Details Page​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#materialization-details-page","content":" When you click on the name of a materialization on the materializations page you will be taken to the detail page to view data stats, sharding information, and general details.  ","version":"Next","tagName":"h2"},{"title":"Overview Tab​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#overview-tab-2","content":"   1: The full name of the materialization.  2: Materialization statistics. The Usage section shows the total amount of data, in bytes and in documents read by a materialization, per hour. The number of hours being displayed in the chart can be changed by clicking the time interval in the header to select from 6 hours, 12 hours, 24 hours, 48 hours, or 30 days.  3: The Details section shows information about the materialization: when it was last updated, when it was created, and the associated collections.  4: Detailed tooltip. You can hover over a section in the graph to see the specific data of that hour.  5: The most recent hour. This will automatically update every 15 seconds with the most recent data and docs.  6: Associated collections. Shows all the collections that provide data to this materialization. Click to go to the collection's detail page.  7: The Shard Information section shows the full identifier of the shard(s) that back your materialization. If there's an error, you'll see an alert identifying the failing shard(s). Use the drop-down to open an expanded view of the failed shard's logs.  ","version":"Next","tagName":"h3"},{"title":"Spec Tab​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#spec-tab-2","content":"   In the Spec tab, you can view the specification of the materialization itself.  ","version":"Next","tagName":"h3"},{"title":"Admin page​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#admin-page","content":" On the Admin page, you can view users' access grants, your organization's cloud storage locations, and a complete list of connectors. You can also get an access token to authenticate with flowctl and manage billing information.  ","version":"Next","tagName":"h2"},{"title":"Account Access​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#account-access","content":" The Account Access tab shows you all provisioned access grants on objects to which you also have access. Both users and catalog prefixes can receive access grants. These are split up into two tables called Organization Membership and Data Sharing. Each access grant has its own row, so a given user or prefix may have multiple rows.  For example, if you had read access to foo/ and write access to bar/, you'd have a separate table row in the Organization Membership table for each of these capabilities. If users Alice, Bob, and Carol each had write access on foo/, you'd see three more table rows representing these access grants.  Taking this a step further, the prefix foo/ could have read access to buz/. You'd see this in the Data Sharing table, and it'd signify that everyone who has access to foo/ also inherits read access to buz/.  Use the search boxes to filter by username, prefix, or object.  You can manage access by generating new user invitations, granting data sharing access, or selecting users or prefixes to revoke access.    Generating a new invitation will create a URL with a grant token parameter. This token will allow access based on the prefix, capability, and type you select. Copy the URL and share it with its intended recipient to invite them to your organization.  Learn more about capabilities and access.  ","version":"Next","tagName":"h3"},{"title":"Settings​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#settings","content":" The Settings tab includes additional configuration, such as organization notifications and storage mappings.  Organization Notifications​  Here, you are able to configure which email address(es) will receive notifications related to your organization or prefix.  Cloud Storage​  This section provides a table of the cloud storage locations that back your Flow collections. You're able to view the table if you're an admin.  Each top-level Flow prefix is backed by one or more cloud storage bucket that you own. You typically have just one prefix: your organization name, which you provided when configuring your Flow organizational account. If you're a trial user, your data is stored temporarily in Estuary's cloud storage bucket for your trial period.  Learn more about storage mappings.  ","version":"Next","tagName":"h3"},{"title":"Billing​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#billing","content":" The Billing tab allows you to view and manage information related to past usage, the current billing cycle, and payment methods.  Your usage is broken down by the amount of data processed and number of task hours. View usage trends across previous months in the Usage by Month chart and preview your bill based on usage for the current month. If you are on the free tier (up to 2 connectors and 10 GB per month), you will still be able to preview your bill breakdown, and will have a &quot;Free tier credit&quot; deduction. To help estimate your bill, also see the Pricing Calculator.  To pay your bill, add a payment method to your account. You can choose to pay via card or bank account. You will not be charged until you exceed the free tier's limits.  ","version":"Next","tagName":"h3"},{"title":"Connectors​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#connectors","content":" The Connectors tab offers a complete view of all connectors that are currently available through the web application, including both capture and materialization connectors. If a connector you need is missing, you can request it.  ","version":"Next","tagName":"h3"},{"title":"CLI-API​","type":1,"pageTitle":"Web application","url":"/concepts/web-app/#cli-api","content":" The CLI-API tab provides the access token required to authenticate with flowctl. You can also revoke old tokens. ","version":"Next","tagName":"h3"},{"title":"Estuary Flow Deployment Options","type":0,"sectionRef":"#","url":"/getting-started/deployment-options/","content":"","keywords":"","version":"Next"},{"title":"Public Deployment​","type":1,"pageTitle":"Estuary Flow Deployment Options","url":"/getting-started/deployment-options/#public-deployment","content":" Public Deployment is Estuary Flow's standard Software-as-a-Service (SaaS) offering, designed for ease of use and quick setup.    Fully managed by Estuary: Estuary handles all operational aspects, including updates and maintenance.Quick setup: Minimal configuration is needed to get started.Multiple data processing regions: You can choose between various regions for data planes, like EU or US.Automatic updates: New features and security patches are automatically applied.Suitable for less stringent security requirements: Best for organizations without strict data compliance needs.    ","version":"Next","tagName":"h2"},{"title":"Private Deployment​","type":1,"pageTitle":"Estuary Flow Deployment Options","url":"/getting-started/deployment-options/#private-deployment","content":" Private Deployment offers the security and control of a dedicated infrastructure while retaining the simplicity of a managed service.  Private Deployment is suited for large enterprises and organizations with strict data governance requirements, such as those in regulated industries (e.g., healthcare, finance) or those handling highly sensitive data.  note If you are interested in setting up Private Deployments, reach out to us via email or join our Slack channel and send us a message!    Enhanced security: Data and all processing remains within your private network, offering improved protection.Immutable infrastructure: Security updates are seamlessly integrated without disruption.Compliant with strict data security standards: Supports industries with rigorous compliance needs.Cross-region data movement: Allows for seamless data migration between regions.    ","version":"Next","tagName":"h2"},{"title":"BYOC (Bring Your Own Cloud)​","type":1,"pageTitle":"Estuary Flow Deployment Options","url":"/getting-started/deployment-options/#byoc-bring-your-own-cloud","content":" With BYOC, customers can deploy Estuary Flow directly within their own cloud environment, allowing for greater flexibility and control.  BYOC is the ideal solution for organizations that have heavily invested in their cloud infrastructure and want to maintain full control while integrating Estuary Flow’s capabilities into their stack. This option offers the highest flexibility in terms of customization and compliance.  note If you are interested in BYOC, reach out to us via email or join our Slack channel and send us a message!    Complete control over cloud infrastructure: Manage the cloud environment according to your organization's policies.Utilize existing cloud resources: Leverage your current cloud setup, including any existing cloud credits or agreements.Customizable: Tailor Estuary Flow's deployment to fit specific needs and compliance requirements.Cost savings: Potential to reduce costs by using existing cloud infrastructure and negotiated pricing.Flexible data residency: You choose where data is stored and processed, ensuring compliance with regional regulations.  ","version":"Next","tagName":"h2"},{"title":"Self-hosting Flow​","type":1,"pageTitle":"Estuary Flow Deployment Options","url":"/getting-started/deployment-options/#self-hosting-flow","content":" The Flow runtime is available under the Business Source License. It's possible to self-host Flow using a cloud provider of your choice.  Beta Setup for self-hosting is not covered in this documentation, and full support is not guaranteed at this time. We recommend using the hosted version of Flow for the best experience. If you'd still like to self-host, refer to the GitHub repository or the Estuary Slack. ","version":"Next","tagName":"h2"},{"title":"Configuring Cloud Storage for Flow","type":0,"sectionRef":"#","url":"/getting-started/installation/","content":"","keywords":"","version":"Next"},{"title":"Google Cloud Storage buckets​","type":1,"pageTitle":"Configuring Cloud Storage for Flow","url":"/getting-started/installation/#google-cloud-storage-buckets","content":" You'll need to grant Estuary Flow access to your GCS bucket.  Create a bucket to use with Flow, if you haven't already. Follow the steps to add a principal to a bucket level policy. As you do so: For the principal, enter flow-258@helpful-kingdom-273219.iam.gserviceaccount.com Select the roles/storage.admin role.  ","version":"Next","tagName":"h2"},{"title":"Amazon S3 buckets​","type":1,"pageTitle":"Configuring Cloud Storage for Flow","url":"/getting-started/installation/#amazon-s3-buckets","content":" You'll need to grant Estuary Flow access to your S3 bucket.  Create a bucket to use with Flow, if you haven't already. Follow the steps to add a bucket policy, pasting in one of the policies below. Policies are available for the US and EU Data Planes. Be sure to replace YOUR-S3-BUCKET with the actual name of your bucket.  US Data Plane policyEU Data Plane policy { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Sid&quot;: &quot;AllowUsersToAccessObjectsUnderPrefix&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: { &quot;AWS&quot;: &quot;arn:aws:iam::789740162118:user/flow-aws&quot; }, &quot;Action&quot;: [ &quot;s3:GetObject&quot;, &quot;s3:PutObject&quot;, &quot;s3:DeleteObject&quot; ], &quot;Resource&quot;: &quot;arn:aws:s3:::YOUR-S3-BUCKET/*&quot; }, { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: { &quot;AWS&quot;: &quot;arn:aws:iam::789740162118:user/flow-aws&quot; }, &quot;Action&quot;: &quot;s3:ListBucket&quot;, &quot;Resource&quot;: &quot;arn:aws:s3:::YOUR-S3-BUCKET&quot; }, { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: { &quot;AWS&quot;: &quot;arn:aws:iam::789740162118:user/flow-aws&quot; }, &quot;Action&quot;: &quot;s3:GetBucketPolicy&quot;, &quot;Resource&quot;: &quot;arn:aws:s3:::YOUR-S3-BUCKET&quot; } ] }   ","version":"Next","tagName":"h2"},{"title":"Azure Blob Storage​","type":1,"pageTitle":"Configuring Cloud Storage for Flow","url":"/getting-started/installation/#azure-blob-storage","content":" You'll need to grant Estuary Flow access to your storage account and container. You'll also need to provide some identifying information.  Create an Azure Blob Storage containerto use with Flow, if you haven't already. Gather the following information. You'll need this when you contact us to complete setup. Your Azure AD tenant ID. You can find this in the Azure Active Directory page. Your Azure Blob Storage account ID. You can find this in the Storage Accounts page. Your Azure Blob Storage container ID. You can find this inside your storage account. You'll grant Flow access to your storage resources by connecting to Estuary'sAzure application. Add Estuary's Azure application to your tenant.      Grant the application access to your storage account via theStorage Blob Data OwnerIAM role. Inside your storage account's Access Control (IAM) tab, click Add Role Assignment. Search for Storage Blob Data Owner and select it. On the next page, make sure User, group, or service principal is selected, then click + Select Members. You must search for the exact name of the application, otherwise it won't show up: Estuary Storage Mappings Prod Once you've selected the application, finish granting the role. For more help, see the Azure docs.  ","version":"Next","tagName":"h2"},{"title":"Add the Bucket​","type":1,"pageTitle":"Configuring Cloud Storage for Flow","url":"/getting-started/installation/#add-the-bucket","content":" If your bucket is for Google Cloud Storage or AWS S3, you can add it yourself. Once you've finished the above steps, head to &quot;Admin&quot;, &quot;Settings&quot; then &quot;Configure Cloud Storage&quot; and enter the relevant information there and we'll start to use your bucket for all data going forward.  If your bucket is for Azure, send support@estuary.dev an email with the name of the storage bucket and any other information you gathered per the steps above. Let us know whether you want to use this storage bucket to for your whole Flow account, or just a specific prefix. We'll be in touch when it's done!  ","version":"Next","tagName":"h2"},{"title":"Migrating your existing data to the new storage mapping​","type":1,"pageTitle":"Configuring Cloud Storage for Flow","url":"/getting-started/installation/#migrating-your-existing-data-to-the-new-storage-mapping","content":" Once you've created your new storage mapping, your collections will be updated to pick up the new storage mapping, so new data will be written there. Existing data from your previous storage mapping is not automatically migrated, to do so you should backfill all of your captures after configuring a storage mapping. Most tenants will have been using the estuary-public storage mapping on the Free plan to begin with, which expires data after 20 days. By backfilling all of your captures you guarantee that even though the data from estuary-public will expire, you’ll still have a full view of your data available on your new storage mapping. ","version":"Next","tagName":"h2"},{"title":"Comparisons","type":0,"sectionRef":"#","url":"/getting-started/comparisons/","content":"","keywords":"","version":"Next"},{"title":"Apache Beam and Google Cloud Dataflow​","type":1,"pageTitle":"Comparisons","url":"/getting-started/comparisons/#apache-beam-and-google-cloud-dataflow","content":" Flow’s most apt comparison is to Apache Beam. You may use a variety of runners (processing engines) for your Beam deployment. One of the most popular, Google Cloud Dataflow, is a more robust redistribution under an additional SDK. Regardless of how you use Beam, there’s a lot of conceptual overlap with Flow. This makes Beam and Flow alternatives rather than complementary technologies, but there are key differences.  Like Beam, Flow’s primary primitive is a collection. You build a processing graph (called a pipeline in Beam and a Data Flow in Flow) by relating multiple collections together through procedural transformations, or lambdas. As with Beam, Flow’s runtime performs automatic data shuffles and is designed to allow fully automatic scaling. Also like Beam, collections have associated schemas.  Unlike Beam, Flow doesn’t distinguish between batch and streaming contexts. Flow unifies these paradigms under a single collection concept, allowing you to seamlessly work with both data types.  Also, while Beam allows you the option to define combine operators, Flow’s runtime always applies combine operators. These are built using the declared semantics of the document’s schema, which makes it much more efficient and cost-effective to work with streaming data.  Finally, Flow allows stateful stream-to-stream joins without the windowing semantics imposed by Beam. Notably, Flow’s modeling of state – via its per-key register concept – is substantially more powerful than Beam's per-key-and-window model. For example, registers can trivially model the cumulative lifetime value of a customer.  ","version":"Next","tagName":"h2"},{"title":"Kafka​","type":1,"pageTitle":"Comparisons","url":"/getting-started/comparisons/#kafka","content":" Flow inhabits a different space than Kafka does by itself. Kafka is an infrastructure that supports streaming applications running elsewhere. Flow is an opinionated framework for working with real-time data. You might think of Flow as an analog to an opinionated bundling of several important features from the broader Kafka ecosystem.  Flow is built on Gazette, a highly-scalable streaming broker similar to log-oriented pub/sub systems. Thus, Kafka is more directly comparable to Gazette. Flow also uses Gazette’s consumer framework, which has similarities to Kafka consumers. Both manage scale-out execution contexts for consumer tasks, offer durable local task stores, and provide exactly-once semantics.  Journals in Gazette and Flow are roughly analogous to Kafka partitions. Each journal is a single append-only log. Gazette has no native notion of a topic, but instead supports label-based selection of subsets of journals, which tends to be more flexible. Gazette journals store data in contiguous chunks called fragments, which typically live in cloud storage. Each journal can have its own separate storage configuration, which Flow leverages to allow users to bring their own cloud storage buckets. Another unique feature of Gazette is its ability to serve reads of historical data by providing clients with pre-signed cloud storage URLs, which enables it to serve many readers very efficiently.  Generally, Flow users don't need to know or care much about Gazette and its architecture, since Flow provides a higher-level interface over groups of journals, called collections.  Flow collections are somewhat similar to Kafka streams, but with some important differences. Collections always store JSON and must have an associated JSON schema. Collections also support automatic logical and physical partitioning. Each collection is backed by one or more journals, depending on the partitioning.  Flow tasks are most similar to Kafka stream processors, but are more opinionated. Tasks fall into one of three categories: captures, derivations, and materializations. Tasks may also have more than one process, which Flow calls shards, to allow for parallel processing. Tasks and shards are fully managed by Flow. This includes transactional state management and zero-downtime splitting of shards, which enables turnkey scaling.  See how Flow compares to popular stream processing platforms that use Kafka:  Flow vs Confluent feature and pricing breakdownFlow vs Debezium feature and pricing breakdown  ","version":"Next","tagName":"h2"},{"title":"Spark​","type":1,"pageTitle":"Comparisons","url":"/getting-started/comparisons/#spark","content":" Spark can be described as a batch engine with stream processing add-ons, where Flow is fundamentally a streaming system that is able to easily integrate with batch systems.  You can think of a Flow collection as a set of RDDs with common associated metadata. In Spark, you can save an RDD to a variety of external systems, like cloud storage or a database. Likewise, you can load from a variety of external systems to create an RDD. Finally, you can transform one RDD into another. You use Flow collections in a similar manner. They represent a logical dataset, which you can materialize to push the data into some external system like cloud storage or a database. You can also create a collection that is derived by applying stateful transformations to one or more source collections.  Unlike Spark RDDs, Flow collections are backed by one or more unbounded append-only logs. Therefore, you don't create a new collection each time data arrives; you simply append to the existing one. Collections can be partitioned and can support extremely large volumes of data.  Spark's processing primitives, applications, jobs, and tasks, don't translate perfectly to Flow, but we can make some useful analogies. This is partly because Spark is not very opinionated about what an application does. Your Spark application could read data from cloud storage, then transform it, then write the results out to a database. The closest analog to a Spark application in Flow is the Data Flow. A Data Flow is a composition of Flow tasks, which are quite different from tasks in Spark.  In Flow, a task is a logical unit of work that does one of capture (ingest), derive (transform), or materialize (write results to an external system). What Spark calls a task is actually closer to a Flow shard. In Flow, a task is a logical unit of work, and shards represent the potentially numerous processes that actually carry out that work. Shards are the unit of parallelism in Flow, and you can easily split them for turnkey scaling.  Composing Flow tasks is also a little different than composing Spark jobs. Flow tasks always produce and/or consume data in collections, instead of piping data directly from one shard to another. This is because every task in Flow is transactional and, to the greatest degree possible, fault-tolerant. This design also affords painless backfills of historical data when you want to add new transformations or materializations.  ","version":"Next","tagName":"h2"},{"title":"Hadoop, HDFS, and Hive​","type":1,"pageTitle":"Comparisons","url":"/getting-started/comparisons/#hadoop-hdfs-and-hive","content":" There are many different ways to use Hadoop, HDFS, and the ecosystem of related projects, several of which are useful comparisons to Flow.  To gain an understanding of Flow's processing model for derivations, see this blog post about MapReduce in Flow.  HDFS is sometimes used as a system of record for analytics data, typically paired with an orchestration system for analytics jobs. If you do this, you likely export datasets from your source systems into HDFS. Then, you use some other tool to coordinate running various MapReduce jobs, often indirectly through systems like Hive.  For this use case, the best way of describing Flow is that it completely changes the paradigm. In Flow, you always append data to existing collections, rather than creating a new one each time a job is run. In fact, Flow has no notion of a job like there is in Hadoop. Flow tasks run continuously and everything stays up to date in real time, so there's never a need for outside orchestration or coordination. Put simply, Flow collections are log-like, and files in HDFS typically store table-like data. This blog post explores those differences in greater depth.  To make this more concrete, imagine a hypothetical example of a workflow in the Hadoop world where you export data from a source system, perform some transformations, and then run some Hive queries.  In Flow, you instead define a capture of data from the source, which runs continuously and keeps a collection up to date with the latest data from the source. Then you transform the data with Flow derivations, which again apply the transformations incrementally and in real time. While you could actually use tools like Hive to directly query data from Flow collections — the layout of collection data in cloud storage is intentionally compatible with this — you could also materialize a view of your transformation results to any database, which is also kept up to date in real time.  ","version":"Next","tagName":"h2"},{"title":"Fivetran, Airbyte, and other ELT solutions​","type":1,"pageTitle":"Comparisons","url":"/getting-started/comparisons/#fivetran-airbyte-and-other-elt-solutions","content":" Tools like Fivetran and Airbyte are purpose-built to move data from one place to another. These ELT tools typically model sources and destinations, and run regularly scheduled jobs to export from the source directly to the destination. Flow models things differently. Instead of modeling the world in terms of independent scheduled jobs that copy data from source to destination, Data Flows model a directed graph ofcaptures (reads from sources),derivations (transforms), andmaterializations (writes to destinations). Collectively, these are called tasks.  Tasks in Flow are only indirectly linked. Captures read data from a source and output to collections. Flow collections store all the data in cloud storage, with configurable retention for historical data. You can then materialize each collection to any number of destination systems. Each one will be kept up to date in real time, and new materializations can automatically backfill all your historical data. Collections in Flow always have an associated JSON schema, and they use that to ensure the validity of all collection data. Tasks are also transactional and generally guarantee end-to-end exactly-once processing (so long as the endpoint system can accommodate them).  Like Airbyte, Flow uses connectors for interacting with external systems in captures and materializations. For captures, Flow integrates the Airbyte specification, so all Airbyte source connectors can be used with Flow. For materializations, Flow uses its own protocol which is not compatible with the Airbyte spec. In either case, the usage of connectors is pretty similar.  In terms of technical capabilities, Flow can do everything that these tools can and more. Both Fivetran and Airbyte both currently have graphical interfaces that make them much easier for non-technical users to configure. Flow, too, is focused on empowering non-technical users through its web application. At the same time, Flow offers declarative YAML for configuration, which works excellently in a GitOps workflow.  Flow vs Fivetran feature and pricing breakdown.  ","version":"Next","tagName":"h2"},{"title":"dbt​","type":1,"pageTitle":"Comparisons","url":"/getting-started/comparisons/#dbt","content":" dbt is a tool that enables data analysts and engineers to transform data in their warehouses more effectively.  In addition to – and perhaps more important than – its transform capability, dbt brought an entirely new workflow for working with data: one that prioritizes version control, testing, local development, documentation, composition, and re-use.  Like dbt, Flow uses a declarative model and tooling, but the similarities end there. dbt is a tool for defining transformations, which are executed within your analytics warehouse. Flow is a tool for delivering data to that warehouse, as well as continuous operational transforms that are applied everywhere else.  These two tools can make lots of sense to use together. First, Flow brings timely, accurate data to the warehouse. Within the warehouse, analysts can use tools like dbt to explore the data. The Flow pipeline is then ideally suited to productionize important insights as materialized views or by pushing to another destination.  Put another way, Flow is a complete ELT platform, but you might choose to perform and manage more complex transformations in a separate, dedicated tool like dbt. While Flow and dbt don’t interact directly, both offer easy integration through your data warehouse.  ","version":"Next","tagName":"h2"},{"title":"Materialize, Rockset, ksqlDB, and other real-time databases​","type":1,"pageTitle":"Comparisons","url":"/getting-started/comparisons/#materialize-rockset-ksqldb-and-other-real-time-databases","content":" Modern real-time databases like Materialize, Rockset, and ksqlDB consume streams of data, oftentimes from Kafka brokers, and can keep SQL views up to date in real time.  These real-time databases have a lot of conceptual overlap with Flow. The biggest difference is that Flow can materialize this same type of incrementally updated view into any database, regardless of whether that database has real-time capabilities or not.  However, this doesn't mean that Flow should replace these systems in your stack. In fact, it can be optimal to use Flow to feed data into them. Flow adds real-time data capture and materialization options that many real-time databases don't support. Once data has arrived in the database, you have access to real-time SQL analysis and other analytical tools not native to Flow. For further explanation, read the section below on OLAP databases.  ","version":"Next","tagName":"h2"},{"title":"Snowflake, BigQuery, and other OLAP databases​","type":1,"pageTitle":"Comparisons","url":"/getting-started/comparisons/#snowflake-bigquery-and-other-olap-databases","content":" Flow differs from OLAP databases mainly in that it's not a database. Flow has no query interface, and no plans to add one. Instead, Flow allows you to use the query interfaces of any database by materializing views into it.  Flow is similar to OLAP databases in that it can be the source of truth for all analytics data (though it's also capable enough to handle operational workloads). Instead of schemas and tables, Flow defines collections. These collections are conceptually similar to database tables in the sense that they are containers for data with an associated (primary) key. Under the hood, Flow collections are each backed by append-only logs, where each document in the log represents a delta update for a given key.  Collections can be easily materialized into a variety of external systems, such as Snowflake or BigQuery. This creates a table in your OLAP database that is continuously kept up to date with the collection. With Flow, there's no need to schedule exports to these systems, and thus no need to orchestrate the timing of those exports. You can also materialize a given collection into multiple destination systems, so you can always use whichever system is best for the type of queries you want to run.  Like Snowflake, Flow uses inexpensive cloud storage for all collection data. It even lets you bring your own storage bucket, so you're always in control. Unlike data warehouses, Flow is able to directly capture data from source systems, and continuously and incrementally keep everything up to date.  A common pattern is to use Flow to capture data from multiple different sources and materialize it into a data warehouse. Flow can also help you avoid expenses associated with queries you frequently pull from a data warehouse by keeping an up-to-date view of them where you want it. Because of Flow’s exactly-once processing guarantees, these materialized views are always correct, consistent, and fault-tolerant. ","version":"Next","tagName":"h2"},{"title":"Flow tutorials","type":0,"sectionRef":"#","url":"/getting-started/tutorials/","content":"Flow tutorials Flow tutorials are complete learning experiences that help you get to know Flow using sample data. You'll find these helpful if: You're a new user looking for practice before you implement production Data Flows. You'd rather learn the Flow concepts in a hands-on setting. If you're looking for more streamlined guidance for your own use case, check out the user guides.","keywords":"","version":"Next"},{"title":"Quickstart for Flow","type":0,"sectionRef":"#","url":"/getting-started/quickstart/","content":"","keywords":"","version":"Next"},{"title":"Step 1. Set up a Capture​","type":1,"pageTitle":"Quickstart for Flow","url":"/getting-started/quickstart/#step-1-set-up-a-capture","content":" Head over to your Flow dashboard (if you haven’t registered yet, you can do so here.) and create a new Capture. A capture is how Flow ingests data from an external source.  Go to the sources page by clicking on the Sources on the left hand side of your screen, then click on + New Capture    Configure the connection to the database and press Next.    On the following page, we can configure how our incoming data should be represented in Flow as collections. As a quick refresher, let’s recap how Flow represents data on a high level.  Documents  The documents of your flows are stored in collections: real-time data lakes of JSON documents in cloud storage. Documents being backed by an object storage mean that once you start capturing data, you won’t have to worry about it not being available to replay – object stores such as S3 can be configured to cheaply store data forever. See docs page for more information.  Schemas  Flow documents and collections always have an associated schema that defines the structure, representation, and constraints of your documents. In most cases, Flow generates a functioning schema on your behalf during the discovery phase of capture, which has already automatically happened - that’s why you’re able to take a peek into the structure of the incoming data!  To see how Flow parsed the incoming records, click on the Collection tab and verify the inferred schema looks correct.    ","version":"Next","tagName":"h2"},{"title":"Step 2. Set up a Materialization​","type":1,"pageTitle":"Quickstart for Flow","url":"/getting-started/quickstart/#step-2-set-up-a-materialization","content":" Similarly to the source side, we’ll need to set up some initial configuration in Snowflake to allow Flow to materialize collections into a table.  Head over to the Destinations page, where you can create a new Materialization.    Choose Snowflake and start filling out the connection details based on the values inside the script you executed in the previous step. If you haven’t changed anything, this is how the connector configuration should look like:    You can grab your Snowflake host URL and account identifier by navigating to these two little buttons on the Snowflake UI.    After the connection details are in place, the next step is to link the capture we just created to Flow is able to see collections we are loading data into from Postgres.  You can achieve this by clicking on the “Source from Capture” button, and selecting the name of the capture from the table.    After pressing continue, you are met with a few configuration options, but for now, feel free to press Next, then Save and Publish in the top right corner, the defaults will work perfectly fine for this tutorial.  A successful deployment will look something like this:    And that’s it, you’ve successfully published a real-time CDC pipeline. Let’s check out Snowflake to see how the data looks.    Looks like the data is arriving as expected, and the schema of the table is properly configured by the connector based on the types of the original table in Postgres.  To get a feel for how the data flow works, head over to the collection details page on the Flow web UI to see your changes immediately. On the Snowflake end, they will be materialized after the next update.  ","version":"Next","tagName":"h2"},{"title":"Next Steps​","type":1,"pageTitle":"Quickstart for Flow","url":"/getting-started/quickstart/#next-steps","content":" That’s it! You should have everything you need to know to create your own data pipeline for loading data into Snowflake!  Now try it out on your own PostgreSQL database or other sources.  If you want to learn more, make sure you read through the Estuary documentation.  You’ll find instructions on how to use other connectors here. There are more tutorials here.  Also, don’t forget to join the Estuary Slack Community! ","version":"Next","tagName":"h2"},{"title":"Create a real-time materialized view in PostgreSQL","type":0,"sectionRef":"#","url":"/getting-started/tutorials/continuous-materialized-view/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Create a real-time materialized view in PostgreSQL","url":"/getting-started/tutorials/continuous-materialized-view/#prerequisites","content":" An Estuary Flow account. If you don't have one, visit the Flow web app to register for free. A GitLab, GitHub, or BitBucket account. You'll use this to log into GitPod, the cloud development environment integrated with Flow. Alternatively, you can complete this tutorial using a local development environment. In that case, you'll need to install flowctl locally. Note that the steps you'll need to take will be different. Refer to this guide for help. A Postgres database set up to allow connections from Flow. Amazon RDS, Amazon Aurora, Google Cloud SQL, Azure Database for PostgreSQL, and self-hosted databases are supported.  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Create a real-time materialized view in PostgreSQL","url":"/getting-started/tutorials/continuous-materialized-view/#introduction","content":" Materialized views in Postgres give you a powerful way to narrow down a huge dataset into a compact one that you can easily monitor. But if your data is updating in real-time, traditional materialized views introduce latency. They're batch workflows — the query is run at a set interval.  To get around this, you'll need to perform a real-time transformation elsewhere. Flow derivations are a great way to do this.  For this example, you'll use Estuary's public data collection of recent changes to Wikipedia, captured from the Wikimedia Foundation's event stream.  The raw dataset is quite large. It captures every change to the platform — about 30 per second — and includes various properties. Written to a Postgres table, it quickly grows to an size that's very expensive to query.  First, you'll scope the raw data down to a small fact table with a derivation.  You'll then materialize both the raw and transformed datasets to your Postgres instance and compare performance.  ","version":"Next","tagName":"h2"},{"title":"Loading the Wikipedia Demo​","type":1,"pageTitle":"Create a real-time materialized view in PostgreSQL","url":"/getting-started/tutorials/continuous-materialized-view/#loading-the-wikipedia-demo","content":" Navigate to the Live Demo page and click on See the capture. After accepting the pop up, Estuary will populate your Sources, Collections and Destinations with the Wikipedia Demo tasks.  ","version":"Next","tagName":"h2"},{"title":"Check out the source data​","type":1,"pageTitle":"Create a real-time materialized view in PostgreSQL","url":"/getting-started/tutorials/continuous-materialized-view/#check-out-the-source-data","content":" Got the the collections page of the Flow web app. Search for demo/wikipedia/recentchange and click on its name. On the Collection Details page, click the Spec tab. The collection schema has many fields. Because Wikipedia sees a lot of edits, this would yield a large, unwieldy table in Postgres. Tip To save on performance, you can also perform this tutorial using the smaller demo/wikipedia/recentchange-sampled collection. Apart from the collection name, all other steps are the same. Learn more about Flow collections and schemas. Now you'll create the derivation. A derivation is a new collection that's defined by a transformation. First, you'll define the collection's schema. Then, you'll write the transformation to shape the data to that schema.  ","version":"Next","tagName":"h2"},{"title":"Add a derivation to transform data​","type":1,"pageTitle":"Create a real-time materialized view in PostgreSQL","url":"/getting-started/tutorials/continuous-materialized-view/#add-a-derivation-to-transform-data","content":" You'll write your derivation using a cloud development environment integrated in the Flow web app.  Go back to the collections page and click the New Transformation button. Set the source collection to the raw Wikipedia data. Search for and select demo/wikipedia/recentchange. Set the transformation language to SQL. Give the derivation a name. From the dropdown, choose the name of your catalog prefix and append a unique name, for example yourprefix/wikipedia/user-fact-table. Click Proceed to GitPod to create your development environment. Sign in with one of the available account types. On the New Workspace screen, keep the Context URL option selected and click Continue. A GitPod development environment opens. A stubbed-out derivation with a SQL transformation has already been created for you. Next, you'll locate and open the source files. Each slash-delimited prefix of your derivation name has become a folder. Open the nested folders to locate a flow.yaml file. Following the example above, you'd open the folders called yourprefix, then wikipedia, to find the correct flow.yaml file. Its contents look like this: collections: yourprefix/wikipedia/user-fact-table: schema: properties: your_key: type: string required: - your_key type: object key: - /your_key derive: using: sqlite: migrations: - user-fact-table.migration.0.sql transforms: - name: recentchange source: demo/wikipedia/recentchange shuffle: any lambda: user-fact-table.lambda.recentchange.sql Your first order of business is to replace the placeholder schema and collection key. As we saw earlier, the source collection's schema and key caused every Wikipedia event to generate a new document. You'll fix that here. Replace the existing schema and key stanzas with the following: schema: properties: edits_this_day: reduce: strategy: sum type: integer date: format: date type: string user: type: string reduce: strategy: merge required: - user - date - edits_this_day type: object key: - /user - /date   The new schema contains reduction annotations. These sum the changes made by a given user on a given date. The collection is now keyed on each unique combination of user ID and date. It has just three fields: the user, date, and the number of changes made by that user on that date.  Next, you'll add the transformation.  In the transforms stanza, give the transformation a new name to differentiate it from the name of the source collection. For example: transforms: - name: dailychangesbyuser Update the shuffle key. Since we're working with a large dataset, this ensures that each user is processed by the same task shard. This way, you'll prevent Flow from creating multiple counts for a given user and date combination. Learn more about shuffles. shuffle: { key: [ /user ] } Now, the transform needs is the SQL lambda function — the function that will shape the source data to fit the new schema. Flow has created another file to contain it. Open the file called user-fact-table.lambda.recentchange.sql. Replace its contents with select $user, 1 as edits_this_day, date($meta$dt) as date where $user is not null; This creates the edits_this_day field we referenced earlier, and starts the counter at 1. It converts the timestamp into a simplified date format. Finally, it filters out null users (which occasionally occur in the Wikipedia data stream and would violate your schema). All pieces of the derivation are in place. Double check your files against these samples:  flow.yamluser-fact-table.lambda.recentchange.sql --- collections: yourprefix/wikipedia/user-fact-table: schema: properties: edits_this_day: reduce: strategy: sum type: integer date: format: date type: string user: type: string reduce: strategy: merge required: - user - date - edits_this_day type: object key: - /user - /date derive: using: sqlite: migrations: - user-fact-table.migration.0.sql transforms: - name: dailychangesbyuser source: demo/wikipedia/recentchange shuffle: { key: [ /user ] } lambda: user-fact-table.lambda.recentchange.sql   Run the derivation locally and preview its output:  flowctl preview --source flow.yaml   In your terminal, you'll see JSON documents that look like:  {&quot;date&quot;:&quot;2023-07-18&quot;,&quot;edits_this_day&quot;:3,&quot;user&quot;:&quot;WMrapids&quot;}   This looks right: it includes the correctly formatted date, the number of edits, and the username. You're ready to publish.  Stop the local derivation with Ctrl-C. Publish the derivation:  flowctl catalog publish --source flow.yaml   The message Publish successful means you're all set. Your transformation will continue in real time based on the raw dataset, which is also updating in real time. You're free to close your GitPod.  ","version":"Next","tagName":"h2"},{"title":"Create the continuous materialized view​","type":1,"pageTitle":"Create a real-time materialized view in PostgreSQL","url":"/getting-started/tutorials/continuous-materialized-view/#create-the-continuous-materialized-view","content":" Now, you'll materialize your new fact table to Postgres. You'll also materialize the source dataset to compare performance.  Go to the Destinations page in the Flow web app. Click New Materialization. Find the PostgreSQL and click Materialization. Add a unique name for the materialization, for example, yourprefix/yourname-materialized-views-demo. Fill out the Basic Config with: A username and password for the Postgres instance. Your database host and port. The database name (if in doubt, use the default, postgres). See the connector documentation if you need help finding these properties. In the Source Collections browser, search for and add the collection demo/wikipedia/recentchange and name the corresponding Postgres Table wikipedia_raw. Also search for and add the collection you just derived, (for example, yourprefix/wikipedia/user-fact-table). Name the corresponding Postgres table wikipedia_data_by_user. Click Next to test the connection. Click Save and Publish.  ","version":"Next","tagName":"h2"},{"title":"Explore the results​","type":1,"pageTitle":"Create a real-time materialized view in PostgreSQL","url":"/getting-started/tutorials/continuous-materialized-view/#explore-the-results","content":" In your Postgres client of choice, note the size of each table and how they quickly change. Try running some basic queries against both and compare performance. See the blog post for ideas.  Once you're satisfied, and to prevent continual resource use, disable or delete your materialization from theDestinations page.  ","version":"Next","tagName":"h2"},{"title":"Resources​","type":1,"pageTitle":"Create a real-time materialized view in PostgreSQL","url":"/getting-started/tutorials/continuous-materialized-view/#resources","content":" About derivations ","version":"Next","tagName":"h2"},{"title":"Create your first dataflow with Amazon S3 and Snowflake","type":0,"sectionRef":"#","url":"/getting-started/tutorials/dataflow-s3-snowflake/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Create your first dataflow with Amazon S3 and Snowflake","url":"/getting-started/tutorials/dataflow-s3-snowflake/#prerequisites","content":" You'll need:  An Estuary Flow account. If you don't have one, visit the Flow web app to register for free. A Snowflake free trial account (or a full account). Snowflake trials are valid for 30 days.  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Create your first dataflow with Amazon S3 and Snowflake","url":"/getting-started/tutorials/dataflow-s3-snowflake/#introduction","content":" The data​  New York City hosts the United States' largest bike share program, Citi Bike. Citi Bike shares ride data in CSV format with the public, including the starting and ending times and locations for every ride. They upload new data monthly to their Amazon S3 bucket as a zipped CSV file.  In this scenario, let's imagine you're interested in urban bike safety, or perhaps you plan to open a bike store and entice Citi Bike renters to buy their own bikes. You'd like to access the Citi Bike data in your Snowflake data warehouse. From there, you plan to use your data analytics platform of choice to explore the data, and perhaps integrate it with business intelligence apps.  You can use Estuary Flow to build a real-time Data Flow that will capture all the new data from Citi Bike as soon as it appears, convert it to Snowflake's format, and land the data in your warehouse.  Estuary Flow​  In Estuary Flow, you create Data Flows to connect data source and destination systems.  The simplest Data Flow comprises three types of entities:  A data capture, which ingests data from the source. In this case, you'll capture from Amazon S3. One or more collections, which Flow uses to store that data inside a cloud-backed data lake A materialization, to push the data to an external destination. In this case, you'll materialize to a Snowflake data warehouse.    graph LR; Capture--&gt;Collection; Collection--&gt;Materialization;  For the capture and materialization to work, they need to integrate with outside systems: in this case, S3 and Snowflake, but many other systems can be used. To accomplish this, Flow uses connectors. Connectors are plug-in components that interface between Flow and an outside system. Today, you'll use Flow's S3 capture connector and Snowflake materialization connector.  You'll start by creating your capture.  ","version":"Next","tagName":"h2"},{"title":"Capture Citi Bike data from S3​","type":1,"pageTitle":"Create your first dataflow with Amazon S3 and Snowflake","url":"/getting-started/tutorials/dataflow-s3-snowflake/#capture-citi-bike-data-from-s3","content":" Go to the Flow web app at dashboard.estuary.dev and sign in. Click the Sources tab and choose New Capture All of the available capture connectors — representing the possible data sources — appear as tiles. Find the Amazon S3 tile and click Capture. A form appears with the properties required for an S3 capture. Every connector requires different properties to configure. First, you'll name your capture. Click inside the Name box. Names of entities in Flow must be unique. They're organized by prefixes, similar to paths in a file system. You'll see one or more prefixes pertaining to your organization. These prefixes represent the namespaces of Flow to which you have access. Click your prefix from the dropdown and append a unique name after it. For example, myOrg/yourname/citibiketutorial. Next, fill out the required properties for S3. AWS Access Key ID and AWS Secret Access Key: The bucket is public, so you can leave these fields blank. AWS Region: us-east-1 Bucket: tripdata Prefix: The storage bucket isn't organized by prefixes, so leave this blank. Match Keys: 2022 The Citi Bike storage bucket has been around for a while. Some of the older datasets have incorrect file extensions or contain data in different formats. By selecting a subset of files from the year 2022, you'll make things easier to manage for the purposes of this tutorial. (In a real-world use case, you'd likely reconcile the different schemas of the various data formats using a derivation.Derivations are a more advanced Flow skill.) Click Next. Flow uses the configuration you provided to initiate a connection with S3. It generates a list of collections that will store the data inside Flow. In this case, there's just one collection from the bucket. Once this process completes, you can move on to the next step. If there's an error, go back and check your configuration. Click Save and Publish. Flow deploys, or publishes, your capture, including your change to the schema. You'll see a notification when the this is complete. A subset of data from the Citi Bike tripdata bucket has been captured to a Flow collection. Now, you can materialize that data to Snowflake. Click Materialize Collections.  ","version":"Next","tagName":"h2"},{"title":"Prepare Snowflake to use with Flow​","type":1,"pageTitle":"Create your first dataflow with Amazon S3 and Snowflake","url":"/getting-started/tutorials/dataflow-s3-snowflake/#prepare-snowflake-to-use-with-flow","content":" Before you can materialize from Flow to Snowflake, you need to complete some setup steps.  Leave the Flow web app open. In a new window or tab, go to your Snowflake console. If you're a new trial user, you should have received instructions by email. For additional help in this section, see the Snowflake documentation. Create a new SQL worksheet if you don't have one open. This provides an interface where you can run queries. Paste the following script into the console, changing the value for estuary_password from secret to a strong password:  set database_name = 'ESTUARY_DB'; set warehouse_name = 'ESTUARY_WH'; set estuary_role = 'ESTUARY_ROLE'; set estuary_user = 'ESTUARY_USER'; set estuary_password = 'secret'; set estuary_schema = 'ESTUARY_SCHEMA'; -- create role and schema for Estuary create role if not exists identifier($estuary_role); grant role identifier($estuary_role) to role SYSADMIN; -- Create snowflake DB create database if not exists identifier($database_name); use database identifier($database_name); create schema if not exists identifier($estuary_schema); -- create a user for Estuary create user if not exists identifier($estuary_user) password = $estuary_password default_role = $estuary_role default_warehouse = $warehouse_name; grant role identifier($estuary_role) to user identifier($estuary_user); grant all on schema identifier($estuary_schema) to identifier($estuary_role); -- create a warehouse for estuary create warehouse if not exists identifier($warehouse_name) warehouse_size = xsmall warehouse_type = standard auto_suspend = 60 auto_resume = true initially_suspended = true; -- grant Estuary role access to warehouse grant USAGE on warehouse identifier($warehouse_name) to role identifier($estuary_role); -- grant Estuary access to database grant CREATE SCHEMA, MONITOR, USAGE on database identifier($database_name) to role identifier($estuary_role); -- change role to ACCOUNTADMIN for STORAGE INTEGRATION support to Estuary (only needed for Snowflake on GCP) use role ACCOUNTADMIN; grant CREATE INTEGRATION on account to role identifier($estuary_role); use role sysadmin; COMMIT;   Click the drop-down arrow next to the Run button and click Run All.  Snowflake runs all the queries and is ready to use with Flow.  Return to the Flow web application.  ","version":"Next","tagName":"h2"},{"title":"Materialize your Flow collection to Snowflake​","type":1,"pageTitle":"Create your first dataflow with Amazon S3 and Snowflake","url":"/getting-started/tutorials/dataflow-s3-snowflake/#materialize-your-flow-collection-to-snowflake","content":" You were directed to the Materializations page. All of the available materialization connectors — representing the possible data destinations — are shown as tiles.  Find the Snowflake tile and click Materialization. A new form appears with the properties required to materialize to Snowflake. Click inside the Name box. Click your prefix from the dropdown and append a unique name after it. For example, myOrg/yourname/citibiketutorial. Next, fill out the required properties for Snowflake (most of these come from the script you just ran). Host URL: This is the URL you use to log into Snowflake. If you recently signed up for a trial, it should be in your email. Omit the protocol from the beginning. For example, ACCOUNTID.region.cloudprovider.snowflakecomputing.com or orgname-accountname.snowflakecomputing.com. Learn more about account identifiers and host URLs. Account: Your account identifier. This is part of the Host URL. Using the previous examples, it would be ACCOUNTID or accountname. User: ESTUARY_USER Password: secret (Substitute the password you set in the script.) Database: ESTUARY_DB Schema: ESTUARY_SCHEMA Warehouse: ESTUARY_WH Role: ESTUARY_ROLE Scroll down to view the Source Collections section and change the default name in the Table field to CitiBikeData or another name of your choosing. Every Flow collection is defined by one or more schemas. Because S3 is a cloud storage bucket, the schema used to ingest the data is quite permissive. You'll add a more detailed schema for Flow to use to materialize the data to Snowflake. This will ensure that each field from the source CSV is mapped to a column in the Snowflake table. With the collection still selected, click its Collection tab. Then, click Schema Inference. Flow examines the data and automatically generates a new readSchema. Scroll through and note the differences between this and the original schema, renamed writeSchema. Click Apply Inferred Schema. Click Next. Flow uses the configuration you provided to initiate a connection with Snowflake and generate a specification with details of the materialization. Once this process completes, you can move on to the next step. If there's an error, go back and check your configuration. Click Save and Publish. Flow publishes the materialization. Return to the Snowflake console and expand ESTUARY_DB and ESTUARY_SCHEMA. You'll find the materialized table there.  ","version":"Next","tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Create your first dataflow with Amazon S3 and Snowflake","url":"/getting-started/tutorials/dataflow-s3-snowflake/#conclusion","content":" You've created a complete Data Flow that ingests the Citi Bike CSV files from an Amazon S3 bucket and materializes them into your Snowflake database.  When Citi Bike uploads new data, it'll be reflected in Snowflake in near-real-time, so long as you don't disable your capture or materialization.  Data warehouses like Snowflake are designed to power data analytics. From here, you can begin any number of analytical workflows.  Want to learn more?​  For more information on the connectors you used today, see the pages on S3 and Snowflake. You can create a Data Flow using any combination of supported connectors with a similar process to the one you followed in this tutorial. For a more generalized procedure, see the guide to create a Data Flow. ","version":"Next","tagName":"h2"},{"title":"Who should use Flow?","type":0,"sectionRef":"#","url":"/getting-started/who-should-use-flow/","content":"","keywords":"","version":"Next"},{"title":"How Flow can help​","type":1,"pageTitle":"Who should use Flow?","url":"/getting-started/who-should-use-flow/#how-flow-can-help","content":" These unique Flow features can help you solve the problems listed above.  ","version":"Next","tagName":"h2"},{"title":"Fully integrated pipelines​","type":1,"pageTitle":"Who should use Flow?","url":"/getting-started/who-should-use-flow/#fully-integrated-pipelines","content":" With Flow, you can build, test, and evolve pipelines that continuously capture, transform, and materialize data across all of your systems. With one tool, you can power workflows that have historically required you to first piece together services, then integrate and operate them in-house to meet your needs.  To achieve comparable capabilities to Flow you would need:  A low-latency streaming system, such as AWS KinesisData lake build-out, such as Kinesis Firehose to S3Custom ETL application development, such as Spark, Flink, or AWS λSupplemental data stores for intermediate transformation statesETL job management and execution, such as a self-hosting or Google Cloud DataflowCustom reconciliation of historical vs streaming datasets, including onerous backfills of new streaming applications from historical data  Flow dramatically simplifies this inherent complexity. It saves you time and costs, catches mistakes before they hit production, and keeps your data fresh across all the places you use it. With both a UI-forward web application and a powerful CLI , more types of professionals can contribute to what would otherwise require a highly specialized set of technical skills.  ","version":"Next","tagName":"h3"},{"title":"Efficient architecture​","type":1,"pageTitle":"Who should use Flow?","url":"/getting-started/who-should-use-flow/#efficient-architecture","content":" Flow mixes a variety of architectural techniques to deliver high throughput, avoid latency, and minimize operating costs. These include:  Leveraging reductions to reduce the amount of data that must be ingested, stored, and processed, often dramaticallyExecuting transformations predominantly in-memoryOptimistic pipelining and vectorization of internal remote procedure calls (RPCs) and operationsA cloud-native design that optimizes for public cloud pricing models  Flow also makes it easy to materialize focused data views directly into your warehouse, so you don't need to repeatedly query the much larger source datasets. This can dramatically lower warehouse costs.  ","version":"Next","tagName":"h3"},{"title":"Powerful transformations​","type":1,"pageTitle":"Who should use Flow?","url":"/getting-started/who-should-use-flow/#powerful-transformations","content":" With Flow, you can build pipelines that join a current event with an event that happened days, weeks, even years in the past. Flow can model arbitrary stream-to-stream joins without the windowing constraints imposed by other systems, which limit how far back in time you can join.  Flow transforms data in durable micro-transactions, meaning that an outcome, once committed, won't be silently re-ordered or changed due to a crash or machine failure. This makes Flow uniquely suited for operational workflows, like assigning a dynamic amount of available inventory to a stream of requests — decisions that, once made, should not be forgotten. You can also evolve transformations as business requirements change, enriching them with new datasets or behaviors without needing to re-compute from scratch.  ","version":"Next","tagName":"h3"},{"title":"Data integrity​","type":1,"pageTitle":"Who should use Flow?","url":"/getting-started/who-should-use-flow/#data-integrity","content":" Flow is architected to ensure that your data is accurate and that changes don't break pipelines. It supports strong schematization, durable transactions with exactly-once semantics, and easy end-to-end testing.  Required JSON schemas ensure that only clean, consistent data is ingested into Flow or written to external systems. If a document violates its schema, Flow pauses the pipeline, giving you a chance to fix the error.Schemas can encode constraints, like that a latitude value must be between +90 and -90 degrees, or that a field must be a valid email address.Flow can project JSON schema into other flavors, like TypeScript types or SQL tables. Strong type checking catches bugs before they're applied to production.Flow's declarative tests verify the integrated, end-to-end behavior of data flows.  ","version":"Next","tagName":"h3"},{"title":"Dynamic scaling​","type":1,"pageTitle":"Who should use Flow?","url":"/getting-started/who-should-use-flow/#dynamic-scaling","content":" The Flow runtime scales from a single process up to a large Kubernetes cluster for high-volume production deployments. Processing tasks are quickly reassigned upon any machine failure for high availability.  Each process can also be scaled independently, at any time, and without downtime. This is unique to Flow. Comparable systems require that an arbitrary data partitioning be decided upfront, a crucial performance knob that's awkward and expensive to change. Instead, Flow can repeatedly split a running task into two new tasks, each half the size, without stopping it or impacting its downstream uses. ","version":"Next","tagName":"h3"},{"title":"Flow user guides","type":0,"sectionRef":"#","url":"/guides/","content":"Flow user guides In this section, you'll find step-by-step guides that walk you through common Flow tasks. These guides are designed to help you work with Data Flows in production — we assume you have your own data and are familiar with your source and destination systems. You might be here to get your data moving with Flow as quickly as possible, reshape your collection with a derivation, or create a secure connection to your database. If you'd prefer a tailored learning experience with sample data, check out the Flow tutorials.","keywords":"","version":"Next"},{"title":"Real-time CDC with MongoDB","type":0,"sectionRef":"#","url":"/getting-started/tutorials/real_time_cdc_with_mongodb/","content":"","keywords":"","version":"Next"},{"title":"Video tutorial​","type":1,"pageTitle":"Real-time CDC with MongoDB","url":"/getting-started/tutorials/real_time_cdc_with_mongodb/#video-tutorial","content":"   ","version":"Next","tagName":"h2"},{"title":"What is CDC?​","type":1,"pageTitle":"Real-time CDC with MongoDB","url":"/getting-started/tutorials/real_time_cdc_with_mongodb/#what-is-cdc","content":" CDC, or Change Data Capture, is a method used to track and capture changes made to data in a database. It enables the real-time capture of insertions, updates, and deletions, providing a continuous stream of changes.  This stream of data is invaluable for keeping downstream systems synchronized and up-to-date with the source database, facilitating real-time analytics, replication, and data integration. In essence, CDC allows organizations to capture and react to data changes as they occur, ensuring data accuracy and timeliness across their systems.  Optionally, if you are interested in the intricacies of change data capture, head over to this article, where we explain the theory behind it - this is not a requirement for this tutorial, so if you want to dive in head first, keep on reading!  ","version":"Next","tagName":"h2"},{"title":"Understanding Change Events in MongoDB​","type":1,"pageTitle":"Real-time CDC with MongoDB","url":"/getting-started/tutorials/real_time_cdc_with_mongodb/#understanding-change-events-in-mongodb","content":" Change events in MongoDB are notifications triggered by modifications to the database's data, configuration, or structure through a mechanism called change streams.  Change Streams in MongoDB are a convenient way for CDC, enabling real-time monitoring of data changes without having to directly interact with the underlying oplog. CDC processes leverage Change Streams to subscribe to data changes, capturing detailed events promptly as insertions, updates, or deletions occur.  This approach ensures efficient data propagation to downstream systems, scalability for high-throughput environments, and robust error handling for operations.  MongoDB supports various types of change events, each catering to different aspects of database operations. For data synchronization, the following three events are the only ones that matter:  Insert Events: Triggered when new documents are inserted into a collection.    Update Events: Fired upon modifications to existing documents, including field updates and replacements.    Delete Events: Signaled when documents are removed from a collection.  note In MongoDB, if you delete a key from a document, the corresponding change event that gets fired is an &quot;update&quot; event. This may seem counterintuitive at first, but in MongoDB, updates are atomic operations that can modify specific fields within a document, including removing keys. So, when a key is deleted from a document, MongoDB interprets it as an update operation where the specific field (i.e., the key) is being removed, resulting in an &quot;update&quot; event being generated in the oplog.    ","version":"Next","tagName":"h2"},{"title":"Introduction to Estuary Flow​","type":1,"pageTitle":"Real-time CDC with MongoDB","url":"/getting-started/tutorials/real_time_cdc_with_mongodb/#introduction-to-estuary-flow","content":" Estuary is the best tool for integrating CDC streams from MongoDB. Here are a few reasons why:  Unlike ELT vendors, Estuary does streaming CDC and incremental snapshots, not full snapshots or batch change data extraction. This puts less of a load on the source database and lowers latency. Estuary also extracts exactly-and-only-once. As it streams snapshots and changes data it also stores it as a collection for later reuse, such as for backfilling or stream replay. ELT vendors require you to re-extract every time to want to resend the data or restart a change data stream. Estuary supports both real-time and batch. Its streaming latency is within 100ms, and it always extracts in real-time. But it supports any latency from real-time to hours or days for destinations. Estuary guarantees change data is delivered exactly once. Estuary can also maintain type I and type II slowly changing dimensions directly into a target. Estuary streaming has 50MB/sec per connection per table throughput rates, which is 5-10x any other benchmarks for CDC. It also scales horizontally to increase throughput.  Time to build a real-time CDC pipeline!  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Real-time CDC with MongoDB","url":"/getting-started/tutorials/real_time_cdc_with_mongodb/#prerequisites","content":" To follow along with the tutorial, you’ll need the following:  An Estuary Flow account. If you haven’t yet, sign up for free here. A fully-managed MongoDB Capture connector is ready for you to get started. A MongoDB Atlas cluster: This tutorial uses Atlas as the source database, but Estuary supports other types of MongoDB deployments as well.  ","version":"Next","tagName":"h2"},{"title":"Setting up MongoDB​","type":1,"pageTitle":"Real-time CDC with MongoDB","url":"/getting-started/tutorials/real_time_cdc_with_mongodb/#setting-up-mongodb","content":" To prepare MongoDB for Estuary Flow, you need to ensure the following prerequisites are met:  ","version":"Next","tagName":"h2"},{"title":"Credentials​","type":1,"pageTitle":"Real-time CDC with MongoDB","url":"/getting-started/tutorials/real_time_cdc_with_mongodb/#credentials","content":" Obtain the necessary credentials for connecting to your MongoDB instance and database. This includes credentials for authentication purposes, typically a username and password.  ","version":"Next","tagName":"h3"},{"title":"Read Access​","type":1,"pageTitle":"Real-time CDC with MongoDB","url":"/getting-started/tutorials/real_time_cdc_with_mongodb/#read-access","content":" Ensure that you have read access to the MongoDB database(s) from which you intend to capture data. MongoDB utilizes Role-Based Access Control (RBAC), so make sure your user account has the appropriate permissions to read data.    In MongoDB Atlas, any of the built-in Roles will work for the tutorial, but Flow needs at least read permissions over the data you wish to capture if you wish to set up more granular, restricted permissions.  ","version":"Next","tagName":"h3"},{"title":"Configuration Considerations​","type":1,"pageTitle":"Real-time CDC with MongoDB","url":"/getting-started/tutorials/real_time_cdc_with_mongodb/#configuration-considerations","content":" If you haven't already, make sure you deploy a Replica Set-type MongoDB cluster. Change streams require a replica set in order to work. A replica set is a group of MongoDB deployments that maintain the same data set. If you are working following along with a fresh MongoDB Atlas project, you shouldn’t need to configure anything manually for this, as the default free-tier instance is a cluster of 3 replicas. To learn more about replica sets, see the Replication Introduction in the MongoDB manual. Ensure that Estuary's IP addresses are allowlisted to allow access. We’ll show you how to do this in the next section.  ","version":"Next","tagName":"h3"},{"title":"Configure MongoDB​","type":1,"pageTitle":"Real-time CDC with MongoDB","url":"/getting-started/tutorials/real_time_cdc_with_mongodb/#configure-mongodb","content":" Let’s start by provisioning our database. As you can see, for this tutorial, you can just deploy an M0-category cluster, which is free to use.    After the cluster has finished provisioning, we’ll need to make sure that Estuary Flow is able to connect to the database. For this, the only requirement with MongoDB Atlas is allowlisting the Estuary Flow IP addresses.  Navigate to the “Network Access” page using the left hand sidebar, and using the “Add new IP address” button, create the list entry which enables the communication between the two services.    Next, find your connection string by navigating to the mongosh setup page by clicking the “Connect” button on the database overview section, then choosing the “Shell” option.  note You’re not going to set up mongosh for this tutorial, but this is the easiest way to get ahold of the connection string we’ll be using.    Copy the connection string and head over to your Estuary Flow dashboard to continue the tutorial.  ","version":"Next","tagName":"h3"},{"title":"Setting up Estuary Flow​","type":1,"pageTitle":"Real-time CDC with MongoDB","url":"/getting-started/tutorials/real_time_cdc_with_mongodb/#setting-up-estuary-flow","content":" On the dashboard, create a new capture by navigating to the “Sources” menu using the sidebar, then pressing the “New Capture” button. In the list of available connectors, search for “MongoDB”, then press “Capture”.    Pressing this button will bring you to the connector configuration page, where you’ll be able to provision your fully managed real-time Data Flow.  ","version":"Next","tagName":"h2"},{"title":"MongoDB Capture Configuration​","type":1,"pageTitle":"Real-time CDC with MongoDB","url":"/getting-started/tutorials/real_time_cdc_with_mongodb/#mongodb-capture-configuration","content":" As a first step, in the Capture Details section, name your capture and optionally add a description for it.    Once you are finished, step down to the second configuration section, called “Endpoint Config”. Here you have to use the address for the database you took a note of in the MongoDB setup section, along with your username and password which you configured when setting up MongoDB Atlas.  If your user has access to all databases, ensure that in your MongoDB address, you specify the ?authSource=admin parameter in the connection address. In this case, authentication is performed through your admin database.    As for the &quot;Database&quot; option, feel free to leave it empty, that way the automated discovery process of Flow will make sure every available database is ready for data extraction.  After you press the blue “Next” button in the top right corner, Flow will automatically crawl through the connection to discover available resources. Next up, you’ll see the third, and final configuration section, where you are able to view and choose from all the databases and collections which are discovered by Flow.    ","version":"Next","tagName":"h3"},{"title":"Documents and Collections​","type":1,"pageTitle":"Real-time CDC with MongoDB","url":"/getting-started/tutorials/real_time_cdc_with_mongodb/#documents-and-collections","content":" Before we initialize the connector, let’s talk a little bit about how incoming data is represented in Flow.  The documents of your flows are stored in collections: real-time data lakes of JSON documents in cloud storage.  note Keep in mind, these are not the same documents and collections as the ones in MongoDB, only the names are similar, but we are talking about separate systems.  Collections being stored in an object storage mean that once you start capturing data, you won’t have to worry about it not being available to replay – object stores such as S3 can be configured to cheaply store data forever. See docs page for more information about documents.  To see how Flow parsed the incoming records, click on the “Collection” tab on the UI.    When you set up a capture from MongoDB using the Flow web app, the underlying collection specifications will look something like this:  key: [ /_id ] writeSchema: type: object properties: _id: { type: string } required: [ _id ] readSchema: allOf: - $ref: flow://write-schema - $ref: flow://inferred-schema   This specification uses separate read and write schemas. The writeSchema is extremely permissive, and only requires an _id property with a string value. The readSchema references flow://inferred-schema, which expands to the current inferred schema when the collection is published.  MongoDB documents have a mandatory _id field that is used as the key of the collection. But that is essentially the only requirement. You can't know what other fields may exist on MongoDB documents until you've read them. On the UI, for this reason, only three fields are visible initially in the collection schema tab.  ","version":"Next","tagName":"h3"},{"title":"Automating schema evolution​","type":1,"pageTitle":"Real-time CDC with MongoDB","url":"/getting-started/tutorials/real_time_cdc_with_mongodb/#automating-schema-evolution","content":" In addition to selecting the collections for capture, this interface provides access to three settings that govern schema evolution. In a NoSQL database environment like MongoDB, schema alterations are frequent occurrences. Manually synchronizing source and destination schemas can end up being a lot of maintenance. To help with this, Estuary introduces a more sophisticated schema evolution strategy.  With Estuary Flow, teams can opt to suspend the Data Flow using data contracts, automate the update of the target schema with the new MongoDB schema, or create a new table in the destination to maintain separation between old and new schemas. Details can be found in our schema evolution docs.  Schema evolutions serve to prevent errors stemming from discrepancies between specifications in a number of ways:  Materializations will automatically apply backward-compatible schema changes, like adding a new column. This doesn't require re-backfilling the target tables or re-creating the Flow collection. For more complex scenarios, the evolution adjusts the affected materialization bindings to increment their backfill counter, prompting the materialization process to reconstruct the resource (such as a database table) and backfill it from the offset. In instances where necessary, such as when the collection key or logical partitioning undergoes changes, the evolution generates a completely new collection with a numerical suffix (e.g., _v2). This new collection initializes as empty and undergoes backfilling from the source. Moreover, the evolution updates all captures and materializations referencing the old collection to point to the new collection, incrementing their backfill counters accordingly. This method is more intricate and is only invoked when essential alterations are identified.  In these scenarios, the names of destination resources remain unaltered. For instance, a materialization to Postgres would drop and re-establish the affected tables with their original names.  ","version":"Next","tagName":"h3"},{"title":"Publishing the Capture​","type":1,"pageTitle":"Real-time CDC with MongoDB","url":"/getting-started/tutorials/real_time_cdc_with_mongodb/#publishing-the-capture","content":" To finalize the connector configuration and kick it off, press the “Save and Publish” button. Flow will test, save and publish your capture. You’ll see a similar screen if everything went well or if there were any issues setting up the connector, you’ll see detailed error messages instead.    During the initial database snapshot – which is triggered automatically – the connector seamlessly captures change events by continuously monitoring change streams while concurrently executing collection scans to backfill pre-existing documents. Following the initial snapshot, the connector perpetually monitors change streams to capture all subsequent changes in real-time.  Incremental backfills in the MongoDB connector follow a straightforward approach to ensure comprehensive data capture with minimal latency. Initially, all tracked change streams are synchronized to maintain data integrity. Then, a targeted backfilling effort occurs across MongoDB collections until all are up to date. Continuous monitoring via change streams ensures ongoing data capture and means there is no need to pause replication during an ongoing backfill.  In the event of a pause in the connector's process, it resumes capturing change events from the point of interruption. However, the connector's ability to accomplish this depends on the size of the replica set oplog. In certain scenarios where the pause duration is significant enough for the oplog to purge old change events, the connector may necessitate redoing the backfill to maintain data consistency.  tip To ensure reliable data capture, it is recommended to adjust the oplog size or set a minimum retention period. A recommended minimum retention period of at least 24 hours is sufficient for most cases.  ","version":"Next","tagName":"h3"},{"title":"Real-time CDC​","type":1,"pageTitle":"Real-time CDC with MongoDB","url":"/getting-started/tutorials/real_time_cdc_with_mongodb/#real-time-cdc","content":" Let’s head over to the collections page to see our arriving documents.    Looks like all of the selected MongoDB collections have been fully replicated into Flow by the initial backfill.  Let’s take a look at the movies collection to see what details Flow can tell us about the documents. You can see some statistics about the integration throughput and you can also take a look at the actual documents in a preview window.    You can also check out the generated specification, which is the Flow’s behind-the-scenes declarative way of representing the Collection resource.  For the movies collection, this is what it looks like:  { &quot;writeSchema&quot;: { &quot;type&quot;: &quot;object&quot;, &quot;required&quot;: [&quot;_id&quot;], &quot;properties&quot;: { &quot;_id&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;_meta&quot;: { &quot;$schema&quot;: &quot;http://json-schema.org/draft/2020-12/schema&quot;, &quot;properties&quot;: { &quot;op&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;c&quot;, &quot;u&quot;, &quot;d&quot;], &quot;title&quot;: &quot;Change Operation&quot;, &quot;description&quot;: &quot;Change operation type: 'c' Create/Insert 'u' Update 'd' Delete.&quot; } }, &quot;type&quot;: &quot;object&quot; } }, &quot;x-infer-schema&quot;: true }, &quot;readSchema&quot;: { &quot;allOf&quot;: [ { &quot;$ref&quot;: &quot;flow://write-schema&quot; }, { &quot;$ref&quot;: &quot;flow://inferred-schema&quot; } ] }, &quot;key&quot;: [&quot;/_id&quot;] }   You can see the flexible readSchema configuration in action we mentioned above.  You can use the preview window on the collections “Overview” page to quickly test how change events propagate from MongoDB. Head over to the MongoDB Atlas UI and insert a new document into the movies collection.    Here’s a sample JSON (describing non-existent but very intriguing movie) you can copy paste into the pop-up modal to spare you the trouble.  { &quot;title&quot;: &quot;Dataflow&quot;, &quot;fullplot&quot;: &quot;In a near-future world driven by data, a team of maverick engineers and programmers set out to revolutionize the way information is processed and transmitted. As they delve deeper into the complexities of real-time data streaming, they uncover dark secrets and face moral dilemmas that threaten to unravel their ambitious project.&quot;, &quot;plot&quot;: &quot;A team of brilliant engineers embark on a groundbreaking project to develop a real-time data streaming platform, but they soon discover unexpected challenges and threats lurking in the digital realm.&quot;, &quot;genres&quot;: [&quot;Drama&quot;, &quot;Sci-Fi&quot;, &quot;Thriller&quot;], &quot;runtime&quot;: 135, &quot;cast&quot;: [ &quot;Emily Blunt&quot;, &quot;Michael B. Jordan&quot;, &quot;Idris Elba&quot;, &quot;Zendaya&quot;, &quot;Oscar Isaac&quot; ], &quot;poster&quot;: &quot;https://example.com/posters/real-time-data-streaming.jpg&quot;, &quot;languages&quot;: [&quot;English&quot;], &quot;released&quot;: 1739808000000, &quot;directors&quot;: [&quot;Christopher Nolan&quot;], &quot;rated&quot;: &quot;PG-13&quot;, &quot;awards&quot;: { &quot;wins&quot;: 3, &quot;nominations&quot;: 8, &quot;text&quot;: &quot;3 wins, 8 nominations&quot; }, &quot;lastupdated&quot;: &quot;2024-04-30 10:15:00.000000&quot;, &quot;year&quot;: 2024, &quot;imdb&quot;: { &quot;rating&quot;: 8.5, &quot;votes&quot;: 15234, &quot;id&quot;: 1001 }, &quot;countries&quot;: [&quot;USA&quot;, &quot;United Kingdom&quot;], &quot;type&quot;: &quot;movie&quot;, &quot;tomatoes&quot;: { &quot;viewer&quot;: { &quot;rating&quot;: 4.2, &quot;numReviews&quot;: 3856, &quot;meter&quot;: 82 }, &quot;fresh&quot;: 34, &quot;critic&quot;: { &quot;rating&quot;: 8.0, &quot;numReviews&quot;: 22, &quot;meter&quot;: 91 }, &quot;rotten&quot;: 2, &quot;lastUpdated&quot;: 1739894400000 }, &quot;num_mflix_comments&quot;: 120 }   After you insert the document, check out the collection preview on the Flow UI to verify it has indeed arrived. The process for updating and deleting collections in MongoDB works similarly.    ","version":"Next","tagName":"h2"},{"title":"Wrapping up​","type":1,"pageTitle":"Real-time CDC with MongoDB","url":"/getting-started/tutorials/real_time_cdc_with_mongodb/#wrapping-up","content":" In this tutorial, you set up a MongoDB Change Data Capture (CDC) integration using Estuary Flow. Throughout the process, you learned about the technical nuances of capturing and synchronizing data changes from MongoDB collections in real-time.  Key takeaways from this tutorial:  MongoDB's document-based approach and schema flexibility present unique challenges and opportunities for Change Data Capture workflows. You explored the prerequisites and configuration settings required to establish a seamless connection between MongoDB and Estuary, ensuring efficient data capture and synchronization. Estuary's schema evolution capabilities enable data teams to manage schema changes effectively, ensuring data consistency and integrity across source and destination systems. You learned how Flow continuously monitors MongoDB change streams and executes backfilling processes to capture changes accurately, even in the event of interruptions or schema alterations.  ","version":"Next","tagName":"h2"},{"title":"Next Steps​","type":1,"pageTitle":"Real-time CDC with MongoDB","url":"/getting-started/tutorials/real_time_cdc_with_mongodb/#next-steps","content":" That’s it! You should have everything you need to know to create your own data pipeline for capturing change events from MongoDB!  Now try it out on your own CloudSQL database or other sources.  If you want to learn more, make sure you read through the Estuary documentation.  You’ll find instructions on how to use other connectors here. There are more tutorials here.  Also, don’t forget to join the Estuary Slack Community! ","version":"Next","tagName":"h2"},{"title":"Configure connections with SSH tunneling","type":0,"sectionRef":"#","url":"/guides/connect-network/","content":"","keywords":"","version":"Next"},{"title":"General setup​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"/guides/connect-network/#general-setup","content":" Activate an SSH implementation on a server, if you don't have one already. Consult the documentation for your server's operating system and/or cloud service provider, as the steps will vary. Configure the server to your organization's standards, or reference the SSH documentation for basic configuration options. Referencing the config files and shell output, collect the following information:  The SSH user, which will be used to log into the SSH server, for example, sshuser. You may choose to create a new user for this workflow.The SSH endpoint for the SSH server, formatted as ssh://user@hostname[:port]. This may look like the any of following: ssh://sshuser@ec2-198-21-98-1.compute-1.amazonaws.com ssh://sshuser@198.21.98.1 ssh://sshuser@198.21.98.1:22 Hint The SSH default port is 22. Depending on where your server is hosted, you may not be required to specify a port, but we recommend specifying :22 in all cases to ensure a connection can be made.  In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key Taken together, these configuration details would allow you to log into the SSH server from your local machine. They'll allow the connector to do the same. Configure your internal network to allow the SSH server to access your capture or materialization endpoint. To grant external access to the SSH server, it's essential to configure your network settings accordingly. The approach you take will be dictated by your organization's IT policies. One recommended step is to allowlist the Estuary IP addresses. This ensures that connections from this specific IP are permitted through your network's firewall or security measures.  ","version":"Next","tagName":"h2"},{"title":"Setup for AWS​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"/guides/connect-network/#setup-for-aws","content":" To allow SSH tunneling to a database instance hosted on AWS, you'll need to create a virtual computing environment, or instance, in Amazon EC2.  Begin by finding your public SSH key on your local machine. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key Import your SSH key into AWS. Launch a new instance in EC2. During setup: Configure the security group to allow SSH connection from anywhere.When selecting a key pair, choose the key you just imported. Connect to the instance, setting the user name to ec2-user. Find and note the instance's public DNS. This will be formatted like: ec2-198-21-98-1.compute-1.amazonaws.com.  ","version":"Next","tagName":"h2"},{"title":"Setup for Google Cloud​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"/guides/connect-network/#setup-for-google-cloud","content":" To allow SSH tunneling to a database instance hosted on Google Cloud, you must set up a virtual machine (VM).  Begin by finding your public SSH key on your local machine. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key If your Google login differs from your local username, generate a key that includes your Google email address as a comment: ssh-keygen -m PEM -t rsa -C user@domain.com Create and start a new VM in GCP, choosing an image that supports OS Login. Add your public key to the VM. Reserve an external IP address and connect it to the VM during setup. Note the generated address.  ","version":"Next","tagName":"h2"},{"title":"Setup for Azure​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"/guides/connect-network/#setup-for-azure","content":" To allow SSH tunneling to a database instance hosted on Azure, you'll need to create a virtual machine (VM) in the same virtual network as your endpoint database.  Begin by finding your public SSH key on your local machine. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key Create and connect to a VM in a virtual network, and add the endpoint database to the network. Create a new virtual network and subnet. Create a Linux or Windows VM within the virtual network, directing the SSH public key source to the public key you generated previously. Note the VM's public IP; you'll need this later. Create a service endpoint for your database in the same virtual network as your VM. Instructions for Azure Database For PostgreSQL can be found here; note that instructions for other database engines may be different.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"/guides/connect-network/#configuration","content":" After you've completed the prerequisites, you should have the following parameters:  SSH Endpoint / sshEndpoint: the remote SSH server's hostname, or public IP address, formatted as ssh://user@hostname[:port] The SSH default port is 22. Depending on where your server is hosted, you may not be required to specify a port, but we recommend specifying :22 in all cases to ensure a connection can be made. Private Key / privateKey: the contents of the SSH private key file  Use these to add SSH tunneling to your capture or materialization definition, either by filling in the corresponding fields in the web app, or by working with the YAML directly. Reference the Connectors page for a YAML sample. ","version":"Next","tagName":"h2"},{"title":"Create a basic Data Flow","type":0,"sectionRef":"#","url":"/guides/create-dataflow/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Create a basic Data Flow","url":"/guides/create-dataflow/#prerequisites","content":" This guide is intended for new Flow users and briefly introduces Flow's key concepts. Though it's not required, you may find it helpful to read the high level concepts documentation for more detail before you begin.  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Create a basic Data Flow","url":"/guides/create-dataflow/#introduction","content":" In Estuary Flow, you create Data Flows to connect data source and destination systems.  The simplest Data Flow comprises three types of entities:  A data capture, which ingests data from an external sourceOne or more collections, which store that data in a cloud-backed data lakeA materialization, to push the data to an external destination  The capture and materialization each rely on a connector. A connector is a plug-in component that interfaces between Flow and whatever data system you need to connect to. Here, we'll walk through how to leverage various connectors, configure them, and deploy your Data Flow.  ","version":"Next","tagName":"h2"},{"title":"Create a capture​","type":1,"pageTitle":"Create a basic Data Flow","url":"/guides/create-dataflow/#create-a-capture","content":" You'll first create a capture to connect to your data source system. This process will create one or more collections in Flow, which you can then materialize to another system.  Go to the Flow web application at dashboard.estuary.dev and sign in using the credentials provided by your Estuary account manager. Click the Sources tab and choose New Capture. Choose the appropriate Connector for your desired data source. A form appears with the properties required for that connector. A documentation page with details about that connector appears in the side panel. You can also browse the connectors reference in your browser. Type a name for your capture. Your capture name must begin with a prefix to which you have access. In the Name field, click the drop-down arrow and select an available prefix. Append a unique capture name after the / to create the full name, for example acmeCo/myFirstCapture. Fill out the required properties and click Next. Flow uses the provided information to initiate a connection to the source system. It identifies one or more data resources — these may be tables, data streams, or something else, depending on the connector. These are each mapped to a collection. The Output Collections browser appears, showing this list of available collections. You can decide which ones you want to capture. Look over the list of available collections. All are selected by default. You can remove collections you don't want to capture, change collection names, and for some connectors, modify other properties.  tip Narrow down a large list of available collections by typing in the Search Bindings box.  If you're unsure which collections you want to keep or remove, you can look at their schemas.  In the Output Collections browser, select a collection and click the Collection tab to view its schema and collection key. For many source systems, you'll notice that the collection schemas are quite permissive. You'll have the option to apply more restrictive schemas later, when you materialize the collections. If you made any changes to output collections, click Next again. Once you're satisfied with the configuration, click Save and Publish. You'll see a notification when the capture publishes successfully. Click Materialize collections to continue.  ","version":"Next","tagName":"h2"},{"title":"Create a materialization​","type":1,"pageTitle":"Create a basic Data Flow","url":"/guides/create-dataflow/#create-a-materialization","content":" Now that you've captured data into one or more collections, you can materialize it to a destination.  Find the tile for your desired data destination and click Materialization. The page populates with the properties required for that connector. More details on each connector are provided in the connectors reference. Choose a unique name for your materialization like you did when naming your capture; for example, acmeCo/myFirstMaterialization. Fill out the required properties in the Endpoint Configuration. Click Next. Flow initiates a connection with the destination system. The Endpoint Config has collapsed and the Source Collections browser is now prominent. It shows each collection you captured previously. All of them will be mapped to a resource in the destination. Again, these may be tables, data streams, or something else. When you publish the Data Flow, Flow will create these new resources in the destination. Now's your chance to make changes to the collections before you materialize them. Optionally remove some collections or add additional collections. Type in the Search Collections box to find a collection. To remove a collection, click the x in its table row. You can also click the Remove All button. Optionally apply a stricter schema to each collection to use for the materialization. Depending on the data source, you may have captured data with a fairly permissive schema. You can tighten up the schema so it'll materialize to your destination in the correct shape. (This isn't necessary for database and SaaS data sources, so the option won't be available.) Choose a collection from the list and click its Collection tab. Click Schema Inference. The Schema Inference window appears. Flow scans the data in your collection and infers a new schema, called the readSchema, to use for the materialization. Review the new schema and click Apply Inferred Schema. You can exert even more control over the output data structure using the Field Selector on the Config tab.Learn how. If you've made any changes to source fields, click Next again. Click Save and publish. You'll see a notification when the full Data Flow publishes successfully.  ","version":"Next","tagName":"h2"},{"title":"What's next?​","type":1,"pageTitle":"Create a basic Data Flow","url":"/guides/create-dataflow/#whats-next","content":" Now that you've deployed your first Data Flow, you can explore more possibilities.  Read the high level concepts to better understand how Flow works and what's possible. Create more complex Data Flows by mixing and matching collections in your captures and materializations. For example: Materialize the same collection to multiple destinations. If a capture produces multiple collections, materialize each one to a different destination. Materialize collections that came from different sources to the same destination. Advanced users can modify collection schemas, apply data reductions, or transform data with a derivation ","version":"Next","tagName":"h2"},{"title":"PostgreSQL CDC streaming to Snowflake","type":0,"sectionRef":"#","url":"/getting-started/tutorials/postgresql_cdc_to_snowflake/","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"PostgreSQL CDC streaming to Snowflake","url":"/getting-started/tutorials/postgresql_cdc_to_snowflake/#introduction","content":" In this tutorial, we'll set up a streaming CDC pipeline from PostgreSQL to Snowflake using Estuary Flow. By the end, you’ll have learned everything you need to know about building a pipeline on your own.  You'll use Flow's PostgreSQL capture connector and Snowflake materialization connector to set up an end-to-end CDC pipeline in three steps:  First, you’ll ingest change event data from a PostgreSQL database, using a table filled with generated realistic product data. Then, you’ll learn how to configure Flow to persist data as collections while maintaining data integrity. And finally, you will see how you can materialize these collections in Snowflake to make them ready for analytics!  By the end of this tutorial, you'll have established a robust and efficient data pipeline with near real-time replication of data from PostgreSQL to Snowflake.  Before you get started, make sure you do two things.  Sign up for Estuary Flow here. It’s simple, fast and free. Make sure you also join the Estuary Slack Community. Don’t struggle. Just ask a question.  ","version":"Next","tagName":"h2"},{"title":"What is CDC?​","type":1,"pageTitle":"PostgreSQL CDC streaming to Snowflake","url":"/getting-started/tutorials/postgresql_cdc_to_snowflake/#what-is-cdc","content":" CDC, or Change Data Capture, is a method used to track and capture changes made to data in a database. It enables the real-time capture of insertions, updates, and deletions, providing a continuous stream of changes.  This stream of change data is invaluable for keeping downstream systems synchronized and up-to-date with the source database, facilitating real-time analytics, replication, and data integration. In essence, CDC allows organizations to capture and react to data changes as they occur, ensuring data accuracy and timeliness across their systems. CDC provides a lower-latency, lower-load way to extract data. It’s also often the only way to capture every change as well as deletes, which are harder to track with batch-based extraction.  If you are interested in the intricacies of change data capture, head over to this article, where we explain the theory behind it - this is not a requirement for this tutorial, so if you want to dive in head first, keep on reading!  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"PostgreSQL CDC streaming to Snowflake","url":"/getting-started/tutorials/postgresql_cdc_to_snowflake/#prerequisites","content":" This tutorial will assume you have access to the following things:  Docker: for convenience, we are providing a docker compose definition which will allow you to spin up a database and a fake data generator service in about 5 seconds! ngrok: Flow is a fully managed service. Because the database used in this tutorial will be running on your machine, you’ll need something to expose it to the internet. ngrok is a lightweight tool that does just that. Snowflake account: The target data warehouse for our flow is Snowflake. In order to follow along with the tutorial, a trial account is perfectly fine.  ","version":"Next","tagName":"h2"},{"title":"Step 1. Set up source database​","type":1,"pageTitle":"PostgreSQL CDC streaming to Snowflake","url":"/getting-started/tutorials/postgresql_cdc_to_snowflake/#step-1-set-up-source-database","content":" ","version":"Next","tagName":"h2"},{"title":"PostgreSQL setup​","type":1,"pageTitle":"PostgreSQL CDC streaming to Snowflake","url":"/getting-started/tutorials/postgresql_cdc_to_snowflake/#postgresql-setup","content":" As this tutorial is focused on CDC replication from PostgreSQL, we’ll need a database. We recommend you create this database first, so you can learn Flow more easily. Then try these steps on your own database. Let’s take a look at what we are working with!  Save the below yaml snippet as a file called docker-compose.yml. This docker-compose.yml file contains the service definitions for the PostgreSQL database and the mock data generator service.  tip Since V2, compose is integrated into your base Docker package, there’s no need to download any separate tooling!  docker-compose.yml services: postgres: image: postgres:latest container_name: postgres_cdc hostname: postgres_cdc restart: unless-stopped user: postgres environment: POSTGRES_USER: postgres POSTGRES_DB: postgres POSTGRES_PASSWORD: postgres command: - &quot;postgres&quot; - &quot;-c&quot; - &quot;wal_level=logical&quot; healthcheck: test: [&quot;CMD-SHELL&quot;, &quot;sh -c 'pg_isready -U flow_capture -d postgres'&quot;] interval: 5s timeout: 10s retries: 120 volumes: - ./init.sql:/docker-entrypoint-initdb.d/init.sql ports: - &quot;5432:5432&quot; datagen: image: materialize/datagen container_name: datagen restart: unless-stopped environment: POSTGRES_HOST: postgres_cdc POSTGRES_PORT: 5432 POSTGRES_DB: postgres POSTGRES_USER: postgres POSTGRES_PASSWORD: postgres entrypoint: &quot;datagen -s /app/schemas/products.sql -n 10000 -f postgres -w 1000&quot; depends_on: postgres: condition: service_healthy volumes: - ./schemas/products.sql:/app/schemas/products.sql   Don’t be alarmed by all these Docker configurations, they are made to be reproducible on any machine, so you don’t have to worry about modifying anything in them! Before you spin up the database, let’s take a quick look at what exactly you can expect to happen.  Next up, create a folder called schemas and paste the below SQL DDL into a file called products.sql. This file contains the schema of the demo data.  note This file defines the schema via a create table statement, but the actual table creation happens in the init.sql file, this is just a quirk of the Datagen data generator tool.  products.sql CREATE TABLE &quot;public&quot;.&quot;products&quot; ( &quot;id&quot; int PRIMARY KEY, &quot;name&quot; varchar COMMENT 'faker.internet.userName()', &quot;merchant_id&quot; int NOT NULL COMMENT 'faker.datatype.number()', &quot;price&quot; int COMMENT 'faker.datatype.number()', &quot;status&quot; varchar COMMENT 'faker.datatype.boolean()', &quot;created_at&quot; timestamp DEFAULT (now()) );   If you take a closer look at the schema definition, you can see that in the COMMENT attribute we define Python snippets which actually tell Datagen how to generate fake data for those fields!  Finally, create the init.sql file, which contains the database-level requirements to enable Flow to stream CDC data.  init.sql CREATE USER flow_capture WITH PASSWORD 'secret' REPLICATION; GRANT pg_read_all_data TO flow_capture; CREATE TABLE products ( &quot;id&quot; int PRIMARY KEY, &quot;name&quot; varchar COMMENT 'faker.internet.userName()', &quot;merchant_id&quot; int NOT NULL COMMENT 'faker.datatype.number()', &quot;price&quot; int COMMENT 'faker.datatype.number()', &quot;status&quot; varchar COMMENT 'faker.datatype.boolean()', &quot;created_at&quot; timestamp DEFAULT (now()) ); CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication; ALTER PUBLICATION flow_publication SET (publish_via_partition_root = true); ALTER PUBLICATION flow_publication ADD TABLE public.flow_watermarks, public.products;   In the init.sql file, you create the products table and all the database objects Flow requires for real-time CDC streaming.  ","version":"Next","tagName":"h3"},{"title":"Configuring PostgreSQL for CDC​","type":1,"pageTitle":"PostgreSQL CDC streaming to Snowflake","url":"/getting-started/tutorials/postgresql_cdc_to_snowflake/#configuring-postgresql-for-cdc","content":" To enable CDC replication in PostgreSQL, several database objects need to be created and configured. These objects facilitate the capture and propagation of data changes to downstream systems. Let's examine each object and its significance in the context of CDC replication:  CREATE USER flow_capture WITH PASSWORD 'secret' REPLICATION;   This user is dedicated to the CDC replication process. It is granted the necessary permissions to read all data from the database, allowing it to capture changes across tables efficiently. In a production environment, make sure you use a more secure password than what is in the example.  GRANT pg_read_all_data TO flow_capture;   Granting the pg_read_all_data privilege to the flow_capture user ensures that it can access and read data from all tables in the database, essential for capturing changes.  note pg_read_all_data is used for convenience, but is not a hard requirement, since it is possible to grant a more granular set of permissions. For more details check out the connector docs.  CREATE TABLE products (...)   The source tables, such as the products table in this example, contain the data whose changes we want to capture and replicate. It is recommended for tables to have a primary key defined, although not a hard requirement for CDC.  CREATE TABLE IF NOT EXISTS public.flow_watermarks (...)   The flow_watermarks table is a small “scratch space” to which the connector occasionally writes a small amount of data to ensure accuracy when backfilling preexisting table contents.  GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture;   The flow_capture user needs full privileges on the flow_watermarks table to insert, update, and query metadata related to the replication process.  CREATE PUBLICATION flow_publication; ALTER PUBLICATION flow_publication SET (publish_via_partition_root = true); ALTER PUBLICATION flow_publication ADD TABLE public.flow_watermarks, public.products;   A publication defines a set of tables whose changes will be replicated. In this case, the flow_publication publication includes the public.flow_watermarks and public.products tables.  These commands configure the flow_publication publication to publish changes via partition root and add the specified tables to the publication. By setting publish_via_partition_root to true, the publication ensures that updates to partitioned tables are correctly captured and replicated.  note The table in this tutorial is not partitioned, but we recommend always setting publish_via_partition_root when creating a publication.  These objects form the backbone of a robust CDC replication setup, ensuring data consistency and integrity across systems. After the initial setup, you will not have to touch these objects in the future, unless you wish to start ingesting change events from a new table.  With that out of the way, you’re ready to start the source database. In order to initialize Postgres and the fake data generator service, all you have to do is execute the following (to free up your current terminal, use the -d flag so the containers run in a daemonized background process):  docker compose up   After a few seconds, you should see that both services are up and running. The postgres_cdc service should print the following on the terminal:  postgres_cdc | LOG: database system is ready to accept connections   While the datagen service will be a little bit more spammy, as it prints every record it generates, but don’t be alarmed, this is enough for us to verify that both are up and running. Let’s see how we can expose the database so Flow can connect to it.  ","version":"Next","tagName":"h3"},{"title":"Expose the database to the internet via ngrok​","type":1,"pageTitle":"PostgreSQL CDC streaming to Snowflake","url":"/getting-started/tutorials/postgresql_cdc_to_snowflake/#expose-the-database-to-the-internet-via-ngrok","content":" As mentioned above, the next step is to make the database available for other services. To do this in one quick command, we can use ngrok, a free CLI tool that enables tunneling of services. In our case we only want to expose the port 5432 and only the tcp protocol.  ngrok tcp 5432     You should immediately be greeted with a screen that contains the public URL for the tunnel we just started! In the example above, the public URL 5.tcp.eu.ngrok.io:14407 is mapped to localhost:5432, which is the address of the Postgres database.  note Don’t close this window while working on the tutorial as this is required to keep the connections between Flow and the database alive.  Before we jump into setting up the replication, you can quickly verify the data being properly generated by connecting to the database and peeking into the products table, as shown below:  ~ psql -h 5.tcp.eu.ngrok.io -p 14407 -U postgres -d postgres Password for user postgres: psql (16.2) Type &quot;help&quot; for help. postgres=# \\d List of relations Schema | Name | Type | Owner --------+-----------------+-------+---------- public | flow_watermarks | table | postgres public | products | table | postgres (2 rows) postgres=# select count(*) from products; count ------- 2637 (1 row) postgres=# select count(*) from products; count ------- 2642 (1 row)   By executing a count(*) statement a few seconds apart you are able to verify that data is in fact being written into the table.  ","version":"Next","tagName":"h3"},{"title":"Step 2. Set up a Capture​","type":1,"pageTitle":"PostgreSQL CDC streaming to Snowflake","url":"/getting-started/tutorials/postgresql_cdc_to_snowflake/#step-2-set-up-a-capture","content":" Good news, the hard part is over! Smooth sailing from here on out. Head over to your Flow dashboard (if you haven’t registered yet, you can do so here.) and create a new Capture. A capture is how Flow ingests data from an external source. Every Data Flow starts with a Capture.  Go to the sources page by clicking on the Sources on the left hand side of your screen, then click on + New Capture    Configure the connection to the database based on the information we gathered in the previous step and press Next.    On the following page, we can configure how our incoming data should be represented in Flow as collections. As a quick refresher, let’s recap how Flow represents data on a high level.  Documents  The documents of your flows are stored in collections: real-time data lakes of JSON documents in cloud storage. Documents being backed by an object storage mean that once you start capturing data, you won’t have to worry about it not being available to replay – object stores such as S3 can be configured to cheaply store data forever. See docs page for more information.  Schemas  Flow documents and collections always have an associated schema that defines the structure, representation, and constraints of your documents. In most cases, Flow generates a functioning schema on your behalf during the discovery phase of capture, which has already automatically happened - that’s why you’re able to take a peek into the structure of the incoming data!  To see how Flow parsed the incoming records, click on the Collection tab and verify the inferred schema looks correct.    Before you advance to the next step, let’s take a look at the other configuration options we have here. You’ll see three toggles, all turned on by default:  Automatically keep schemas up to date Automatically add new source collections Breaking changes re-version collections  All of these settings relate to how Flow handles schema evolution, so let’s take a quick detour to explain them from a high-level perspective.  Estuary Flow's schema evolution feature seamlessly handles updates to dataset structures within a Data Flow, ensuring uninterrupted operation. Collection specifications define each dataset, including key, schema, and partitions. When specs change, schema evolution automatically updates associated components to maintain compatibility.  It addresses breaking changes by updating materializations or recreating collections with new names, preventing disruptions. Common causes of breaking changes include modifications to collection schemas, which require updates to materializations.  Overall, schema evolution streamlines adaptation to structural changes, maintaining smooth data flow within the system.  For more information, check out the dedicated documentation page for schema evolution.  For the sake of this tutorial, feel free to leave everything at its default setting and press Next again, then Save and Publish to deploy the connector and kick off a backfill.  ","version":"Next","tagName":"h2"},{"title":"Step 3. Set up a Materialization​","type":1,"pageTitle":"PostgreSQL CDC streaming to Snowflake","url":"/getting-started/tutorials/postgresql_cdc_to_snowflake/#step-3-set-up-a-materialization","content":" Similarly to the source side, we’ll need to set up some initial configuration in Snowflake to allow Flow to materialize collections into a table.  Preparing Snowflake for use with Estuary Flow involves the following steps:  1. Keep the Flow web app open and open a new tab or window to access your Snowflake console.  2. Create a new SQL worksheet. This provides a platform to execute queries.  3. Paste the provided script into the SQL console, adjusting the value for estuary_password to a strong password.  set database_name = 'ESTUARY_DB'; set warehouse_name = 'ESTUARY_WH'; set estuary_role = 'ESTUARY_ROLE'; set estuary_user = 'ESTUARY_USER'; set estuary_password = 'secret'; set estuary_schema = 'ESTUARY_SCHEMA'; -- create role and schema for Estuary create role if not exists identifier($estuary_role); grant role identifier($estuary_role) to role SYSADMIN; -- Create snowflake DB create database if not exists identifier($database_name); use database identifier($database_name); create schema if not exists identifier($estuary_schema); -- create a user for Estuary create user if not exists identifier($estuary_user) password = $estuary_password default_role = $estuary_role default_warehouse = $warehouse_name; grant role identifier($estuary_role) to user identifier($estuary_user); grant all on schema identifier($estuary_schema) to identifier($estuary_role); -- create a warehouse for estuary create warehouse if not exists identifier($warehouse_name) warehouse_size = xsmall warehouse_type = standard auto_suspend = 60 auto_resume = true initially_suspended = true; -- grant Estuary role access to warehouse grant USAGE on warehouse identifier($warehouse_name) to role identifier($estuary_role); -- grant Estuary access to database grant CREATE SCHEMA, MONITOR, USAGE on database identifier($database_name) to role identifier($estuary_role); -- change role to ACCOUNTADMIN for STORAGE INTEGRATION support to Estuary (only needed for Snowflake on GCP) use role ACCOUNTADMIN; grant CREATE INTEGRATION on account to role identifier($estuary_role); use role sysadmin; COMMIT;   4. Execute all the queries by clicking the drop-down arrow next to the Run button and selecting &quot;Run All.&quot;  5. Snowflake will process the queries, setting up the necessary roles, databases, schemas, users, and warehouses for Estuary Flow.  6. Once the setup is complete, return to the Flow web application to continue with the integration process.  Back in Flow, head over to the Destinations page, where you can create a new Materialization.    Choose Snowflake and start filling out the connection details based on the values inside the script you executed in the previous step. If you haven’t changed anything, this is how the connector configuration should look like:    You can grab your Snowflake host URL and account identifier by navigating to these two little buttons on the Snowflake UI.    If you scroll down to the Advanced Options section, you will be able to configure the &quot;Update Delay&quot; parameter. If you leave this parameter unset, the default value of 30 minutes will be used.    The Update Delay parameter in Estuary materializations offers a flexible approach to data ingestion scheduling. It represents the amount of time the system will wait before it begins materializing the latest data.  For example, if an update delay is set to 2 hours, the materialization task will pause for 2 hours before processing the latest available data. This delay ensures that data is not pulled in immediately after it becomes available, allowing your Snowflake warehouse to go idle and be suspended in between updates, which can significantly reduce the number of credits consumed.  After the connection details are in place, the next step is to link the capture we just created to Flow is able to see collections we are loading data into from Postgres.  You can achieve this by clicking on the “Source from Capture” button, and selecting the name of the capture from the table.    After pressing continue, you are met with a few configuration options, but for now, feel free to press Next, then Save and Publish in the top right corner, the defaults will work perfectly fine for this tutorial.  A successful deployment will look something like this:    And that’s pretty much it, you’ve successfully published a real-time CDC pipeline. Let’s check out Snowflake to see how the data looks.    Looks like the data is arriving as expected, and the schema of the table is properly configured by the connector based on the types of the original table in Postgres.  To get a feel for how the data flow works, head over to the collection details page on the Flow web UI to see your changes immediately. On the Snowflake end, they will be materialized after the next update.  note Based on your configuration of the &quot;Update Delay&quot; parameter when setting up the Snowflake Materialization, you might have to wait until the configured amount of time passes for your changes to make it to the destination.  ","version":"Next","tagName":"h2"},{"title":"Party time!​","type":1,"pageTitle":"PostgreSQL CDC streaming to Snowflake","url":"/getting-started/tutorials/postgresql_cdc_to_snowflake/#party-time","content":" Congratulations! 🎉 You've successfully set up a CDC pipeline from PostgreSQL to Snowflake using Estuary Flow. In just a few minutes, you've learned how to configure log-based CDC replication, handle schema evolution, and deploy a robust data integration solution.  Take a moment to celebrate your achievement! You've not only gained valuable technical knowledge but also demonstrated the agility and efficiency of modern data engineering practices. With your newfound skills, you're well-equipped to tackle complex data challenges and drive innovation in your organization.  ","version":"Next","tagName":"h2"},{"title":"Clean up​","type":1,"pageTitle":"PostgreSQL CDC streaming to Snowflake","url":"/getting-started/tutorials/postgresql_cdc_to_snowflake/#clean-up","content":" After every party, it’s customary to clean up after ourselves. After you are done exploring the flow, make sure to remove any resources which you won’t use anymore!  Postgres  Simply stop the Docker containers &amp; terminate the ngrok process. This will terminate both the database process and the tunnel which exposed it to the internet.  Snowflake  To clean up resources in Snowflake use the following SQL script.  snowflake_cleanup.sql -- Revoke all privileges from Estuary role revoke all privileges on schema ESTUARY_SCHEMA from role ESTUARY_ROLE; revoke all privileges on database ESTUARY_DB from role ESTUARY_ROLE; -- Drop the warehouse drop warehouse if exists ESTUARY_WH; -- Drop the role and user drop user if exists ESTUARY_USER; drop role if exists ESTUARY_ROLE; -- Drop the schema and database drop schema if exists ESTUARY_SCHEMA; drop database if exists ESTUARY_DB;   Flow  In the UI, disable or delete any resources you don’t wish to keep.  ","version":"Next","tagName":"h2"},{"title":"Next Steps​","type":1,"pageTitle":"PostgreSQL CDC streaming to Snowflake","url":"/getting-started/tutorials/postgresql_cdc_to_snowflake/#next-steps","content":" That’s it! You should have everything you need to know to create your own data pipeline for loading data into Snowflake!   Now try it out on your own PostgreSQL database or other sources.  If you want to learn more, make sure you read through the Estuary documentation.  You’ll find instructions on how to use other connectors here. There are more tutorials here.   Also, don’t forget to join the Estuary Slack Community! ","version":"Next","tagName":"h2"},{"title":"Customize materialized fields","type":0,"sectionRef":"#","url":"/guides/customize-materialization-fields/","content":"","keywords":"","version":"Next"},{"title":"Capture desired fields and generate projections​","type":1,"pageTitle":"Customize materialized fields","url":"/guides/customize-materialization-fields/#capture-desired-fields-and-generate-projections","content":" Any field you eventually want to materialize must be included in the collection's schema. It's ok if the field is nested in the JSON structure; you'll flatten the structure with projections.  caution In this workflow, you'll edit a collection. This change can impact other downstream materializations and derivations. Use caution and be mindful of any edit's consequences before publishing.  ","version":"Next","tagName":"h2"},{"title":"Captured collections​","type":1,"pageTitle":"Customize materialized fields","url":"/guides/customize-materialization-fields/#captured-collections","content":" If the collection you're using was captured directly, follow these steps.  Go to the Captures page of the Flow web app and locate the capture that produced the collection. Click the Options button and choose Edit Specification. Under Output Collections, choose the binding that corresponds to the collection. Then, click the Collection tab. In the list of fields, look for the fields you want to materialize. If they're present and correctly named, you can skip toincluding them in the materialization.  hint: Compare the field name and pointer. For nested pointers, you'll probably want to change the field name to omit slashes.  If your desired fields aren't present or need to be re-named, edit the collection schema manually: Click Edit. Add missing fields to the schema in the correct location based on the source data structure. Click Close. Generate projections for new or incorrectly named fields. If available, click the Schema Inference button. The Schema Inference Window appears. Flow cleans up your schema and adds projections for new fields. Manually change the names of projected fields. These names will be used by the materialization and shown in the endpoint system as column names or the equivalent. Click Next. info Schema Inference isn't available for all capture types. You can also add projections manually with flowctl. Refer to the guide to editing with flowctl andhow to format projections. Repeat steps 3 through 6 with other collections, if necessary. Click Save and Publish.  ","version":"Next","tagName":"h3"},{"title":"Derived collections​","type":1,"pageTitle":"Customize materialized fields","url":"/guides/customize-materialization-fields/#derived-collections","content":" If the collection you're using came from a derivation, follow these steps.  Pull the derived collection's specification locally using flowctl.  flowctl catalog pull-specs --name &lt;yourOrg/full/collectionName&gt;   Review the collection's schema to see if the fields of interest are included. If they're present, you can skip toincluding them in the materialization. If your desired fields aren't present or are incorrectly named, add any missing fields to the schema in the correct location based on the source data structure. Use schema inference to generate projections for the fields.  flowctl preview --infer-schema --source &lt;full\\path\\to\\flow.yaml&gt; --collection &lt;yourOrg/full/collectionName&gt;   Review the updated schema. Manually change the names of projected fields. These names will be used by the materialization and shown in the endpoint system as column names or the equivalent. Re-publish the collection specification.  ","version":"Next","tagName":"h3"},{"title":"Include desired fields in your materialization​","type":1,"pageTitle":"Customize materialized fields","url":"/guides/customize-materialization-fields/#include-desired-fields-in-your-materialization","content":" Now that all your fields are present in the collection schema as projections, you can choose which ones to include in the materialization.  Every included field will be mapped to a table column or equivalent in the endpoint system.  If you haven't created the materialization, begin the process. Pause once you've selected the collections to materialize. If your materialization already exists, navigate to the edit materialization page. In the Collection Selector, choose the collection whose output fields you want to change. Click its Collection tab. Review the listed field. In most cases, Flow automatically detects all fields to materialize, projected or otherwise. However, a projected field may still be missing, or you may want to exclude other fields. If you want to make changes, click Edit. Use the editor to add the fields stanza to the collection's binding specification. Learn more about configuring fields and view a sample specification. Choose whether to start with Flow's recommended fields. Under fields, set recommended to true or false. If you choose true, you can exclude fields later. Use include to add missing projections, or exclude to remove fields. Click Close. Repeat steps 2 through 8 with other collections, if necessary. Click Save and Publish.  The named, included fields will be reflected in the endpoint system. ","version":"Next","tagName":"h2"},{"title":"dbt Cloud Integration","type":0,"sectionRef":"#","url":"/guides/dbt-integration/","content":"","keywords":"","version":"Next"},{"title":"How to Configure dbt Cloud Integration​","type":1,"pageTitle":"dbt Cloud Integration","url":"/guides/dbt-integration/#how-to-configure-dbt-cloud-integration","content":" Follow these steps to configure the dbt Cloud Job Trigger within an Estuary Flow materialization connector:  ","version":"Next","tagName":"h2"},{"title":"Required Parameters​","type":1,"pageTitle":"dbt Cloud Integration","url":"/guides/dbt-integration/#required-parameters","content":" To configure the dbt Cloud Job Trigger, you’ll need the following information:  Access URL: The dbt access URL can be found in your dbt Account Settings. Use this URL if your dbt account requires a specific access endpoint. For more information, visit go.estuary.dev/dbt-cloud-trigger. If you have not yet migrated to the new API, your Access URL is: https://cloud.getdbt.com/Job ID: The unique identifier for the dbt job you wish to trigger.Account ID: Your dbt account identifier.API Key: The dbt API key associated with your account. This allows Estuary Flow to authenticate with dbt Cloud and trigger jobs.  ","version":"Next","tagName":"h3"},{"title":"Optional Parameters​","type":1,"pageTitle":"dbt Cloud Integration","url":"/guides/dbt-integration/#optional-parameters","content":" Cause Message: Set a custom message that will appear as the &quot;cause&quot; for each triggered job. This is useful for tracking the context of each run, especially in complex workflows. If left empty, it defaults to &quot;Estuary Flow.&quot;Job Trigger Mode: skip: Skips the trigger if a job is already running (default).replace: Cancels any currently running job and starts a new one.ignore: Initiates a new job regardless of any existing jobs. Run Interval: Defines the interval at which the dbt job should run. This interval only triggers if new data has been materialized. The default is 30m (30 minutes).  ","version":"Next","tagName":"h3"},{"title":"Use Cases​","type":1,"pageTitle":"dbt Cloud Integration","url":"/guides/dbt-integration/#use-cases","content":" ","version":"Next","tagName":"h2"},{"title":"Regular Data Transformation on New Data​","type":1,"pageTitle":"dbt Cloud Integration","url":"/guides/dbt-integration/#regular-data-transformation-on-new-data","content":" In scenarios where data arrival may be delayed (for example, a materialization connector's Sync Frequency is set to1hr), the dbt Cloud Job Trigger mechanism in Estuary Flow is designed to ensure transformations are consistent without overwhelming the dbt job queue. Here’s how the process works:  Connector Initialization: When the connector starts, it immediately triggers a dbt job. This initial job ensures that data is consistent, even if the connector has restarted. Materializing Initial Data: The connector materializes an initial chunk of data and then starts a timer, set to trigger the dbt job in a specified interval (Run Interval). Handling Subsequent Data Chunks: The connector continues materializing the remaining data chunks and, after completing the initial data load, starts a scheduled delay (e.g., 1 hour if no new data is arriving). dbt Job Trigger Timing: The dbt job triggers once the set interval (e.g., 30 minutes) has passed from the initial timer, regardless of whether there is backfilled data. If the data arrival is sparse or infrequent, such as once per day, the default 30-minute interval allows for timely but controlled job triggers without excessive job runs.During periods without backfilling, the 30-minute interval provides a balance—triggering jobs at regular intervals while avoiding rapid job initiation and reducing load on the dbt Cloud system. Minimizing Latency: The Run Interval ensures that the dbt job runs shortly after the first bulk of data is committed, without triggering too frequently, particularly during backfills.  Defaulting to a lower (e.g. 30-minute) interval—supports various use cases, such as cases where connectors don’t useSync Interval or where data arrival is infrequent.  ","version":"Next","tagName":"h3"},{"title":"Job Management​","type":1,"pageTitle":"dbt Cloud Integration","url":"/guides/dbt-integration/#job-management","content":" The default behaviour is to avoid triggering multiple overlapping dbt jobs, set Job Trigger Mode to skip. This way, if a job is already running, the trigger will not start a new job, helping you manage resources efficiently.  Alternatively, if you need each transformation job to run regardless of current jobs, set Job Trigger Mode to ignore to initiate a new dbt job each time data is materialized. ","version":"Next","tagName":"h3"},{"title":"Connecting to Estuary Flow from Kafka using Dekaf","type":0,"sectionRef":"#","url":"/guides/dekaf_reading_collections_from_kafka/","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Connecting to Estuary Flow from Kafka using Dekaf","url":"/guides/dekaf_reading_collections_from_kafka/#overview","content":" Collections represent datasets within Estuary Flow. All captured documents are written to a collection, and all materialized documents are read from a collection.Dekaf enables you to interact with these collections as though they were Kafka topics, providing seamless integration with existing Kafka-based tools and workflows.  ","version":"Next","tagName":"h2"},{"title":"Key Features​","type":1,"pageTitle":"Connecting to Estuary Flow from Kafka using Dekaf","url":"/guides/dekaf_reading_collections_from_kafka/#key-features","content":" Kafka Topic Emulation: Access Estuary Flow collections as if they were Kafka topics.Schema Registry Emulation: Manage and retrieve schemas assigned to Estuary Flow collections, emulating Confluent's Schema Registry.  ","version":"Next","tagName":"h2"},{"title":"Connection Details​","type":1,"pageTitle":"Connecting to Estuary Flow from Kafka using Dekaf","url":"/guides/dekaf_reading_collections_from_kafka/#connection-details","content":" To connect to Estuary Flow via Dekaf, you need the following connection details:  Broker Address: dekaf.estuary-data.comSchema Registry Address: https://dekaf.estuary-data.comSecurity Protocol: SASL_SSLSASL Mechanism: PLAINSASL Username: {}SASL Password: Estuary Refresh Token (Generate a refresh token in the dashboard)Schema Registry Username: {}Schema Registry Password: The same Estuary Refresh Token as above  ","version":"Next","tagName":"h2"},{"title":"How to Connect to Dekaf​","type":1,"pageTitle":"Connecting to Estuary Flow from Kafka using Dekaf","url":"/guides/dekaf_reading_collections_from_kafka/#how-to-connect-to-dekaf","content":" ","version":"Next","tagName":"h2"},{"title":"1. Generate an Estuary Flow refresh token​","type":1,"pageTitle":"Connecting to Estuary Flow from Kafka using Dekaf","url":"/guides/dekaf_reading_collections_from_kafka/#1-generate-an-estuary-flow-refresh-token","content":" ","version":"Next","tagName":"h3"},{"title":"2. Set Up Your Kafka Client​","type":1,"pageTitle":"Connecting to Estuary Flow from Kafka using Dekaf","url":"/guides/dekaf_reading_collections_from_kafka/#2-set-up-your-kafka-client","content":" Configure your Kafka client using the connection details provided.  Example Kafka Client Configuration​  Below is an example configuration for a Kafka client using Python’s kafka-python library:  from kafka import KafkaConsumer # Configuration details conf = { 'bootstrap_servers': 'dekaf.estuary-data.com:9092', 'security_protocol': 'SASL_SSL', 'sasl_mechanism': 'PLAIN', 'sasl_plain_username': '{}', 'sasl_plain_password': 'Your_Estuary_Refresh_Token', 'group_id': 'your_group_id', 'auto_offset_reset': 'earliest' } # Create Consumer instance consumer = KafkaConsumer( 'your_topic_name', bootstrap_servers=conf['bootstrap_servers'], security_protocol=conf['security_protocol'], sasl_mechanism=conf['sasl_mechanism'], sasl_plain_username=conf['sasl_plain_username'], sasl_plain_password=conf['sasl_plain_password'], group_id=conf['group_id'], auto_offset_reset=conf['auto_offset_reset'], enable_auto_commit=True, value_deserializer=lambda x: x.decode('utf-8') ) # Poll for messages try: for msg in consumer: print(f&quot;Received message: {msg.value}&quot;) except KeyboardInterrupt: pass finally: consumer.close()   You can also use kcat (formerly known as kafkacat) to test reading messages from an Estuary Flow collection as if it were a Kafka topic.  kcat -C \\ -X broker.address.family=v4 \\ -X security.protocol=SASL_SSL \\ -X sasl.mechanism=PLAIN \\ -X sasl.username=&quot;{}&quot; \\ -X sasl.password=&quot;Your_Estuary_Refresh_Token&quot; \\ -b dekaf.estuary-data.com:9092 \\ -t &quot;full/nameof/estuarycollection&quot; \\ -p 0 \\ -o beginning \\ -s avro \\ -r https://{}:{Your_Estuary_Refresh_Token}@dekaf.estuary-data.com  ","version":"Next","tagName":"h3"},{"title":"Edit Data Flows in the web app","type":0,"sectionRef":"#","url":"/guides/edit-data-flows/","content":"","keywords":"","version":"Next"},{"title":"Edit a capture​","type":1,"pageTitle":"Edit Data Flows in the web app","url":"/guides/edit-data-flows/#edit-a-capture","content":" Go to the Sources page of the web app. Locate the capture you'd like to edit. Click the Options button in its table row, then click Edit specification. The Edit Capture page opens. Edit the connection to the destination system, if desired. You can either update fields in the Endpoint Config section or manually update the JSON in the Advanced Specification Editor.  caution You may have to re-authenticate with the source system. Be sure to have current credentials on hand before editing the endpoint configuration.  Use the Output Collections browser to add or remove collections from the capture, if desired. To refresh your connection with the source and see an updated list of possible collections, click the Refresh button, but be aware that it will overwrite all existing collection selections. Use the Schema Inference tool, if desired. This option is available for source systems with permissive schemas, such as NoSQL databases and cloud storage. Flow can help you tighten up the schema to be used for downstream tasks in your Data Flow. In the Output Collections browser, choose a collection and click its Collection tab. Click Schema Inference The Schema Inference window appears. Flow scans the data in your collection and infers a new schema, called the readSchema, to use for downstream tasks like materializations and derivations. Review the new schema and click Apply Inferred Schema. When you're done making changes, click Next. Click Save and Publish.  Editing a capture only affects how it will work going forward. Data that was captured before editing will reflect the original configuration.  Edit a materialization  To edit a materialization:  Go to the Destinations page of the web app. Locate the materialization you'd like to edit. Click the Options button in its table row, then click Edit specification. The Edit Materialization page opens. Edit the connection to the destination system, if desired. You can either update fields in the Endpoint Config section or manually update the JSON in the Advanced Specification Editor.  caution You may have to re-authenticate with the destination system. Be sure to have current credentials on hand before editing the endpoint configuration.  Use the Source Collections browser to add or remove collections from the materialization, if desired. Optionally apply a stricter schema to each collection to use for the materialization. This option is available for collections captured from source systems with permissive schemas, such as NoSQL databases and cloud storage. Flow can help you tighten up the schema to be used for downstream tasks in your Data Flow. In the Source Collections browser, choose a collection and click its Collection tab. Click Schema Inference The Schema Inference window appears. Flow scans the data in your collection and infers a new schema, called the readSchema, to use for the materialization. Review the new schema and click Apply Inferred Schema. When you're done making changes, click Next. Click Save and Publish.  Editing a materialization only affects how it will work going forward. Data that was materialized before editing will reflect the original configuration.  ","version":"Next","tagName":"h2"},{"title":"Advanced editing​","type":1,"pageTitle":"Edit Data Flows in the web app","url":"/guides/edit-data-flows/#advanced-editing","content":" For more fine-grain control over editing, you can use flowctl and work directly on specification files in your local environment.View the tutorial. ","version":"Next","tagName":"h2"},{"title":"flowctl guides","type":0,"sectionRef":"#","url":"/guides/flowctl/","content":"flowctl guides The guides in this section cover common workflows using the Estuary Flow CLI, flowctl. Learn how to edit published Flow entities, create derivations, and more. To get to know flowctl, see the concepts page.","keywords":"","version":"Next"},{"title":"How to transform data using SQL","type":0,"sectionRef":"#","url":"/guides/derivation_tutorial_sql/","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"How to transform data using SQL","url":"/guides/derivation_tutorial_sql/#introduction","content":" This tutorial will show you how to implement a stateless transformation using SQL. You’ll learn how to implement a flow that transforms events coming from the live, real-time Wikipedia API.  ","version":"Next","tagName":"h2"},{"title":"Setting up your development environment​","type":1,"pageTitle":"How to transform data using SQL","url":"/guides/derivation_tutorial_sql/#setting-up-your-development-environment","content":" In order to implement transformations through derivations, you’ll need to set up your development environment. You’ll need a text editor and flowctl, the CLI-tool for Flow installed on your machine. Check out the docs page on installation instructions.  Before continuing, sign in to the Estuary Flow dashboard, make sure you enable access to the Wikipedia demo. Using flowctl, quickly verify you are able to view the demo collections used in this guide.  Execute the below command to display the documents in the demo/wikipedia/recentchange-sampled collection:  note This collection is a 3% sample of the enormous demo/wikipedia/recentchange collection which contains millions of documents. Since the purpose of this tutorial is to demonstrate a proof of concept, we avoid publishing a derivation that processes hundreds of gigabytes of data.  flowctl collections read --collection demo/wikipedia/recentchange-sampled --uncommitted   If you see a stream of JSON documents on your terminal, you’re all good - feel free to cancel the process by pressing C^C.  Examine a sample JSON that lives in the demo collection, as this is the data you’ll be using as the input for our derivation.  { &quot;$schema&quot;: &quot;/mediawiki/recentchange/1.0.0&quot;, &quot;_meta&quot;: { &quot;file&quot;: &quot;recentchange&quot;, &quot;offset&quot;: 12837, &quot;uuid&quot;: &quot;f8f07d87-f5bf-11ee-8401-4fdf95f7b91a&quot; }, &quot;bot&quot;: false, &quot;comment&quot;: &quot;[[:File:Jeton. Ordinaire des guerres - btv1b10405460g (1 of 2).jpg]] added to category&quot;, &quot;id&quot;: 2468434138, &quot;meta&quot;: { &quot;domain&quot;: &quot;commons.wikimedia.org&quot;, &quot;dt&quot;: &quot;2024-04-08T15:52:13Z&quot;, &quot;id&quot;: &quot;d9e8698f-4eac-4262-a451-b7ca247e401c&quot;, &quot;offset&quot;: 5008568732, &quot;partition&quot;: 0, &quot;request_id&quot;: &quot;b5372124-63fa-45e1-b35e-86784f1692bc&quot;, &quot;stream&quot;: &quot;mediawiki.recentchange&quot;, &quot;topic&quot;: &quot;eqiad.mediawiki.recentchange&quot;, &quot;uri&quot;: &quot;https://commons.wikimedia.org/wiki/Category:Jetons&quot; }, &quot;namespace&quot;: 14, &quot;notify_url&quot;: &quot;https://commons.wikimedia.org/w/index.php?diff=866807860&amp;oldid=861559382&amp;rcid=2468434138&quot;, &quot;parsedcomment&quot;: &quot;&lt;a href=\\&quot;/wiki/File:Jeton._Ordinaire_des_guerres_-_btv1b10405460g_(1_of_2).jpg\\&quot; title=\\&quot;File:Jeton. Ordinaire des guerres - btv1b10405460g (1 of 2).jpg\\&quot;&gt;File:Jeton. Ordinaire des guerres - btv1b10405460g (1 of 2).jpg&lt;/a&gt; added to category&quot;, &quot;server_name&quot;: &quot;commons.wikimedia.org&quot;, &quot;server_script_path&quot;: &quot;/w&quot;, &quot;server_url&quot;: &quot;https://commons.wikimedia.org&quot;, &quot;timestamp&quot;: 1712591533, &quot;title&quot;: &quot;Category:Jetons&quot;, &quot;title_url&quot;: &quot;https://commons.wikimedia.org/wiki/Category:Jetons&quot;, &quot;type&quot;: &quot;categorize&quot;, &quot;user&quot;: &quot;DenghiùComm&quot;, &quot;wiki&quot;: &quot;commonswiki&quot; }   The transformation in this tutorial will make use of the length, bot and user_id fields to calculate how many lines a given non-bot user has modified on a day.  { ... &quot;user_id&quot;: &quot;User&quot; &quot;bot&quot;: 0 &quot;length&quot;: 1253 ... }   ","version":"Next","tagName":"h2"},{"title":"Writing the derivation​","type":1,"pageTitle":"How to transform data using SQL","url":"/guides/derivation_tutorial_sql/#writing-the-derivation","content":" Set up your folder structure so you can organize the resources required for the derivation. Create a working directory to follow along, and inside, create a flow.yaml file.  Inside your flow.yaml file, add the following contents:  --- collections: Dani/derivation-tutorial/edits-by-users: schema: type: object properties: user_id: type: string date: format: date type: string total_edits: reduce: strategy: sum type: number total_new_lines: reduce: strategy: sum type: number reduce: strategy: merge required: - date - user_id key: - /date - /user_id derive: using: sqlite: {} transforms: - name: edits_by_users source: demo/wikipedia/recentchange-sampled shuffle: any lambda: | select $user as user_id, substr($meta$dt,1,10) as date, 1 as total_edits, coalesce($length$new - $length$old, 0) as total_new_lines where $type = 'edit' and $user is not null and $bot = 0;      The Flow consists of just one collection, which is what you define here, called edits-by-users.  Let’s go over this in a bit more detail.  First of all, the collection needs a schema. The schema of the incoming data (also called the “write” schema) is already defined by the demo, you only have to define the schema of the documents the transformation will output, which is the “read” schema.  In the flow.yaml file, the schema is defined in-line with the rest of the configuration.  schema: type: object properties: user_id: type: string date: format: date type: string total_edits: reduce: strategy: sum type: number total_new_lines: reduce: strategy: sum type: number reduce: strategy: merge required: - date - user_id   As you can see, this schema includes less fields than what is available in the incoming documents, this is expected, but if you wish to include more, this is where you would add them first.  The user_id and date fields do not contain any modifications, but the other two have their reduction strategy defined as well to be sum. This strategy reduces two numbers or integers by adding their values.  To learn more about how reduction strategies work, check out the documentation page.  Moving on, the next section in the yaml file defines the key of the documents.  key: - /date - /user_id   Every Flow collection must declare a key which is used to group its documents. Keys are specified as an array of JSON pointers to document locations. The important detail here is to know that a collection key instructs Flow how documents of a collection are to be reduced, such as while being materialized to an endpoint.  The final section is where you specify that this collection is derived from another collection.  derive: using: sqlite: {} transforms: - name: edits_by_users source: demo/wikipedia/recentchange-sampled shuffle: any lambda: | select $user as user_id, substr($meta$dt,1,10) as date, 1 as total_edits, coalesce($length$new - $length$old, 0) as total_new_lines where $type = 'edit' and $user is not null and $bot = 0;   Here you define the SQL statement that gets executed on the documents of the source collection.  The source: demo/wikipedia/recentchange-sampled property lets Flow know that the source collection is the demo collection from mentioned at in the beginning of the tutorial while shuffle tells Flow how to colocate documents while processing, which in this case is set to any, meaning source documents can be processed by any available compute.  The SQL is straightforward  select $user as user_id, substr($meta$dt,1,10) as date, 1 as total_edits, coalesce($length$new - $length$old, 0) as total_new_lines where $type = 'edit' and $user is not null and $bot = 0   We select the user_id, parse the event date and calculate the amount of line changes. We also select 1 for the value of total_edits, this is important because during the reduction phase, due to having selected sum as the strategy, these values will get added together to form the total number of edits in the result. We also filter out non-edit events, bot users or events without a user_id to have a somewhat clean dataset.  ","version":"Next","tagName":"h2"},{"title":"Verify​","type":1,"pageTitle":"How to transform data using SQL","url":"/guides/derivation_tutorial_sql/#verify","content":" You can use flowctl to quickly verify your derivation before publishing it. Use the preview command to get an idea of the resulting collections.  flowctl preview --source flow.yaml --name Dani/derivation-tutorial/edits-by-users {&quot;date&quot;:&quot;2024-04-08&quot;,&quot;total_edits&quot;:3,&quot;total_new_lines&quot;:110,&quot;user_id&quot;:&quot;Renamerr&quot;} {&quot;date&quot;:&quot;2024-04-08&quot;,&quot;total_edits&quot;:1,&quot;total_new_lines&quot;:769,&quot;user_id&quot;:&quot;Sebring12Hrs&quot;} {&quot;date&quot;:&quot;2024-04-08&quot;,&quot;total_edits&quot;:5,&quot;total_new_lines&quot;:3360,&quot;user_id&quot;:&quot;Sic19&quot;} {&quot;date&quot;:&quot;2024-04-08&quot;,&quot;total_edits&quot;:1,&quot;total_new_lines&quot;:82,&quot;user_id&quot;:&quot;Simeon&quot;} ^C   As you can see, the output format matches the defined schema. The last step would be to publish your derivation to Flow, which you can also do using flowctl.  warning Publishing the derivation will initialize the transformation on the live, real-time Wikipedia stream, make sure to delete it after completing the tutorial.  flowctl catalog publish --source flow.yaml   After successfully publishing your derivation, head over to the Collections page on the Web UI and you will be able to see your derivation in action!    ","version":"Next","tagName":"h2"},{"title":"Wrapping up​","type":1,"pageTitle":"How to transform data using SQL","url":"/guides/derivation_tutorial_sql/#wrapping-up","content":" In this guide you learned how to write your first stateless SQL derivation to filter data in a collection. ","version":"Next","tagName":"h2"},{"title":"Edit a draft created in the web app","type":0,"sectionRef":"#","url":"/guides/flowctl/edit-draft-from-webapp/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Edit a draft created in the web app","url":"/guides/flowctl/edit-draft-from-webapp/#prerequisites","content":" To complete this workflow, you need:  An Estuary account flowctl installed locally  ","version":"Next","tagName":"h2"},{"title":"Identify the draft and pull it locally​","type":1,"pageTitle":"Edit a draft created in the web app","url":"/guides/flowctl/edit-draft-from-webapp/#identify-the-draft-and-pull-it-locally","content":" Drafts aren't currently visible in the Flow web app, but you can get a list with flowctl.  Authorize flowctl. Generate an Estuary Flow refresh token. Run flowctl auth token --token &lt;paste-token-here&gt; Run flowctl draft list flowctl outputs a table of all the drafts to which you have access, from oldest to newest. Use the name and timestamp to find the draft you're looking for. Each draft has an ID, and most have a name in the Details column. Note the # of Specs column. For drafts created in the web app, materialization drafts will always contain one specification. A number higher than 1 indicates a capture with its associated collections. Copy the draft ID. Select the draft: flowctl draft select --id &lt;paste-id-here&gt;. Pull the draft source files to your working directory: flowctl draft develop. Browse the source files. The source files and their directory structure will look slightly different depending on the draft. Regardless, there will always be a top-level file called flow.yaml that imports all other YAML files, which you'll find in a subdirectory named for your catalog prefix. These, in turn, contain the specifications you'll want to edit.  ","version":"Next","tagName":"h2"},{"title":"Edit the draft and publish​","type":1,"pageTitle":"Edit a draft created in the web app","url":"/guides/flowctl/edit-draft-from-webapp/#edit-the-draft-and-publish","content":" Next, you'll make changes to the specification(s), test, and publish the draft.  Open the YAML files that contain the specification you want to edit. Make changes. For guidance on how to construct Flow specifications, see the documentation for the entity type: CapturesCollectionsMaterializations When you're done, sync the local work to the global draft: flowctl draft author --source flow.yaml. Specifying the top-level flow.yaml file as the source ensures that all entities in the draft are imported. Publish the draft: flowctl draft publish Once this operation completes successfully, check to verify if the entity or entities are live. You can: Go to the appropriate tab in the Flow web app. Run flowctl catalog list, filtering by --name, --prefix, or entity type, for example --capture.  If you're not satisfied with the published entities, you can continue to edit them. See the other guides for help:  Edit in the web app.Edit with flowctl. ","version":"Next","tagName":"h2"},{"title":"Implementing Derivations for AcmeBank","type":0,"sectionRef":"#","url":"/getting-started/tutorials/derivations_acmebank/","content":"","keywords":"","version":"Next"},{"title":"Filtering Large Transfers​","type":1,"pageTitle":"Implementing Derivations for AcmeBank","url":"/getting-started/tutorials/derivations_acmebank/#filtering-large-transfers","content":" note This section introduces SQLite derivations, SQL lambda blocks and $parameters.  Your compliance department has reached out, and they require an understanding of the last large transfer (if any) made by each user account.  You create a SQL derivation to help them out. The transfers collection is keyed on the transfer /id, so you'll need to re-key your derivation on the /sender account. You also need to filter out transfers that aren't large enough.  Putting this all together:  last-large-send.flow.yamllast-large-send-test.flow.yaml collections: acmeBank/last-large-send: schema: transfers.schema.yaml key: [/sender] derive: using: sqlite: {} transforms: - name: filterTransfers source: acmeBank/transfers shuffle: any lambda: SELECT $id, $sender, $recipient, $amount WHERE $amount &gt; 100;   derive: using: sqlite: {} tells Flow that collectionacmeBank/last-large-send is derived using Flow's SQLite derivation connector.  This derivation has just one transform, which sources from the transfers collection. As source documents become available, they're evaluated by the SQL lambdaand its SELECT output is published to the derived collection. Your SQL queries access locations of source documents through $parameter bindings.  The compliance department then materializes this collection to their preferred destination, for an always up-to-date view indexed by each account.  ","version":"Next","tagName":"h2"},{"title":"Finding New Account Pairs​","type":1,"pageTitle":"Implementing Derivations for AcmeBank","url":"/getting-started/tutorials/derivations_acmebank/#finding-new-account-pairs","content":" note This section introduces SQLite migrations and internal task tables.  The fraud team needs your help: they have a new process they must run the first time some sending account sends funds to a receiving account. They would like to see only those transfers which reflect a new account pair of (sender, recipient). To tackle this you need to know which account pairs have been seen before.  SQLite derivations run within the context of a persistent, managed SQLite database. You can apply database migrations that create whatever tables, triggers, or views you might need. Then, the statements of your SQL lambda code can INSERT, UPDATE, or DELETEfrom those tables, query from them, or any other operation supported by SQLite. The tables and other schema you create through your migrations are the internal state of your task.  first-send.flow.yamlfirst-send-test.flow.yaml collections: acmeBank/first-send: schema: transfers.schema.yaml key: [/id] derive: using: sqlite: migrations: - CREATE TABLE seen_pairs ( sender TEXT NOT NULL, recipient TEXT NOT NULL, PRIMARY KEY (sender, recipient) ); transforms: - name: fromTransfers source: acmeBank/transfers shuffle: key: [/sender, /recipient] lambda: INSERT INTO seen_pairs (sender, recipient) VALUES ($sender, $recipient) ON CONFLICT DO NOTHING RETURNING $id, $sender, $recipient, $amount;   This time, the derivation attempts to INSERT into the seen_pairs table, and uses SQLite's RETURNINGsyntax to only publish documents for rows which were successfully inserted.  You can evolve the internal SQLite tables of your derivation as needed, by appending SQL blocks which perform a database migration to the migrations array. Any migrations appended to the list are automatically applied by Flow.  ","version":"Next","tagName":"h2"},{"title":"Grouped Windows of Transfers​","type":1,"pageTitle":"Implementing Derivations for AcmeBank","url":"/getting-started/tutorials/derivations_acmebank/#grouped-windows-of-transfers","content":" note This section introduces delayed reads, and applies them to implement a custom window policy.  The fraud team is back, and now needs to know the othertransfers which an account has made in the last day. They want you to enrich each transfer with the grouping of all transfers initiated by that account in the prior 24 hours.  You may have encountered &quot;windowing&quot; in other tools for stream processing. Some systems even require that you define a window policy in order to function. Flow does not use windows, but sometimes you do want a time-bound grouping of recent events.  All collection documents contain a wall-clock timestamp of when they were published. The transforms of a derivation will generally process source documents in ascending wall-time order. You can augment this behavior by using a read delay to refine the relative order in which source documents are read, which is useful for implementing arbitrary window policies:  grouped.flow.yamlenrichAndAddToWindow.sqlgrouped-test.flow.yaml collections: acmeBank/grouped-transfers: schema: # Enrich transfer with a window of *other* transfers. $ref: transfers.schema.yaml required: [window] properties: window: { type: array } key: [/id] derive: using: sqlite: migrations: - CREATE TABLE transfers ( id INTEGER PRIMARY KEY NOT NULL, sender TEXT NOT NULL, recipient TEXT NOT NULL, amount REAL NOT NULL ); CREATE INDEX idx_transfers_sender ON transfers (sender); transforms: - name: enrichAndAddToWindow source: acmeBank/transfers shuffle: { key: [/sender] } lambda: enrichAndAddToWindow.sql - name: removeFromWindow source: acmeBank/transfers shuffle: { key: [/sender] } readDelay: 24h lambda: DELETE FROM transfers WHERE id = $id;   ","version":"Next","tagName":"h2"},{"title":"Approving Transfers​","type":1,"pageTitle":"Implementing Derivations for AcmeBank","url":"/getting-started/tutorials/derivations_acmebank/#approving-transfers","content":" note This section expands usage of SQLite task tables and introduces a recursive data flow.  Your users don't always check if they have sufficient funds before starting a transfer, and account overdrafts are becoming common. The product team has tapped you to fix this by enriching each transfer with an approve or deny outcome based on the account balance of the sender.  To do this, you first need to track the sender's current account balance. Clearly an account balance is debited when it's used to sends funds. It's also credited when it receives funds.  But there's a catch: an account can only be credited for funds received from approved transfers! This implies you need a collection of transfer outcomes in order to derive your collection of transfer outcomes 🤯.  This is an example of a self-referential, recursive data-flow. You may have used tools which require that data flow in a Directed Acyclic Graph (DAG). Flow does not require that your data flows are acyclic, and it also supports a derivation that reads from itself, which lets you tackle this task:  outcomes.flow.yamldebitSender.sqloutcomes-test.flow.yaml collections: acmeBank/transfer-outcomes: schema: # Enrich transfer schema with outcome and the sender's balance. $ref: transfers.schema.yaml required: [outcome, sender_balance] properties: outcome: description: Transfer was approved, or denied for insufficient funds. enum: [approve, deny] sender_balance: { type: number } key: [/id] derive: using: sqlite: migrations: - CREATE TABLE current_balances ( account TEXT PRIMARY KEY NOT NULL, balance REAL NOT NULL ); transforms: - name: debitSender source: acmeBank/transfers # Shuffle on the sender, as we'll debit their balance. shuffle: { key: [/sender] } lambda: debitSender.sql - name: creditRecipient # When a transfer is approved, we've debited the sender but still need to # credit the recipient. Read approved transfers from ourselves to do so. source: name: acmeBank/transfer-outcomes partitions: include: outcome: [approve] shuffle: { key: [/recipient] } lambda: INSERT INTO current_balances (account, balance) VALUES ($recipient, $amount) ON CONFLICT DO UPDATE SET balance = balance + $amount; # Partition output based on the transfer outcome. projections: outcome: location: /outcome partition: true   ","version":"Next","tagName":"h2"},{"title":"Current Account Balances​","type":1,"pageTitle":"Implementing Derivations for AcmeBank","url":"/getting-started/tutorials/derivations_acmebank/#current-account-balances","content":" note This section introduces TypeScript derivations and reduction annotations.  Your product team is back, and they want a database table keyed by account that contains its up-to-date current balance.  As shown in the previous section, you could create a task table which aggregates each account balance, and then SELECT the current balance after every transfer. For most use cases, this is a great place to start. For interest and variety, you'll solve this problem using TypeScript.  TypeScript derivations require a module which you write. You don't know how to write that module yet, so first implement the derivation specification in balances.flow.yaml. Next run the flowctl generate command, which generates two files:  A module stub for you to fill out.A file of TypeScript interfaces which are used by your module.  balances.flow.yamlModule StubInterfaces collections: acmeBank/balances: schema: balances.schema.yaml key: [/user] derive: using: typescript: module: balances.ts transforms: - name: fromOutcomes source: name: acmeBank/transfer-outcomes partitions: include: outcome: [approve] shuffle: any   Next fill out the body of your TypeScript module and write a test:  balances.tsbalances-test.flow.yaml import { IDerivation, Document, SourceFromOutcomes } from 'flow/acmeBank/balances.ts'; // Implementation for derivation acmeBank/balances. export class Derivation extends IDerivation { fromOutcomes(read: { doc: SourceFromOutcomes }): Document[] { const doc = read.doc; return [ // Debit the sender. { user: doc.sender, balance: -doc.amount }, // Credit the recipient. { user: doc.recipient, balance: doc.amount }, ]; } }   One piece is still missing. Your TypeScript module is publishing the change in account balance for each transfer. That's not the same thing as the current balance for each account.  You can ask Flow to sum up the balance changes into a current account balance through reduction annotations. Here's the balances schema, with reduce annotations for summing the account balance:  type: object required: [user, balance] reduce: { strategy: merge } properties: user: { type: string } balance: type: number reduce: { strategy: sum }   This section has more moving parts that the previous SQL-based examples. You might be wondering, why bother? Fair question! This is just an illustrative example, after all.  While they're more verbose, TypeScript derivations do have certain advantages:  TypeScript derivations are strongly typed, and those checks often catch meaningful bugs and defects before they're deployed. Your derivation modules also play nicely with VSCode and other developer tooling.TypeScript derivations can use third-party libraries, as well as your native code compiled to WASM.TypeScript can be easier when working with nested or complex document structures.  Reduction annotations also have some benefits over task state (like SQLite tables):  Internal task state is managed by Flow. If it grows to be large (say, you have a lot of accounts), then your task must be scaled and could require performance tuning. Reduction annotations, on the other hand, require no internal state and are extremely efficient.Certain aggregations, such as recursive merging of tree-like structures, are much simpler to express through reduction annotations vs implementing yourself.  See &quot;Where to Accumulate?&quot; for more discussion. ","version":"Next","tagName":"h2"},{"title":"Create a derivation with flowctl","type":0,"sectionRef":"#","url":"/guides/flowctl/create-derivation/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Create a derivation with flowctl","url":"/guides/flowctl/create-derivation/#prerequisites","content":" A Flow account and access to the web app. If you don't have an account yet, go to the web app to register for free. An existing Flow collection. Typically, you create this through a capture in the Flow web application. If you need help, see the guide to create a Data Flow. A development environment to work with flowctl. Choose between: GitPod, the cloud development environment integrated with Flow. GitPod comes ready for derivation writing, with stubbed out files and flowctl installed. You'll need a GitLab, GitHub, or BitBucket account to log in. Your local development environment. Install flowctl locally.  ","version":"Next","tagName":"h2"},{"title":"Get started with GitPod​","type":1,"pageTitle":"Create a derivation with flowctl","url":"/guides/flowctl/create-derivation/#get-started-with-gitpod","content":" You'll write your derivation using GitPod, a cloud development environment integrated in the Flow web app.  Navigate to the Collections page in Flow. Click on the New Transformation button. The Derive A New Collection pop-up window appears. In the Available Collections dropdown, select the collection you want to use as the source. For example, if your organization is acmeCo, you might choose the acmeCo/resources/anvils collection. Set the transformation language to either SQL or TypeScript. SQL transformations can be a more approachable place to start if you're new to derivations. TypeScript transformations can provide more resiliency against failures through static type checking. Give your derivation a name. From the dropdown, choose the name of your catalog prefix and append a unique name, for example acmeCo/resources/anvil-status. Click Proceed to GitPod to create your development environment. Sign in with one of the available account types. On the New Workspace screen, keep the Context URL option selected and click Continue. A GitPod development environment opens. A stubbed-out derivation with a transformation has already been created for you in the language you chose. Next, you'll locate and open the source files. Each slash-delimited prefix of your derivation name has become a folder. Open the nested folders to find the flow.yaml file with the derivation specification. Following the example above, you'd open the folders called acmeCo, then resources to find the correct flow.yaml file. The file contains a placeholder collection specification and schema for the derivation. In the same folder, you'll also find supplementary TypeScript or SQL files you'll need for your transformation.  Continue with SQL  Continue with TypeScript  Authentication When you first connect to GitPod, you will have already authenticated Flow, but if you leave GitPod opened for too long, you may have to reauthenticate Flow. To do this: Generate an Estuary Flow refresh token. Run flowctl auth token --token &lt;paste-token-here&gt; in the GitPod terminal.  ","version":"Next","tagName":"h2"},{"title":"Add a SQL derivation in GitPod​","type":1,"pageTitle":"Create a derivation with flowctl","url":"/guides/flowctl/create-derivation/#add-a-sql-derivation-in-gitpod","content":" If you chose SQL as your transformation language, follow these steps.  Along with the derivation's flow.yaml you found in the previous steps, there are two other files:  A lambda file. This is where you'll write your first SQL transformation. Its name follows the pattern derivation-name.lambda.source-collection-name.sql. Using the example above, it'd be called anvil-status.lambda.anvils.sql. A migrations file. Migrations allow you to leverage other features of the sqlite database that backs your derivation by creating tables, indices, views, and more. Its name follows the pattern derivation-name.migration.0.sql. Using the example above, it'd be called anvil-status.migration.0.sql.  Open the flow.yaml file for your derivation. It looks something like this: collections: acmeCo/resources/anvil-status: schema: properties: your_key: type: string required: - your_key type: object key: - /your_key derive: using: sqlite: migrations: - anvil-status.migration.0.sql transforms: - name: anvils source: acmeCo/resources/anvils shuffle: any lambda: anvil-status.lambda.anvils.sql Note the stubbed out schema and key. Write the schema you'd like your derivation to conform to and specify its collection key. Keep in mind: The source collection's schema. The transformation required to get from the source schema to the new schema. Give the transform a unique name (by default, it's the name of the source collection). In the lambda file, write your SQL transformation.  Tip For help writing your derivation, start with these examples: Continuous materialized view tutorialAcme Bank examples The main derivations page includes many other examples and in-depth explanations of how derivations work.  If necessary, open the migration file and write your migration. Preview the derivation locally. flowctl preview --source flow.yaml If the preview output appears as expected, publish the derivation. flowctl catalog publish --source flow.yaml   The derivation you created is now live and ready for further use. You can access it from the web application and materialize it to a destination, just as you would any other Flow collection.  ","version":"Next","tagName":"h2"},{"title":"Add a TypeScript derivation in GitPod​","type":1,"pageTitle":"Create a derivation with flowctl","url":"/guides/flowctl/create-derivation/#add-a-typescript-derivation-in-gitpod","content":" If you chose TypeScript as your transformation language, follow these steps.  Along with the derivation's flow.yaml you found in the previous steps, there's another file for the TypeScript transformation. It follows the naming convention derivation-name.ts. Using the example above, it'd be called anvil-status.ts.  Open the flow.yaml file for your derivation. It looks something like this: collections: acmeCo/resources/anvil-status: schema: properties: your_key: type: string required: - your_key type: object key: - /your_key derive: using: typescript: module: anvil-status.ts transforms: - name: anvils source: acmeCo/resources/anvils shuffle: any Note the stubbed out schema and key. Write the schema you'd like your derivation to conform to and specify the collection key. Keep in mind: The source collection's schema. The transformation required to get from the source schema to the new schema. Give the transform a unique name (by default, it's the name of the source collection). In the TypeScript file, write your transformation.  Tip For help writing a TypeScript derivation, start with this example. The main derivations page includes many other examples and in-depth explanations of how derivations work.  Preview the derivation locally. flowctl preview --source flow.yaml If the preview output appears how you'd expect, publish the derivation. flowctl catalog publish --source flow.yaml   The derivation you created is now live and ready for further use. You can access it from the web application and materialize it to a destination, just as you would any other Flow collection.  ","version":"Next","tagName":"h2"},{"title":"Create a derivation locally​","type":1,"pageTitle":"Create a derivation with flowctl","url":"/guides/flowctl/create-derivation/#create-a-derivation-locally","content":" Creating a derivation locally is largely the same as using GitPod, but has some extra steps. Those extra steps are explained here, but you'll find more useful context in the sections above.  Authorize flowctl. Generate an Estuary Flow refresh token. Run flowctl auth token --token &lt;paste-token-here&gt; in your local environment. Locate the source collection for your derivation. Check the web app's Collections. All published entities to which you have access are listed and can be searched. Run flowctl catalog list --collections. This command returns a complete list of collections to which you have access. You can refine by specifying a --prefix. Pull the source collection locally using the full collection name. flowctl catalog pull-specs --name acmeCo/resources/anvils The source files are written to your current working directory. Each slash-delimited prefix of your collection name has become a folder. Open the nested folders to find the flow.yaml file with the collection specification. Following the example above, you'd open the folders called acmeCo, then resources to find the correct flow.yaml file. The file contains the source collection specification and schema. Add the derivation as a second collection in the flow.yaml file. Write the schema you'd like your derivation to conform to and specify the collection key. Reference the source collection's schema, and keep in mind the transformation required to get from the source schema to the new schema. Add the derive stanza. See examples for SQL and TypeScript above. Give your transform a unique name. Stub out the SQL or TypeScript files for your transform. flowctl generate --source flow.yaml Locate the generated file, likely in the same subdirectory as the flow.yaml file you've been working in. Write your transformation. Preview the derivation locally.  flowctl preview --source flow.yaml   If the preview output appears how you'd expect, publish the derivation.  flowctl catalog publish --source flow.yaml   The derivation you created is now live and ready for further use. You can access it from the web application and materialize it to a destination, just as you would any other Flow collection.  ","version":"Next","tagName":"h2"},{"title":"Updating an existing derivation​","type":1,"pageTitle":"Create a derivation with flowctl","url":"/guides/flowctl/create-derivation/#updating-an-existing-derivation","content":" Derivations are applied on a go-forward basis only.  If you would like to make an update to an existing derivation (for example, adding columns to the derived collection), you can add a new transform by changing the name of your existing transform to a new name, and at the same time updating your lambda or TypeScript module.  From the Flow's perspective, this is equivalent to deleting the old transform and adding a new one. This will backfill over the source collection again with the updated SQL statement. ","version":"Next","tagName":"h2"},{"title":"Troubleshoot a task with flowctl","type":0,"sectionRef":"#","url":"/guides/flowctl/troubleshoot-task/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Troubleshoot a task with flowctl","url":"/guides/flowctl/troubleshoot-task/#prerequisites","content":" To complete this workflow, you need:  An Estuary account flowctl installed locally  ","version":"Next","tagName":"h2"},{"title":"Print task logs​","type":1,"pageTitle":"Troubleshoot a task with flowctl","url":"/guides/flowctl/troubleshoot-task/#print-task-logs","content":" Authorize flowctl. Go to the CLI-API tab of the web app and copy your access token. Run flowctl auth token --token &lt;paste-token-here&gt; Identify the name of the failing task in the web app; for example myOrg/marketing/leads. Use the tables on the Captures or Materializations pages of the web app to do so. Run flowctl logs --task &lt;task-name&gt;. You have several options to get more specific. For example: flowctl logs --task myOrg/marketing/leads --follow — If the task hasn't failed, continuously print logs as they're generated. flowctl logs --task myOrg/marketing/leads --since 1h — Print logs from approximately the last hour. The actual output window is approximate and may somewhat exceed this time boundary. You may use any time, for example 10m and 1d.  ","version":"Next","tagName":"h2"},{"title":"Change log level​","type":1,"pageTitle":"Troubleshoot a task with flowctl","url":"/guides/flowctl/troubleshoot-task/#change-log-level","content":" If your logs aren't providing enough detail, you can change the log level.  Flow offers several log levels. From least to most detailed, these are:  errorwarninfo (default)debugtrace  Follow the guide to edit a specification with flowctl. Working in your local specification file, add the shards stanza to the capture or materialization specification: myOrg/marketing/leads: shards: logLevel: debug endpoint: {} Finish the workflow as described, re-publishing the task.  Learn more about working with logs ","version":"Next","tagName":"h2"},{"title":"Edit a Flow specification locally","type":0,"sectionRef":"#","url":"/guides/flowctl/edit-specification-locally/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Edit a Flow specification locally","url":"/guides/flowctl/edit-specification-locally/#prerequisites","content":" To complete this workflow, you need:  An Estuary account flowctl installed locally One or more published Flow entities. (To edit unpublished drafts, use this guide.)  ","version":"Next","tagName":"h2"},{"title":"Pull specifications locally​","type":1,"pageTitle":"Edit a Flow specification locally","url":"/guides/flowctl/edit-specification-locally/#pull-specifications-locally","content":" Every entity (including active tasks, like captures and materializations, and static collections) has a globally unique name in the Flow catalog.  For example, a given Data Flow may comprise:  A capture, myOrg/marketing/leads, which writes to...Two collections, myOrg/marketing/emailList and myOrg/marketing/socialMedia, which are materialized as part of...A materialization, myOrg/marketing/contacts.  Using these names, you'll identify and pull the relevant specifications for editing.  Authorize flowctl. Go to the CLI-API tab of the web app and copy your access token. Run flowctl auth token --token &lt;paste-token-here&gt; Determine which entities you need to pull from the catalog. You can: Check the web app's Sources, Collections, and Destinations pages. All published entities to which you have access are listed and can be searched. Run flowctl catalog list. This command returns a complete list of entities to which you have access. You can refine by specifying a --prefix and filter by entity type: --captures, --collections, --materializations, or --tests. From the above example, flowctl catalog list --prefix myOrg/marketing --captures --materializations would returnmyOrg/marketing/leads and myOrg/marketing/contacts. Pull the specifications you need by running flowctl catalog pull-specs: Pull one or more specifications by name, for example: flowctl catalog pull-specs --name myOrg/marketing/emailList Pull a group of specifications by prefix or type filter, for example: flowctl catalog pull-specs --prefix myOrg/marketing --collections The source files are written to your current working directory. Browse the source files.  flowctl pulls specifications into subdirectories organized by entity name, and specifications sharing a catalog prefix are written to the same YAML file.  Regardless of what you pull, there is always a top-level file called flow.yaml that imports all other nested YAML files. These, in turn, contain the entities' specifications.  ","version":"Next","tagName":"h2"},{"title":"Edit source files and re-publish specifications​","type":1,"pageTitle":"Edit a Flow specification locally","url":"/guides/flowctl/edit-specification-locally/#edit-source-files-and-re-publish-specifications","content":" Next, you'll complete your edits, test that they were performed correctly, and re-publish everything.  Open the YAML files that contain the specification you want to edit. Make changes. For guidance on how to construct Flow specifications, see the documentation for the task type: CapturesCollectionsMaterializationsDerivationsTests When you're done, you can test your changes:flowctl catalog test --source flow.yaml You'll almost always use the top-level flow.yaml file as the source here because it imports all other Flow specifications in your working directory. Once the test has passed, you can publish your specifications. Re-publish all the specifications you pulled: flowctl catalog publish --source flow.yaml Again you'll almost always want to use the top-level flow.yaml file. If you want to publish only certain specifications, you can provide a path to a different file. Return to the web app or use flowctl catalog list to check the status of the entities you just published. Their publication time will be updated to reflect the work you just did.  If you're not satisfied with the results of your edits, repeat the process iteratively until you are. ","version":"Next","tagName":"h2"},{"title":"How to generate an Estuary Flow Refresh Token","type":0,"sectionRef":"#","url":"/guides/how_to_generate_refresh_token/","content":"How to generate an Estuary Flow Refresh Token To generate a Refresh Token, navigate to the Admin page, then head over to the CLI-API section. Press the Generate token button to bring up the modal where you are able to give your token a name. Choose a name that you will be able to use to identify which service your token is meant to give access to.","keywords":"","version":"Next"},{"title":"Getting Started With flowctl","type":0,"sectionRef":"#","url":"/guides/get-started-with-flowctl/","content":"Getting Started With flowctl After your account has been activated through the web app, you can begin to work with your data flows from the command line. This is not required, but it enables more advanced workflows or might simply be your preference. Flow has a single binary, flowctl. flowctl is available for: Linux x86-64. All distributions are supported.MacOS 11 (Big Sur) or later. Both Intel and M1 chips are supported. To install, copy and paste the appropriate script below into your terminal. This will download flowctl, make it executable, and add it to your PATH. For Linux: sudo curl -o /usr/local/bin/flowctl -L 'https://github.com/estuary/flow/releases/latest/download/flowctl-x86_64-linux' &amp;&amp; sudo chmod +x /usr/local/bin/flowctl For Mac: sudo curl -o /usr/local/bin/flowctl -L 'https://github.com/estuary/flow/releases/latest/download/flowctl-multiarch-macos' &amp;&amp; sudo chmod +x /usr/local/bin/flowctl Alternatively, Mac users can install with Homebrew: brew tap estuary/flowctl brew install flowctl flowctl isn't currently available for Windows. For Windows users, we recommend running the Linux version inside WSL, or using a remote development environment. The flowctl source files are also on GitHub here. Once you've installed flowctl and are ready to begin working, authenticate your session using an access token. Ensure that you have an Estuary account and have signed into the Flow web app before. In the terminal of your local development environment, run: flowctl auth login In a browser window, the web app opens to the CLI-API tab. Copy the access token. Return to the terminal, paste the access token, and press Enter. The token will expire after a predetermined duration. Repeat this process to re-authenticate. Next steps flowctl concepts: Learn more about using flowctl.User guides: Check out some of the detailed user guides to see flowctl in action.","keywords":"","version":"Next"},{"title":"System-specific Data Flows","type":0,"sectionRef":"#","url":"/guides/system-specific-dataflows/","content":"System-specific Data Flows The guides in this section cover popular Estuary Flow use cases. Each guide walks you through the process of capturing data from a specific source system and materializing it to a specific destination. These are supplemental to the main guide to create a Data Flow. If you don't see your exact Data Flow here, use the main guide and the connector referenceto mix and match your required source and destination systems.","keywords":"","version":"Next"},{"title":"Google Cloud Firestore to Snowflake","type":0,"sectionRef":"#","url":"/guides/system-specific-dataflows/firestore-to-dwh/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Google Cloud Firestore to Snowflake","url":"/guides/system-specific-dataflows/firestore-to-dwh/#prerequisites","content":" You'll need:  (Recommended) understanding of the basic Flow concepts. Access to the Flow web application through an Estuary account. If you don't have one, visit the web app to register for free. A Firestore database that contains the data you'd like to move to Snowflake. You create this as part of a Google Firebase project. A Google service account with: Read access to your Firestore database, via roles/datastore.viewer. You can assign this role when you create the service account, or add it to an existing service account. A generated JSON service account key for the account. A Snowflake account with: A target database, schema, and virtual warehouse; and a user with a role assigned that grants the appropriate access levels to these resources.You can use a script to quickly create all of these items. Have these details on hand for setup with Flow. The account identifier and host URL noted. The URL is formatted using the account identifier. For example, you might have the account identifier orgname-accountname.snowflakecomputing.com.  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Google Cloud Firestore to Snowflake","url":"/guides/system-specific-dataflows/firestore-to-dwh/#introduction","content":" In Estuary Flow, you create Data Flows to transfer data from source systems to destination systems in real time. In this use case, your source is a Google Cloud Firestore NoSQL database and your destination is a Snowflake data warehouse.  After following this guide, you'll have a Data Flow that comprises:  A capture, which ingests data from FirestoreSeveral collections, cloud-backed copies of Firestore collections in the Flow systemA materialization, which pushes the collections to Snowflake  The capture and materialization rely on plug-in components called connectors. We'll walk through how to configure the Firestore and Snowflake connectors to integrate these systems with Flow.  ","version":"Next","tagName":"h2"},{"title":"Capture from Firestore​","type":1,"pageTitle":"Google Cloud Firestore to Snowflake","url":"/guides/system-specific-dataflows/firestore-to-dwh/#capture-from-firestore","content":" You'll first create a capture to connect to your Firestore database, which will yield one Flow collection for each Firestore collection in your database.  Go to the Flow web application at dashboard.estuary.dev and sign in using the credentials provided by your Estuary account manager. Click the Sources tab and choose New Capture. Find the Google Firestore tile and click Capture. A form appears with the properties required for a Firestore capture. Type a name for your capture. Your capture name must begin with a prefix to which you have access. In the Name field, use the drop-down to select your prefix. Append a unique capture name after the / to create the full name, for example, acmeCo/myFirestoreCapture. Fill out the required properties for Firestore. Database: Flow can autodetect the database name, but you may optionally specify it here. This is helpful if the service account used has access to multiple Firebase projects. Your database name usually follows the format projects/$PROJECTID/databases/(default). Credentials: The JSON service account key created per the prerequisites. Click Next. Flow uses the provided configuration to initiate a connection with Firestore. It maps each available Firestore collection to a possible Flow collection. It also generates minimal schemas for each collection. You can use the Source Collections browser to remove or modify collections. You'll have the chance to tighten up each collection's JSON schema later, when you materialize to Snowflake.  tip If you make any changes to collections, click Next again.  Once you're satisfied with the collections to be captured, click Save and Publish. You'll see a notification when the capture publishes successfully. The data currently in your Firestore database has been captured, and future updates to it will be captured continuously. Click Materialize Collections to continue.  ","version":"Next","tagName":"h2"},{"title":"Materialize to Snowflake​","type":1,"pageTitle":"Google Cloud Firestore to Snowflake","url":"/guides/system-specific-dataflows/firestore-to-dwh/#materialize-to-snowflake","content":" Next, you'll add a Snowflake materialization to connect the captured data to its destination: your data warehouse.  Locate the Snowflake tile and click Materialization. A form appears with the properties required for a Snowflake materialization. Choose a unique name for your materialization like you did when naming your capture; for example, acmeCo/mySnowflakeMaterialization. Fill out the required properties for Snowflake (you should have most of these handy from the prerequisites). Host URLAccountUserPasswordDatabaseSchemaWarehouse: optionalRole: optional Click Next. Flow uses the provided configuration to initiate a connection to Snowflake. You'll be notified if there's an error. In that case, fix the configuration form or Snowflake setup as needed and click Next to try again. Once the connection is successful, the Endpoint Config collapses and the Source Collections browser becomes prominent. It shows the collections you captured previously. Each of them will be mapped to a Snowflake table. In the Source Collections browser, optionally change the name in the Table field for each collection. These will be the names of the output tables in Snowflake. For each table, choose whether to enable delta updates. For each collection, apply a stricter schema to be used for the materialization. Firestore has a flat data structure. To materialize data effectively from Firestore to Snowflake, you should apply a schema that can translate to a table structure. Flow's Schema Inference tool can help. In the Source Collections browser, choose a collection and click its Collection tab. Click Schema Inference The Schema Inference window appears. Flow scans the data in your collection and infers a new schema, called the readSchema, to use for the materialization. Review the new schema and click Apply Inferred Schema. Click Next to apply the changes you made to collections. Click Save and Publish. You'll see a notification when the full Data Flow publishes successfully.  ","version":"Next","tagName":"h2"},{"title":"What's next?​","type":1,"pageTitle":"Google Cloud Firestore to Snowflake","url":"/guides/system-specific-dataflows/firestore-to-dwh/#whats-next","content":" Your Data Flow has been deployed, and will run continuously until it's stopped. Updates in your Firestore database will be reflected in your Snowflake table as they occur.  You can advance your Data Flow by adding a derivation. Derivations are real-time data transformations. See the guide to create a derivation. ","version":"Next","tagName":"h2"},{"title":"Amazon S3 to Snowflake","type":0,"sectionRef":"#","url":"/guides/system-specific-dataflows/s3-to-snowflake/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon S3 to Snowflake","url":"/guides/system-specific-dataflows/s3-to-snowflake/#prerequisites","content":" You'll need:  (Recommended) understanding of the basic Flow concepts. Access to the Flow web application through an Estuary account. If you don't have one, visit the web app to register for free. An S3 bucket that contains the data you'd like to move to Snowflake. For public buckets, verify that the access policy allows anonymous reads. For buckets accessed by a user account, you'll need the AWS access key and secret access key for the user. See the AWS blog for help finding these credentials. A Snowflake account with: A target database, schema, and virtual warehouse; and a user with a role assigned that grants the appropriate access levels to these resources.You can use a script to quickly create all of these items. Have these details on hand for setup with Flow. The account identifier and host URL noted. The URL is formatted using the account identifier. For example, you might have the account identifier orgname-accountname.snowflakecomputing.com.  ","version":"Next","tagName":"h2"},{"title":"Introduction​","type":1,"pageTitle":"Amazon S3 to Snowflake","url":"/guides/system-specific-dataflows/s3-to-snowflake/#introduction","content":" In Estuary Flow, you create Data Flows to transfer data from source systems to destination systems in real time. In this use case, your source is an Amazon S3 bucket and your destination is a Snowflake data warehouse.  After following this guide, you'll have a Data Flow that comprises:  A capture, which ingests data from S3A collection, a cloud-backed copy of that data in the Flow systemA materialization, which pushes the data to Snowflake  The capture and materialization rely on plug-in components called connectors. We'll walk through how to configure the S3 and Snowflake connectors to integrate these systems with Flow.  ","version":"Next","tagName":"h2"},{"title":"Capture from S3​","type":1,"pageTitle":"Amazon S3 to Snowflake","url":"/guides/system-specific-dataflows/s3-to-snowflake/#capture-from-s3","content":" You'll first create a capture to connect to your S3 bucket, which will yield one or more Flow collections.  Go to the Flow web application at dashboard.estuary.dev and sign in using the credentials provided by your Estuary account manager. Click the Sources tab and choose New Capture. Find the Amazon S3 tile and click Capture. A form appears with the properties required for an S3 capture. Type a name for your capture. Your capture name must begin with a prefix to which you have access. In the Name field, use the drop-down to select your prefix. Append a unique capture name after the / to create the full name, for example, acmeCo/myS3Capture. Fill out the required properties for S3. AWS Access Key ID and AWS Secret Access Key: Required for private buckets. AWS Region and Bucket: These are listed in your S3 console. Prefix: You might organize your S3 bucket using prefixes, which emulate a directory structure. To capture only from a specific prefix, add it here. Match Keys: Filters to apply to the objects in the S3 bucket. If provided, only data whose absolute path matches the filter will be captured. For example, *\\.json will only capture JSON files. See the S3 connector documentation for information on advanced fields and parser settings. (You're unlikely to need these for most use cases.) Click Next. Flow uses the provided configuration to initiate a connection to S3. It generates a permissive schema and details of the Flow collection that will store the data from S3. You'll have the chance to tighten up each collection's JSON schema later, when you materialize to Snowflake. Click Save and publish. You'll see a notification when the capture publishes successfully. The data currently in your S3 bucket has been captured, and future updates to it will be captured continuously. Click Materialize Collections to continue.  ","version":"Next","tagName":"h2"},{"title":"Materialize to Snowflake​","type":1,"pageTitle":"Amazon S3 to Snowflake","url":"/guides/system-specific-dataflows/s3-to-snowflake/#materialize-to-snowflake","content":" Next, you'll add a Snowflake materialization to connect the captured data to its destination: your data warehouse.  Locate the Snowflake tile and click Materialization. A form appears with the properties required for a Snowflake materialization. Choose a unique name for your materialization like you did when naming your capture; for example, acmeCo/mySnowflakeMaterialization. Fill out the required properties for Snowflake (you should have most of these handy from the prerequisites). Host URLAccountUserPasswordDatabaseSchemaWarehouse: optionalRole: optional Click Next. Flow uses the provided configuration to initiate a connection to Snowflake. You'll be notified if there's an error. In that case, fix the configuration form or Snowflake setup as needed and click Next to try again. Once the connection is successful, the Endpoint Config collapses and the Source Collections browser becomes prominent. It shows the collection you captured previously, which will be mapped to a Snowflake table. In the Collection Selector, optionally change the name in the Table field. This will be the name of the output table in Snowflake. Choose whether to enable delta updates. Apply a stricter schema to the collection for the materialization. S3 has a flat data structure. To materialize this data effectively to Snowflake, you should apply a schema that can translate to a table structure. Flow's Schema Inference tool can help. In the Source Collections browser, click the collection's Collection tab. Click Schema Inference The Schema Inference window appears. Flow scans the data in your collection and infers a new schema, called the readSchema, to use for the materialization. Review the new schema and click Apply Inferred Schema. Click Next to apply the changes you made to the collection. Click Save and Publish. You'll see a notification when the full Data Flow publishes successfully.  ","version":"Next","tagName":"h2"},{"title":"What's next?​","type":1,"pageTitle":"Amazon S3 to Snowflake","url":"/guides/system-specific-dataflows/s3-to-snowflake/#whats-next","content":" Your Data Flow has been deployed, and will run continuously until it's stopped. Updates in your S3 bucket will be reflected in your Snowflake table as they occur.  You can advance your Data Flow by adding a derivation. Derivations are real-time data transformations. See the guide to create a derivation. ","version":"Next","tagName":"h2"},{"title":"Allowlisting IP Addresses for Estuary Flow","type":0,"sectionRef":"#","url":"/reference/allow-ip-addresses/","content":"","keywords":"","version":"Next"},{"title":"IP Addresses to Allowlist​","type":1,"pageTitle":"Allowlisting IP Addresses for Estuary Flow","url":"/reference/allow-ip-addresses/#ip-addresses-to-allowlist","content":" Ensure that the following IP addresses are allowlisted on both the source and destination systems that interact with Estuary Flow:  34.121.207.12835.226.75.13534.68.62.148 ","version":"Next","tagName":"h2"},{"title":"Authorizing users and authenticating with Flow","type":0,"sectionRef":"#","url":"/reference/authentication/","content":"","keywords":"","version":"Next"},{"title":"Subjects, objects, and inherited capabilities​","type":1,"pageTitle":"Authorizing users and authenticating with Flow","url":"/reference/authentication/#subjects-objects-and-inherited-capabilities","content":" The entity to which you grant a capability is called the subject, and the entity over which access is granted is called the object. The subject can be either a user or a prefix, and the object is always a prefix. This allows subjects to inherit nested capabilities, so long as they are granted admin.  For example, user X of Acme Co has admin access to the acmeCo/ prefix, and user Y has write access. A third party has granted acmeCo/ read access to shared data at outside-org/acmeCo-share/. User X automatically inherits read access to outside-org/acmeCo-share/, but user Y does not.  ","version":"Next","tagName":"h2"},{"title":"Default authorization settings​","type":1,"pageTitle":"Authorizing users and authenticating with Flow","url":"/reference/authentication/#default-authorization-settings","content":" When you first sign up to use Flow, your organization is provisioned a prefix, and your username is granted admin access to the prefix. Your prefix is granted write access to itself and read access to its logs, which are stored under a unique sub-prefix of the global ops/ prefix.  Using the same example, say user X signs up on behalf of their company, AcmeCo. User X is automatically granted admin access to the acmeCo/ prefix.acmeCo/, in turn, has write access to acmeCo/ and read access to ops/acmeCo/.  As more users and prefixes are added, admins can provision capabilities using the CLI.  ","version":"Next","tagName":"h2"},{"title":"Authenticating Flow in the web app​","type":1,"pageTitle":"Authorizing users and authenticating with Flow","url":"/reference/authentication/#authenticating-flow-in-the-web-app","content":" You must sign in to begin a new session using the Flow web application. For the duration of the session, you'll be able to perform actions depending on the capabilities granted to the user profile.  You can view the capabilities currently provisioned in your organization on the Admin tab.  ","version":"Next","tagName":"h2"},{"title":"Authenticating Flow using the CLI​","type":1,"pageTitle":"Authorizing users and authenticating with Flow","url":"/reference/authentication/#authenticating-flow-using-the-cli","content":" You can use the flowctl CLI to work with your organization's catalogs and drafts in your local development environment.  To authenticate a local development session using the CLI, do the following:  Ensure that you have an Estuary account and have signed into the Flow web app before. In the terminal of your local development environment, run: flowctl auth login In a browser window, the web app opens to the CLI-API tab. Copy the access token. Return to the terminal, paste the access token, and press Enter.  The token will expire after a predetermined duration. Repeat this process to re-authenticate.  ","version":"Next","tagName":"h2"},{"title":"Provisioning capabilities​","type":1,"pageTitle":"Authorizing users and authenticating with Flow","url":"/reference/authentication/#provisioning-capabilities","content":" As an admin, you can provision capabilities using the CLI with the subcommands of flowctl auth roles.  For example:  flowctl auth roles list returns a list of all currently provisioned capabilities flowctl auth roles grant --object-role=acmeCo/ --capability=admin --subject-user-id=userZ grants user Z admin access to acmeCo flowctl auth roles revoke --object-role=outside-org/acmeCo-share/ --capability=read --subject-role=acmeCo/ would be used by an admin of outside-orgto revoke acmeCo/'s read access to outside-org/acmeCo-share/.  You can find detailed help for all subcommands using the --help or -h flag. ","version":"Next","tagName":"h2"},{"title":"Schema evolution","type":0,"sectionRef":"#","url":"/guides/schema-evolution/","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Schema evolution","url":"/guides/schema-evolution/#introduction","content":" Flow collections serve not only as your real-time data storage, but also as a contract between tasks that produce and consume their data. Captures are producers, materializations are consumers, and derivations can act as either.  This contract helps prevent data loss and error in your Data Flows, and is defined in terms of the collection specification, or spec, which includes:  The JSON schemaThe collection keyProjections, if any  There are many reasons a collection spec might change. Often, it's due to a change in the source data. Regardless, you'll need to make changes to downstream tasks — most often, materializations — to avoid errors.  ","version":"Next","tagName":"h2"},{"title":"Schema evolution scenarios​","type":1,"pageTitle":"Schema evolution","url":"/guides/schema-evolution/#schema-evolution-scenarios","content":" This guide is broken down into sections for different common scenarios, depending on which properties of the collection spec have changed.  The key pointers have changedThe logical partitioning configuration has changedThe schema (or readSchema if defined separately) has changed A new field is addedA field's data type has changedA field was removed  info There are a variety of reasons why these properties may change, and also different mechanisms for detecting changes in source data. In general, it doesn't matter why the collection spec has changed, only what has changed. However, AutoDiscovers are able to handle some of these scenarios automatically. Where applicable, AutoDiscover behavior will be called out under each section.  ","version":"Next","tagName":"h2"},{"title":"Re-creating a collection​","type":1,"pageTitle":"Schema evolution","url":"/guides/schema-evolution/#re-creating-a-collection","content":" Scenario: the key pointer or logical partitioning configurations have changed.  The key of a Flow collection cannot be changed after the collection is created. The same is true of the logical partitioning, which also cannot be changed after the collection is created.  If you need to change either of those parts of a collection spec, you'll need to create a new collection and update the bindings of any captures or materializations that reference the old collection.  Web app workflow  If you're working in the Flow web app, you'll see an error message and an option to re-create the collection as shown in the example below.    Click Apply to re-create the collection and update any tasks that reference the old collection with the new name.  flowctl workflow:  If you're working with flowctl, you'll need to re-create the collection manually in your flow.yaml file. You must also update any captures or materializations that reference it. For example, say you have a data flow defined by the following specs:  captures: acmeCo/inventory/source-postgres: endpoint: connector: image: ghcr.io/estuary/source-postgres:v1 config: encrypted-pg-config.sops.yaml bindings: - resource: namespace: public stream: anvils mode: Normal target: acmeCo/inventory/anvils collections: acmeCo/inventory/anvils: key: [/sku] schema: type: object properties: sku: { type: string } warehouse_id: { type: string } quantity: { type: integer } required: [sku, warehouse_id, quantity] materializations: acmeCo/data-warehouse/materialize-snowflake: endpoint: connector: image: ghcr.io/estuary/materialize-snowflake:v1 config: encrypted-snowflake-config.sops.yaml bindings: - source: acmeCo/inventory/anvils resource: table: anvils schema: inventory   To change the collection key, you would update the YAML like so. Note the capture target, collection name, and materialization source.  captures: acmeCo/inventory/source-postgres: endpoint: connector: image: ghcr.io/estuary/source-postgres:v1 config: encrypted-pg-config.sops.yaml bindings: - resource: namespace: public stream: anvils mode: Normal backfill: 1 target: acmeCo/inventory/anvils_v2 collections: acmeCo/inventory/anvils_v2: key: [/sku] schema: type: object properties: sku: { type: string } warehouse_id: { type: string } quantity: { type: integer } required: [sku, warehouse_id, quantity] materializations: acmeCo/data-warehouse/materialize-snowflake: endpoint: connector: image: ghcr.io/estuary/materialize-snowflake:v1 config: encrypted-snowflake-config.sops.yaml bindings: - source: acmeCo/inventory/anvils_v2 backfill: 1 resource: table: anvils schema: inventory   The existing acmeCo/inventory/anvils collection will not be modified and will remain in place, but won't update because no captures are writing to it.  Also note the addition of the backfill property. If the backfill property already exists, just increment its value. For the materialization, this will ensure that the destination table in Snowflake gets dropped and re-created, and that the materialization will backfill it from the beginning. In the capture, it similarly causes it to start over from the beginning, writing the captured data into the new collection.  Auto-Discovers:  If you enabled the option to Automatically keep schemas up to date (autoDiscover) and selected Breaking change re-versions collections (evolveIncompatibleCollections) for the capture, this evolution would be performed automatically.  ","version":"Next","tagName":"h3"},{"title":"A new field is added​","type":1,"pageTitle":"Schema evolution","url":"/guides/schema-evolution/#a-new-field-is-added","content":" Scenario: this is one way in which the schema can change.  When a new field appears in the collection schema, it may automatically be added to any materializations that use recommended fields. Recommended fields are enabled by default in each binding. See the materialization docs for more info about how to enable or disable recommended fields.  When recommended fields are enabled, new fields are added automatically if they meet the criteria for the particular materialization connector. For example, scalar fields (strings, numbers, and booleans) are considered &quot;recommended&quot; fields when materializing to database tables.  If your materialization binding is set to recommended: false, or if the new field is not recommended, you can manually add it to the materialization.  To manually add a field:  In the Flow web app, edit the materialization, find the affected binding, and click Show Fields.Using flowctl, add the field to fields.include in the materialization specification as shown here.  info Newly added fields will not be set for rows that have already been materialized. If you want to ensure that all rows have the new field, just increment the backfill counter in the affected binding to have it re-start from the beginning.  ","version":"Next","tagName":"h3"},{"title":"A field's data type has changed​","type":1,"pageTitle":"Schema evolution","url":"/guides/schema-evolution/#a-fields-data-type-has-changed","content":" Scenario: this is one way in which the schema can change.  When a field's data type has changed, the effect on your materialization depends on the specific connector you're using.  warning Note that these restrictions only apply to fields that are actively being materialized. If a field is excluded from your materialization, either explicitly or because it's not recommended, then the data types may change in any way. Regardless of whether the field is materialized or not, it must still pass schema validation tests. Therefore, you must still make sure existing data remains valid against the new schema. For example, if you changed excluded_field: { type: string } to type: integer while there was existing data with string values, your materialization would fail due to a schema validation error.  Database and data warehouse materializations tend to be somewhat restrictive about changing column types. They typically only allow dropping NOT NULL constraints. This means that you can safely change a schema to make a required field optional, or to add null as a possible type, and the materialization will continue to work normally. Most other types of changes will require materializing into a new table.  The best way to find out whether a change is acceptable to a given connector is to run a test or attempt to re-publish. Failed attempts to publish won't affect any tasks that are already running.  Web app workflow  If you're working in the Flow web app, and attempt to publish a change that's unacceptable to the connector, you'll see an error message and an offer to increment the necessary backfill counters, or, in rare cases, to re-create the collection.  Click Apply to to accept this solution and continue to publish.  flowctl workflow  If you test or attempt to publish a change that's unacceptable to the connector, you'll see an error message pointing to the field that's changed. In most cases, you can work around the issue by manually updating the materialization to materialize into a new table.  For example, say you have a data flow defined by the following specs:  collections: acmeCo/inventory/anvils: key: [/sku] schema: type: object properties: sku: { type: string } quantity: { type: integer } description: { type: string } required: [sku, quantity] materializations: acmeCo/data-warehouse/materialize-snowflake: endpoint: connector: image: ghcr.io/estuary/materialize-snowflake:v1 config: encrypted-snowflake-config.sops.yaml bindings: - source: acmeCo/inventory/anvils backfill: 3 resource: table: anvils schema: inventory   Let's say the type of description was broadened to allow object values in addition to string. You'd update your specs as follows:  collections: acmeCo/inventory/anvils: key: [/sku] schema: type: object properties: sku: { type: string } quantity: { type: integer } description: { type: [string, object] } required: [sku, quantity] materializations: acmeCo/data-warehouse/materialize-snowflake: endpoint: connector: image: ghcr.io/estuary/materialize-snowflake:v1 config: encrypted-snowflake-config.sops.yaml bindings: - source: acmeCo/inventory/anvils backfill: 4 resource: table: anvils schema: inventory   Note that the only change was to increment the backfill counter. If the previous binding spec did not specify backfill, then just add backfill: 1.  This works because the type is broadened, so existing values will still validate against the new schema. If this were not the case, then you'd likely need to re-create the whole collection.  Auto-Discovers:  If you enabled the option to Automatically keep schemas up to date (autoDiscover) and selected Breaking change re-versions collections (evolveIncompatibleCollections) for the capture, this evolution would be performed automatically.  ","version":"Next","tagName":"h3"},{"title":"A field was removed​","type":1,"pageTitle":"Schema evolution","url":"/guides/schema-evolution/#a-field-was-removed","content":" Scenario: this is one way in which the schema can change.  Removing fields is generally allowed by all connectors, and does not require new tables or collections. Note that for database materializations, the existing column will not be dropped, and will just be ignored by the materialization going forward. A NOT NULL constraint would be removed from that column, but it will otherwise be left in place. ","version":"Next","tagName":"h3"},{"title":"Backfilling Data","type":0,"sectionRef":"#","url":"/reference/backfilling-data/","content":"","keywords":"","version":"Next"},{"title":"Preventing backfills​","type":1,"pageTitle":"Backfilling Data","url":"/reference/backfilling-data/#preventing-backfills","content":" Preventing backfills when possible can help save costs and computational resources. You may find it appropriate to skip the backfill, especially for extremely large datasets or tables.  In this case, many connectors allow you to turn off backfilling on a per-stream or per-table basis. See each individual connector's properties for details.  ","version":"Next","tagName":"h2"},{"title":"Preventing backfills during database upgrades​","type":1,"pageTitle":"Backfilling Data","url":"/reference/backfilling-data/#preventing-backfills-during-database-upgrades","content":" It is common to want to prevent backfills when performing database maintenance, as database upgrades can kick off a new backfill with Flow. Whether or not a database upgrade automatically performs a backfill depends on the database itself.  During an upgrade, some databases invalidate a replication slot, binlog position, CDC tables, or similar. As Flow relies on these methods to keep its place, upgrades will disrupt the Flow pipeline in these cases.  If a database upgrade will affect these or similar resources, you can manually prevent a backfill. If a database upgrade will not affect these resources, the Flow connector should simply resume when the upgrade completes.  For example, Postgres currently deletes or requires users to drop logical replication slots during a major version upgrade. To prevent a backfill during the upgrade, follow these steps:  Pause database writes so no further changes can occur. Monitor the current capture to ensure captures are fully up-to-date. These two steps ensure the connector won't miss any changes. Perform the database upgrade. Backfill each binding of the capture using the &quot;Only Changes&quot; backfill mode. This will not cause a full backfill. &quot;Backfilling&quot; the bindings resets the WAL (Write-Ahead Log) position for the capture, essentially resetting its place. The &quot;Only Changes&quot; mode will skip re-reading existing table content. Resume database writes.  ","version":"Next","tagName":"h3"},{"title":"Backfill modes​","type":1,"pageTitle":"Backfilling Data","url":"/reference/backfilling-data/#backfill-modes","content":" The connectors that use CDC (Change Data Capture) allow fine-grained control of backfills for individual tables. These bindings include a &quot;Backfill Mode&quot; dropdown in their resource configuration. This setting then translates to a mode field for that resource in the specification. For example:  &quot;bindings&quot;: [ { &quot;resource&quot;: { &quot;namespace&quot;: &quot;public&quot;, &quot;stream&quot;: &quot;tableName&quot;, &quot;mode&quot;: &quot;Only Changes&quot; }, &quot;target&quot;: &quot;Artificial-Industries/postgres/public/tableName&quot; } ]   warning In general, you should not change this setting. Make sure you understand your use case, such as preventing backfills.  The following modes are available:  Normal: backfills chunks of the table and emits all replication events regardless of whether they occur within the backfilled portion of the table or not. In Normal mode, the connector fetches key-ordered chunks of the table for the backfill while performing reads of the WAL. All WAL changes are emitted immediately, whether or not they relate to an unread portion of the table. Therefore, if a change is made, it shows up quickly even if its table is still backfilling. Precise: backfills chunks of the table and filters replication events in portions of the table which haven't yet been reached. In Precise mode, the connector fetches key-ordered chunks of the table for the backfill while performing reads of the WAL. Any WAL changes for portions of the table that have already been backfilled are emitted. In contrast to Normal mode, however, WAL changes are suppressed if they relate to a part of the table that hasn't been backfilled yet. WAL changes and backfill chunks get stitched together to produce a fully consistent logical sequence of changes for each key. For example, you are guaranteed to see an insert before an update or delete. Note that Precise backfill is not possible in some cases due to equality comparison challenges when using varying character encodings. Only Changes: skips backfilling the table entirely and jumps directly to replication streaming for the entire dataset. No backfill of the table content is performed at all. Only WAL changes are emitted. Without Primary Key: can be used to capture tables without any form of unique primary key. The connector uses an alternative physical row identifier (such as a Postgres ctid) to scan backfill chunks, rather than walking the table in key order. This mode lacks the exact correctness properties of the Normal backfill mode.  If you do not choose a specific backfill mode, Flow will default to an automatic mode. ","version":"Next","tagName":"h2"},{"title":"How to join two collections (TypeScript)","type":0,"sectionRef":"#","url":"/guides/howto_join_two_collections_typescript/","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"How to join two collections (TypeScript)","url":"/guides/howto_join_two_collections_typescript/#introduction","content":" This tutorial will show you how to implement a stateless transformation using TypeScript. You’ll learn how to implement a flow that matches orders to customers in real-time.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"How to join two collections (TypeScript)","url":"/guides/howto_join_two_collections_typescript/#prerequisites","content":" An Estuary accountflowctl installed and authenticatedA Google Drive account to work with Google SheetsDocker  ","version":"Next","tagName":"h3"},{"title":"Setting up your development environment​","type":1,"pageTitle":"How to join two collections (TypeScript)","url":"/guides/howto_join_two_collections_typescript/#setting-up-your-development-environment","content":" Example datasets for this tutorial are available in two Google Sheets: this one for orders and this one for customers. Make a copy of each to your own Drive so you’ll be able to test out the pipeline by adding, editing or removing records.  tip Ensure that the first row in your copied sheets is frozen. Otherwise, Flow will not pick up the headers as field names and instead assign field names in high-case alphabet order (A, B, C...). If the field names in your collection schema are not correct, the flowctl generate command later in the tutorial will fail with the error: /customer_id is prohibited from ever existing by the schema.  Customers table sample  customer_id\temail\tname\tphone101\tcustomer1@email.com\tJohn Doe\t123-456-7890 102\tcustomer2@email.com\tJane Smith\t987-654-3210 103\tcustomer3@email.com\tAlex Lee\t555-123-4567  Orders table sample  order_id\tcustomer_id\torder_date\ttotal_amount1\t101\t2024-05-10 8:00:00\t50 2\t102\t2024-05-09 12:00:00\t75.5 3\t103\t2024-05-08 15:30:00\t100.25  As you can see, both tables contain a field called customer_id. This is what we’re going to use as the key in our join operation. One customer can have multiple orders, but one order can only belong to one customer. There are also some customers without any orders.  Let’s say you want to see all customers and all of their orders in the results. This means you’ll want to implement a full outer join.  To create the collections in Estuary Flow, head over to the dashboard and create a new Google Sheet capture. Give it a name and add one of the previously copied sheet’s URLs as the “Spreadsheet Link”. Authenticate your Google account and Save and Publish the capture. Repeat this process for the other sheet, which should leave you with 2 collections.  You can take a look into each via the data preview window on the Collections page to verify that the sample data has already landed in Flow.    In order to implement transformations through derivations, you’ll need to set up your development environment. You’ll need a text editor and flowctl, the CLI-tool for Flow, installed on your machine.  To verify that you’re able to access Flow via flowctl, try executing the flowctl catalog list command.  If you see your new Google Sheets captures and their associated collections, you’re good to continue!  ","version":"Next","tagName":"h2"},{"title":"Writing the derivation​","type":1,"pageTitle":"How to join two collections (TypeScript)","url":"/guides/howto_join_two_collections_typescript/#writing-the-derivation","content":" Set up your folder structure to organize the resources required for the derivation:  Create a new working directory.Inside, create a flow.yaml file.  Inside your flow.yaml file, add the following contents, making sure to update the collection names accordingly:  collections: &lt;your-tenant&gt;/&lt;your-prefix&gt;/customers_with_orders: schema: description: &gt;- A document that represents the joined result of orders with customer information type: object properties: customer_id: type: string email: type: string name: type: string phone: type: string orders: type: array items: type: object properties: order_id: type: string reduce: strategy: merge key: - /order_id required: - customer_id reduce: strategy: merge key: - /customer_id derive: using: typescript: module: full-outer-join.flow.ts transforms: - name: fromOrders source: name: &lt;your-tenant&gt;/&lt;your-orders-collection/Sheet1 shuffle: key: - /customer_id - name: fromCustomers source: name: &lt;your-tenant&gt;/&lt;your-customers-collection&gt;/Sheet1 shuffle: key: - /customer_id   Let’s take a look at this in a bit more detail. Essentially, we define one collection which is a derivation that is the result of two transformations.  In the schema definition, we specify what structure we want the documents of the result collection to take on.   &lt;your-tenant&gt;/&lt;your-prefix&gt;/customers_with_orders: schema: description: &gt;- A document that represents the joined result of orders with customer information type: object properties: customer_id: type: string email: type: string name: type: string phone: type: string orders: type: array items: type: object properties: order_id: type: string reduce: strategy: merge key: - /order_id required: - customer_id reduce: strategy: merge key: - /customer_id   Because you are going to implement a 1-to-many join using the two source collections, it’s important to pay attention to what reduction strategy Flow uses.  There are two merge strategies defined here, one for the customers_with_orders collection and for the nested orders array.  note Merge reduces the left-hand side and right-hand side by recursively reducing shared document locations. The LHS and RHS must either both be objects, or both be arrays.  For the nested merge, you have to define a key, which is one or more JSON pointers that are relative to the reduced location. If both sides are arrays and a merge key is present, then a deep sorted merge of the respective items is done, as ordered by the key. In this case, setting it to order_id will cause the reduction to collect all orders for a given customer.  The derivation details are defined in the next section of the yaml:   derive: using: typescript: module: full-outer-join.flow.ts transforms: - name: fromOrders source: name: &lt;your-tenant&gt;/&lt;your-orders-collection&gt;/Sheet1 shuffle: key: - /customer_id - name: fromCustomers source: name: &lt;your-tenant&gt;/&lt;your-customers-collection&gt;/Sheet1 shuffle: key: - /customer_id   This tells Flow that the transformation code is defined in a TypeScript file called full-outer-join.flow.ts (which doesn’t exist – yet!) and that there are in fact two transformations that it expects, one for each source collection.  Shuffles let Flow identify the shard that should process a particular source document, in order to co-locate that processing with other documents it may need to know about.  Both transformations shuffle data on the same key. An important detail is that if a derivation has more than one transformation, the shuffle keys of all transformations must align with one another in terms of the extracted key types (string vs integer) as well as the number of components in a composite key.  Let’s generate the scaffolding for the derivation using flowctl.  flowctl generate --source flow.yaml   tip Is your Docker daemon running? Some flowctl commands, like flowctl generate, use a Docker container to help with more involved processes, like creating stub files. You don't need to set up a Docker container yourself for the command to work, but the Docker daemon should be running.  This command will create a few new files in your current working directory.  ➜ tree . ├── deno.json ├── flow.yaml ├── flow_generated │ └── typescript │ └── Dani │ └── join-tutorial-typescript │ └── customers_with_orders.ts └── full-outer-join.flow.ts 5 directories, 4 files   You won't need to modify the flow_generated folder or the deno.json file in this tutorial. If you take a look at the file that flowctl generated under flow_generated/typescript/&lt;your_tenant&gt;/&lt;your_prefix&gt;/customers_with_orders.ts you can see the types you are able to use in your transformations.  // Generated for published documents of derived collection customers_with_orders. export type Document = /* A document that represents the joined result of orders with customer information */ { customer_id: string; email?: string; name?: string; orders?: unknown[]; phone?: string; }; // Generated for read documents of sourced collection Sheet1. export type SourceFromOrders = { customer_id?: string; order_date?: string; order_id?: string; row_id: number; total_amount?: string; }; // Generated for read documents of sourced collection Sheet1. export type SourceFromCustomers = { customer_id?: string; email?: string; name?: string; phone?: string; row_id: number; };   Now, the actual transformation code will live in the following file: full-outer-join.flow.ts. Take a look at its contents.  import { IDerivation, Document, SourceFromOrders, SourceFromCustomers } from 'flow/Dani/join-tutorial-typescript/customers_with_orders.ts'; // Implementation for derivation Dani/join-tutorial-typescript/customers_with_orders. export class Derivation extends IDerivation { fromOrders(_read: { doc: SourceFromOrders }): Document[] { throw new Error(&quot;Not implemented&quot;); } fromCustomers(_read: { doc: SourceFromCustomers }): Document[] { throw new Error(&quot;Not implemented&quot;); } }   Helpfully, flowctl provides two skeleton functions. Update the function body to implement the filter functionality. Modify the Derivation class like this:  import { IDerivation, Document, SourceFromOrders, SourceFromCustomers } from 'flow/Dani/join-tutorial-typescript/customers_with_orders.ts'; // Implementation for derivation Dani/join-tutorial-typescript/customers_with_orders. export class Derivation extends IDerivation { fromOrders(_read: { doc: SourceFromOrders }): Document[] { return [{ customer_id: _read.doc.customer_id || &quot;&quot;, orders: [_read.doc], }]; } fromCustomers(_read: { doc: SourceFromCustomers }): Document[] { return [{ customer_id: _read.doc.customer_id || &quot;&quot;, email: _read.doc.email, name: _read.doc.name, phone: _read.doc.phone }]; } }   As you can see here, all we do is return the fields we need from each document. There’s no code required to define the actual “join” – all the heavy lifting is done in the reduction phase during materialization by the Flow runtime based on the schema you defined earlier.  Publish the derivation using flowctl:  flowctl catalog publish --source flow.yaml   After it’s successfully published, head over to the Flow dashboard to see the new collection.    If you take a look at the preview window at the bottom of the page, you might notice that the documents are not yet in their final, reduced form. As mentioned earlier, the reduction happens during materialization. Let's create one to show the results!  Head over to the materialization creation page, search for Google Sheets and configure a new connector. Create a fresh Google Sheet and copy its URL as the Spreadsheet Link.  In the third configuration step, select the derivation you created as the source collection.    Refresh &quot;Field Selection&quot; to populate the list of available fields from the collection schema. Make sure all your desired fields are selected, including the orders array.  After everything looks good, press the “Save and Publish” button in the top-right corner to provision your materialization connector.  And that’s it! Go check out the sheet you created to store the results. You should see all orders associated with their respective customer in the nested array.    To test the data flow, head over to the source “Orders” sheet, and add a new order for a customer. After a few seconds, you should see the new order added to the array of existing orders of the customer. Take a few minutes to play around with different actions as well: deleting an order, adding a customer, or editing details of either entity.  ","version":"Next","tagName":"h2"},{"title":"Wrapping up​","type":1,"pageTitle":"How to join two collections (TypeScript)","url":"/guides/howto_join_two_collections_typescript/#wrapping-up","content":" In this guide you learned how to write a TypeScript derivation to join two collections. After finishing with the tutorial, don’t forget to delete resources you don’t need anymore.  To learn more about joining collections with Estuary Flow, check out Streaming Joins Are Hard and How to Join Two Collections in Estuary Flow using SQL. ","version":"Next","tagName":"h2"},{"title":"How to transform data using TypeScript","type":0,"sectionRef":"#","url":"/guides/transform_data_using_typescript/","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"How to transform data using TypeScript","url":"/guides/transform_data_using_typescript/#introduction","content":" This tutorial will show you how to implement a stateless transformation using TypeScript. You’ll learn how to implement a flow that filters events coming from the live, real-time Wikipedia API.  ","version":"Next","tagName":"h2"},{"title":"Setting up your development environment​","type":1,"pageTitle":"How to transform data using TypeScript","url":"/guides/transform_data_using_typescript/#setting-up-your-development-environment","content":" In order to implement transformations through derivations, you’ll need to set up your development environment. You’ll need a text editor and flowctl, the CLI-tool for Flow installed on your machine. Check out the docs page on installation instructions.  Before continuing, sign in to the Estuary Flow dashboard, make sure you enable access to the Wikipedia demo. Using flowctl, quickly verify you are able to view the demo collections used in this guide.  Execute the below command to display the documents in the demo/wikipedia/recentchange-sampled collection:  note This collection is a 3% sample of the enormous demo/wikipedia/recentchange collection which contains millions of documents. Since the purpose of this tutorial is to demonstrate a proof of concept, we avoid publishing a derivation that processes hundreds of gigabytes of data.  flowctl collections read --collection demo/wikipedia/recentchange-sampled --uncommitted   If you see a stream of JSON documents on your terminal, you’re all good - feel free to cancel the process by pressing C^C.  Examine a sample JSON that lives in the demo collection, as this is the data you’ll be using as the input for our derivation.  { &quot;$schema&quot;: &quot;/mediawiki/recentchange/1.0.0&quot;, &quot;_meta&quot;: { &quot;file&quot;: &quot;recentchange&quot;, &quot;offset&quot;: 12837, &quot;uuid&quot;: &quot;f8f07d87-f5bf-11ee-8401-4fdf95f7b91a&quot; }, &quot;bot&quot;: false, &quot;comment&quot;: &quot;[[:File:Jeton. Ordinaire des guerres - btv1b10405460g (1 of 2).jpg]] added to category&quot;, &quot;id&quot;: 2468434138, &quot;meta&quot;: { &quot;domain&quot;: &quot;commons.wikimedia.org&quot;, &quot;dt&quot;: &quot;2024-04-08T15:52:13Z&quot;, &quot;id&quot;: &quot;d9e8698f-4eac-4262-a451-b7ca247e401c&quot;, &quot;offset&quot;: 5008568732, &quot;partition&quot;: 0, &quot;request_id&quot;: &quot;b5372124-63fa-45e1-b35e-86784f1692bc&quot;, &quot;stream&quot;: &quot;mediawiki.recentchange&quot;, &quot;topic&quot;: &quot;eqiad.mediawiki.recentchange&quot;, &quot;uri&quot;: &quot;https://commons.wikimedia.org/wiki/Category:Jetons&quot; }, &quot;namespace&quot;: 14, &quot;notify_url&quot;: &quot;https://commons.wikimedia.org/w/index.php?diff=866807860&amp;oldid=861559382&amp;rcid=2468434138&quot;, &quot;parsedcomment&quot;: &quot;&lt;a href=\\&quot;/wiki/File:Jeton._Ordinaire_des_guerres_-_btv1b10405460g_(1_of_2).jpg\\&quot; title=\\&quot;File:Jeton. Ordinaire des guerres - btv1b10405460g (1 of 2).jpg\\&quot;&gt;File:Jeton. Ordinaire des guerres - btv1b10405460g (1 of 2).jpg&lt;/a&gt; added to category&quot;, &quot;server_name&quot;: &quot;commons.wikimedia.org&quot;, &quot;server_script_path&quot;: &quot;/w&quot;, &quot;server_url&quot;: &quot;https://commons.wikimedia.org&quot;, &quot;timestamp&quot;: 1712591533, &quot;title&quot;: &quot;Category:Jetons&quot;, &quot;title_url&quot;: &quot;https://commons.wikimedia.org/wiki/Category:Jetons&quot;, &quot;type&quot;: &quot;categorize&quot;, &quot;user&quot;: &quot;DenghiùComm&quot;, &quot;wiki&quot;: &quot;commonswiki&quot; }   There’s a bunch of fields available, but as mentioned earlier, the scope of the transformation for this tutorial is limited to only one field, which lives nested inside the meta object.  { ... &quot;meta&quot;: { ... &quot;domain&quot;: &quot;commons.wikimedia.org&quot;, ... }, ... }   This field is composed of the various wikipedia domains that are used to serve different sites of the organization. This is what you’ll use as the base of the filter derivation. Let's say that the goal is to only keep events that originate from the English-language wikipedia page, which is running under the domain en.wikipedia.org.  ","version":"Next","tagName":"h2"},{"title":"Writing the derivation​","type":1,"pageTitle":"How to transform data using TypeScript","url":"/guides/transform_data_using_typescript/#writing-the-derivation","content":" Set up your folder structure so you can organize the resources required for the derivation. Create a working directory to follow along, and inside, create a flow.yaml file.  Inside your flow.yaml file, add the following contents:  --- collections: Dani/derivation-tutorial/recentchange-filtered-typescript: schema: recentchange-filtered.schema.yaml key: - /_meta/file - /_meta/offset derive: using: typescript: module: recentchange-filtered.ts transforms: - name: filter_values_typescript source: demo/wikipedia/recentchange-sampled shuffle: any      The Flow consists of just one collection, which is what you define here, called Dani/derivation-tutorial/recentchange-filtered-typescript.  Let’s go over this in a bit more detail.  First of all, the collection needs a schema. The schema of the incoming data (also called the “write” schema) is already defined by the demo, you only have to define the schema of the documents the transformation will output, which is the “read” schema.  Let’s define what the final documents will look like.  --- $schema: &quot;http://json-schema.org/draft-07/schema#&quot; properties: _meta: properties: file: type: string offset: type: integer uuid: type: string required: - file - offset type: object domain: type: string title: type: string user: type: string type: object   Save this schema as recentchange-filtered.schema.yaml next to your flow.yaml file.  As you can see, this schema definition includes a lot less fields than what is available in the incoming documents, this is expected, but if you wish to include more, this is where you would add them first.  In the collection yaml definition, the next section defines the key of the documents.  key: - /_meta/file - /_meta/offset   Every Flow collection must declare a key which is used to group its documents. Keys are specified as an array of JSON pointers to document locations. The important detail here is to know that a collection key instructs Flow how documents of a collection are to be reduced, such as while being materialized to an endpoint. For this tutorial, you are just going to reuse the key definition of the base collection.  The final section is where you specify that this collection is derived from another collection.  derive: using: typescript: module: recentchange-filtered.ts transforms: - name: filter_values_typescript source: demo/wikipedia/recentchange-sampled shuffle: any   Here you configure the name of the Typescript file that will contain the code for the actual transformation (don’t worry about the file not existing yet!) and give a name to the transformation.   The source: demo/wikipedia/recentchange-sampled property lets Flow know that the source collection is the demo collection from mentioned at in the beginning of the tutorial while shuffle tells Flow how to colocate documents while processing, which in this case is set to any, meaning source documents can be processed by any available compute.  Alright, the configuration required for the derivation is in place, all that’s left is to write some TypeScript!  ","version":"Next","tagName":"h2"},{"title":"The transformation code​","type":1,"pageTitle":"How to transform data using TypeScript","url":"/guides/transform_data_using_typescript/#the-transformation-code","content":" The next step is to use flowctl to generate TypeScript stubs you can use as aid when writing the transformation code.  Execute the following command:  flowctl generate --source flow.yaml   If everything went well, you’ll see a bunch of new files that flowctl generated for you in your working directory.  ➜ tree . ├── deno.json ├── flow.yaml ├── flow_generated │ └── typescript │ └── Dani │ └── derivation-tutorial │ └── recentchange-filtered-typescript.ts ├── recentchange-filtered.schema.yaml └── recentchange-filtered.ts 5 directories, 5 files   The folder flow_generated along with the deno.json file are two things you won’t have to modify during this tutorial. If you take a look at file that flowctl generated under flow_generated/typescript/&lt;your_working_directory&gt;/&lt;your_prefix&gt;/recentchange-filtered-typescript.ts you can see the types you are able to use in your transformations.  // Generated for published documents of derived collection Dani/derivation-tutorial/recentchange-filtered-typescript. export type Document = { &quot;_meta&quot;?: { file: string; offset: number; uuid?: string; }; domain?: string; title?: string; user?: string; };   Now, the actual transformation code will live in the following file: recentchange-filtered.ts. Take a look at the default contents.  import { IDerivation, Document, SourceFilterValuesTypescript } from 'flow/Dani/derivation-tutorial/recentchange-filtered-typescript.ts'; // Implementation for derivation Dani/derivation-tutorial/recentchange-filtered-typescript. export class Derivation extends IDerivation { filterValuesTypescript(_read: { doc: SourceFilterValuesTypescript }): Document[] { throw new Error(&quot;Not implemented&quot;); } }   Helpfully, flowctl provides a skeleton function. Update the function body to implement the filter functionality.  export class Derivation extends IDerivation { filterValuesTypescript(_read: { doc: SourceFilterValuesTypescript }): Document[] { if (_read.doc.meta?.domain == 'en.wikipedia.org') { return [{ &quot;_meta&quot;: { &quot;file&quot;: _read.doc._meta.file, &quot;offset&quot;: _read.doc._meta.offset, &quot;uuid&quot;: _read.doc._meta.uuid, }, &quot;domain&quot;: _read.doc.meta.domain, &quot;title&quot;: _read.doc.title, &quot;user&quot;: _read.doc.user }]; } else { return [] } } }   As you can see, only documents which contain the “en.wikipedia.org” domain are being returned, in addition to discarding most fields from the incoming record, and just keeping the ones defined in the collection schema.  ","version":"Next","tagName":"h2"},{"title":"Verify​","type":1,"pageTitle":"How to transform data using TypeScript","url":"/guides/transform_data_using_typescript/#verify","content":" You can use flowctl to quickly verify your derivation before publishing it. Use the preview command to get an idea of the resulting collections.  ➜ flowctl preview --source flow.yaml --name Dani/derivation-tutorial/recentchange-filtered-typescript {&quot;_meta&quot;:{&quot;file&quot;:&quot;recentchange&quot;,&quot;offset&quot;:13757,&quot;uuid&quot;:&quot;079296fe-f5c0-11ee-9401-4fdf95f7b91a&quot;},&quot;domain&quot;:&quot;en.wikipedia.org&quot;,&quot;title&quot;:&quot;Adoption&quot;,&quot;user&quot;:&quot;JustBeCool&quot;} {&quot;_meta&quot;:{&quot;file&quot;:&quot;recentchange&quot;,&quot;offset&quot;:13772,&quot;uuid&quot;:&quot;082ae4fc-f5c0-11ee-8801-4fdf95f7b91a&quot;},&quot;domain&quot;:&quot;en.wikipedia.org&quot;,&quot;title&quot;:&quot;Wikipedia:Teahouse&quot;,&quot;user&quot;:&quot;Subanark&quot;} {&quot;_meta&quot;:{&quot;file&quot;:&quot;recentchange&quot;,&quot;offset&quot;:13774,&quot;uuid&quot;:&quot;082ae4fc-f5c0-11ee-9001-4fdf95f7b91a&quot;},&quot;domain&quot;:&quot;en.wikipedia.org&quot;,&quot;title&quot;:&quot;Islandia, New York&quot;,&quot;user&quot;:&quot;204.116.28.102&quot;} ^C   As you can see, the output format matches the defined schema. The last step would be to publish your derivation to Flow, which you can also do using flowctl.  warning Publishing the derivation will initialize the transformation on the live, real-time Wikipedia stream, make sure to delete it after completing the tutorial.  flowctl catalog publish --source flow.yaml   After successfully publishing your derivation, head over to the Collections page on the Web UI and you will be able to see your derivation in action!    ","version":"Next","tagName":"h2"},{"title":"Wrapping up​","type":1,"pageTitle":"How to transform data using TypeScript","url":"/guides/transform_data_using_typescript/#wrapping-up","content":" In this guide you learned how to write your first stateless TypeScript derivation to filter data in a collection. ","version":"Next","tagName":"h2"},{"title":"Connectors","type":0,"sectionRef":"#","url":"/reference/Connectors/","content":"Connectors A current list and configuration details for Estuary's connectors can be found on the following pages: Capture connectorsMaterialization connectorsDekaf integrations You can learn more about how connectors work and how to use them in their conceptual documentation.","keywords":"","version":"Next"},{"title":"Configuring task shards","type":0,"sectionRef":"#","url":"/reference/Configuring-task-shards/","content":"","keywords":"","version":"Next"},{"title":"Properties​","type":1,"pageTitle":"Configuring task shards","url":"/reference/Configuring-task-shards/#properties","content":" Property\tTitle\tDescription\tType/disable\tDisable\tDisable processing of the task's shards.\tBoolean /logLevel\tLog level\tLog levels may currently be &quot;error&quot;, &quot;warn&quot;, &quot;info&quot;, &quot;debug&quot;, or &quot;trace&quot;. If not set, the effective log level is &quot;info&quot;.\tString /maxTxnDuration\tMaximum transaction duration\tThis duration upper-bounds the amount of time during which a transaction may process documents before it must initiate a commit. Note that it may take some additional time for the commit to complete after it is initiated. The shard may run for less time if there aren't additional ready documents for it to process. If not set, the maximum duration defaults to one second for captures and derivations, and 5 minutes for materializations.\tString /minTxnDuration\tMinimum transaction duration\tThis duration lower-bounds the amount of time during which a transaction must process documents before it must flush and commit. It may run for more time if additional documents are available. The default value is zero seconds.\tString  For more information about these controls and when you might need to use them, see:  TransactionsLog level  ","version":"Next","tagName":"h2"},{"title":"Sample​","type":1,"pageTitle":"Configuring task shards","url":"/reference/Configuring-task-shards/#sample","content":" materializations: acmeCo/snowflake-materialization: endpoint: connector: config: account: acmeCo database: acmeCo_db password: secret cloud_provider: aws region: us-east-1 schema: acmeCo_flow_schema user: snowflake_user warehouse: acmeCo_warehouse image: ghcr.io/estuary/materialize-snowflake:dev bindings: - resource: table: anvils source: acmeCo/anvils shards: logLevel: debug minTxnDuration: 30s maxTxnDuration: 4m  ","version":"Next","tagName":"h2"},{"title":"Airtable","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/airtable/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Airtable","url":"/reference/Connectors/capture-connectors/airtable/#prerequisites","content":" An active Airtable account  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Airtable","url":"/reference/Connectors/capture-connectors/airtable/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Airtable source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Airtable","url":"/reference/Connectors/capture-connectors/airtable/#properties","content":" Endpoint​  The following properties reflect the API Key authentication method.  Property\tTitle\tDescription\tType\tRequired/Default/api_key\tAPI Key\tAPI Key\tstring\tRequired /access_token\tPersonal Access Token\tThe Personal Access Token for the Airtable account.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Airtable project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Airtable","url":"/reference/Connectors/capture-connectors/airtable/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-airtable:dev config: access_token: &lt;secret&gt; api_key: &lt;secret&gt; bindings: - resource: stream: users syncMode: full_refresh target: ${PREFIX}/users {...}  ","version":"Next","tagName":"h3"},{"title":"Aircall","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/aircall/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Aircall","url":"/reference/Connectors/capture-connectors/aircall/#prerequisites","content":" To set up the Aircall connector, you need the following prerequisite:  Access Token: An access token acting as a bearer token is required for the connector to work. You can find the access token in the settings of Aircall.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Aircall","url":"/reference/Connectors/capture-connectors/aircall/#setup","content":" Follow the steps below to set up the Aircall connector.  Obtain an Aircall access token from the Aircall settings.  ","version":"Next","tagName":"h2"},{"title":"Set up the Aircall connector in Estuary Flow​","type":1,"pageTitle":"Aircall","url":"/reference/Connectors/capture-connectors/aircall/#set-up-the-aircall-connector-in-estuary-flow","content":" Log into your Estuary Flow account.In the left navigation bar, click on &quot;Captures&quot;. In the top-left corner, click &quot;Connector Search&quot;.Enter the name for the Aircall connector and select &quot;Aircall&quot; from the dropdown.Fill out the following endpoint configurations: api_id: The auto-generated ID.api_token: The access token obtained from Aircall settings.start_date: Date filter for eligible streams. Enter the desired start date.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Aircall","url":"/reference/Connectors/capture-connectors/aircall/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Aircall source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Aircall","url":"/reference/Connectors/capture-connectors/aircall/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/api_id\tAPI ID\tApp ID found at settings\tstring\tRequired /api_token\tAPI Token\tApp token found at settings\tstring\tRequired /start_date\tStart Date\tDate time filter for incremental filter, Specify which date to extract from.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Aircall project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Aircall","url":"/reference/Connectors/capture-connectors/aircall/#sample","content":" { &quot;properties&quot;: { &quot;start_date&quot;: { &quot;default&quot;: &quot;2023-01-01T00:00:00.000Z&quot;, &quot;format&quot;: null } } }   ","version":"Next","tagName":"h3"},{"title":"Supported Streams​","type":1,"pageTitle":"Aircall","url":"/reference/Connectors/capture-connectors/aircall/#supported-streams","content":" The Aircall connector supports the following streams:  callscompanycontactsnumberstagsuser_availablityusersteamswebhooks  ","version":"Next","tagName":"h2"},{"title":"API Method Example​","type":1,"pageTitle":"Aircall","url":"/reference/Connectors/capture-connectors/aircall/#api-method-example","content":" An example of an API method call for Aircall:  GET https://api.aircall.io/v1/numbers  ","version":"Next","tagName":"h2"},{"title":"Performance Considerations​","type":1,"pageTitle":"Aircall","url":"/reference/Connectors/capture-connectors/aircall/#performance-considerations","content":" The Aircall API currently uses v1. The connector defaults to using v1. ","version":"Next","tagName":"h2"},{"title":"Capture connectors","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/","content":"","keywords":"","version":"Next"},{"title":"Available capture connectors​","type":1,"pageTitle":"Capture connectors","url":"/reference/Connectors/capture-connectors/#available-capture-connectors","content":" ","version":"Next","tagName":"h2"},{"title":"Estuary connectors​","type":1,"pageTitle":"Capture connectors","url":"/reference/Connectors/capture-connectors/#estuary-connectors","content":" These connectors are created by Estuary. We prioritize high-scale technology systems for development.  All Estuary connectors capture data in real time, as it appears in the source system.  Airtable ConfigurationPackage - ghcr.io/estuary/source-airtable:dev AlloyDB ConfigurationPackage - ghcr.io/estuary/source-alloydb:dev Alpaca ConfigurationPackage - ghcr.io/estuary/source-alpaca:dev Amazon DocumentDB ConfigurationPackage - ghcr.io/estuary/source-amazon-documentdb:dev Amazon Dynamodb ConfigurationPackage - ghcr.io/estuary/source-dynamodb:dev Amazon Kinesis ConfigurationPackage — ghcr.io/estuary/source-kinesis:dev Amazon RDS SQL Server ConfigurationPackage - ghcr.io/estuary/source-amazon-rds-sqlserver:dev Amazon Redshift ConfigurationPackage - ghcr.io/estuary/source-redshift-batch:dev Amazon S3 ConfigurationPackage — ghcr.io/estuary/source-s3:dev Apache Kafka ConfigurationPackage — ghcr.io/estuary/source-kafka:dev Asana ConfigurationPackage - ghcr.io/estuary/source-asana:dev Azure Blob Storage ConfigurationPackage — ghcr.io/estuary/azure-blob-storage:dev Azure Cosmos DB ConfigurationPackage - ghcr.io/estuary/source-cosmosdb-mongodb:dev Azure SQL Server ConfigurationPackage - ghcr.io/estuary/source-azure-sqlserver:dev BigQuery ConfigurationPackage — ghcr.io/estuary/source-bigquery-batch:dev Braintree ConfigurationPackage - ghcr.io/estuary/source-braintree-native:dev Brevo ConfigurationPackage - ghcr.io/estuary/source-brevo:dev Criteo ConfigurationPackage - ghcr.io/estuary/source-criteo:dev Dropbox ConfigurationPackage - ghcr.io/estuary/source-dropbox:dev Facebook Marketing ConfigurationPackage - ghcr.io/estuary/source-facebook-marketing:dev Front ConfigurationPackage - ghcr.io/estuary/source-front:dev Genesys ConfigurationPackage - ghcr.io/estuary/source-genesys:dev GitHub ConfigurationPackage - ghcr.io/estuary/source-github:dev Gladly ConfigurationPackage - ghcr.io/estuary/source-gladly:dev Google Ads ConfigurationPackage - ghcr.io/estuary/source-google-ads:dev Google Cloud Storage ConfigurationPackage — ghcr.io/estuary/source-gcs:dev Google Firestore ConfigurationPackage - ghcr.io/estuary/source-firestore:dev Google Cloud Pub/Sub ConfigurationPackage — ghcr.io/estuary/source-google-pubsub:dev Google Cloud SQL Server ConfigurationPackage - ghcr.io/estuary/source-google-cloud-sql-sqlserver:dev Google Drive ConfigurationPackage - ghcr.io/estuary/source-google-drive:dev Google Sheets ConfigurationPackage - ghcr.io/estuary/source-google-sheets-native:dev HTTP file ConfigurationPackage - ghcr.io/estuary/source-http-file:dev HTTP ingest (webhook) ConfigurationPackage - ghcr.io/estuary/source-http-ingest:dev Hubspot (Real-Time) ConfigurationPackage - ghcr.io/estuary/source-hubspot-native:dev Intercom ConfigurationPackage - ghcr.io/estuary/source-intercom-native:dev Iterable ConfigurationPackage - ghcr.io/estuary/source-iterable:dev Iterate ConfigurationPackage - ghcr.io/estuary/source-iterate:dev Jira ConfigurationPackage - ghcr.io/estuary/source-jira-native:dev Klaviyo ConfigurationPackage - ghcr.io/estuary/source-klaviyo:dev LinkedIn Pages ConfigurationPackage - ghcr.io/estuary/source-linkedin-pages:dev MariaDB ConfigurationPackage - ghcr.io/estuary/source-mariadb:dev Microsoft SQL Server ConfigurationPackage - ghcr.io/estuary/source-sqlserver:dev MongoDB ConfigurationPackage - ghcr.io/estuary/source-mongodb:dev MySQL ConfigurationPackage - ghcr.io/estuary/source-mysql:dev NetSuite ConfigurationPackage - ghcr.io/estuary/source-netsuite:dev Pendo ConfigurationPackage - ghcr.io/estuary/source-pendo:dev PostgreSQL ConfigurationPackage — ghcr.io/estuary/source-postgres:dev Salesforce (for real-time data) ConfigurationPackage - ghcr.io/estuary/source-salesforce-next:dev SFTP ConfigurationPackage - ghcr.io/estuary/source-sftp:dev Shopify ConfigurationPackage - ghcr.io/estuary/source-shopify:dev Shopify (GraphQL) ConfigurationPackage - ghcr.io/estuary/source-shopify-native.dev Snowflake ConfigurationPackage - ghcr.io/estuary/source-snowflake:dev Stripe Real-time ConfigurationPackage - ghcr.io/estuary/source-stripe-native:dev Stripe (deprecated) ConfigurationPackage - ghcr.io/estuary/source-stripe:dev Twilio ConfigurationPackage - ghcr.io/estuary/source-twilio:dev Zendesk Chat ConfigurationPackage - ghcr.io/estuary/source-zendesk-chat:dev Zendesk Support ConfigurationPackage - ghcr.io/estuary/source-zendesk-support:dev  ","version":"Next","tagName":"h3"},{"title":"Third party connectors​","type":1,"pageTitle":"Capture connectors","url":"/reference/Connectors/capture-connectors/#third-party-connectors","content":" Estuary supports open-source connectors from third parties. These connectors operate in a batch fashion, capturing data in increments. When you run these connectors in Flow, you'll get as close to real time as possible within the limitations set by the connector itself.  Typically, we enable SaaS connectors from third parties to allow more diverse data flows.  Aircall ConfigurationPackage - ghcr.io/estuary/source-aircall:dev Amazon Ads ConfigurationPackage - ghcr.io/estuary/source-amazon-ads:dev Amazon SQS ConfigurationPackage - ghcr.io/estuary/source-amazon-sqs:dev Amplitude ConfigurationPackage - ghcr.io/estuary/source-amplitude:dev Bing Ads ConfigurationPackage - ghcr.io/estuary/source-bing-ads:dev Braze ConfigurationPackage - ghcr.io/estuary/source-braze:dev Chargebee ConfigurationPackage - ghcr.io/estuary/source-chargebee:dev Confluence ConfigurationPackage - ghcr.io/estuary/source-confluence:dev Exchange Rates API ConfigurationPackage - ghcr.io/estuary/source-exchange-rates:dev Freshdesk ConfigurationPackage - ghcr.io/estuary/source-freshdesk:dev GitLab ConfigurationPackage - ghcr.io/estuary/source-gitlab:dev Google Analytics 4 ConfigurationPackage - ghcr.io/estuary/source-google-analytics-data-api:dev Google Universal Analytics ConfigurationPackage - ghcr.io/estuary/source-google-analytics-ua:dev Google Search Console ConfigurationPackage - ghcr.io/estuary/source-google-search-console:dev Greenhouse ConfigurationPackage - ghcr.io/estuary/source-greenhouse:dev Harvest ConfigurationPackage - ghcr.io/estuary/source-harvest:dev Instagram ConfigurationPackage - ghcr.io/estuary/source-instagram:dev Intercom (deprecated) ConfigurationPackage - ghcr.io/estuary/source-intercom:dev Jira (deprecated) ConfigurationPackage - ghcr.io/estuary/source-jira:dev LinkedIn Ads ConfigurationPackage - ghcr.io/estuary/source-linkedin-ads:dev Mailchimp ConfigurationPackage - ghcr.io/estuary/source-mailchimp:dev Marketo ConfigurationPackage - ghcr.io/estuary/source-marketo:dev MixPanel ConfigurationPackage - ghcr.io/estuary/source-mixpanel:dev Notion ConfigurationPackage - ghcr.io/estuary/source-notion:dev Paypal Transaction ConfigurationPackage - ghcr.io/estuary/source-paypal-transaction:dev Pinterest ConfigurationPackage - ghcr.io/estuary/source-pinterest:dev Recharge ConfigurationPackage - ghcr.io/estuary/source-recharge:dev Salesforce (For historical data) ConfigurationPackage - ghcr.io/estuary/source-salesforce:dev SendGrid ConfigurationPackage - ghcr.io/estuary/source-sendgrid:dev Sentry ConfigurationPackage - ghcr.io/estuary/source-sentry:dev Slack ConfigurationPackage - ghcr.io/estuary/source-slack:dev Snapchat ConfigurationPackage - ghcr.io/estuary/source-snapchat:dev SurveyMonkey ConfigurationPackage - ghcr.io/estuary/source-surveymonkey:dev TikTok Marketing ConfigurationPackage - ghcr.io/estuary/source-tiktok-marketing:dev WooCommerce ConfigurationPackage - ghcr.io/estuary/source-woocommerce:dev YouTube Analytics ConfigurationPackage - ghcr.io/estuary/source-youtube-analytics:dev ","version":"Next","tagName":"h3"},{"title":"AlloyDB","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/alloydb/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"AlloyDB","url":"/reference/Connectors/capture-connectors/alloydb/#prerequisites","content":" You'll need an AlloyDB database setup with the following:  Logical decoding enabledUser role with REPLICATION attributeA replication slot. This represents a “cursor” into the PostgreSQL write-ahead log from which change events can be read. Optional; if none exist, one will be created by the connector.If you wish to run multiple captures from the same database, each must have its own slot. You can create these slots yourself, or by specifying a name other than the default in the advanced configuration. A publication. This represents the set of tables for which change events will be reported. In more restricted setups, this must be created manually, but can be created automatically if the connector has suitable permissions. A watermarks table. The watermarks table is a small “scratch space” to which the connector occasionally writes a small amount of data to ensure accuracy when backfilling preexisting table contents. In more restricted setups, this must be created manually, but can be created automatically if the connector has suitable permissions.  You'll also need a virtual machine to connect securely to the instance via SSH tunnelling (AlloyDB doesn't support IP allowlisting).  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"AlloyDB","url":"/reference/Connectors/capture-connectors/alloydb/#setup","content":" To meet the prerequisites, complete these steps.  Set the alloydb.logical_decoding flag to on to enable logical replication on your AlloyDB instance. In your psql client, connect to your instance and issue the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table and publication.  CREATE USER flow_capture WITH REPLICATION IN ROLE alloydbsuperuser LOGIN PASSWORD 'secret'; GRANT SELECT ON ALL TABLES IN SCHEMA public TO flow_capture; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO flow_capture; CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication FOR ALL TABLES;   Follow the instructions to create a virtual machine for SSH tunnelingin the same Google Cloud project as your instance.  ","version":"Next","tagName":"h3"},{"title":"Backfills and performance considerations​","type":1,"pageTitle":"AlloyDB","url":"/reference/Connectors/capture-connectors/alloydb/#backfills-and-performance-considerations","content":" When the AlloyDB capture is initiated, by default, the connector first backfills, or captures the targeted tables in their current state. It then transitions to capturing change events on an ongoing basis.  This is desirable in most cases, as it ensures that a complete view of your tables is captured into Flow. However, you may find it appropriate to skip the backfill, especially for extremely large tables.  In this case, you may turn off backfilling on a per-table basis. See properties for details.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"AlloyDB","url":"/reference/Connectors/capture-connectors/alloydb/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the PostgreSQL source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"AlloyDB","url":"/reference/Connectors/capture-connectors/alloydb/#properties","content":" Endpoint​  The SSH config section is required for this connector. You'll fill in the database address with a localhost IP address, and specify your VM's IP address as the SSH address. See the table below and the sample config.  Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tThe host or host:port at which the database can be reached.\tstring\tRequired /database\tDatabase\tLogical database name to capture from.\tstring\tRequired, &quot;postgres&quot; /user\tUser\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tPassword\tPassword for the specified database user.\tstring\tRequired /advanced\tAdvanced Options\tOptions for advanced users. You should not typically need to modify these.\tobject /advanced/backfill_chunk_size\tBackfill Chunk Size\tThe number of rows which should be fetched from the database in a single backfill query.\tinteger\t4096 /advanced/publicationName\tPublication Name\tThe name of the PostgreSQL publication to replicate from.\tstring\t&quot;flow_publication&quot; /advanced/skip_backfills\tSkip Backfills\tA comma-separated list of fully-qualified table names which should not be backfilled.\tstring /advanced/slotName\tSlot Name\tThe name of the PostgreSQL replication slot to replicate from.\tstring\t&quot;flow_slot&quot; /advanced/watermarksTable\tWatermarks Table\tThe name of the table used for watermark writes during backfills. Must be fully-qualified in '&lt;schema&gt;.&lt;table&gt;' form.\tstring\t&quot;public.flow_watermarks&quot; networkTunnel\tNetwork Tunnel\tConnect to your system through an SSH server that acts as a bastion host for your network.\tObject networkTunnel/sshForwarding\tSSH Forwarding Object networkTunnel/sshForwarding/sshEndpoint\tSSH Endpoint\tEndpoint of the remote SSH server (in this case, your Google Cloud VM) that supports tunneling (in the form of ssh://user@address).\tString networkTunnel/sshForwarding/privateKey\tSSH Private Key\tPrivate key to connect to the remote SSH server.\tString\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe namespace/instance of the table.\tstring\tRequired /stream\tStream\tTable name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"AlloyDB","url":"/reference/Connectors/capture-connectors/alloydb/#sample","content":" A minimal capture definition will look like the following:  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: &quot;ghcr.io/estuary/source-alloydb:dev&quot; config: address: &quot;127.0.0.1:5432&quot; database: &quot;postgres&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; networkTunnel: sshForwarding: sshEndpoint: ssh://sshUser@vm-ip-address privateKey: |2 -----BEGIN RSA PRIVATE KEY----- MIICXAIBAAKBgQCJO7G6R+kv2MMS8Suw21sk2twHg8Vog0fjimEWJEwyAfFM/Toi EJ6r5RTaSvN++/+MPWUll7sUdOOBZr6ErLKLHEt7uXxusAzOjMxFKZpEARMcjwHY v/tN1A2OYU0qay1DOwknEE0i+/Bvf8lMS7VDjHmwRaBtRed/+iAQHf128QIDAQAB AoGAGoOUBP+byAjDN8esv1DCPU6jsDf/Tf//RbEYrOR6bDb/3fYW4zn+zgtGih5t CR268+dwwWCdXohu5DNrn8qV/Awk7hWp18mlcNyO0skT84zvippe+juQMK4hDQNi ywp8mDvKQwpOuzw6wNEitcGDuACx5U/1JEGGmuIRGx2ST5kCQQDsstfWDcYqbdhr 5KemOPpu80OtBYzlgpN0iVP/6XW1e5FCRp2ofQKZYXVwu5txKIakjYRruUiiZTza QeXRPbp3AkEAlGx6wMe1l9UtAAlkgCFYbuxM+eRD4Gg5qLYFpKNsoINXTnlfDry5 +1NkuyiQDjzOSPiLZ4Abpf+a+myjOuNL1wJBAOwkdM6aCVT1J9BkW5mrCLY+PgtV GT80KTY/d6091fBMKhxL5SheJ4SsRYVFtguL2eA7S5xJSpyxkadRzR0Wj3sCQAvA bxO2fE1SRqbbF4cBnOPjd9DNXwZ0miQejWHUwrQO0inXeExNaxhYKQCcnJNUAy1J 6JfAT/AbxeSQF3iBKK8CQAt5r/LLEM1/8ekGOvBh8MAQpWBW771QzHUN84SiUd/q xR9mfItngPwYJ9d/pTO7u9ZUPHEoat8Ave4waB08DsI= -----END RSA PRIVATE KEY----- bindings: - resource: stream: ${TABLE_NAME} namespace: ${TABLE_NAMESPACE} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}   Your capture definition will likely be more complex, with additional bindings for each table in the source database.  Learn more about capture definitions. ","version":"Next","tagName":"h3"},{"title":"Alpaca","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/alpaca/","content":"","keywords":"","version":"Next"},{"title":"Real-time and historical trade data​","type":1,"pageTitle":"Alpaca","url":"/reference/Connectors/capture-connectors/alpaca/#real-time-and-historical-trade-data","content":" The Alpaca Market Data API comprises multiple APIs for stock trades, including the Trades REST API for historical trade data and websocket streaming via the Data API for real-time trade data.  Historical trade data is available from the Alpaca Market Data API starting 01-01-2016. As such, the connector configuration requires a start date for the backfill to be on or after 01-01-2016.  This connector uses both APIs to capture historical and real-time data in parallel. It uses the Trades API to perform a historical backfill starting from the start date you specify and stopping when it reaches the present. At the same time, the connector uses websocket streaming to initiate a real-time stream of trade data starting at the present moment and continuing indefinitely until you stop the capture process.  As a result, you'll get data from a historical time period you specify, as well as the lowest-latency possible updates of new trade data, but there will be some overlap in the two data streams. See limitations to learn more about reconciling historical and real-time data.  ","version":"Next","tagName":"h2"},{"title":"Supported data resources​","type":1,"pageTitle":"Alpaca","url":"/reference/Connectors/capture-connectors/alpaca/#supported-data-resources","content":" Alpaca supports over 8000 stocks and ETFs. You simply supply a list of symbols to Flow when you configure the connector. To check whether Alpaca supports a symbol, you can use the Alpaca Broker API.  You can use this connector to capture data from up to 20 stock symbols into Flow collections in a single capture (to add more than 20, set up multiple captures). For a given capture, data from all symbols is captured to a single collection.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Alpaca","url":"/reference/Connectors/capture-connectors/alpaca/#prerequisites","content":" To use this connector, you'll need:  An Alpaca account. To access complete stock data in real-time, you'll need the Unlimited plan. To access a smaller sample of trade data with a 15-minute delay, you can use a Free plan, making sure to set Feed to iex and choose the Free Plan option when configuring the connector. Your Alpaca API Key ID and Secret Key.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Alpaca","url":"/reference/Connectors/capture-connectors/alpaca/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Alpaca source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Alpaca","url":"/reference/Connectors/capture-connectors/alpaca/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/advanced\tAdvanced Options\tOptions for advanced users. You should not typically need to modify these.\tobject /advanced/disable_backfill\tDisable Historical Data Backfill\tDisables historical data backfill via the historical data API. Data will only be collected via streaming.\tboolean /advanced/disable_real_time\tDisable Real-Time Streaming\tDisables real-time streaming via the websocket API. Data will only be collected via the backfill mechanism.\tboolean /advanced/is_free_plan\tFree Plan\tSet this if you are using a free plan. Delays data by 15 minutes.\tboolean /advanced/max_backfill_interval\tMaximum Backfill Interval\tThe largest time interval that will be requested for backfills. Using smaller intervals may be useful when tracking many symbols. Must be a valid Go duration string.\tstring /advanced/min_backfill_interval\tMinimum Backfill Interval\tThe smallest time interval that will be requested for backfills after the initial backfill is complete. Must be a valid Go duration string.\tstring /advanced/stop_date\tStop Date\tStop backfilling historical data at this date.\tstring /api_key_id\tAlpaca API Key ID\tYour Alpaca API key ID.\tstring\tRequired /api_secret_key\tAlpaca API Secret Key\tYour Alpaca API Secret key.\tstring\tRequired /feed\tFeed\tThe feed to pull market data from. Choose from iex or sip; set iex if using a free plan.\tstring\tRequired /start_date\tStart Date\tGet trades starting at this date. Has no effect if changed after the capture has started. Must be no earlier than 2016-01-01T00:00:00Z.\tstring\tRequired /symbols\tSymbols\tComma separated list of symbols to monitor.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/name\tName\tUnique name for this binding. Cannot be changed once set.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Alpaca","url":"/reference/Connectors/capture-connectors/alpaca/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: &quot;ghcr.io/estuary/source-alpaca:dev&quot; config: api_key_id: &lt;SECRET&gt; api_secret_key: &lt;SECRET&gt; feed: iex start_date: 2022-11-01T00:00:00Z symbols: AAPL,MSFT,AMZN,TSLA,GOOGL,GOOG,NVDA,BRK.B,META,UNH advanced: is_free_plan: true bindings: - resource: name: trades target: ${PREFIX}/${CAPTURE_NAME}/trades   ","version":"Next","tagName":"h3"},{"title":"Limitations​","type":1,"pageTitle":"Alpaca","url":"/reference/Connectors/capture-connectors/alpaca/#limitations","content":" Capturing data for more than 20 symbols in a single capture could result in API errors.​  If you need to capture data for more than 20 symbols, we recommend splitting them between two captures. Support for a larger number of symbols in a single capture is planned for a future release.  Separate historical and real-time data streams will result in some duplicate trade documents.​  As discussed above, the connector captures historical and real-time data in two different streams. As the historical data stream catches up to the present, it will overlap with the beginning of the real-time data stream, resulting in some duplicated documents. These will have identical properties from Alpaca, but different metadata from Flow.  There are several ways to resolve this:  If you plan to materialize to an endpoint for which standard (non-delta) updates are supported, Flow will resolve the duplicates during the materialization process. Unless otherwise specified in their documentation page, materialization connectors run in standard updates mode. If a connector supports both modes, it will default to standard updates. If you plan to materialize to an endpoint for which delta updates is the only option, ensure that the endpoint system supports the equivalent of lastWriteWins reductions. ","version":"Next","tagName":"h2"},{"title":"Amazon DynamoDB","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/amazon-dynamodb/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon DynamoDB","url":"/reference/Connectors/capture-connectors/amazon-dynamodb/#prerequisites","content":" To use this connector, you'll need:  One or more DynamoDB tables with DynamoDB streams enabled. To enable DynamoDB streams for a table: Select the table in the AWS consoleGo to the Exports and streams tabClick Turn on in the DynamoDB stream details sectionSelect New and old images for the View typeClick Turn on stream An IAM user with the following permissions: ListTables on all resourcesDescribeTable on all resourcesDescribeStream on all resourcesScan on all tables usedGetRecords on all streams usedGetShardIterator on all streams used These permissions should be specified with the dynamodb: prefix in an IAM policy document. For more details and examples, see Using identity-based policies with Amazon DynamoDB in the Amazon docs. The AWS access key and secret access key for the user. See the AWS blog for help finding these credentials.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Amazon DynamoDB","url":"/reference/Connectors/capture-connectors/amazon-dynamodb/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the DynamoDB source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Amazon DynamoDB","url":"/reference/Connectors/capture-connectors/amazon-dynamodb/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/awsAccessKeyId\tAccess Key ID\tAWS Access Key ID for capturing from DynamoDB tables.\tstring\tRequired /awsSecretAccessKey\tSecret Access Key\tAWS Secret Access Key for capturing from DynamoDB tables.\tstring\tRequired /region\tAWS Region\tThe name of the AWS region where the DynamoDB tables are located.\tstring\tRequired advanced/backfillSegments\tBackfill Table Segments\tNumber of segments to use for backfill table scans. Has no effect if changed after the backfill has started.\tinteger advanced/endpoint\tAWS Endpoint\tThe AWS endpoint URI to connect to. Use if you're capturing from a compatible API that isn't provided by AWS.\tstring advanced/scanLimit\tScan Limit\tLimit the number of items to evaluate for each table backfill scan request.\tinteger\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/table\tTable Name\tThe name of the table to be captured.\tstring\tRequired /rcuAllocation\tRCU Allocation\tRead capacity units the capture will attempt to consume during the table backfill. Leave blank to automatically determine based on the provisioned capacity of the table.\tinteger\t  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Amazon DynamoDB","url":"/reference/Connectors/capture-connectors/amazon-dynamodb/#sample","content":" A minimal capture definition will look like the following:  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-dynamodb:dev config: awsAccessKeyId: &quot;example-aws-access-key-id&quot; awsSecretAccessKey: &quot;example-aws-secret-access-key&quot; region: &quot;us-east-1&quot; bindings: - resource: table: ${TABLE_NAME} target: ${PREFIX}/${COLLECTION_NAME}   Your capture definition may be more complex, with additional bindings for each DynamoDB table.  Learn more about capture definitions. ","version":"Next","tagName":"h3"},{"title":"Amazon Ads","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/amazon-ads/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Amazon Ads","url":"/reference/Connectors/capture-connectors/amazon-ads/#supported-data-resources","content":" The following data resources are supported:  ProfilesSponsored brands ad groupsSponsored brands campaignsSponsored brands keywordsSponsored brands report streamSponsored brands video report streamSponsored display ad groupsSponsored display ad campaignsSponsored display product ads Sponsored display report streamSponsored display targetingsSponsored product ad groupsSponsored product adsSponsored product campaignsSponsored product keywordsSponsored product negative keywordsSponsored product targetingsSponsored product report stream  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon Ads","url":"/reference/Connectors/capture-connectors/amazon-ads/#prerequisites","content":" This connector uses OAuth2 to authenticate with Amazon. You can do this in the Flow web app, or configure manually if you're using the flowctl CLI.  ","version":"Next","tagName":"h2"},{"title":"Using OAuth2 to authenticate with Amazon in the Flow web app​","type":1,"pageTitle":"Amazon Ads","url":"/reference/Connectors/capture-connectors/amazon-ads/#using-oauth2-to-authenticate-with-amazon-in-the-flow-web-app","content":" You'll need an Amazon user account with access to the Amazon Ads account from which you wish to capture data.  You'll use these credentials to sign in.  ","version":"Next","tagName":"h3"},{"title":"Authenticating manually using the CLI​","type":1,"pageTitle":"Amazon Ads","url":"/reference/Connectors/capture-connectors/amazon-ads/#authenticating-manually-using-the-cli","content":" When you configure this connector manually, you provide the same credentials that OAuth2 would automatically fetch if you used the web app. These are:  Client IDClient secretRefresh token  To obtain these credentials:  Complete the Amazon Ads API onboarding process. Retrieve your client ID and client secret. Retrieve a refresh token.  ","version":"Next","tagName":"h3"},{"title":"Selecting data region and profiles​","type":1,"pageTitle":"Amazon Ads","url":"/reference/Connectors/capture-connectors/amazon-ads/#selecting-data-region-and-profiles","content":" When you configure the endpoint for this connector, you must choose an Amazon region from which to capture data. Optionally, you may also select profiles from which to capture data.  The region must be one of:  NA (North America)EU (European Union)FE (Far East)  These represent the three URL endpoints provided by Amazon through which you can access the marketing API. Each region encompasses multiple Amazon marketplaces, which are broken down by country. See the Amazon docs for details.  If you run your Amazon ads in multiple marketplaces, you may have separate profiles for each. If this is the case, you can specify the profiles from which you wish to capture data by supplying their profile IDs. Be sure to specify only profiles that correspond to marketplaces within the region you chose.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Amazon Ads","url":"/reference/Connectors/capture-connectors/amazon-ads/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Amazon Ads source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Amazon Ads","url":"/reference/Connectors/capture-connectors/amazon-ads/#properties","content":" Endpoint​  The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so many of these properties aren't required.  Property\tTitle\tDescription\tType\tRequired/Default/credentials object\tRequired /credentials/auth_type\tAuth Type\tSet to oauth2.0 for manual integration (in this method, you're re-creating the same credentials of the OAuth user interface, but doing so manually)\tstring /credentials/client_id\tClient ID\tThe client ID of your Amazon Ads developer application.\tstring\tRequired /credentials/client_secret\tClient Secret\tThe client secret of your Amazon Ads developer application.\tstring\tRequired /credentials/refresh_token\tRefresh Token\tAmazon Ads refresh token.\tstring\tRequired /profiles\tProfile IDs (Optional)\tProfile IDs you want to fetch data for.\tarray /region\tRegion *\tRegion to pull data from (EU/NA/FE).\tstring\t&quot;NA&quot; /report_generation_max_retries\tReport Generation Maximum Retries *\tMaximum retries the connector will attempt for fetching report data.\tinteger\t5 /report_wait_timeout\tReport Wait Timeout *\tTimeout duration in minutes for reports.\tinteger\t60 /start_date\tStart Date (Optional)\tThe start date for collecting reports, in YYYY-MM-DD format. This should not be more than 60 days in the past.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tAmazon Ads resource from which a collection is captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Amazon Ads","url":"/reference/Connectors/capture-connectors/amazon-ads/#sample","content":" This sample specification reflects the manual authentication method.  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-amazon-ads:dev config: credentials: auth_type: oauth2.0 client_id: amzn1.application-oa2-client.XXXXXXXXX client_secret: &lt;secret&gt; refresh_token: Atzr|XXXXXXXXXXXX region: NA report_generation_max_retries: 5 report_wait_timeout: 60 start_date: 2022-03-01 bindings: - resource: stream: profiles syncMode: full_refresh target: ${PREFIX}/profiles {}  ","version":"Next","tagName":"h3"},{"title":"Amazon Redshift","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/amazon-redshift/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon Redshift","url":"/reference/Connectors/capture-connectors/amazon-redshift/#prerequisites","content":" To use this connector, you'll need:  Access credentials for connecting to your Amazon Redshift cluster.Properly configured IAM roles for the necessary permissions.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Amazon Redshift","url":"/reference/Connectors/capture-connectors/amazon-redshift/#configuration","content":" You can configure the Redshift source connector either through the Flow web app or by directly editing the Flow specification file. For more information on using this connector, see our guide on connectors. The values and specification sample below provide configuration details that are specific to the Amazon Redshift source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Amazon Redshift","url":"/reference/Connectors/capture-connectors/amazon-redshift/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/host\tHost\tHostname or IP address of your Redshift cluster.\tstring\tRequired /port\tPort\tPort number for the cluster.\tinteger\tDefault /database\tDatabase Name\tName of the database to capture data from.\tstring\tRequired /user\tUser\tDatabase user with necessary permissions.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /schemas\tSchemas\tList of schemas to include.\tstring /jdbc_params\tJDBC URL Params\tAdditional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/table\tTable Name\tName of the table to capture.\tstring\tRequired /cursor_field\tUser-defined Cursor\tField for incremental syncs. Uses ascending values to ensure queries are sequential.\tstring or integer\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Amazon Redshift","url":"/reference/Connectors/capture-connectors/amazon-redshift/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-redshift:dev config: host: &quot;example-redshift-cluster.us-east-2.redshift.amazonaws.com&quot; port: 5439 database: &quot;sample_db&quot; user: &quot;sample_user&quot; password: &quot;sample_password&quot; schemas: &quot;public&quot; jdbc_params: &quot;key1=value1&amp;key2=value2&amp;key3=value3&quot; bindings: - resource: table: users cursor_field: cursor target: ${PREFIX}/users  ","version":"Next","tagName":"h3"},{"title":"Amazon Kinesis","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/amazon-kinesis/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon Kinesis","url":"/reference/Connectors/capture-connectors/amazon-kinesis/#prerequisites","content":" To use this connector, you'll need:  One or more Amazon Kinesis streams. For a given capture, all streams must: Contain JSON data onlyBe in the same AWS region An IAM user with the following permissions: ListShards on all resourcesGetRecords on all streams usedGetShardIterator on all streams usedDescribeStream on all streams usedDescribeStreamSummary on all streams used These permissions should be specified with the kinesis: prefix in an IAM policy document. For more details and examples, see Controlling Access to Amazon Kinesis Data in the Amazon docs. The AWS access key and secret access key for the user. See the AWS blog for help finding these credentials.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Amazon Kinesis","url":"/reference/Connectors/capture-connectors/amazon-kinesis/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Amazon Kinesis source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Amazon Kinesis","url":"/reference/Connectors/capture-connectors/amazon-kinesis/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/awsAccessKeyId\tAWS access key ID\tPart of the AWS credentials that will be used to connect to Kinesis.\tstring\tRequired, &quot;example-aws-access-key-id&quot; /awsSecretAccessKey\tAWS secret access key\tPart of the AWS credentials that will be used to connect to Kinesis.\tstring\tRequired, &quot;example-aws-secret-access-key&quot; /endpoint\tAWS endpoint\tThe AWS endpoint URI to connect to, useful if you're capturing from a kinesis-compatible API that isn't provided by AWS.\tstring /region\tAWS region\tThe name of the AWS region where the Kinesis stream is located.\tstring\tRequired, &quot;us-east-1&quot;  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tStream name.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Amazon Kinesis","url":"/reference/Connectors/capture-connectors/amazon-kinesis/#sample","content":" A minimal capture definition will look like the following:  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-kinesis:dev config: awsAccessKeyId: &quot;example-aws-access-key-id&quot; awsSecretAccessKey: &quot;example-aws-secret-access-key&quot; region: &quot;us-east-1&quot; bindings: - resource: stream: ${STREAM_NAME} target: ${PREFIX}/${COLLECTION_NAME}   Your capture definition will likely be more complex, with additional bindings for each Kinesis stream.  Learn more about capture definitions.. ","version":"Next","tagName":"h3"},{"title":"Amplitude","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/amplitude/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Amplitude","url":"/reference/Connectors/capture-connectors/amplitude/#supported-data-resources","content":" The following data resources are supported through the Amplitude APIs:  Active User CountsAnnotationsAverage Session LengthCohortsEvents  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Amplitude","url":"/reference/Connectors/capture-connectors/amplitude/#prerequisites","content":" An Amplitude project with an API Key and Secret Key  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Amplitude","url":"/reference/Connectors/capture-connectors/amplitude/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Amplitude source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Amplitude","url":"/reference/Connectors/capture-connectors/amplitude/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/api_key\tAPI Key\tAmplitude API Key.\tstring\tRequired /secret_key\tSecret Key\tAmplitude Secret Key.\tstring\tRequired /start_date\tReplication Start Date\tUTC date and time in the format 2021-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Amplitude project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Amplitude","url":"/reference/Connectors/capture-connectors/amplitude/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-amplitude:dev config: api_key: &lt;secret&gt; secret_key: &lt;secret&gt; start_date: 2022-06-18T00:00:00Z bindings: - resource: stream: cohorts syncMode: full_refresh target: ${PREFIX}/cohorts - resource: stream: annotations syncMode: full_refresh target: ${PREFIX}/annotations - resource: stream: events syncMode: incremental target: ${PREFIX}/events - resource: stream: active_users syncMode: incremental target: ${PREFIX}/activeusers - resource: stream: average_session_length syncMode: incremental target: ${PREFIX}/averagesessionlength  ","version":"Next","tagName":"h3"},{"title":"Amazon SQS","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/amazon-sqs/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon SQS","url":"/reference/Connectors/capture-connectors/amazon-sqs/#prerequisites","content":" AWS IAM Access KeyAWS IAM Secret KeyAWS SQS Queue  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Amazon SQS","url":"/reference/Connectors/capture-connectors/amazon-sqs/#setup","content":" Follow these steps to set up the Amazon SQS connector:  Create AWS IAM KeysCreate an SQS QueueEnter a Primary Key and Cursor Field using the standard form editor. Note that these values currently have to be a string or timestamp.  note If Delete Messages After Read is false, the IAM User only requires sqs:ReceiveMessage permission in the AWS IAM Policy.If Delete Messages After Read is true, both sqs:ReceiveMessage and sqs:DeleteMessage permissions are needed in the AWS IAM Policy.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Amazon SQS","url":"/reference/Connectors/capture-connectors/amazon-sqs/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the AmazonSQS source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Amazon SQS","url":"/reference/Connectors/capture-connectors/amazon-sqs/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/queue_url\tQueue URL\tURL of the SQS Queue\tstring\tRequired /region\tAWS Region\tAWS Region of the SQS Queue\tstring\tRequired /access_key\tAWS IAM Access Key ID\tThe Access Key ID of the AWS IAM Role to use for pulling messages\tstring /secret_key\tAWS IAM Secret Key\tThe Secret Key of the AWS IAM Role to use for pulling messages\tstring /delete_messages\tDelete Messages After Read\tDelete messages from the SQS Queue after reading them\tboolean\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Amazon SQS project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Amazon SQS","url":"/reference/Connectors/capture-connectors/amazon-sqs/#sample","content":" { &quot;properties&quot;: { &quot;queue_url&quot;: { &quot;order&quot;: 0 }, &quot;region&quot;: { &quot;order&quot;: 1 }, &quot;access_key&quot;: { &quot;order&quot;: 2 }, &quot;secret_key&quot;: { &quot;order&quot;: 3 }, &quot;delete_messages&quot;: { &quot;order&quot;: 4 } } }   ","version":"Next","tagName":"h3"},{"title":"Performance Considerations​","type":1,"pageTitle":"Amazon SQS","url":"/reference/Connectors/capture-connectors/amazon-sqs/#performance-considerations","content":" Consider the following performance aspects:  Max Batch Size: Set the maximum number of messages to consume in a single poll.Max Wait Time: Define the maximum time (in seconds) to poll for messages before committing a batch.Message Visibility Timeout: Determine how long a message should be hidden from other consumers after being read.  ","version":"Next","tagName":"h3"},{"title":"Data Loss Warning​","type":1,"pageTitle":"Amazon SQS","url":"/reference/Connectors/capture-connectors/amazon-sqs/#data-loss-warning","content":" When enabling Delete Messages After Read, messages are deleted from the SQS Queue after being read. However, there is no guarantee that the downstream destination has committed or persisted the message. Exercise caution before enabling this option to avoid permanent message loss. ","version":"Next","tagName":"h3"},{"title":"Amazon S3","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/amazon-s3/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon S3","url":"/reference/Connectors/capture-connectors/amazon-s3/#prerequisites","content":" You can use this connector to capture data from an entire S3 bucket or for a prefix within a bucket. This bucket or prefix must be either be:  Publicly accessible and allowing anonymous reads. Accessible via a root or IAM user.  In either case, you'll need an access policy. Policies in AWS are JSON objects that define permissions. You attach them to resources, which include both IAM users and S3 buckets.  See the steps below to set up access.  ","version":"Next","tagName":"h2"},{"title":"Setup: Public buckets​","type":1,"pageTitle":"Amazon S3","url":"/reference/Connectors/capture-connectors/amazon-s3/#setup-public-buckets","content":" For a public bucket, the bucket access policy must allow anonymous reads on the whole bucket or a specific prefix.  Create a bucket policy using the templates below.  Anonymous reads policy - Full bucketAnonymous reads policy - Specific prefix { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Sid&quot;: &quot;BucketAnonymousRead&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: &quot;*&quot;, &quot;Action&quot;: [ &quot;s3:ListBucket&quot; ], &quot;Resource&quot;: [ &quot;arn:aws:s3:::EXAMPLE_BUCKET&quot; ] }, { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: &quot;*&quot;, &quot;Action&quot;: [ &quot;s3:GetObject&quot; ], &quot;Resource&quot;: [ &quot;arn:aws:s3:::EXAMPLE_BUCKET/*&quot; ] } ] }   Add the policy to your bucket. Paste over the existing policy and resolve any errors or warnings before saving. Confirm that the Block public access setting on the bucket is disabled.  ","version":"Next","tagName":"h3"},{"title":"Setup: Accessing with a user account​","type":1,"pageTitle":"Amazon S3","url":"/reference/Connectors/capture-connectors/amazon-s3/#setup-accessing-with-a-user-account","content":" For buckets accessed by a user account, you'll need the AWS access key and secret access key for the user. You'll also need to apply an access policy to the user to grant access to the specific bucket or prefix.  Create an IAM user if you don't yet have one to use with Flow. Note the user's access key and secret access key. See the AWS blog for help finding these credentials. Create an IAM policy using the templates below.  IAM user access policy - Full bucketIAM user access policy - Specific prefix { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Sid&quot;: &quot;UserAccessFullBucket&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;s3:ListBucket&quot; ], &quot;Resource&quot;: [ &quot;arn:aws:s3:::EXAMPLE_BUCKET&quot; ] }, { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;s3:GetObject&quot; ], &quot;Resource&quot;: [ &quot;arn:aws:s3:::EXAMPLE_BUCKET/*&quot; ] } ] }   Add the policy to AWS. Attach the policy to the IAM user.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Amazon S3","url":"/reference/Connectors/capture-connectors/amazon-s3/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the S3 source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Amazon S3","url":"/reference/Connectors/capture-connectors/amazon-s3/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/advanced Options for advanced users. You should not typically need to modify these.\tobject /advanced/ascendingKeys\tAscending Keys\tImprove sync speeds by listing files from the end of the last sync, rather than listing the entire bucket prefix. This requires that you write objects in ascending lexicographic order, such as an RFC-3339 timestamp, so that key ordering matches modification time ordering. If data is not ordered correctly, using ascending keys could cause errors.\tboolean\tfalse /advanced/endpoint\tAWS Endpoint\tThe AWS endpoint URI to connect to. Use if you're capturing from a S3-compatible API that isn't provided by AWS\tstring /awsAccessKeyId\tAWS Access Key ID\tPart of the AWS credentials that will be used to connect to S3. Required unless the bucket is public and allows anonymous listings and reads.\tstring /awsSecretAccessKey\tAWS Secret Access Key\tPart of the AWS credentials that will be used to connect to S3. Required unless the bucket is public and allows anonymous listings and reads.\tstring /bucket\tBucket\tName of the S3 bucket\tstring\tRequired /matchKeys\tMatch Keys\tFilter applied to all object keys under the prefix. If provided, only objects whose absolute path matches this regex will be read. For example, you can use &quot;.*\\.json&quot; to only capture json files.\tstring /parser\tParser Configuration\tConfigures how files are parsed (optional, see below)\tobject /parser/compression\tCompression\tDetermines how to decompress the contents. The default, 'Auto', will try to determine the compression automatically.\tnull, string\tnull /parser/format\tFormat\tDetermines how to parse the contents. The default, 'Auto', will try to determine the format automatically based on the file extension or MIME type, if available.\tobject\t{&quot;type&quot;:&quot;auto&quot;} /parser/format/type\tType string /prefix\tPrefix\tPrefix within the bucket to capture from. Use this to limit the data in your capture.\tstring /region\tAWS Region\tThe name of the AWS region where the S3 bucket is located. &quot;us-east-1&quot; is a popular default you can try, if you're unsure what to put here.\tstring\tRequired, &quot;us-east-1&quot;  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tPrefix\tPath to dataset in the bucket, formatted as bucket-name/prefix-name.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Amazon S3","url":"/reference/Connectors/capture-connectors/amazon-s3/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-s3:dev config: bucket: &quot;my-bucket&quot; parser: compression: zip format: type: csv config: delimiter: &quot;,&quot; encoding: UTF-8 errorThreshold: 5 headers: [ID, username, first_name, last_name] lineEnding: &quot;\\\\r&quot; quote: &quot;\\&quot;&quot; region: &quot;us-east-1&quot; bindings: - resource: stream: my-bucket/${PREFIX} target: ${PREFIX}/${COLLECTION_NAME}   Your capture definition may be more complex, with additional bindings for different S3 prefixes within the same bucket.  Learn more about capture definitions.  ","version":"Next","tagName":"h3"},{"title":"Advanced: Parsing cloud storage data​","type":1,"pageTitle":"Amazon S3","url":"/reference/Connectors/capture-connectors/amazon-s3/#advanced-parsing-cloud-storage-data","content":" Cloud storage platforms like S3 can support a wider variety of file types than other data source systems. For each of these file types, Flow must parse and translate data into collections with defined fields and JSON schemas.  By default, the parser will automatically detect the type and shape of the data in your bucket, so you won't need to change the parser configuration for most captures.  However, the automatic detection may be incorrect in some cases. To fix or prevent this, you can provide explicit information in the parser configuration, which is part of the endpoint configuration for this connector.  The parser configuration includes:  Compression: Specify how the bucket contents are compressed. If no compression type is specified, the connector will try to determine the compression type automatically. Options are: zipgzipzstdnone Format: Specify the data format, which determines how it will be parsed. Options are: Auto: If no format is specified, the connector will try to determine it automatically.AvroCSVJSONProtobufW3C Extended Log info At this time, Flow only supports S3 captures with data of a single file type. Support for multiple file types, which can be configured on a per-binding basis, will be added in the future. For now, use a prefix in the endpoint configuration to limit the scope of each capture to data of a single file type.  CSV configuration​  CSV files include several additional properties that are important to the parser. In most cases, Flow is able to automatically determine the correct values, but you may need to specify for unusual datasets. These properties are:  Delimiter. Options are: Comma (&quot;,&quot;)Pipe (&quot;|&quot;)Space (&quot;0x20&quot;)Semicolon (&quot;;&quot;)Tab (&quot;0x09&quot;)Vertical tab (&quot;0x0B&quot;)Unit separator (&quot;0x1F&quot;)SOH (&quot;0x01&quot;)Auto Encoding type, specified by its WHATWG label. Optionally, an Error threshold, as an acceptable percentage of errors. If set to a number greater than zero, malformed rows that fall within the threshold will be excluded from the capture. Escape characters. Options are: Backslash (&quot;\\\\&quot;)Disable escapes (&quot;&quot;)Auto Optionally, a list of column Headers, if not already included in the first row of the CSV file. If any headers are provided, it is assumed that the provided list of headers is complete and authoritative. The first row of your CSV file will be assumed to be data (not headers), and you must provide a header value for every column in the file. Line ending values CRLF (&quot;\\\\r\\\\n&quot;) (Windows)CR (&quot;\\\\r&quot;)LF (&quot;\\\\n&quot;)Record Separator (&quot;0x1E&quot;)Auto Quote character Double Quote (&quot;\\&quot;&quot;)Single Quote (&quot;)Disable Quoting (&quot;&quot;)Auto  The sample specification above includes these fields. ","version":"Next","tagName":"h3"},{"title":"Asana","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/asana/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Asana","url":"/reference/Connectors/capture-connectors/asana/#supported-data-resources","content":" The following data resources are supported through the Asana APIs:  ","version":"Next","tagName":"h2"},{"title":"Default Streams​","type":1,"pageTitle":"Asana","url":"/reference/Connectors/capture-connectors/asana/#default-streams","content":" AttachmentsAttachments (Compact)Custom FieldsEventsPortfoliosPortfolios (Compact)Portfolios MembershipsProjectsSectionsSections (Compact)StoriesStories (Compact)TagsTasksTeam MembershipsTeamsUsersWorkspaces  ","version":"Next","tagName":"h3"},{"title":"Streams Available for Enterprise+ Organizations​","type":1,"pageTitle":"Asana","url":"/reference/Connectors/capture-connectors/asana/#streams-available-for-enterprise-organizations","content":" Organization Exports (available for service accounts)  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h3"},{"title":"Prerequisites​","type":1,"pageTitle":"Asana","url":"/reference/Connectors/capture-connectors/asana/#prerequisites","content":" You will need an Asana account.  You can authenticate your account with Estuary either via OAuth or using an Asana personal access token.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Asana","url":"/reference/Connectors/capture-connectors/asana/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Asana source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Asana","url":"/reference/Connectors/capture-connectors/asana/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/credentials\tCredentials object\tRequired /credentials/option_title\tCredentials Title\tDenotes the authentication type. Can either be OAuth Credentials or PAT Credentials.\tstring /credentials/client_id\tClient ID\tThe client ID for Asana OAuth.\tstring\tRequired when using the OAuth Credentials option /credentials/client_secret\tClient Secret\tThe client secret for Asana OAuth.\tstring\tRequired when using the OAuth Credentials option /credentials/refresh_token\tRefresh Token\tThe refresh token for Asana OAuth.\tstring\tRequired when using the OAuth Credentials option /credentials/personal_access_token\tPersonal Access Token\tThe access token to authenticate with the Asana API.\tstring\tRequired when using the PAT Credentials option  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tAsana resource from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Asana","url":"/reference/Connectors/capture-connectors/asana/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-asana:dev config: credentials: option_title: PAT Credentials personal_access_token: &lt;secret&gt; bindings: - resource: stream: attachments syncMode: full_refresh target: ${PREFIX}/attachments {...}  ","version":"Next","tagName":"h3"},{"title":"BigQuery Batch Query Connector","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/bigquery-batch/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"BigQuery Batch Query Connector","url":"/reference/Connectors/capture-connectors/bigquery-batch/#prerequisites","content":" To use this connector, you will need the following prerequisites:  A Google Cloud Project with BigQuery enabledA Google Cloud Service Account with the &quot;BigQuery User&quot; and &quot;BigQuery Data Viewer&quot; roles in your GCP projectA Service Account Key to authenticate into your Service Account  See the setup guide for more information about how to create the required resources.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"BigQuery Batch Query Connector","url":"/reference/Connectors/capture-connectors/bigquery-batch/#setup","content":" Follow the steps below to set up the BigQuery connector.  ","version":"Next","tagName":"h2"},{"title":"Service Account​","type":1,"pageTitle":"BigQuery Batch Query Connector","url":"/reference/Connectors/capture-connectors/bigquery-batch/#service-account","content":" To sync data from BigQuery, you need credentials for a Service Account with the &quot;BigQuery User&quot; and &quot;BigQuery Data Viewer&quot; roles. These roles grant the necessary permissions to run BigQuery jobs, discover tables within the dataset, and read the contents of those tables. It is recommended to create a dedicated Service Account to facilitate permission management and auditing. However, if you already have a Service Account with the correct permissions, you can use it.  Here's how to provision a suitable service account:  Follow Google Cloud Platform's instructions for Creating a Service Account.Note down the ID of the service account you just created. Service Account IDs typically follow the format &lt;account-name&gt;@&lt;project-name&gt;.iam.gserviceaccount.com.Follow Google Cloud Platform's instructions for Granting IAM Roles to the new service account. The &quot;principal&quot; email address should be the ID of the service account you just created, and the roles granted should be &quot;BigQuery User&quot; and &quot;BigQuery Data Viewer&quot;.  ","version":"Next","tagName":"h3"},{"title":"Service Account Key​","type":1,"pageTitle":"BigQuery Batch Query Connector","url":"/reference/Connectors/capture-connectors/bigquery-batch/#service-account-key","content":" Service Account Keys are used to authenticate as Google Service Accounts. To be able to utilize the permissions granted to the Service Account in the previous step, you'll need to provide its Service Account Key when creating the capture. It is a good practice, though not required, to create a new key for Flow even if you're reusing a preexisting account.  To create a new key for a service account, follow Google Cloud Platform's instructions for Creating a Service Account Key. Be sure to create the key in JSON format. Once the linked instructions have been followed you should have a key file, which will need to be uploaded to Flow when setting up your capture.  ","version":"Next","tagName":"h3"},{"title":"Set up the BigQuery connector in Estuary Flow​","type":1,"pageTitle":"BigQuery Batch Query Connector","url":"/reference/Connectors/capture-connectors/bigquery-batch/#set-up-the-bigquery-connector-in-estuary-flow","content":" Log into your Estuary Flow account.In the left navigation bar, click on &quot;Sources&quot;. In the top-left corner, click &quot;New Capture&quot;.Locate and select the &quot;BigQuery&quot; connector.Enter a name and optional description for the capture task.Enter the Project ID and Dataset name that you intend to capture from, and paste or upload the service account key in the appropriate field.Click the &quot;Next&quot; button and wait while the connector automatically discovers the available tables in the specified project and dataset.Select the tables you wish to capture from the bindings list.For each binding you selected, you will likely wish to specify cursor columns and a shorter &quot;Poll Interval&quot; setting. Otherwise the default behavior will be to recapture the entire contents of the table, once per day.Once you are satisfied with your binding selection, click the &quot;Save and Publish&quot; button.  ","version":"Next","tagName":"h3"},{"title":"Specifying Cursor Columns​","type":1,"pageTitle":"BigQuery Batch Query Connector","url":"/reference/Connectors/capture-connectors/bigquery-batch/#specifying-cursor-columns","content":" This connector operates by periodically executing a SELECT * FROM table query and outputting the resulting rows as JSON documents into a Flow collection. In some cases doing this once or twice a day is entirely sufficient, but when working with larger tables (or if a faster update rate is desired) it pays to manually configure cursor columns.  The cursor must be a column (or ordered tuple of columns) which is expected to strictly increase for newly added or updated rows. Common examples of suitable cursors include:  Update timestamps, which are often the best choice if available since they can often be used to identify changed rows as well as new insertions.Creation timestamps, which can be used to identify newly added rows in append-only datasets but won't help to identify changes to preexisting rows.Monotonically increasing IDs, which are another way of identifying newly added rows but often don't help with update detection.  When a cursor is specified, the update query will take the form SELECT * FROM $table WHERE $cursorName &gt; $lastCursorValue ORDER BY $cursorNameand the capture connector will keep track of the highest observed cursor value between polling intervals. If multiple cursor columns are specified, they will be treated as an ordered tuple of columns which collectively form the cursor, and the obvious lexicographic tuple ordering will apply.  Once you have specified a suitable cursor for a table, you will likely want to lower the polling interval for that binding. The default polling interval is &quot;24h&quot; to keep data volumes low, but once a cursor is specified there is usually no downside to frequent polling, so you may wish to lower the interval to &quot;5m&quot; or even &quot;5s&quot; for that table.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"BigQuery Batch Query Connector","url":"/reference/Connectors/capture-connectors/bigquery-batch/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the BigQuery source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"BigQuery Batch Query Connector","url":"/reference/Connectors/capture-connectors/bigquery-batch/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/project_id\tProject ID\tThe GCP project ID for the project containing the source BigQuery dataset\tstring\tRequired /dataset\tDataset\tThe BigQuery dataset to discover tables within\tstring\tRequired /credentials_json\tCredentials JSON\tThe contents of your Service Account Key JSON file\tstring\tRequired /advanced/poll\tPoll Interval\tHow often to poll bindings (may be overridden for a specific binding)\tstring\t&quot;24h&quot;  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/name\tName\tA name which uniquely identifies this binding.\tstring\tRequired /cursor\tCursor\tThe column name(s) which should be used as the incremental capture cursor\tarray\t[] /template\tTemplate\tThe query (template) which will be executed every polling interval\tstring\tRequired /poll\tPoll Interval\tOverride the global polling interval for this binding.\tstring\t&quot;&quot;  ","version":"Next","tagName":"h3"},{"title":"Query Templates​","type":1,"pageTitle":"BigQuery Batch Query Connector","url":"/reference/Connectors/capture-connectors/bigquery-batch/#query-templates","content":" The query template property of a binding defines what query will be executed against the database, given inputs describing the configured cursor columns and whether any prior cursor state exists. The default template implements the behavior described inspecifying cursor columns.  In principle you are free to modify this template to implement whatever query you need. You could for instance create a new binding which queries a view, or which performs a more complex analytics query. However this should not be combined with table auto-discovery in a single capture, as this can produce some counterintuitive results. Instead create two separate capture tasks from the same database, one for autodiscovered tables and a separate one with the setting &quot;Automatically Add New Collections&quot; disabled for your custom bindings. ","version":"Next","tagName":"h3"},{"title":"Google Analytics 4 BigQuery Native Connector","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/bigquery-ga4-native/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Google Analytics 4 BigQuery Native Connector","url":"/reference/Connectors/capture-connectors/bigquery-ga4-native/#prerequisites","content":" To use this connector, ensure you have the following prerequisites:  A Google Cloud Project with BigQuery enabled.Administrative access to your Google Analytics 4 property.Editor or Administrator roles on the Google Cloud Project where BigQuery is located.A Google Cloud Service Account with the &quot;BigQuery User&quot; and &quot;BigQuery Data Viewer&quot; roles in your GCP projectA Service Account Key to authenticate into your Service Account  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Google Analytics 4 BigQuery Native Connector","url":"/reference/Connectors/capture-connectors/bigquery-ga4-native/#setup","content":" Follow the steps below to set up the native GA4 to BigQuery export and the BigQuery Connector.  ","version":"Next","tagName":"h2"},{"title":"Step 1: Link GA4 to BigQuery​","type":1,"pageTitle":"Google Analytics 4 BigQuery Native Connector","url":"/reference/Connectors/capture-connectors/bigquery-ga4-native/#step-1-link-ga4-to-bigquery","content":" Access Google Analytics: Sign in to your Google Analytics account at analytics.google.com. Navigate to BigQuery Linking: In the Admin section, find the property you want to link to BigQuery.Under the property column, click on BigQuery Linking. Create a Link: Click on the Link button and follow the prompts.Select the Google Cloud Project where your BigQuery dataset is located. You may need to provide necessary permissions if you haven't done so already. Configure Export Options: Choose between Daily Export and Streaming Export options.Daily Export exports all events from the previous day, while Streaming Export provides near real-time data but incurs additional BigQuery costs. Review and Submit: Review the settings, including your dataset name and export frequency.Click Submit to complete the linking process.  For detailed instructions, refer to the official Google documentation on Setting up BigQuery Export for GA4.  ","version":"Next","tagName":"h3"},{"title":"Step 2: Understand the Exported Tables​","type":1,"pageTitle":"Google Analytics 4 BigQuery Native Connector","url":"/reference/Connectors/capture-connectors/bigquery-ga4-native/#step-2-understand-the-exported-tables","content":" For each day, streaming export creates one new table:  events_intraday_YYYYMMDD: An internal staging table that includes records of session activity that took place during the day. Streaming export is a best-effort operation and may not include all data for reasons such as the processing of late events and/or failed uploads. Data is exported continuously throughout the day. This table can include records of a session when that session spans multiple export operations.This table is deleted when events_YYYYMMDD is complete. If you select the daily option when you set up BigQuery Export, then the following table is also created each day.  events_YYYYMMDD: The full daily export of events.  ","version":"Next","tagName":"h3"},{"title":"Step 3: Set up the BigQuery Connector​","type":1,"pageTitle":"Google Analytics 4 BigQuery Native Connector","url":"/reference/Connectors/capture-connectors/bigquery-ga4-native/#step-3-set-up-the-bigquery-connector","content":" Service Account​  To sync data from BigQuery, you need credentials for a Service Account with the &quot;BigQuery User&quot; and &quot;BigQuery Data Viewer&quot; roles. These roles grant the necessary permissions to run BigQuery jobs, discover tables within the dataset, and read the contents of those tables. It is recommended to create a dedicated Service Account to facilitate permission management and auditing. However, if you already have a Service Account with the correct permissions, you can use it.  Here's how to provision a suitable service account:  Follow Google Cloud Platform's instructions for Creating a Service Account.Note down the ID of the service account you just created. Service Account IDs typically follow the format &lt;account-name&gt;@&lt;project-name&gt;.iam.gserviceaccount.com.Follow Google Cloud Platform's instructions for Granting IAM Roles to the new service account. The &quot;principal&quot; email address should be the ID of the service account you just created, and the roles granted should be &quot;BigQuery User&quot; and &quot;BigQuery Data Viewer&quot;.  Service Account Key​  Service Account Keys are used to authenticate as Google Service Accounts. To be able to utilize the permissions granted to the Service Account in the previous step, you'll need to provide its Service Account Key when creating the capture. It is a good practice, though not required, to create a new key for Flow even if you're reusing a preexisting account.  To create a new key for a service account, follow Google Cloud Platform's instructions for Creating a Service Account Key. Be sure to create the key in JSON format. Once the linked instructions have been followed you should have a key file, which will need to be uploaded to Flow when setting up your capture.  Set up the BigQuery connector in Estuary Flow​  Log into your Estuary Flow account.In the left navigation bar, click on &quot;Sources&quot;. In the top-left corner, click &quot;New Capture&quot;.Locate and select the &quot;BigQuery&quot; connector.Enter a name and optional description for the capture task.Enter the Project ID and Dataset name that you intend to capture from, and paste or upload the service account key in the appropriate field.Click the &quot;Next&quot; button and wait while the connector automatically discovers the available tables in the specified project and dataset.Select the tables you wish to capture from the bindings list.For each binding you selected, you will likely wish to specify cursor columns and a shorter &quot;Poll Interval&quot; setting. Otherwise the default behavior will be to recapture the entire contents of the table, once per day.Once you are satisfied with your binding selection, click the &quot;Save and Publish&quot; button.  Specifying Cursor Columns​  This connector operates by periodically executing a SELECT * FROM table query and outputting the resulting rows as JSON documents into a Flow collection. In some cases doing this once or twice a day is entirely sufficient, but when working with larger tables (or if a faster update rate is desired) it pays to manually configure cursor columns.  The cursor must be a column (or ordered tuple of columns) which is expected to strictly increase for newly added or updated rows. Common examples of suitable cursors include:  Update timestamps, which are often the best choice if available since they can often be used to identify changed rows as well as new insertions.Creation timestamps, which can be used to identify newly added rows in append-only datasets but won't help to identify changes to preexisting rows.Monotonically increasing IDs, which are another way of identifying newly added rows but often don't help with update detection.  When a cursor is specified, the update query will take the form SELECT * FROM $table WHERE $cursorName &gt; $lastCursorValue ORDER BY $cursorNameand the capture connector will keep track of the highest observed cursor value between polling intervals. If multiple cursor columns are specified, they will be treated as an ordered tuple of columns which collectively form the cursor, and the obvious lexicographic tuple ordering will apply.  Once you have specified a suitable cursor for a table, you will likely want to lower the polling interval for that binding. The default polling interval is &quot;24h&quot; to keep data volumes low, but once a cursor is specified there is usually no downside to frequent polling, so you may wish to lower the interval to &quot;5m&quot; or even &quot;5s&quot; for that table.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Google Analytics 4 BigQuery Native Connector","url":"/reference/Connectors/capture-connectors/bigquery-ga4-native/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the BigQuery source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Google Analytics 4 BigQuery Native Connector","url":"/reference/Connectors/capture-connectors/bigquery-ga4-native/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/project_id\tProject ID\tThe GCP project ID for the project containing the source BigQuery dataset\tstring\tRequired /dataset\tDataset\tThe BigQuery dataset to discover tables within\tstring\tRequired /credentials_json\tCredentials JSON\tThe contents of your Service Account Key JSON file\tstring\tRequired /advanced/poll\tPoll Interval\tHow often to poll bindings (may be overridden for a specific binding)\tstring\t&quot;24h&quot;  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/name\tName\tA name which uniquely identifies this binding.\tstring\tRequired /cursor\tCursor\tThe column name(s) which should be used as the incremental capture cursor\tarray\t[] /template\tTemplate\tThe query (template) which will be executed every polling interval\tstring\tRequired /poll\tPoll Interval\tOverride the global polling interval for this binding.\tstring\t&quot;&quot;  ","version":"Next","tagName":"h3"},{"title":"Query Templates​","type":1,"pageTitle":"Google Analytics 4 BigQuery Native Connector","url":"/reference/Connectors/capture-connectors/bigquery-ga4-native/#query-templates","content":" The query template property of a binding defines what query will be executed against the database, given inputs describing the configured cursor columns and whether any prior cursor state exists. The default template implements the behavior described inspecifying cursor columns.  In principle you are free to modify this template to implement whatever query you need. You could for instance create a new binding which queries a view, or which performs a more complex analytics query. However this should not be combined with table auto-discovery in a single capture, as this can produce some counterintuitive results. Instead create two separate capture tasks from the same database, one for autodiscovered tables and a separate one with the setting &quot;Automatically Add New Collections&quot; disabled for your custom bindings. ","version":"Next","tagName":"h3"},{"title":"Apache Kafka","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/apache-kafka/","content":"","keywords":"","version":"Next"},{"title":"Supported message formats​","type":1,"pageTitle":"Apache Kafka","url":"/reference/Connectors/capture-connectors/apache-kafka/#supported-message-formats","content":" This connectors supports Kafka messages encoded in Avro or JSON format.  For Avro messages, the connector must be configured to use a schema registry.  JSON messages may be read without a schema registry. If the JSON messages were encoded with a JSON schema, configuring a schema registry is recommended to enable discovery of collection keys if the message key has an associated schema.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Apache Kafka","url":"/reference/Connectors/capture-connectors/apache-kafka/#prerequisites","content":" A Kafka cluster with: bootstrap.servers configured so that clients may connect via the desired host and portAn authentication mechanism of choice set up (highly recommended for production environments)Connection security enabled with TLS (highly recommended for production environments) If using schema registry: The endpoint to use for connecting to the schema registryUsername for authenticationPassword for authentication  tip If you are using the Confluent Cloud Schema Registry, your schema registry username and password will be the key and secret from your schema registry API key. See the Confluent Cloud Schema Registry Documentationfor help setting up a schema registry API key.  ","version":"Next","tagName":"h2"},{"title":"Discovered collection schemas​","type":1,"pageTitle":"Apache Kafka","url":"/reference/Connectors/capture-connectors/apache-kafka/#discovered-collection-schemas","content":" If no schema registry is configured, all available topics will be discovered and use a collection key composed of the captured message's partition andoffset. If schema registry is configured, Flow collections for Kafka topics will be discovered using the latest version of the registered key schema for the topic.  For a collection key to be discovered from a registered topic key schema, the topic key schema must be compatible with a Flow collection key, with the following additional considerations:  Key fields must not contain null as a typeKey fields can be a single type onlyKeys may contain nested fields, such as types with nested Avro records  If a topic has a registered key schema but it does not fit these requirements, the default collection key of parition and offset will be used instead.  ","version":"Next","tagName":"h3"},{"title":"Authentication and connection security​","type":1,"pageTitle":"Apache Kafka","url":"/reference/Connectors/capture-connectors/apache-kafka/#authentication-and-connection-security","content":" Neither authentication nor connection security are enabled by default in your Kafka cluster, but both are important considerations. Similarly, Flow's Kafka connectors do not strictly require authentication or connection security mechanisms. You may choose to omit them for local development and testing; however, both are strongly encouraged for production environments.  A wide variety of authentication methods is available in Kafka clusters. Flow supports SASL/SCRAM-SHA-256, SASL/SCRAM-SHA-512, and SASL/PLAIN. Behavior using other authentication methods is not guaranteed. When authentication details are not provided, the client connection will attempt to use PLAINTEXT (insecure) protocol.  If you don't already have authentication enabled on your cluster, Estuary recommends either of listed SASL/SCRAM methods. With SCRAM, you set up a username and password, making it analogous to the traditional authentication mechanisms you use in other applications.  tip If you are connecting to Kafka hosted on Confluent Cloud, select the PLAINSASL mechanism.  For connection security, Estuary recommends that you enable TLS encryption for your SASL mechanism of choice, as well as all other components of your cluster. Note that because TLS replaced now-deprecated SSL encryption, Kafka still uses the acronym &quot;SSL&quot; to refer to TLS encryption. See Confluent's documentation for details.  Beta TLS encryption is currently the only supported connection security mechanism for this connector. Other connection security methods may be enabled in the future.  ","version":"Next","tagName":"h3"},{"title":"AWS Managed Streaming Kafka (MSK)​","type":1,"pageTitle":"Apache Kafka","url":"/reference/Connectors/capture-connectors/apache-kafka/#aws-managed-streaming-kafka-msk","content":" If using AWS Managed Streaming for Apache Kafka (MSK), you can use IAM authentication with our connector. Read more about IAM authentication with MSK in AWS docs: IAM access control.  Additionally, you want to make sure that your VPC configuration allows inbound and outbound requests to Estuary Flow IP addresses.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Apache Kafka","url":"/reference/Connectors/capture-connectors/apache-kafka/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Apache Kafka source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Apache Kafka","url":"/reference/Connectors/capture-connectors/apache-kafka/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/bootstrap_servers\tBootstrap servers\tThe initial servers in the Kafka cluster to connect to, separated by commas. The Kafka client will be informed of the rest of the cluster nodes by connecting to one of these nodes.\tstring\tRequired /tls\tTLS\tTLS connection settings.\tstring\t&quot;system_certificates&quot; /credentials\tCredentials\tConnection details used to authenticate a client connection to Kafka via SASL.\tnull, object /credentials/auth_type\tAuthentication type\tOne of UserPassword for SASL or AWS for IAM authentication\tstring /credentials/mechanism\tSASL Mechanism\tSASL mechanism describing how to exchange and authenticate client servers.\tstring /credentials/password\tPassword\tPassword, if applicable for the authentication mechanism chosen.\tstring /credentials/username\tUsername\tUsername, if applicable for the authentication mechanism chosen.\tstring /credentials/aws_access_key_id\tAWS Access Key ID\tSupply if using auth_type: AWS\tstring /credentials/aws_secret_access_key\tAWS Secret Access Key\tSupply if using auth_type: AWS\tstring /credentials/region\tAWS Region\tSupply if using auth_type: AWS\tstring /schema_registry\tSchema Registry\tConnection details for interacting with a schema registry.\tobject\tRequired schema_registry/schema_registry_type\tSchema Registry Type\tEither confluent_schema_registry or no_schema_registry.\tobject\tRequired /schema_registry/endpoint\tSchema Registry Endpoint\tSchema registry API endpoint. For example: https://registry-id.us-east-2.aws.confluent.cloud.\tstring /schema_registry/username\tSchema Registry Username\tSchema registry username to use for authentication. If you are using Confluent Cloud, this will be the 'Key' from your schema registry API key.\tstring /schema_registry/password\tSchema Registry Password\tSchema registry password to use for authentication. If you are using Confluent Cloud, this will be the 'Secret' from your schema registry API key.\tstring /schema_registry/enable_json_only\tCapture Messages in JSON Format Only\tIf no schema registry is configured the capture will attempt to parse all data as JSON, and discovered collections will use a key of the message partition &amp; offset.\tboolean\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/topic\tStream\tKafka topic name.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Apache Kafka","url":"/reference/Connectors/capture-connectors/apache-kafka/#sample","content":" User and password authentication (SASL):  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-kafka:dev config: bootstrap_servers: server1:9092,server2:9092 tls: system_certificates credentials: auth_type: UserPassword mechanism: SCRAM-SHA-512 username: bruce.wayne password: definitely-not-batman schema_registry: schema_registry_type: confluent_schema_registry endpoint: https://schema.registry.com username: schemaregistry.username password: schemaregistry.password bindings: - resource: topic: ${TOPIC_NAME} target: ${PREFIX}/${COLLECTION_NAME}   AWS IAM authentication:  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-kafka:dev config: bootstrap_servers: server1:9092,server2:9092 tls: system_certificates credentials: auth_type: AWS aws_access_key_id: AK... aws_secret_access_key: secret region: us-east-1 schema_registry: schema_registry_type: confluent_schema_registry endpoint: https://schema.registry.com username: schemaregistry.username password: schemaregistry.password bindings: - resource: topic: ${TOPIC_NAME} target: ${PREFIX}/${COLLECTION_NAME}   Your capture definition will likely be more complex, with additional bindings for each Kafka topic.  Learn more about capture definitions.. ","version":"Next","tagName":"h3"},{"title":"Azure Blob Storage","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/azure-blob-storage/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Azure Blob Storage","url":"/reference/Connectors/capture-connectors/azure-blob-storage/#prerequisites","content":" You will need the following values to authenticate to Azure and an active subscription  Subscription IDClient IDClient SecretTenant ID  ","version":"Next","tagName":"h2"},{"title":"Setup a Microsoft Entra application​","type":1,"pageTitle":"Azure Blob Storage","url":"/reference/Connectors/capture-connectors/azure-blob-storage/#setup-a-microsoft-entra-application","content":" These values can be obtained from the portal, here's the instructions:  Get Subscription ID Login into your Azure account Select Subscriptions in the left sidebar Select whichever subscription is neededClick on OverviewCopy the Subscription ID Get Client ID / Client Secret / Tenant ID Go to Azure Active Directory, then select App registrations.Click New registration, fill out the required fields like Name and Supported account types.Click Register to create the new app registration.After registration, note down the Application (client) ID value.Go to Certificates &amp; secrets and click New client secret. Fill in a Description, choose a Secret value type and length, then click Add to save the secret. A pop-up will appear with your new client secret value; copy it immediately as you won't be able to view it again.Go back to Overview page and copy the Directory (tenant) ID under Properties.Your service principal is now created, and you have its Application (client) ID, Directory (tenant) ID, and a client secret key. Use these values when configuring your application or service to interact with Azure services that require authentication through AAD.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Azure Blob Storage","url":"/reference/Connectors/capture-connectors/azure-blob-storage/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Azure Blob Storage source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Azure Blob Storage","url":"/reference/Connectors/capture-connectors/azure-blob-storage/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/advanced Options for advanced users. You should not typically need to modify these.\tobject /advanced/ascendingKeys\tAscending Keys\tImprove sync speeds by listing files from the end of the last sync, rather than listing the entire bucket prefix. This requires that you write objects in ascending lexicographic order, such as an RFC-3339 timestamp, so that key ordering matches modification time ordering. If data is not ordered correctly, using ascending keys could cause errors.\tboolean\tfalse /credentials\tCredentials\tAzure credentials used to authenticate with Azure Blob Storage.\tobject /credentials/storageAccountName\tStorage Account Name\tThe name of the Azure Blob Storage account.\tstring\tRequired. /credentials/azureClientID\tAzure Client ID\tThe client ID used to authenticate with Azure Blob Storage.\tstring\tRequired if using OAuth2 /credentials/azureClientSecret\tAzure Client Secret\tThe client secret used to authenticate with Azure Blob Storage.\tstring\tRequired if using OAuth2 /credentials/azureTenantID\tAzure Tenant ID\tThe ID of the Azure tenant where the Azure Blob Storage account is located.\tstring\tRequired if using OAuth2 /credentials/azureSubscriptionID\tAzure Subscription ID\tThe ID of the Azure subscription that contains the Azure Blob Storage account.\tstring\tRequired if using OAuth2 /credentials/ConnectionString\tConnection String\tThe connection string used to authenticate with Azure Blob Storage.\tstring\tRequired if using the Connection String authentication. /containerName\tContainer Name\tThe name of the Azure Blob Storage container to read from.\tstring\tRequired. /matchKeys\tMatch Keys\tFilter applied to all object keys under the prefix. If provided, only objects whose absolute path matches this regex will be read. For example, you can use &quot;.*\\.json&quot; to only capture json files.\tstring\t  Bindings​  Property\tTitle\tTitle\tType\tRequired/Default/stream\tContainer\tThe container name\tstring\trequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Azure Blob Storage","url":"/reference/Connectors/capture-connectors/azure-blob-storage/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: &quot;ghcr.io/estuary/source-azure-blob-storage:dev&quot; config: containerName: example credentials: azureClientID: e2889d31-aaaa-bbbb-cccc-85bb5a33d7a5 azureClientSecret: just-a-secret azureSubscriptionID: f1a5bc81-aaaa-bbbb-cccc-b926c154ecc7 azureTenantID: d494a2c6-aaaa-bbbb-cccc-ef1e5eaa64a6 storageAccountName: example parser: compression: zip format: type: csv config: delimiter: &quot;,&quot; encoding: UTF-8 errorThreshold: 5 headers: [ID, username, first_name, last_name] lineEnding: &quot;\\\\r&quot; quote: &quot;\\&quot;&quot; bindings: - resource: stream: example target: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Advanced: Parsing cloud storage data​","type":1,"pageTitle":"Azure Blob Storage","url":"/reference/Connectors/capture-connectors/azure-blob-storage/#advanced-parsing-cloud-storage-data","content":" Cloud storage platforms like Azure Blob Storage can support a wider variety of file types than other data source systems. For each of these file types, Flow must parse and translate data into collections with defined fields and JSON schemas.  By default, the parser will automatically detect the type and shape of the data in your bucket, so you won't need to change the parser configuration for most captures.  However, the automatic detection may be incorrect in some cases. To fix or prevent this, you can provide explicit information in the parser configuration, which is part of the endpoint configuration for this connector.  The parser configuration includes:  Compression: Specify how the bucket contents are compressed. If no compression type is specified, the connector will try to determine the compression type automatically. Options are: zipgzipzstdnone Format: Specify the data format, which determines how it will be parsed. Options are: Auto: If no format is specified, the connector will try to determine it automatically.AvroCSVJSONProtobufW3C Extended Log info At this time, Flow only supports S3 captures with data of a single file type. Support for multiple file types, which can be configured on a per-binding basis, will be added in the future. For now, use a prefix in the endpoint configuration to limit the scope of each capture to data of a single file type.  ","version":"Next","tagName":"h3"},{"title":"CSV configuration​","type":1,"pageTitle":"Azure Blob Storage","url":"/reference/Connectors/capture-connectors/azure-blob-storage/#csv-configuration","content":" CSV files include several additional properties that are important to the parser. In most cases, Flow is able to automatically determine the correct values, but you may need to specify for unusual datasets. These properties are:  Delimiter. Options are: Comma (&quot;,&quot;)Pipe (&quot;|&quot;)Space (&quot;0x20&quot;)Semicolon (&quot;;&quot;)Tab (&quot;0x09&quot;)Vertical tab (&quot;0x0B&quot;)Unit separator (&quot;0x1F&quot;)SOH (&quot;0x01&quot;)Auto Encoding type, specified by its WHATWG label. Optionally, an Error threshold, as an acceptable percentage of errors. If set to a number greater than zero, malformed rows that fall within the threshold will be excluded from the capture. Escape characters. Options are: Backslash (&quot;\\\\&quot;)Disable escapes (&quot;&quot;)Auto Optionally, a list of column Headers, if not already included in the first row of the CSV file. If any headers are provided, it is assumed that the provided list of headers is complete and authoritative. The first row of your CSV file will be assumed to be data (not headers), and you must provide a header value for every column in the file. Line ending values CRLF (&quot;\\\\r\\\\n&quot;) (Windows)CR (&quot;\\\\r&quot;)LF (&quot;\\\\n&quot;)Record Separator (&quot;0x1E&quot;)Auto Quote character Double Quote (&quot;\\&quot;&quot;)Single Quote (&quot;)Disable Quoting (&quot;&quot;)Auto  The sample specification above includes these fields. ","version":"Next","tagName":"h3"},{"title":"Bing Ads","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/bing-ads/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Bing Ads","url":"/reference/Connectors/capture-connectors/bing-ads/#supported-data-resources","content":" The following data resources are supported:  AccountsAccount performance reports: hourly, daily, weekly, and monthly (four resources)Ad groupsAd group performance reports: hourly, daily, weekly, and monthly (four resources)AdsAd performance reports: hourly, daily, weekly, and monthly (four resources).Budget summary reportCampaignsCampaign performance reports: hourly, daily, weekly, and monthly (four resources).Keyword performance reports: hourly, daily, weekly, and monthly (four resources).  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Bing Ads","url":"/reference/Connectors/capture-connectors/bing-ads/#prerequisites","content":" This connector uses OAuth2 to authenticate with Microsoft. You can do this in the Flow web app, or configure manually if you're using the flowctl CLI.  ","version":"Next","tagName":"h2"},{"title":"Using OAuth2 to authenticate with Microsoft in the Flow web app​","type":1,"pageTitle":"Bing Ads","url":"/reference/Connectors/capture-connectors/bing-ads/#using-oauth2-to-authenticate-with-microsoft-in-the-flow-web-app","content":" You'll need:  User credentials with access to the Bing Ads account. A developer token associated with the user.  ","version":"Next","tagName":"h3"},{"title":"Authenticating manually using the CLI​","type":1,"pageTitle":"Bing Ads","url":"/reference/Connectors/capture-connectors/bing-ads/#authenticating-manually-using-the-cli","content":" You'll need:  A registered Bing Ads application with the following credentials retrieved: Client ID Client Secret Refresh Token  To set get these items, complete the following steps:  Register your Bing Ads Application in the Azure Portal. During setup, note the client_id and client_secret. Get a user access token. Redeem the user authorization code for OAuth tokens, and note the refresh_token.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Bing Ads","url":"/reference/Connectors/capture-connectors/bing-ads/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Bing Ads source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Bing Ads","url":"/reference/Connectors/capture-connectors/bing-ads/#properties","content":" Endpoint​  The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so many of these properties aren't required.  Property\tTitle\tDescription\tType\tRequired/Default/credentials\tCredentials object\tRequired /credentials/auth_method\tAuthentication method\tSet to oauth2.0\tString\toauth2.0 /credentials/client_id\tClient ID\tThe Client ID of your Microsoft Advertising developer application.\tString\tRequired /credentials/client_secret\tClient Secret\tThe Client Secret of your Microsoft Advertising developer application.\tString\tRequired /credentials/refresh_token\tRefresh Token\tRefresh Token to renew the expired Access Token.\tString\tRequired /developer_token\tDeveloper Token\tDeveloper token associated with user.\tString\tRequired /reports_start_date\tCredentials\tThe start date from which to begin replicating report data. Any data generated before this date will not be replicated in reports. This is a UTC date in YYYY-MM-DD format.\tString\tRequired, 2020-01-01 /tenant_id\tCredentials\tThe Tenant ID of your Microsoft Advertising developer application. Set this to common unless you know you need a different value.\tString\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tBing Ads resource from which a collection is captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Bing Ads","url":"/reference/Connectors/capture-connectors/bing-ads/#sample","content":" This sample specification reflects the manual authentication method.  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-bing-ads:dev config: credentials: auth_type: oauth2.0 client_id: 6731de76-14a6-49ae-97bc-6eba6914391e client_secret: &lt;secret&gt; refresh_token: &lt;token&gt; developer_token: &lt;token&gt; reports_start_date: 2020-01-01 tenant_id: common bindings: - resource: stream: accounts syncMode: full_refresh target: ${PREFIX}/accounts {}  ","version":"Next","tagName":"h3"},{"title":"Braintree","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/braintree/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Braintree","url":"/reference/Connectors/capture-connectors/braintree/#supported-data-resources","content":" The connector automatically discovers bindings for the Braintree resources listed below. By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Full Refresh Streams​","type":1,"pageTitle":"Braintree","url":"/reference/Connectors/capture-connectors/braintree/#full-refresh-streams","content":" Add OnsDiscountsMerchant AccountsPlans  ","version":"Next","tagName":"h3"},{"title":"Incremental Streams​","type":1,"pageTitle":"Braintree","url":"/reference/Connectors/capture-connectors/braintree/#incremental-streams","content":" Credit Card VerificationsCustomersDisputesSubscriptionsTransactions  tip All incremental streams except Transactions only capture creates, not updates, of resources due to Braintree API limitations. To capture updates to these resources, regular backfills are required. Please reach out via email or Slack to set up and schedule regular backfills.  ","version":"Next","tagName":"h3"},{"title":"Prerequisites​","type":1,"pageTitle":"Braintree","url":"/reference/Connectors/capture-connectors/braintree/#prerequisites","content":" To set up the Braintree source connector, you'll need the following from your Braintree account:  Merchant IDPublic KeyPrivate Key  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Braintree","url":"/reference/Connectors/capture-connectors/braintree/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Braintree source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Braintree","url":"/reference/Connectors/capture-connectors/braintree/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/merchant_id\tMerchant ID\tThe unique identifier for your Braintree gateway account.\tstring\tRequired /start_date\tStart Date\tUTC date and time in the format &quot;YYYY-MM-DDTHH:MM:SSZ&quot;. Any data before this date will not be replicated.\tstring\t30 days prior to the current date /credentials/public_key\tPublic Key\tBraintree Public Key.\tstring\tRequired /credentials/private_key\tPrivate Key\tBraintree Private Key.\tstring\tRequired /advanced/is_sandbox\tSandbox Environment\tSet to true if the credentials are for a sandbox Braintree environment.\tboolean\tfalse /advanced/window_size\tWindow Size\tThe window size in hours to use when fetching data from Braintree. Typically, this is left as the default value unless the connector raises an error stating that the window size needs to be reduced.\tinteger\t24  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/name\tData resource\tName of the data resource.\tstring\tRequired /interval\tInterval\tInterval between data syncs\tstring\tPT5M  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Braintree","url":"/reference/Connectors/capture-connectors/braintree/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-braintree-native:dev config: merchant_id: my_merchant_id start_date: &quot;2024-12-04T00:00:00Z&quot; credentials: public_key: my_public_key private_key: my_private_key advanced: is_sandbox: false window_size: 15 bindings: - resource: name: add_ons interval: PT5M target: ${PREFIX}/add_ons - resource: name: credit_card_verifications interval: PT5M target: ${PREFIX}/credit_card_verifications - resource: name: customers interval: PT5M target: ${PREFIX}/customers - resource: name: discounts interval: PT5M target: ${PREFIX}/discounts - resource: name: disputes interval: PT5M target: ${PREFIX}/disputes - resource: name: merchant_accounts interval: PT5M target: ${PREFIX}/merchant_accounts - resource: name: merchant_accounts interval: PT5M target: ${PREFIX}/merchant_accounts - resource: name: plans interval: PT5M target: ${PREFIX}/plans - resource: name: subscriptions interval: PT5M target: ${PREFIX}/subscriptions - resource: name: transactions interval: PT5M target: ${PREFIX}/transactions {...}  ","version":"Next","tagName":"h3"},{"title":"Braze","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/braze/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Braze","url":"/reference/Connectors/capture-connectors/braze/#supported-data-resources","content":" The following data resources are supported through the Braze APIs:  campaignscampaigns_analyticscanvasescanvases_analyticseventsevents_analyticskpi_daily_new_userskpi_daily_active_userskpi_daily_app_uninstallscardscards_analyticssegmentssegments_analytics  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Braze","url":"/reference/Connectors/capture-connectors/braze/#prerequisites","content":" It is required to have an account on Braze to provide us with URL and Rest API Key during set up.  Rest API Key could be found on Braze Dashboard -&gt; Developer Console tab -&gt; API Settings -&gt; Rest API KeysURL could be found on Braze Dashboard -&gt; Manage Settings -&gt; Settings tab -&gt; Your App name -&gt; SDK Endpoint  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Braze","url":"/reference/Connectors/capture-connectors/braze/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Braze source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Braze","url":"/reference/Connectors/capture-connectors/braze/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/api_key\tBraze REST API Key\tBraze API Key.\tstring\tRequired /start_date\tStart Date\tRows after this date will be synced.\tstring\tRequired /url\tURL\tBraze REST API endpoint.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Braze project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Braze","url":"/reference/Connectors/capture-connectors/braze/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-braze:dev config: api_key: &lt;key&gt; start_date: 2017-01-25T00:00:00Z url: &lt;url&gt; bindings: - resource: stream: events syncMode: full_refresh target: ${PREFIX}/events {...}  ","version":"Next","tagName":"h3"},{"title":"Brevo","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/brevo/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Brevo","url":"/reference/Connectors/capture-connectors/brevo/#supported-data-resources","content":" The following data resources are supported through the Brevo APIs:  ContactsContacts AttributesContacts Lists  By default, each resource is mapped to a Flow collection through a separate binding.  If your use case requires additional Brevo APIs, such as Campaigns, Events, or Accounts, contact us to discuss the possibility of expanding this connector.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Brevo","url":"/reference/Connectors/capture-connectors/brevo/#prerequisites","content":" You will need a Brevo API key. See Brevo's documentation for instructions on creating one.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Brevo","url":"/reference/Connectors/capture-connectors/brevo/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Brevo source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Brevo","url":"/reference/Connectors/capture-connectors/brevo/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/api-key\tAPI Key\tThe Brevo API key used for authentication.\tstring\tRequired /start_date\tStart Date\tEarliest date to read data from. Uses date-time format, ex. YYYY-MM-DDT00:00:00.000Z.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tBrevo resource from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Brevo","url":"/reference/Connectors/capture-connectors/brevo/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-brevo:dev config: api-key: {secret} start_date: 2025-01-01T00:00:00.000Z bindings: - resource: stream: contacts syncMode: full_refresh target: ${PREFIX}/contacts {...}  ","version":"Next","tagName":"h3"},{"title":"Chargebee","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/chargebee/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Chargebee","url":"/reference/Connectors/capture-connectors/chargebee/#supported-data-resources","content":" The following data resources are supported through the Chargebee APIs:  SubscriptionsCustomersInvoicesOrdersPlansAddonsItemsItem PricesAttached Items  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Chargebee","url":"/reference/Connectors/capture-connectors/chargebee/#prerequisites","content":" To set up the Chargebee source connector, you'll need the Chargebee API key and the Product Catalog version.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Chargebee","url":"/reference/Connectors/capture-connectors/chargebee/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Chargebee source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Chargebee","url":"/reference/Connectors/capture-connectors/chargebee/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/site_api_key\tAPI Key\tChargebee API Key.\tstring\tRequired /site\tSite\tThe site prefix for your Chargebee instance.\tstring\tRequired /start_date\tStart Date\tUTC date and time in the format 2021-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired /product_catalog\tProduct Catalog\tProduct Catalog version of your Chargebee site. Instructions on how to find your version you may find under 'API Version' section in the Chargebee docs.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Chargebee project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Chargebee","url":"/reference/Connectors/capture-connectors/chargebee/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-chargebee:dev config: site_api_key: &lt;secret&gt; site: &lt;your site&gt; start_date: 2017-01-25T00:00:00Z product_catalog: &lt;your product catalog&gt; bindings: - resource: stream: items syncMode: full_refresh target: ${PREFIX}/items {...}  ","version":"Next","tagName":"h3"},{"title":"Confluence","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/confluence/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Confluence","url":"/reference/Connectors/capture-connectors/confluence/#supported-data-resources","content":" When you configure the connector, you specify your email, api and domain name  From your selection, the following data resources are captured:  ","version":"Next","tagName":"h2"},{"title":"resources​","type":1,"pageTitle":"Confluence","url":"/reference/Connectors/capture-connectors/confluence/#resources","content":" AuditBlog PostsGroupPagesSpace  Each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h3"},{"title":"Prerequisites​","type":1,"pageTitle":"Confluence","url":"/reference/Connectors/capture-connectors/confluence/#prerequisites","content":" Atlassian API TokenYour Confluence domain nameYour Confluence login email  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Confluence","url":"/reference/Connectors/capture-connectors/confluence/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the GitHub source connector.  Create an API Token  For detailed instructions on creating an Atlassian API Token, please refer to the official documentation.  Set up the Confluence connector in Estuary Flow  Log into Estuary Flow and click &quot;Captures&quot;.Select &quot;Create Capture&quot; search for and click on &quot;Confluence&quot;Enter a Capture NameIn the &quot;API Token&quot; field, enter your Atlassian API TokenIn the &quot;Domain Name&quot; field, enter your Confluence Domain nameIn the &quot;Email&quot; field, enter your Confluence login emailClick &quot;Save and Publish&quot;  ","version":"Next","tagName":"h2"},{"title":"Sample​","type":1,"pageTitle":"Confluence","url":"/reference/Connectors/capture-connectors/confluence/#sample","content":" This sample specification reflects the manual authentication method.  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-confluence:dev config: credentials: api_token: PAT Credentials domain_name: estuary1.atlassian.net email: dave@estuary.dev bindings: - resource: stream: audit syncMode: full_refresh target: ${PREFIX}/audit {...}  ","version":"Next","tagName":"h3"},{"title":"Criteo","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/criteo/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Criteo","url":"/reference/Connectors/capture-connectors/criteo/#supported-data-resources","content":" The following data resources are supported through the Criteo APIs:  Ad SetsAdvertisersAudiencesAudiences (Legacy)Campaigns (Legacy)Campaigns (Preview)Categories (Legacy)  You may also configure multiple Report resources based on desired dimensions and metrics.  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Criteo","url":"/reference/Connectors/capture-connectors/criteo/#prerequisites","content":" To set up a Criteo source connector in Flow, you will need:  A Criteo Client IDA Criteo Client SecretOne or more Advertiser IDs  See Criteo's documentation for information on authentication and where to find your IDs.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Criteo","url":"/reference/Connectors/capture-connectors/criteo/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Criteo source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Criteo","url":"/reference/Connectors/capture-connectors/criteo/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/client_id\tClient ID\tThe Criteo client ID used for authentication.\tstring\tRequired /client_secret\tClient Secret\tThe Criteo client secret used for authentication.\tstring\tRequired /advertiser_ids\tAdvertiser IDs\tOne or more Criteo advertiser IDs.\tstring[]\tRequired /start_date\tStart Date\tEarliest date to read data from. Uses UTC date-time format, ex. YYYY-MM-DDT00:00:00.000Z.\tstring\tRequired /reports\tReports\tOptional configuration for additional report streams.\tobject[] /reports/-/name\tReport Name\tThe report's name.\tstring /reports/-/dimensions\tReport Dimensions\tAn array of dimensions. See Criteo's documentation for possible options.\tstring /reports/-/metrics\tReport Metrics\tAn array of metrics. See Criteo's documentation for possible options.\tstring /reports/-/currency\tReport Currency\tThe report's currency.\tstring\tUSD  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tCriteo resource from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Criteo","url":"/reference/Connectors/capture-connectors/criteo/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-criteo:dev config: client_id: {secret} client_secret: {secret} advertiser_ids: - &quot;12345&quot; - &quot;67890&quot; start_date: 2025-01-01T00:00:00.000Z bindings: - resource: stream: advertisers syncMode: full_refresh target: ${PREFIX}/advertisers {...}  ","version":"Next","tagName":"h3"},{"title":"Dropbox","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/dropbox/","content":"","keywords":"","version":"Next"},{"title":"Supported data types​","type":1,"pageTitle":"Dropbox","url":"/reference/Connectors/capture-connectors/dropbox/#supported-data-types","content":" This connector automatically captures the data within the specified Dropbox folder into a single Flow collection.  The following file types are supported:  AvroCSVJSONProtobufW3C Extended Log  The following compression methods are supported:  ZIPGZIPZSTD  By default, Flow automatically detects the file type and compression method. If necessary, you can specify the correct file type, compression, and other properties (CSV only) using the optional parser configuration.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Dropbox","url":"/reference/Connectors/capture-connectors/dropbox/#prerequisites","content":" To use this connector, make sure you have an active Dropbox account. Authentication is handled using OAuth2 in the Flow web app.  Note: This connector is designed for files located in a specific Dropbox folder.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Dropbox","url":"/reference/Connectors/capture-connectors/dropbox/#configuration","content":" You configure the Dropbox source connector in the Flow web app. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Dropbox source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Dropbox","url":"/reference/Connectors/capture-connectors/dropbox/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/path\tPath\tThe path to the Dropbox folder to read from. For example, &quot;/my_folder&quot;.\tstring\tRequired /matchKeys\tMatch Keys\tFilter applied to all object keys under the prefix. If provided, only objects whose absolute path matches this regex will be read. For example, you can use .*\\.json to only capture JSON files.\tobject /credentials\tCredentials\tOAuth2 credentials for Dropbox. These are automatically handled by the Web UI.\tobject\tRequired /advanced\tAdvanced\tOptions for advanced users. You should not typically need to modify these.\tobject /advanced/ascendingKeys\tAscending Keys\tImprove sync speeds by listing files from the end of the last sync, rather than listing the entire bucket prefix. This requires that you write objects in ascending lexicographic order, such as an RFC-3339 timestamp, so that key ordering matches modification time ordering. If data is not ordered correctly, using ascending keys could cause errors.\tboolean\tfalse /parser\tParser Configuration\tConfigures how files are parsed (optional, see below)\tobject /parser/compression\tCompression\tDetermines how to decompress the contents. The default, 'Auto', will try to determine the compression automatically.\tnull, string\tnull /parser/format\tFormat\tDetermines how to parse the contents. The default, 'Auto', will try to determine the format automatically based on the file extension or MIME type, if available.\tobject\t{&quot;type&quot;:&quot;auto&quot;} /parser/format/type\tType string\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tPath\tThe path to the Dropbox folder.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Dropbox","url":"/reference/Connectors/capture-connectors/dropbox/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-http-file:dev config: path: &quot;/my_folder&quot; matchKeys: &quot;.*\\.json&quot; bindings: - resource: stream: /my_folder target: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Advanced: Parsing Dropbox files​","type":1,"pageTitle":"Dropbox","url":"/reference/Connectors/capture-connectors/dropbox/#advanced-parsing-dropbox-files","content":" Dropbox folders can contain a variety of file types. For each file type, Flow must parse and translate data into collections with defined fields and JSON schemas.  By default, the parser will automatically detect the type and shape of the data in the Dropbox folder, so you won't need to change the parser configuration for most captures.  However, the automatic detection may be incorrect in some cases. To fix or prevent this, you can provide explicit information in the parser configuration, which is part of the endpoint configuration for this connector.  The parser configuration includes:  Compression: Specify how the data is compressed. If no compression type is specified, the connector will try to determine the compression type automatically. Options are: zipgzipzstdnone Format: Specify the data format, which determines how it will be parsed. If no file type is specified, the connector will try to determine the file type automatically Options are: AvroCSVJSONProtobufW3C Extended Log  CSV configuration​  CSV files include several additional properties that are important to the parser. In most cases, Flow is able to automatically determine the correct values, but you may need to specify for unusual datasets. These properties are:  Delimiter. Options are: Comma (&quot;,&quot;)Pipe (&quot;|&quot;)Space (&quot;0x20&quot;)Semicolon (&quot;;&quot;)Tab (&quot;0x09&quot;)Vertical tab (&quot;0x0B&quot;)Unit separator (&quot;0x1F&quot;)SOH (&quot;0x01&quot;)Auto Encoding type, specified by its WHATWG label. Optionally, an Error threshold, as an acceptable percentage of errors. If set to a number greater than zero, malformed rows that fall within the threshold will be excluded from the capture. Escape characters. Options are: Backslash (&quot;\\\\&quot;)Disable escapes (&quot;&quot;)Auto Optionally, a list of column Headers, if not already included in the first row of the CSV file. If any headers are provided, it is assumed that the provided list of headers is complete and authoritative. The first row of your CSV file will be assumed to be data (not headers), and you must provide a header value for every column in the file. Line ending values CRLF (&quot;\\\\r\\\\n&quot;) (Windows)CR (&quot;\\\\r&quot;)LF (&quot;\\\\n&quot;)Record Separator (&quot;0x1E&quot;)Auto Quote character Double Quote (&quot;\\&quot;&quot;)Single Quote (&quot;)Disable Quoting (&quot;&quot;)Auto ","version":"Next","tagName":"h3"},{"title":"Exchange Rates API","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/exchange-rates/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Exchange Rates API","url":"/reference/Connectors/capture-connectors/exchange-rates/#prerequisites","content":" An API key generated through an Exchange Rate API account. After you sign up, your API key can be found on your account page. You may use the free account, but note that you'll be limited to the default base currency, EUR.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Exchange Rates API","url":"/reference/Connectors/capture-connectors/exchange-rates/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Exchange Rates source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Exchange Rates API","url":"/reference/Connectors/capture-connectors/exchange-rates/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/access_key\tAccess key\tYour API access key. The key is case sensitive.\tstring\tRequired /base\tBase currency\tISO reference currency. See the documentation. Free plan doesn't support Source Currency Switching, default base currency is EUR\tstring\tEUR /ignore_weekends\tIgnore weekends\tIgnore weekends? (Exchanges don't run on weekends)\tboolean\ttrue /start_date\tStart date\tThe date in the format YYYY-MM-DD. Data will begin from this date.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tData stream from which Flow captures data. Always set to exchange_rates.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Exchange Rates API","url":"/reference/Connectors/capture-connectors/exchange-rates/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-exchange-rates:dev config: base: EUR access_key: &lt;secret&gt; start_date: 2022-01-01 ignore_weekends: true bindings: - resource: stream: exchange_rates syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}   This capture definition should only have one binding, as exchange_rates is the only available data stream.  Learn more about capture definitions. ","version":"Next","tagName":"h3"},{"title":"Freshdesk","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/freshdesk/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Freshdesk","url":"/reference/Connectors/capture-connectors/freshdesk/#supported-data-resources","content":" The following data resources are supported:  AgentsBusiness hoursCanned response foldersCanned responsesCompaniesContactsConversationsDiscussion categoriesDiscussion commentsDiscussion forumsDiscussion topicsEmail configsEmail mailboxesGroupsProductsRolesSatisfaction ratingsScenario automationsSettingsSkillsSLA policiesSolution articlesSolution categoriesSolution foldersSurveysTicket fieldsTicketsTime entries  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Freshdesk","url":"/reference/Connectors/capture-connectors/freshdesk/#prerequisites","content":" To use this connector, you'll need:  Your Freshdesk account URLYour Freshdesk API key  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Freshdesk","url":"/reference/Connectors/capture-connectors/freshdesk/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Freshdesk source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Freshdesk","url":"/reference/Connectors/capture-connectors/freshdesk/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/api_key\tAPI Key\tFreshdesk API Key.\tstring\tRequired /domain\tDomain\tFreshdesk domain\tstring\tRequired /requests_per_minute\tRequests per minute\tThe number of requests per minute that this source is allowed to use. There is a rate limit of 50 requests per minute per app per account.\tinteger /start_date\tStart Date\tUTC date and time. Any data created after this date will be replicated. If this parameter is not set, all data will be replicated.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource from the Freshdesk API from which a collection is captured.\tstring\tRequired /syncMode\tSync mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Freshdesk","url":"/reference/Connectors/capture-connectors/freshdesk/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-freshdesk:dev config: api_key: xxxxxxxxxxxxxxxx domain: acmesupport.freshdesk.com bindings: - resource: stream: agents syncMode: incremental target: ${PREFIX}/agents {...}  ","version":"Next","tagName":"h3"},{"title":"Facebook Marketing","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/facebook-marketing/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Facebook Marketing","url":"/reference/Connectors/capture-connectors/facebook-marketing/#supported-data-resources","content":" The following data resources are supported:  AdsAd activitiesAd creativesAd insightsAd setsBusiness ad accountsCampaignsCustom ConversionsImagesVideos  By default, each resource associated with your Facebook Business account is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Facebook Marketing","url":"/reference/Connectors/capture-connectors/facebook-marketing/#prerequisites","content":" There are two ways to authenticate with Facebook when capturing data into Flow: signing in with OAuth2, and manually supplying an access token. Their prerequisites differ.  OAuth is recommended for simplicity in the Flow web app; the manual method is the only supported method using the command line.  ","version":"Next","tagName":"h2"},{"title":"Signing in with OAuth2​","type":1,"pageTitle":"Facebook Marketing","url":"/reference/Connectors/capture-connectors/facebook-marketing/#signing-in-with-oauth2","content":" To use OAuth2 in the Flow web app, you'll need a Facebook Business account and its Ad Account ID.  ","version":"Next","tagName":"h3"},{"title":"Configuring manually with an access token​","type":1,"pageTitle":"Facebook Marketing","url":"/reference/Connectors/capture-connectors/facebook-marketing/#configuring-manually-with-an-access-token","content":" To configure manually with an access token, you'll need:  A Facebook Business account, and its Ad Account ID.A Facebook app with: The Marketing API enabled.A Marketing API access token generated.Access upgrade from Standard Access (the default) to Advanced Access. This allows a sufficient rate limit to support the connector.  Follow the steps below to meet these requirements.  Setup​  Find your Facebook Ad Account ID. In Meta for Developers, create a new app of the type Business. On your new app's dashboard, click the button to set up the Marketing API. On the Marketing API Tools tab, generate a Marketing API access token with all available permissions (ads_management, ads_read, read_insights, and business_management). Request Advanced Access for your app. Specifically request the Advanced Access to the following: The feature Ads Management Standard Access The permission ads_read The permission ads_management Once your request is approved, you'll have a high enough rate limit to proceed with running the connector.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Facebook Marketing","url":"/reference/Connectors/capture-connectors/facebook-marketing/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Facebook Marketing source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Facebook Marketing","url":"/reference/Connectors/capture-connectors/facebook-marketing/#properties","content":" Endpoint​  By default, this connector captures all data associated with your Business Ad Account.  You can refine the data you capture from Facebook Marketing using the optional Custom Insights configuration. You're able to specify certain fields to capture and apply data breakdowns.Breakdowns are a feature of the Facebook Marketing Insights API that allows you to group API output by common metrics.Action breakdownsare a subset of breakdowns that must be specified separately.  Property\tTitle\tDescription\tType\tRequired/Default/access_token\tAccess Token\tThe value of the access token generated.\tstring\tRequired /account_ids\tAccount IDs\tA comma delimited string of Facebook Ad account IDs to use when pulling data from the Facebook Marketing API.\tstring\tRequired /custom_insights\tCustom Insights\tA list which contains insights entries. Each entry must have a name and can contains fields, breakdowns or action_breakdowns\tarray /custom_insights/-/action_breakdowns\tAction Breakdowns\tA list of chosen action_breakdowns to apply\tarray\t[] /custom_insights/-/action_breakdowns/-\tValidActionBreakdowns\tGeneric enumeration. Derive from this class to define new enumerations.\tstring /custom_insights/-/breakdowns\tBreakdowns\tA list of chosen breakdowns to apply\tarray\t[] /custom_insights/-/breakdowns/-\tValidBreakdowns\tGeneric enumeration. Derive from this class to define new enumerations.\tstring /custom_insights/-/end_date\tEnd Date\tThe date until which you'd like to replicate data for this stream, in the format YYYY-MM-DDT00:00:00Z. All data generated between the start date and this date will be replicated. Not setting this option will result in always syncing the latest data.\tstring /custom_insights/-/fields\tFields\tA list of chosen fields to capture\tarray\t[] /custom_insights/-/fields/-\tValidEnums\tGeneric enumeration. Derive from this class to define new enumerations.\tstring /custom_insights/-/name\tName\tThe name of the insight\tstring /custom_insights/-/start_date\tStart Date\tThe date from which you'd like to replicate data for this stream, in the format YYYY-MM-DDT00:00:00Z.\tstring /custom_insights/-/time_increment\tTime Increment\tTime window in days by which to aggregate statistics. The sync will be chunked into N day intervals, where N is the number of days you specified. For example, if you set this value to 7, then all statistics will be reported as 7-day aggregates by starting from the start_date. If the start and end dates are October 1st and October 30th, then the connector will output 5 records: 01 - 06, 07 - 13, 14 - 20, 21 - 27, and 28 - 30 (3 days only).\tinteger\t1 /end_date\tEnd Date\tThe date until which you'd like to capture data, in the format YYYY-MM-DDT00:00:00Z. All data generated between start_date and this date will be replicated. Not setting this option will result in always syncing the latest data.\tstring /fetch_thumbnail_images\tFetch Thumbnail Images\tIn each Ad Creative, fetch the thumbnail_url and store the result in thumbnail_data_url\tboolean\tfalse /include_deleted\tInclude Deleted\tInclude data from deleted Campaigns, Ads, and AdSets\tboolean\tfalse /insights_lookback_window\tInsights Lookback Window\tThe attribution window\tinteger\t28 /max_batch_size\tMaximum size of Batched Requests\tMaximum batch size used when sending batch requests to Facebook API. Most users do not need to set this field unless they specifically need to tune the connector to address specific issues or use cases.\tinteger\t50 /page_size\tPage Size of Requests\tPage size used when sending requests to Facebook API to specify number of records per page when response has pagination. Most users do not need to set this field unless they specifically need to tune the connector to address specific issues or use cases.\tinteger\t25 /start_date\tStart Date\tThe date from which you'd like to begin capturing data, in the format YYYY-MM-DDT00:00:00Z. All data generated after this date will be replicated.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Facebook Marketing account from which collections are captured.\tstring\tRequired /syncMode\tSync mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Facebook Marketing","url":"/reference/Connectors/capture-connectors/facebook-marketing/#sample","content":" This sample specification reflects the manual authentication method.  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-facebook-marketing:dev config: access_token: &lt;secret&gt; account_ids: &quot;000000000000000,111111111111111&quot; start_date: 2022-03-01T00:00:00Z custom_insights: - name: my-custom-insight fields: [ad_id, account_currency] breakdowns: [device_platform] action_breakdowns: [action_type] start_date: 2022-03-01T00:00:00Z bindings: - resource: stream: ad_account syncMode: incremental target: ${PREFIX}/ad_account - resource: stream: ad_sets syncMode: incremental target: ${PREFIX}/ad_sets - resource: stream: ads_insights syncMode: incremental target: ${PREFIX}/ads_insights - resource: stream: ads_insights_age_and_gender syncMode: incremental target: ${PREFIX}/ads_insights_age_and_gender - resource: stream: ads_insights_country syncMode: incremental target: ${PREFIX}/ads_insights_country - resource: stream: ads_insights_region syncMode: incremental target: ${PREFIX}/ads_insights_region - resource: stream: ads_insights_dma syncMode: incremental target: ${PREFIX}/ads_insights_dma - resource: stream: ads_insights_platform_and_device syncMode: incremental target: ${PREFIX}/ads_insights_platform_and_device - resource: stream: ads_insights_action_type syncMode: incremental target: ${PREFIX}/ads_insights_action_type - resource: stream: campaigns syncMode: incremental target: ${PREFIX}/campaigns - resource: stream: custom_conversions syncMode: full_refresh target: ${PREFIX}/custom_conversions - resource: stream: activities syncMode: incremental target: ${PREFIX}/activities - resource: stream: ads syncMode: incremental target: ${PREFIX}/ads - resource: stream: ad_creatives syncMode: full_refresh target: ${PREFIX}/ad_creatives   Learn more about capture definitions. ","version":"Next","tagName":"h3"},{"title":"Datadog HTTP Ingest (Webhook)","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/datadog-ingest/","content":"","keywords":"","version":"Next"},{"title":"Usage​","type":1,"pageTitle":"Datadog HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/datadog-ingest/#usage","content":" This connector is distinct from all other capture connectors in that it's not designed to pull data from a specific system or endpoint. It requires no endpoint-specific configuration, and can accept any and all valid JSON objects from any source.  This is useful primarily if you want to test out Flow or see how your webhook data will come over.  To begin, use the web app to create a capture. Once published, the confirmation dialog displays a unique URL for your public endpoint.  You're now ready to send data to Flow.  ","version":"Next","tagName":"h2"},{"title":"Send sample data to Flow​","type":1,"pageTitle":"Datadog HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/datadog-ingest/#send-sample-data-to-flow","content":" After publishing the capture, click the endpoint link from the confirmation dialog to open the Swagger UI page for your capture. Expand POST or PUT and click Try it out to send some example JSON documents using the UI. You can also copy the provided curl commands to send data via the command line. After sending data, go to the Collections page of the Flow web app and find the collection associated with your capture. Click Details to view the data preview.  ","version":"Next","tagName":"h3"},{"title":"Configure a Datadog webhook​","type":1,"pageTitle":"Datadog HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/datadog-ingest/#configure-a-datadog-webhook","content":" In the Datadog Cloud Monitoring Platform, navigate to the Integrations tab and click on the Integrations option in the dropdown. Using the search bar, search for the Webhook Integration and install it. Within the Webhook Integration configuration, select new Webhook and enter in the following information:  Field\tValue\tDescriptionName\tyour-webhook\tThe name of your webhook within Datadog URL\thttps://your-unique-webhook-url/webhook-data\tThe unique Estuary URL created for ingesting webhook data  In the Datadog Cloud Monitoring Platform, navigate to Monitors/New Monitor and select Metric for the type. Define the alert conditions and under Notify your team select @your-webhook from the dropdown.  ","version":"Next","tagName":"h3"},{"title":"Webhook URLs​","type":1,"pageTitle":"Datadog HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/datadog-ingest/#webhook-urls","content":" To determine the full URL, start with the base URL from the Flow web app (for example https://abc123-8080.us-central1.v1.estuary-data.dev), and then append the path.  The path will be whatever is in the paths endpoint configuration field (/webhook-data by default). For example, your full webhook URL would be https://&lt;your-unique-hostname&gt;/webhook-data. You can add additional paths to paths, and the connector will accept webhook requests on each of them. Each path will correspond to a separate binding. If you're editing the capture via the UI, click the &quot;re-fresh&quot; button after editing the URL paths in the endpoint config to see the resulting collections in the bindings editor. For example, if you set the path to /my-webhook.json, then the full URL for that binding would be https://&lt;your-unique-hostname&gt;/my-webhook.json.  Any URL query parameters that are sent on the request will be captured and serialized under /_meta/query/* the in documents. For example, a webhook request that's sent to /webhook-data?testKey=testValue would result in a document like:  { &quot;_meta&quot;: { &quot;webhookId&quot;: &quot;...&quot;, &quot;query&quot;: { &quot;testKey&quot;: &quot;testValue&quot; }, ... } ... }   ","version":"Next","tagName":"h3"},{"title":"Authentication​","type":1,"pageTitle":"Datadog HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/datadog-ingest/#authentication","content":" The connector can optionally require each request to present an authentication token as part of an Authorization: Bearer HTTP header. To enable authentication, generate a secret and paste it into the &quot;Require Auth Token&quot; field. We recommend using a password manager to generate these values, but keep in mind that not all systems will be able to send values with certain special characters, so you may want to disable special characters when you generate the secret. If you enable authentication, then each incoming request must have an Authorization header with the value of your token. For example, if you use an auth token value of mySecretToken, then the header on each request must be Authorization: Bearer mySecretToken.  If you don't enable authentication, then anyone who knows the URL will be able to publish data to your collection. We recommend using authentication whenever possible.  ","version":"Next","tagName":"h3"},{"title":"Webhook signature verification​","type":1,"pageTitle":"Datadog HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/datadog-ingest/#webhook-signature-verification","content":" This connector does not yet support webhook signature verification. If this is a requirement for your use case, please contact support@estuary.dev and let us know.  ","version":"Next","tagName":"h3"},{"title":"Endpoint Configuration​","type":1,"pageTitle":"Datadog HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/datadog-ingest/#endpoint-configuration","content":" Property\tTitle\tDescription\tType\tRequired/Default****\tEndpointConfig object\tRequired /require_auth_token Optional bearer token to authenticate webhook requests. WARNING: If this is empty or unset, then anyone who knows the URL of the connector will be able to write data to your collections.\tnull, string\tnull /paths\tURL Paths\tList of URL paths to accept requests at. Discovery will return a separate collection for each given path. Paths must be provided without any percent encoding, and should not include any query parameters or fragment.\tnull, string\tnull  ","version":"Next","tagName":"h2"},{"title":"Resource configuration​","type":1,"pageTitle":"Datadog HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/datadog-ingest/#resource-configuration","content":" Property\tTitle\tDescription\tType\tRequired/Default****\tResourceConfig object\tRequired /idFromHeader Set the /_meta/webhookId from the given HTTP header in each request. If not set, then a random id will be generated automatically. If set, then each request will be required to have the header, and the header value will be used as the value of `/_meta/webhookId`.\tnull, string /path The URL path to use for adding documents to this binding. Defaults to the name of the collection.\tnull, string /stream The name of the binding, which is used as a merge key when doing Discovers.\tnull, string\t ","version":"Next","tagName":"h2"},{"title":"Front","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/front/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Front","url":"/reference/Connectors/capture-connectors/front/#supported-data-resources","content":" The following data resources are supported through the Front API:  channelscontactsconversationseventsinboxestagsteammatesteams  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Front","url":"/reference/Connectors/capture-connectors/front/#prerequisites","content":" A Front API token with the appropriate scope(s).  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Front","url":"/reference/Connectors/capture-connectors/front/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification files. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Front source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Front","url":"/reference/Connectors/capture-connectors/front/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/credentials/access_token\tAPI Key\tYour Front API token.\tstring\tRequired /start_date\tReplication Start Date\tUTC date and time in the format &quot;YYYY-MM-DDTHH:MM:SSZ&quot;. Data prior to this date will not be replicated.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/name\tData resource\tResource in Front from which collections are captured.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Front","url":"/reference/Connectors/capture-connectors/front/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-front:dev config: credentials: access_token: &lt;secret&gt; start_date: &quot;2024-11-15T00:00:00Z&quot; bindings: - resource: name: channels target: ${PREFIX}/channels - resource: name: contacts target: ${PREFIX}/contacts - resource: name: conversations target: ${PREFIX}/conversations - resource: name: events target: ${PREFIX}/events - resource: name: inboxes target: ${PREFIX}/inboxes - resource: name: tags target: ${PREFIX}/tags - resource: name: teammates target: ${PREFIX}/teammates - resource: name: teams target: ${PREFIX}/teams  ","version":"Next","tagName":"h3"},{"title":"Genesys","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/genesys/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Genesys","url":"/reference/Connectors/capture-connectors/genesys/#supported-data-resources","content":" The following data resources are supported through the Genesys API:  ConversationsUsers  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Genesys","url":"/reference/Connectors/capture-connectors/genesys/#prerequisites","content":" A Genesys account with API access.A OAuth app created within your Genesys account with the Client Credentials grant type. See Authentication for instructions on how to create this.  ","version":"Next","tagName":"h2"},{"title":"Authentication​","type":1,"pageTitle":"Genesys","url":"/reference/Connectors/capture-connectors/genesys/#authentication","content":" Genesys requires an OAuth client for authentication. To create an OAuth client in your Genesys account that will allow Flow to access data, follow the below steps or refer to Genesys' documentation.  Log into your Genesys account.Click Admin.Under the Integrations section, click OAuth.Click the + Add Client button.Enter an App Name, like &quot;Estuary Flow OAuth Client&quot;.Under Grant Types, select Client Credentials.In the Roles tab, select the appropriate role for the OAuth app.Click Save.Note the Client ID and Client Secret for when you set up the connector.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Genesys","url":"/reference/Connectors/capture-connectors/genesys/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification files. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Genesys source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Genesys","url":"/reference/Connectors/capture-connectors/genesys/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/start_date\tReplication Start Date\tUTC date and time in the format &quot;YYYY-MM-DDTHH:MM:SSZ&quot;. Data prior to this date will not be replicated.\tstring\t30 days before the current date /genesys_cloud_domain\tGenesys Cloud Domain\tThe cloud region where the Genesys organization is deployed. Cloud regions and their domains can be found here. The genesys_cloud_domain is the part of the login URL after https://login.. For example, for the ap-south-1 region that has a login URL of https://login.aps1.pure.cloud, the genesys_cloud_domain is aps1.pure.cloud\tstring\tRequired /credentials/client_id\tOAuth Client ID\tThe client ID for your Genesys OAuth app.\tstring\tRequired /credentials/client_secret\tOAuth Client Secret\tThe client secret for your Genesys OAuth app.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/name\tData resource\tResource in Genesys from which collections are captured.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Genesys","url":"/reference/Connectors/capture-connectors/genesys/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-genesys:dev config: start_date: &quot;2024-11-11T00:00:00Z&quot; genesys_cloud_domain: mypurecloud.com credentials: client_id: my_client_id client_secret: my_client_secret bindings: - resource: name: conversations target: ${PREFIX}/conversations - resource: name: users target: ${PREFIX}/users  ","version":"Next","tagName":"h3"},{"title":"GitLab","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/gitlab/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"GitLab","url":"/reference/Connectors/capture-connectors/gitlab/#supported-data-resources","content":" When you configure the connector, you may provide a list of GitLab Groups or Projects from which to capture data.  From your selection, the following data resources are captured:  ","version":"Next","tagName":"h2"},{"title":"Resources​","type":1,"pageTitle":"GitLab","url":"/reference/Connectors/capture-connectors/gitlab/#resources","content":" BranchesCommitsIssuesGroup Issue BoardsPipelinesJobsProjectsProject MilestonesProject Merge RequestsUsersGroupsGroup MilestonesGroup and Project MembersTagsReleasesGroup LabelsProject LabelsEpics (only available for GitLab Ultimate and GitLab.com Gold accounts)Epic Issues (only available for GitLab Ultimate and GitLab.com Gold accounts)  Each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h3"},{"title":"Prerequisites​","type":1,"pageTitle":"GitLab","url":"/reference/Connectors/capture-connectors/gitlab/#prerequisites","content":" There are two ways to authenticate with GitLab when capturing data into Flow: using OAuth2, and manually, by generating a personal access token. Their prerequisites differ.  OAuth is recommended for simplicity in the Flow web app; the access token method is the only supported method using the command line. Which authentication method you choose depends on the policies of your organization. GitLab has special organization settings that need to be enabled in order for users to be able to access repos that are part of an organization.  ","version":"Next","tagName":"h2"},{"title":"Using OAuth2 to authenticate with GitLab in the Flow web app​","type":1,"pageTitle":"GitLab","url":"/reference/Connectors/capture-connectors/gitlab/#using-oauth2-to-authenticate-with-gitlab-in-the-flow-web-app","content":" A GitLab user account with access to the repositories of interest, and which is a member of organizations of interest. How to add a member.  ","version":"Next","tagName":"h3"},{"title":"Configuring the connector specification manually using personal access token​","type":1,"pageTitle":"GitLab","url":"/reference/Connectors/capture-connectors/gitlab/#configuring-the-connector-specification-manually-using-personal-access-token","content":" A GitLab user account with access to all entities of interest. A GitLab personal access token.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"GitLab","url":"/reference/Connectors/capture-connectors/gitlab/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the GitHub source connector.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"GitLab","url":"/reference/Connectors/capture-connectors/gitlab/#setup","content":" Complete authentication using Oauth or a PATSelect your start date in the format 2023-08-31T00:00:00Optionally select Groups and Projects  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"GitLab","url":"/reference/Connectors/capture-connectors/gitlab/#sample","content":" This sample specification reflects the manual authentication method.  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-gitlab:dev config: credentials: option_title: PAT Credentials personal_access_token: {secret} groups: estuary.dev projects: estuary/flow start_date: 2022-01-01T00:00:00Z bindings: - resource: stream: branches syncMode: full_refresh target: ${PREFIX}/assignees {...}  ","version":"Next","tagName":"h3"},{"title":"Google Cloud Storage","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/gcs/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Google Cloud Storage","url":"/reference/Connectors/capture-connectors/gcs/#prerequisites","content":" To use this connector, either your GCS bucket must be public, or you must have access via a Google service account.  For public buckets, verify that objects in the bucket are publicly readable.For buckets accessed by a Google Service Account: Ensure that the user has been assigned a role with read access.Create a JSON service account key. Google's Application Default Credentials will use this file for authentication.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Google Cloud Storage","url":"/reference/Connectors/capture-connectors/gcs/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the GCS source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Google Cloud Storage","url":"/reference/Connectors/capture-connectors/gcs/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/advanced Options for advanced users. You should not typically need to modify these.\tobject /advanced/ascendingKeys\tAscending Keys\tImprove sync speeds by listing files from the end of the last sync, rather than listing the entire bucket prefix. This requires that you write objects in ascending lexicographic order, such as an RFC-3339 timestamp, so that key ordering matches modification time ordering. If data is not ordered correctly, using ascending keys could cause errors.\tboolean\tfalse /bucket\tBucket\tName of the Google Cloud Storage bucket\tstring\tRequired /googleCredentials\tGoogle Service Account\tService account JSON key to use as Application Default Credentials\tstring /matchKeys\tMatch Keys\tFilter applied to all object keys under the prefix. If provided, only objects whose key (relative to the prefix) matches this regex will be read. For example, you can use &quot;.*\\.json&quot; to only capture json files.\tstring /parser\tParser Configuration\tConfigures how files are parsed\tobject /parser/compression\tCompression\tDetermines how to decompress the contents. The default, 'Auto', will try to determine the compression automatically.\tnull, string\tnull /parser/format\tFormat\tDetermines how to parse the contents. The default, 'Auto', will try to determine the format automatically based on the file extension or MIME type, if available.\tobject\t{&quot;type&quot;:&quot;auto&quot;} /parser/format/type\tType string /prefix\tPrefix\tPrefix within the bucket to capture from. Use this to limit the data in your capture.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tPrefix\tPath to dataset in the bucket, formatted as bucket-name/prefix-name\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Google Cloud Storage","url":"/reference/Connectors/capture-connectors/gcs/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-gcs:dev config: bucket: my-bucket googleCredentials: &quot;type&quot;: &quot;service_account&quot;, &quot;project_id&quot;: &quot;project-id&quot;, &quot;private_key_id&quot;: &quot;key-id&quot;, &quot;private_key&quot;: &quot;-----BEGIN PRIVATE KEY-----\\nprivate-key\\n-----END PRIVATE KEY-----\\n&quot;, &quot;client_email&quot;: &quot;service-account-email&quot;, &quot;client_id&quot;: &quot;client-id&quot;, &quot;auth_uri&quot;: &quot;https://accounts.google.com/o/oauth2/auth&quot;, &quot;token_uri&quot;: &quot;https://accounts.google.com/o/oauth2/token&quot;, &quot;auth_provider_x509_cert_url&quot;: &quot;https://www.googleapis.com/oauth2/v1/certs&quot;, &quot;client_x509_cert_url&quot;: &quot;https://www.googleapis.com/robot/v1/metadata/x509/service-account-email&quot; parser: compression: zip format: type: csv config: delimiter: &quot;,&quot; encoding: UTF-8 errorThreshold: 5 headers: [ID, username, first_name, last_name] lineEnding: &quot;\\\\r&quot; quote: &quot;\\&quot;&quot; bindings: - resource: stream: my-bucket/${PREFIX} target: ${PREFIX}/${COLLECTION_NAME}   Your capture definition may be more complex, with additional bindings for different GCS prefixes within the same bucket.  Learn more about capture definitions.  ","version":"Next","tagName":"h3"},{"title":"Advanced: Parsing cloud storage data​","type":1,"pageTitle":"Google Cloud Storage","url":"/reference/Connectors/capture-connectors/gcs/#advanced-parsing-cloud-storage-data","content":" Cloud storage platforms like GCS can support a wider variety of file types than other data source systems. For each of these file types, Flow must parse and translate data into collections with defined fields and JSON schemas.  By default, the parser will automatically detect the type and shape of the data in your bucket, so you won't need to change the parser configuration for most captures.  However, the automatic detection may be incorrect in some cases. To fix or prevent this, you can provide explicit information in the parser configuration, which is part of the endpoint configuration for this connector.  The parser configuration includes:  Compression: Specify how the bucket contents are compressed. If no compression type is specified, the connector will try to determine the compression type automatically. Options are: zipgzipzstdnone Format: Specify the data format, which determines how it will be parsed. Options are: Auto: If no format is specified, the connector will try to determine it automatically.AvroCSVJSONProtobufW3C Extended Log info At this time, Flow only supports GCS captures with data of a single file type. Support for multiple file types, which can be configured on a per-binding basis, will be added in the future. For now, use a prefix in the endpoint configuration to limit the scope of each capture to data of a single file type.  CSV configuration​  CSV files include several additional properties that are important to the parser. In most cases, Flow is able to automatically determine the correct values, but you may need to specify for unusual datasets. These properties are:  Delimiter. Options are: Comma (&quot;,&quot;)Pipe (&quot;|&quot;)Space (&quot;0x20&quot;)Semicolon (&quot;;&quot;)Tab (&quot;0x09&quot;)Vertical tab (&quot;0x0B&quot;)Unit separator (&quot;0x1F&quot;)SOH (&quot;0x01&quot;)Auto Encoding type, specified by its WHATWG label. Optionally, an Error threshold, as an acceptable percentage of errors. If set to a number greater than zero, malformed rows that fall within the threshold will be excluded from the capture. Escape characters. Options are: Backslash (&quot;\\\\&quot;)Disable escapes (&quot;&quot;)Auto Optionally, a list of column Headers, if not already included in the first row of the CSV file. If any headers are provided, it is assumed that the provided list of headers is complete and authoritative. The first row of your CSV file will be assumed to be data (not headers), and you must provide a header value for every column in the file. Line ending values CRLF (&quot;\\\\r\\\\n&quot;) (Windows)CR (&quot;\\\\r&quot;)LF (&quot;\\\\n&quot;)Record Separator (&quot;0x1E&quot;)Auto Quote character Double Quote (&quot;\\&quot;&quot;)Single Quote (&quot;)Disable Quoting (&quot;&quot;)Auto  The sample specification above includes these fields.  ","version":"Next","tagName":"h3"},{"title":"Advanced: Configure Google service account impersonation​","type":1,"pageTitle":"Google Cloud Storage","url":"/reference/Connectors/capture-connectors/gcs/#advanced-configure-google-service-account-impersonation","content":" As part of your Google IAM management, you may have configured one service account to impersonate another service account. You may find this useful when you want to easily control access to multiple service accounts with only one set of keys.  If necessary, you can configure this authorization model for a GCS capture in Flow using the GitOps workflow. To do so, you'll enable sops encryption and impersonate the target account with JSON credentials.  Before you begin, make sure you're familiar with how to encrypt credentials in Flow using sops.  Use the following sample as a guide to add the credentials JSON to the capture's endpoint configuration. The sample uses the encrypted suffix feature of sops to encrypt only the sensitive credentials, but you may choose to encrypt the entire configuration.  config: bucket: &lt;bucket-name&gt; googleCredentials_sops: # URL containing the account to impersonate and the associated project service_account_impersonation_url: https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/&lt;target-account&gt;@&lt;project&gt;.iam.gserviceaccount.com:generateAccessToken # Credentials for the account that has been configured to impersonate the target. source_credentials: # In addition to the listed fields, copy and paste the rest of your JSON key file as your normally would # for the `googleCredentials` field client_email: &lt;origin-account&gt;@&lt;anotherproject&gt;.iam.gserviceaccount.com token_uri: https://oauth2.googleapis.com/token type: service_account type: impersonated_service_account  ","version":"Next","tagName":"h3"},{"title":"GitHub","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/github/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"GitHub","url":"/reference/Connectors/capture-connectors/github/#supported-data-resources","content":" When you configure the connector, you specify a list of GitHub organizations and/or repositories from which to capture data.  From your selection, the following data resources are captured:  Full refresh (batch) resources\tIncremental (real-time supported) resourcesAssignees\tComments Branches\tCommit comment reactions Collaborators\tCommit comments Issue labels\tCommits Pull request commits\tDeployments Tags\tEvents Team members\tIssue comment reactions Team memberships\tIssue events Teams\tIssue milestones Users\tIssue reactions Issues Project cards Project columns Projects Pull request comment reactions Pull request stats Pull requests Releases Repositories Review comments Reviews Stargazers Workflow runs Workflows  Each resource is mapped to a Flow collection through a separate binding.  info The /start_date field is not applicable to the following resources: AssigneesBranchesCollaboratorsIssue labelsOrganizationsPull request commitsPull request statsRepositoriesTagsTeamsUsers  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"GitHub","url":"/reference/Connectors/capture-connectors/github/#prerequisites","content":" There are two ways to authenticate with GitHub when capturing data into Flow: using OAuth2, and manually, by generating a personal access token. Their prerequisites differ.  OAuth is recommended for simplicity in the Flow web app; the access token method is the only supported method using the command line. Which authentication method you choose depends on the policies of your organization. Github has special organization settings that need to be enabled in order for users to be able to access repos that are part of an organization.  ","version":"Next","tagName":"h2"},{"title":"Using OAuth2 to authenticate with GitHub in the Flow web app​","type":1,"pageTitle":"GitHub","url":"/reference/Connectors/capture-connectors/github/#using-oauth2-to-authenticate-with-github-in-the-flow-web-app","content":" A GitHub user account with access to the repositories of interest, and which is a member of organizations of interest. User may need to request access in Github under the user's personal settings (not the organization settings) by going to Applications then Authorized OAuth Apps on Github. Click the app or the image next to the app and request access under &quot;Organization access&quot;. After a user has made the request, the organization administrator can grant access on the &quot;Third-party application access policy&quot; page. See additional details on this Github doc.  ","version":"Next","tagName":"h3"},{"title":"Configuring the connector specification manually using personal access token​","type":1,"pageTitle":"GitHub","url":"/reference/Connectors/capture-connectors/github/#configuring-the-connector-specification-manually-using-personal-access-token","content":" A GitHub user account with access to the repositories of interest, and which is a member of organizations of interest. A GitHub personal access token. You may use multiple tokens to balance the load on your API quota. User may need to get the organization's administrator to grant access under &quot;Third-party Access&quot; then &quot;Personal access tokens&quot;.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"GitHub","url":"/reference/Connectors/capture-connectors/github/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the GitHub source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"GitHub","url":"/reference/Connectors/capture-connectors/github/#properties","content":" Endpoint​  The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so some of these properties aren't required.  Property\tTitle\tDescription\tType\tRequired/Default/branch\tBranch (Optional)\tSpace-delimited list of GitHub repository branches to pull commits for, e.g. `estuary/flow/your-branch`. If no branches are specified for a repository, the default branch will be pulled.\tstring /credentials\tAuthentication\tChoose how to authenticate to GitHub\tobject\tRequired /credentials/option_title\tAuthentication method\tSet to PAT Credentials for manual authentication\tstring /credentials/personal_access_token\tAccess token\tPersonal access token, used for manual authentication. You may include multiple access tokens as a comma separated list. /page_size_for_large_streams\tPage size for large streams (Optional)\tThe Github connector captures from several resources with a large amount of data. The page size of such resources depends on the size of your repository. We recommended that you specify values between 10 and 30.\tinteger\t10 /repository\tGitHub Repositories\tSpace-delimited list of GitHub organizations/repositories, e.g. `estuary/flow` for a single repository, `estuary/*` to get all repositories from an organization and `estuary/flow estuary/another-repo` for multiple repositories.\tstring\tRequired /start_date\tStart date\tThe date from which you'd like to replicate data from GitHub in the format YYYY-MM-DDT00:00:00Z. For the resources that support this configuration, only data generated on or after the start date will be replicated. This field doesn't apply to all resources.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tGitHub resource from which collection is captured.\tstring\tRequired /syncMode\tSync mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"GitHub","url":"/reference/Connectors/capture-connectors/github/#sample","content":" This sample specification reflects the manual authentication method.  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-github:dev config: credentials: option_title: PAT Credentials personal_access_token: {secret} page_size_for_large_streams: 10 repository: estuary/flow start_date: 2022-01-01T00:00:00Z bindings: - resource: stream: assignees syncMode: full_refresh target: ${PREFIX}/assignees {...}  ","version":"Next","tagName":"h3"},{"title":"Gladly","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/gladly/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Gladly","url":"/reference/Connectors/capture-connectors/gladly/#supported-data-resources","content":" This connector can be used to sync the following Event entity types from Gladly:  AGENT_AVAILABILITYAGENT_STATUSCONTACTCONVERSATIONCUSTOMERPAYMENT_REQUESTTASK  By default, each entity type is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Gladly","url":"/reference/Connectors/capture-connectors/gladly/#prerequisites","content":" To set up the Gladly source connector, you'll need a Gladly account with an API token.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Gladly","url":"/reference/Connectors/capture-connectors/gladly/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Gladly source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Gladly","url":"/reference/Connectors/capture-connectors/gladly/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/organization\tOrganization\tOrganization to Request Data From\tstring\tRequired /agentEmail\tAgent Email\tAgent Email Address to use for Authentication\tstring\tRequired /apiToken\tAPI Token\tAPI Token to use for Authentication\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/name\tName\tName of this resource\tstring\tRequired /interval\tInterval\tInterval between updates for this resource\tstring\t  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Gladly","url":"/reference/Connectors/capture-connectors/gladly/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-gladly:dev config: organization: agentEmail: apiToken: &lt;secret&gt; bindings: - resource: name: AgentAvailabilityEvents interval: PT30S target: ${PREFIX}/AgentAvailabilityEvents {...}  ","version":"Next","tagName":"h3"},{"title":"Google Ads","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/google-ads/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Google Ads","url":"/reference/Connectors/capture-connectors/google-ads/#supported-data-resources","content":" The following data resources are supported. Resources ending in _report represent legacy resources from the Google Adwords API.  ad_group_adsad_group_ad_labelad_groupsad_group_labelcampaignscampaign_labelsclick_viewcustomergeographic_viewkeyword_viewuser_location_viewaccount_performance_reportad_performance_reportdisplay_keyword_performance_reportdisplay_topics_performance_reportshopping_performance_report  By default, each resource is mapped to a Flow collection through a separate binding.  You may also generate custom resources using GAQL queries.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Google Ads","url":"/reference/Connectors/capture-connectors/google-ads/#prerequisites","content":" There are two ways to authenticate with Google when capturing data into Flow: using OAuth2, and manually, using tokens and secret credentials. Their prerequisites differ.  OAuth is recommended for simplicity in the Flow web app; the manual method is the only supported method using the command line.  ","version":"Next","tagName":"h2"},{"title":"Customer Id & Login Customer Id​","type":1,"pageTitle":"Google Ads","url":"/reference/Connectors/capture-connectors/google-ads/#customer-id--login-customer-id","content":" The Login Customer Id setting refers to your MCC Google Ads account Id. One can easily find this number by accessing their Google Ads Dashboard and look to the far right corner of their screen.  Example:    In the above example, my login_customer_id would be 1234567890.  The Customer Id setting refers to your Client Accounts under a MCC account. One can easily find this number by accessing their Google Ads Dashboard and look to the far left corner of their screen, after selecting a client account.  Example:    In the above example, my customer_id would be 9876543210.  Multiple Customer Ids​  This Source allows for multiple Customer Ids to be selected. To allow this, simply add your customer_id followed by a comma.  Example:  Customer1 = 1234567890 Customer2 = 9876543210  customer_id = 1234567890,9876543210  ","version":"Next","tagName":"h3"},{"title":"Using OAuth2 to authenticate with Google in the Flow web app​","type":1,"pageTitle":"Google Ads","url":"/reference/Connectors/capture-connectors/google-ads/#using-oauth2-to-authenticate-with-google-in-the-flow-web-app","content":" One or more Google Ads accounts. Note each account's customer ID A Google Account that has access to the Google Ads account(s). This account may be a manager account. If so, ensure that it is linked to each Google Ads account and make note of its customer ID.  ","version":"Next","tagName":"h3"},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"Google Ads","url":"/reference/Connectors/capture-connectors/google-ads/#configuring-the-connector-specification-manually","content":" One or more Google Ads accounts. Note each account's customer ID A Google Ads manager account that has been linked to each Google Ads account A Google Ads developer token. Your Google Ads manager account must be configured prior to applying for a developer token.  caution Developer token applications are independently reviewed by Google and may take one or more days to be approved. Be sure to carefully review Google's requirements before submitting an application.  A refresh token, which fetches a new developer tokens for you as the previous token expires. A generated Client ID and Client Secret, used for authentication.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Google Ads","url":"/reference/Connectors/capture-connectors/google-ads/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the Flow specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Google Ads source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Google Ads","url":"/reference/Connectors/capture-connectors/google-ads/#properties","content":" Endpoint​  The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so many of these properties aren't required.  Property\tTitle\tDescription\tType\tRequired/Default/conversion_window_days\tConversion Window (Optional)\tA conversion window is the period of time after an ad interaction (such as an ad click or video view) during which a conversion, such as a purchase, is recorded in Google Ads. For more information, see Google's docs.\tinteger\t14 /credentials\tGoogle Credentials object\tRequired /credentials/client_id\tClient ID\tThe Client ID of your Google Ads developer application.\tstring\tRequired /credentials/client_secret\tClient Secret\tThe Client Secret of your Google Ads developer application.\tstring\tRequired /credentials/developer_token\tDeveloper Token\tDeveloper token granted by Google to use their APIs.\tstring\tRequired /credentials/refresh_token\tRefresh Token\tThe token for obtaining a new access token.\tstring\tRequired /custom_queries\tCustom GAQL Queries (Optional) array /custom_queries/-/query\tCustom Query\tA custom defined GAQL query for building the report. Should not contain segments.date expression. See Google's query builder for more information.\tstring /custom_queries/-/table_name\tDestination Table Name\tThe table name in your destination database for chosen query.\tstring /customer_id\tCustomer ID(s)\tComma separated list of (client) customer IDs. Each customer ID must be specified as a 10-digit number without dashes. More instruction on how to find this value in our docs. Metrics streams like AdGroupAdReport cannot be requested for a manager account.\tstring\tRequired /end_date\tEnd Date (Optional)\tUTC date in the format 2017-01-25. Any data after this date will not be replicated.\tstring /login_customer_id\tLogin Customer ID for Managed Accounts (Optional)\tIf your access to the customer account is through a manager account, this field is required and must be set to the customer ID of the manager account (10-digit number without dashes).\tstring /start_date\tStart Date\tUTC date in the format 2017-01-25. Any data before this date will not be replicated.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tGoogle Ad resource from which a collection is captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Google Ads","url":"/reference/Connectors/capture-connectors/google-ads/#sample","content":" This sample specification reflects the manual authentication method.  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-google-ads:dev config: conversion_window_days: 7 credentials: client_id: {secret_client_ID} client_secret: {secret_secret} developer_token: {access_token} refresh_token: {refresh_token} customer_id: 0123456789, 1234567890 login_customer_id: 0987654321 end_date: 2022-01-01 start_date: 2020-01-01 custom_queries: - query: SELECT campaign.id, campaign.name, campaign.status FROM campaign ORDER BY campaign.id table_name: campaigns_custom bindings: - resource: stream: campaign syncMode: incremental target: ${PREFIX}/campaign {...}   ","version":"Next","tagName":"h3"},{"title":"Custom queries​","type":1,"pageTitle":"Google Ads","url":"/reference/Connectors/capture-connectors/google-ads/#custom-queries","content":" You can create custom resources using Google Analytics Query Language (GAQL) queries. Each generated resource will be mapped to a Flow collection. For help generating a valid query, see Google's query builder documentation.  If a query fails to validate against a given Google Ads account, it will be skipped.  ","version":"Next","tagName":"h2"},{"title":"Stream Limitations​","type":1,"pageTitle":"Google Ads","url":"/reference/Connectors/capture-connectors/google-ads/#stream-limitations","content":" ","version":"Next","tagName":"h2"},{"title":"ClickView​","type":1,"pageTitle":"Google Ads","url":"/reference/Connectors/capture-connectors/google-ads/#clickview","content":" Due to Google Ads API limitations, ClickView stream queries are executed with a time range limited to one day. Also, data can only be requested for periods 90 days before the time of the request.  In pratical terms, this means that you can only search ClickView data limited to 3 months ago, anything before this is not returned.  For more information, check Google's Ads API documentation ","version":"Next","tagName":"h3"},{"title":"(Deprecated) Google Analytics UA","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/google-analytics/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"(Deprecated) Google Analytics UA","url":"/reference/Connectors/capture-connectors/google-analytics/#supported-data-resources","content":" The following data resources are captured to Flow collections by default:  Website overviewTraffic sourcesPagesLocationsMonthly active usersFour weekly active usersTwo weekly active usersWeekly active usersDaily active usersDevices  Each resource is mapped to a Flow collection through a separate binding.  You can also configure custom reports.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"(Deprecated) Google Analytics UA","url":"/reference/Connectors/capture-connectors/google-analytics/#prerequisites","content":" There are two ways to authenticate with Google when capturing data from a Google Analytics view: using OAuth2, and manually, by generating a service account key. Their prerequisites differ.  OAuth is recommended for simplicity in the Flow web app; the service account key method is the only supported method using the command line.  ","version":"Next","tagName":"h2"},{"title":"Using OAuth2 to authenticate with Google in the Flow web app​","type":1,"pageTitle":"(Deprecated) Google Analytics UA","url":"/reference/Connectors/capture-connectors/google-analytics/#using-oauth2-to-authenticate-with-google-in-the-flow-web-app","content":" The View ID for your Google Analytics account. You can find this using Google's Account Explorer tool. Your Google account username and password.  ","version":"Next","tagName":"h3"},{"title":"Authenticating manually with a service account key​","type":1,"pageTitle":"(Deprecated) Google Analytics UA","url":"/reference/Connectors/capture-connectors/google-analytics/#authenticating-manually-with-a-service-account-key","content":" The View ID for your Google Analytics account. You can find this using Google's Account Explorer tool. Google Analytics and Google Analytics Reporting APIs enabled on your Google account. A Google service account with: A JSON key generated.Access to the source Google Analytics view.  Follow the steps below to meet these prerequisites:  Enable the Google Analytics and Google Analytics Reporting APIs for the Google project with which your Analytics view is associated. (Unless you actively develop with Google Cloud, you'll likely just have one option). Create a service account and generate a JSON keyDuring setup, grant the account the Viewer role on your project. You'll copy the contents of the downloaded key file into the Service Account Credentials parameter when you configure the connector. Add the service account to the Google Analytics view. Grant the account Viewer permissions (formerly known as Read &amp; Analyze permissions).  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"(Deprecated) Google Analytics UA","url":"/reference/Connectors/capture-connectors/google-analytics/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Google Analytics source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"(Deprecated) Google Analytics UA","url":"/reference/Connectors/capture-connectors/google-analytics/#properties","content":" Endpoint​  The following properties reflect the Service Account Key authentication method. If you're working in the Flow web app, you'll use OAuth2, so some of these properties aren't required.  Property\tTitle\tDescription\tType\tRequired/Default/credentials\tCredentials\tCredentials for the service\tobject /credentials/auth_type\tAuthentication Type\tAuthentication method. Set to Service for manual configuration, or use OAuth in the web app.\tstring\tRequired credentials/credentials_json\tService Account Credentials\tContents of the JSON key file generated during setup.\tstring\tRequired /custom_reports\tCustom Reports (Optional)\tA JSON array describing the custom reports you want to sync from GA.\tstring /start_date\tStart Date\tThe date in the format YYYY-MM-DD. Any data before this date will not be replicated.\tstring\tRequired /view_id\tView ID\tThe ID for the Google Analytics View you want to fetch data from. This can be found from the Google Analytics Account Explorer: https://ga-dev-tools.appspot.com/account-explorer/\tstring\tRequired /window_in_days\tWindow in days (Optional)\tThe amount of days each stream slice would consist of beginning from start_date. Bigger the value - faster the fetch. (Min=1, as for a Day; Max=364, as for a Year).\tinteger\t1  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tData resource from the Google Analytics view.\tstring\tRequired /syncMode\tSync Mode\tConnection method. Always set to incremental.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Custom reports​","type":1,"pageTitle":"(Deprecated) Google Analytics UA","url":"/reference/Connectors/capture-connectors/google-analytics/#custom-reports","content":" You can include data beyond the default data resources with Custom Reports. These replicate the functionality of Custom Reports in the Google Analytics Web console.  To do so, fill out the Custom Reports property with a JSON array as a string with the following schema:  [{&quot;name&quot;: string, &quot;dimensions&quot;: [string], &quot;metrics&quot;: [string]}]   You may specify default Google Analytics dimensions and metrics from the table below, or custom dimensions and metrics you've previously defined. Each custom report may contain up to 7 unique dimensions and 10 unique metrics. You must include the ga:date dimension for proper data flow.  Supported GA dimensions\tSupported GA metricsga:browser\tga:14dayUsers ga:city\tga:1dayUsers ga:continent\tga:28dayUsers ga:country\tga:30dayUsers ga:date\tga:7dayUsers ga:deviceCategory\tga:avgSessionDuration ga:hostname\tga:avgTimeOnPage ga:medium\tga:bounceRate ga:metro\tga:entranceRate ga:operatingSystem\tga:entrances ga:pagePath\tga:exits ga:region\tga:newUsers ga:socialNetwork\tga:pageviews ga:source\tga:pageviewsPerSession ga:subContinent\tga:sessions ga:sessionsPerUser ga:uniquePageviews ga:users  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"(Deprecated) Google Analytics UA","url":"/reference/Connectors/capture-connectors/google-analytics/#sample","content":" This sample reflects the manual authentication method.  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-google-analytics-v4:dev config: view_id: 000000000 start_date: 2022-03-01 credentials: auth_type: service credentials_json: &lt;secret&gt; window_in_days: 1 bindings: - resource: stream: daily_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: devices syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: four_weekly_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: locations syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: monthly_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: pages syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: traffic_sources syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: two_weekly_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: website_overview syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: weekly_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}   Learn more about capture definitions.  ","version":"Next","tagName":"h3"},{"title":"Performance considerations​","type":1,"pageTitle":"(Deprecated) Google Analytics UA","url":"/reference/Connectors/capture-connectors/google-analytics/#performance-considerations","content":" ","version":"Next","tagName":"h2"},{"title":"Data sampling​","type":1,"pageTitle":"(Deprecated) Google Analytics UA","url":"/reference/Connectors/capture-connectors/google-analytics/#data-sampling","content":" The Google Analytics Reporting API enforces compute thresholds for ad-hoc queries and reports. If a threshold is exceeded, the API will apply sampling to limit the number of sessions analyzed for the specified time range. These thresholds can be found here.  If your account is on the Analytics 360 tier, you're less likely to run into these limitations. For Analytics Standard accounts, you can avoid sampling by keeping the window_in_days parameter set to its default value, 1. This makes it less likely that you will exceed the threshold. When sampling occurs, a warning is written to the capture log.  ","version":"Next","tagName":"h3"},{"title":"Processing latency​","type":1,"pageTitle":"(Deprecated) Google Analytics UA","url":"/reference/Connectors/capture-connectors/google-analytics/#processing-latency","content":" Data in Google Analytics reports may continue to update up to 48 hours after it appears.  To ensure data correctness, each time it reads from Google Analytics, this connector automatically applies a lookback window of 2 days prior to its last read. This allows it to double-check and correct for any changes in reports resulting from latent data updates.  This mechanism relies on the isDataGolden flag in the Google Analytics Reporting API. ","version":"Next","tagName":"h3"},{"title":"Google Analytics 4","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/google-analytics-4/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Google Analytics 4","url":"/reference/Connectors/capture-connectors/google-analytics-4/#supported-data-resources","content":" The following data resources are supported:  Daily active usersDevicesFour-weekly active usersLocationsPagesTraffic sourcesWebsite overviewWeekly active users  Each is fetched as a report and mapped to a Flow collection through a separate binding.  You can also capture custom reports.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Google Analytics 4","url":"/reference/Connectors/capture-connectors/google-analytics-4/#prerequisites","content":" To use this connector, you'll need:  The Google Analytics Data API enabled on your Google project with which your Analytics property is associated. (Unless you actively develop with Google Cloud, you'll likely just have one option). Your Google Analytics 4 property ID.  There are two ways to authenticate this connector with Google:  Directly with Google using OAuth through the Flow web app. You'll only need your username and password. Manually, by generating a service account key. Using this method, there are more prerequisites.  ","version":"Next","tagName":"h2"},{"title":"Authenticating manually with a service account key​","type":1,"pageTitle":"Google Analytics 4","url":"/reference/Connectors/capture-connectors/google-analytics-4/#authenticating-manually-with-a-service-account-key","content":" In addition to the above prerequisites, you'll need a Google service account with:  A JSON key generated. Access to the Google Analytics 4 property.  To set this up:  Create a service account and generate a JSON key. During setup, grant the account the Viewer role on your project. You'll copy the contents of the downloaded key file into the Service Account Credentials parameter when you configure the connector. Add the service account to the Google Analytics property. Grant the account Viewer permissions.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Google Analytics 4","url":"/reference/Connectors/capture-connectors/google-analytics-4/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing a specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Google Analytics 4 source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Google Analytics 4","url":"/reference/Connectors/capture-connectors/google-analytics-4/#properties","content":" Endpoint​  The following properties reflect the manual authentication method. If you authenticate directly with Google in the Flow web app, some of these properties aren't required.  Property\tTitle\tDescription\tType\tRequired/Default/credentials\tCredentials\tCredentials for the service\tobject /credentials/auth_type\tAuthentication Method\tSet to Service for manual authentication.\tstring /credentials/credentials_json\tService Account Credentials\tContents of the JSON key file generated during setup.\tstring /custom_reports\tCustom Reports (Optional)\tA JSON array describing the custom reports you want to sync from Google Analytics. Learn more about custom reports.\tstring /date_ranges_start_date\tDate Range Start Date\tThe start date. One of the values &lt;N&gt;daysago, yesterday, today or in the format YYYY-MM-DD.\tstring\tRequired /property_id\tProperty ID\tA Google Analytics GA4 property identifier whose events are tracked.\tstring\tRequired /window_in_days\tData request time increment in days (Optional)\tThe time increment used by the connector when requesting data from the Google Analytics API. We recommend setting this to 1 unless you have a hard requirement to make the sync faster at the expense of accuracy. The minimum allowed value for this field is 1, and the maximum is 364. See data sampling for details.\tinteger\t1  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tData resource from Google Analytics.\tstring\tRequired /syncMode\tSync Mode\tConnection method. Always set to incremental.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Custom reports​","type":1,"pageTitle":"Google Analytics 4","url":"/reference/Connectors/capture-connectors/google-analytics-4/#custom-reports","content":" You can include data beyond the default data resources with Custom Reports. These replicate the functionality of Custom Reports in the Google Analytics Web console.  Fill out the Custom Reports property with a JSON array as a string with the following schema:  [{&quot;name&quot;: &quot;&lt;report-name&gt;&quot;, &quot;dimensions&quot;: [&quot;&lt;dimension-name&gt;&quot;, ...], &quot;metrics&quot;: [&quot;&lt;metric-name&gt;&quot;, ...]}]   Segments and filters are also supported. When using segments, you must include the ga:segment dimension:  [{&quot;name&quot;: &quot;&lt;report-name&gt;&quot;, &quot;dimensions&quot;: [&quot;ga:segment&quot;, &quot;&lt;other-dimension-name&gt;&quot;, ...], &quot;metrics&quot;: [&quot;&lt;metric-name&gt;&quot;, ...], &quot;segments&quot;: &quot;&lt;segment-id&gt;&quot;, &quot;filter&quot;: &quot;&lt;filter-expression&gt;&quot;}]   ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Google Analytics 4","url":"/reference/Connectors/capture-connectors/google-analytics-4/#sample","content":" This sample reflects the manual authentication method.  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-google-analytics-data-api:dev config: credentials: auth_type: Service credentials_json: &lt;secret&gt; date_ranges_start_date: 2023-01-01 property_id: 000000000 window_in_days: 1 bindings: - resource: stream: daily_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: devices syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: four_weekly_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: locations syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: pages syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: traffic_sources syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: website_overview syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: weekly_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Performance considerations​","type":1,"pageTitle":"Google Analytics 4","url":"/reference/Connectors/capture-connectors/google-analytics-4/#performance-considerations","content":" ","version":"Next","tagName":"h2"},{"title":"Data sampling​","type":1,"pageTitle":"Google Analytics 4","url":"/reference/Connectors/capture-connectors/google-analytics-4/#data-sampling","content":" The Google Analytics Data API enforces compute thresholds for ad-hoc queries and reports. If a threshold is exceeded, the API will apply sampling to limit the number of sessions analyzed for the specified time range. These thresholds can be found here.  If your account is on the Analytics 360 tier, you're less likely to run into these limitations. For Analytics Standard accounts, you can avoid sampling by keeping the window_in_days parameter set to its default value, 1. This makes it less likely that you will exceed the threshold. ","version":"Next","tagName":"h3"},{"title":"Google Drive","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/google-drive/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Google Drive","url":"/reference/Connectors/capture-connectors/google-drive/#prerequisites","content":" To use this connector, make sure you have the following:  An active Google Drive account with access credentials.Properly configured permissions for your Google Drive resources.  Note: This connector is designed specifically for .csv files located in a specified Google Drive folder.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Google Drive","url":"/reference/Connectors/capture-connectors/google-drive/#configuration","content":" You can set up the Google Drive source connector either through the Flow web app or by editing the Flow specification file directly. For more information on setting up this connector, refer to our guide on using connectors.  The values and specification sample below provide configuration details specific to the Google Drive connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Google Drive","url":"/reference/Connectors/capture-connectors/google-drive/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/credentials\tCredentials\tJSON file containing authentication credentials for Google Drive.\tfile\tRequired  ","version":"Next","tagName":"h3"},{"title":"Bindings​","type":1,"pageTitle":"Google Drive","url":"/reference/Connectors/capture-connectors/google-drive/#bindings","content":" Property\tTitle\tDescription\tType\tRequired/Default/file_id\tFile ID\tUnique identifier of the Google Drive file.\tstring\tRequired /path\tPath\tPath to the file or directory in Google Drive.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Google Drive","url":"/reference/Connectors/capture-connectors/google-drive/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-google-drive:dev config: credentials: /path/to/your/credentials.json bindings: - resource: file_id: &quot;your_google_drive_file_id&quot; path: &quot;/path/in/google/drive&quot; target: ${PREFIX}/target_name  ","version":"Next","tagName":"h3"},{"title":"Google Firestore","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/google-firestore/","content":"","keywords":"","version":"Next"},{"title":"Data model​","type":1,"pageTitle":"Google Firestore","url":"/reference/Connectors/capture-connectors/google-firestore/#data-model","content":" Firestore is a NoSQL database. Its data model consists of documents (lightweight records that contain mappings of fields and values) organized in collections.  Collections are organized hierarchically. A given document in a collection can, in turn, be associated with a subcollection.  For example, you might have a collection called users, which contains two documents, alice and bob. Each document has a subcollection called messages (for example, users/alice/messages), which contain more documents (for example, users/alice/messages/1).  users ├── alice │ └── messages │ ├── 1 │ └── 2 └── bob └── messages └── 1   The connector works by identifying documents associated with a particular sequence of Firestore collection names, regardless of documents that split the hierarchy. These document groupings are mapped to Flow collections using a path in the pattern collection/*/subcollection.  In this example, we'd end up with users and users/*/messages Flow collections, with the latter contain messages from both users. The /_meta/path property for each document contains its full, original path, so we'd still know which messages were Alice's and which were Bob's.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Google Firestore","url":"/reference/Connectors/capture-connectors/google-firestore/#prerequisites","content":" You'll need:  A Google service account with: Read access to your Firestore database, via roles/datastore.viewer. You can assign this role when you create the service account, or add it to an existing service account. A generated JSON service account key for the account.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Google Firestore","url":"/reference/Connectors/capture-connectors/google-firestore/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the Flow specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Firestore source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Google Firestore","url":"/reference/Connectors/capture-connectors/google-firestore/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/googleCredentials\tCredentials\tGoogle Cloud Service Account JSON credentials.\tstring\tRequired /database\tDatabase\tOptional name of the database to capture from. Leave blank to autodetect. Typically &quot;projects/$PROJECTID/databases/(default)&quot;.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/backfillMode\tBackfill Mode\tConfigures the handling of data already in the collection. See below for details or just stick with 'async'\tstring\tRequired /path\tPath to Collection\tSupports parent/*/nested to capture all nested collections of parent's children\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Google Firestore","url":"/reference/Connectors/capture-connectors/google-firestore/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-firestore:dev config: googleCredentials: &quot;type&quot;: &quot;service_account&quot;, &quot;project_id&quot;: &quot;project-id&quot;, &quot;private_key_id&quot;: &quot;key-id&quot;, &quot;private_key&quot;: &quot;-----BEGIN PRIVATE KEY-----\\nprivate-key\\n-----END PRIVATE KEY-----\\n&quot;, &quot;client_email&quot;: &quot;service-account-email&quot;, &quot;client_id&quot;: &quot;client-id&quot;, &quot;auth_uri&quot;: &quot;https://accounts.google.com/o/oauth2/auth&quot;, &quot;token_uri&quot;: &quot;https://accounts.google.com/o/oauth2/token&quot;, &quot;auth_provider_x509_cert_url&quot;: &quot;https://www.googleapis.com/oauth2/v1/certs&quot;, &quot;client_x509_cert_url&quot;: &quot;https://www.googleapis.com/robot/v1/metadata/x509/service-account-email&quot; bindings: - resource: #The below `path` will capture all Firestore documents that match the pattern #`orgs/&lt;orgID&gt;/runs/&lt;runID&gt;/runResults/&lt;runResultID&gt;/queryResults`. #See the Data Model section above for details. path: orgs/*/runs/*/runResults/*/queryResults backfillMode: async target: ${PREFIX}/orgs_runs_runResults_queryResults - resource: path: orgs/*/runs/*/runResults backfillMode: async target: ${PREFIX}/orgs_runs_runResults - resource: path: orgs/*/runs backfillMode: async target: ${PREFIX}/orgs_runs - resource: path: orgs backfillMode: async target: ${PREFIX}/orgs   ","version":"Next","tagName":"h3"},{"title":"Backfill mode​","type":1,"pageTitle":"Google Firestore","url":"/reference/Connectors/capture-connectors/google-firestore/#backfill-mode","content":" In each captured collection's binding configuration, you can choose whether and how to backfill historical data. There are three options:  none: Skip preexisting data in the Firestore collection. Capture only new documents and changes to existing documents that occur after the capture is published. async: Use two threads to capture data. The first captures new documents, as with none. The second progressively ingests historical data in chunks. This mode is most reliable for Firestore collections of all sizes but provides slightly weaker guarantees against data duplication. The connector uses a reduction to reconcile changes to the same document found on the parallel threads. The version with the most recent timestamp the document metadata will be preserved ({&quot;strategy&quot;: &quot;maximize&quot;, &quot;key&quot;: &quot;/_meta/mtime&quot;}). For most collections, this produces an accurate copy of your Firestore collections in Flow. sync: Request that Firestore stream all changes to the collection since its creation, in order. This mode provides the strongest guarantee against duplicated data, but can cause errors for large datasets. Firestore may terminate the process if the backfill of historical data has not completed within about ten minutes, forcing the capture to restart from the beginning. If this happens once it is likely to recur continuously. If left unattended for an extended time this can result in a massive number of read operations and a correspondingly large bill from Firestore. This mode should only be used when somebody can keep an eye on the backfill and shut it down if it has not completed within half an hour at most, and on relatively small collections. 100,000 documents or fewer should generally be safe, although this can vary depending on the average document size in the collection.  If you're unsure which backfill mode to use, choose async. ","version":"Next","tagName":"h2"},{"title":"Google Cloud Pub/Sub","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/google-pubsub/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"/reference/Connectors/capture-connectors/google-pubsub/#prerequisites","content":" To use this connector, you will need the following prerequisites:  A Google Cloud Project with Pub/Sub enabledA Google Cloud Service Account with the &quot;Pub/Sub Editor&quot; roles in your GCP projectA Service Account Key to authenticate into your Service Account  See the setup guide for more information about how to create the required resources.  ","version":"Next","tagName":"h2"},{"title":"Service Account​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"/reference/Connectors/capture-connectors/google-pubsub/#service-account","content":" To sync data from Pub/Sub, you need credentials for a Service Account with the &quot;Pub/Sub Editor&quot; role. This role grants the necessary permissions to discover topics, create subscriptions to those topics, and read messages from the subscriptions. It is recommended to create a dedicated Service Account to facilitate permission management and auditing. However, if you already have a Service Account with the correct permissions, you can use it.  Here's how to provision a suitable service account:  Follow Google Cloud Platform's instructions for Creating a Service Account.Note down the ID of the service account you just created. Service Account IDs typically follow the format&lt;account-name&gt;@&lt;project-name&gt;.iam.gserviceaccount.com.Follow Google Cloud Platform's instructions for Granting IAM Rolesto the new service account. The &quot;principal&quot; email address should be the ID of the service account you just created, and the role granted should be &quot;Pub/Sub Editor&quot;.  ","version":"Next","tagName":"h3"},{"title":"Service Account Key​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"/reference/Connectors/capture-connectors/google-pubsub/#service-account-key","content":" Service Account Keys are used to authenticate as Google Service Accounts. To be able to utilize the permissions granted to the Service Account in the previous step, you'll need to provide its Service Account Key when creating the capture. It is a good practice, though not required, to create a new key for Flow even if you're reusing a preexisting account.  To create a new key for a service account, follow Google Cloud Platform's instructions for Creating a Service Account Key. Be sure to create the key in JSON format. Once the linked instructions have been followed you should have a key file, which will need to be uploaded to Flow when setting up your capture.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"/reference/Connectors/capture-connectors/google-pubsub/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. Seeconnectors to learn more about using connectors.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"/reference/Connectors/capture-connectors/google-pubsub/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/projectId\tProject ID\tGoogle Cloud Project ID that contains the PubSub topics.\tstring\tRequired /credentialsJson\tService Account JSON\tGoogle Cloud Service Account JSON credentials to use for authentication.\tstring\tRequired /subscriptionPrefix\tSubscription Prefix\tPrefix to prepend to the PubSub topics subscription names.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/topic\tTopic\tName of the PubSub topic to subscribe to.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"At-Least-Once Message Capture​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"/reference/Connectors/capture-connectors/google-pubsub/#at-least-once-message-capture","content":" Received messages are acknowledged to Pub/Sub after they have been durably committed to your Flow collection. After Pub/Sub receives acknowledgement of messages, it will not attempt to re-deliver messages to subscribers if subscriptions are created with &quot;exactly-once delivery&quot;, which this connector does set when it creates subscriptions. Occasionally messages will be captured to your Flow collection more than once if the connector is restarted after it has durably committed the document to the collection but before it has acknowledged the message to Pub/Sub.  In this way the committing of the message to your Flow collection is considered a &quot;side effect&quot; of processing the message, and Pub/Sub does not provide guarantees around exactly-once side effects.  If you materialize the collections using standard updates, duplicate documents will automatically be de-duplicated in the destination based on the ID of the documents. Materializations that use delta updates need to consider the potential for more than one document with the same ID.  ","version":"Next","tagName":"h3"},{"title":"Message Format​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"/reference/Connectors/capture-connectors/google-pubsub/#message-format","content":" Currently only messages with data in JSON format can be processed. Data in other formats will cause errors with the capture connector. Support for other formats is planned - reach out to support@estuary.dev if your use case requires processing data in a different format. ","version":"Next","tagName":"h3"},{"title":"Google Sheets","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/google-sheets/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Google Sheets","url":"/reference/Connectors/capture-connectors/google-sheets/#prerequisites","content":" There are two ways to authenticate with Google when capturing data from a Sheet: using OAuth2, and manually, by generating a service account key. Their prerequisites differ.  OAuth is recommended for simplicity in the Flow web app; the service account key method is the only supported method using the command line.  ","version":"Next","tagName":"h2"},{"title":"Using OAuth2 to authenticate with Google in the Flow web app​","type":1,"pageTitle":"Google Sheets","url":"/reference/Connectors/capture-connectors/google-sheets/#using-oauth2-to-authenticate-with-google-in-the-flow-web-app","content":" A link to a Google spreadsheet. Simply copy the link from your browser. Your Google account username and password.  ","version":"Next","tagName":"h3"},{"title":"Spreadsheet Formatting​","type":1,"pageTitle":"Google Sheets","url":"/reference/Connectors/capture-connectors/google-sheets/#spreadsheet-formatting","content":" For a more efficient usage, the connector expects a few basic formatting rules inside each spreadsheet:  The first row must be frozen and contain header names for each column. If the first row is not frozen or does not contain header names, header names will be set using high-case alphabet letters (A,B,C,D...Z). Sheet is not an image sheet or contains images.Sheet is not empty. If a Sheet is empty, the connector will not break and wait for changes inside the Sheet. When new data arrives, you will be prompted by Flow to allow for schema changes. Sheet does not contain formulaValue inside any cell.  ","version":"Next","tagName":"h3"},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"Google Sheets","url":"/reference/Connectors/capture-connectors/google-sheets/#configuring-the-connector-specification-manually","content":" A link to a Google spreadsheet. Simply copy the link from your browser. Google Sheets and Google Drive APIs enabled on your Google account. A Google service account with: A JSON key generated.Access to the source spreadsheet.  Follow the steps below to meet these prerequisites:  Enable the Google Sheets and Google Drive APIs for the Google project with which your spreadsheet is associated. (Unless you actively develop with Google Cloud, you'll likely just have one option). Create a service account and generate a JSON key. During setup, grant the account the Viewer role on your project. You'll copy the contents of the downloaded key file into the Service Account Credentials parameter when you configure the connector. Share your Google spreadsheet with the service account. You may either share the sheet so that anyone with the link can view it, or share explicitly with the service account's email address.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Google Sheets","url":"/reference/Connectors/capture-connectors/google-sheets/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Google Sheets source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Google Sheets","url":"/reference/Connectors/capture-connectors/google-sheets/#properties","content":" Endpoint​  The following properties reflect the Service Account Key authentication method.  Property\tTitle\tDescription\tType\tRequired/Default/credentials\tCredentials\tGoogle API Credentials for connecting to Google Sheets and Google Drive APIs\tobject\tRequired /credentials/auth_type\tAuthentication Type\tAuthentication method. Set to Service.\tstring\tRequired credentials/service_account_info\tService Account Credentials\tContents of the JSON key file generated during setup.\tstring\tRequired /spreadsheet_id\tSpreadsheet Link\tThe link to your spreadsheet.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tSheet\tEach sheet in your Google Sheets document.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to full_refresh.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Google Sheets","url":"/reference/Connectors/capture-connectors/google-sheets/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-google-sheets:dev config: credentials: auth_type: Service service_account_info: &lt;secret&gt; spreadsheet_id: https://docs.google.com/spreadsheets/... bindings: - resource: stream: Sheet1 syncMode: full_refresh target: ${PREFIX}/${COLLECTION_NAME}   Learn more about capture definitions. ","version":"Next","tagName":"h3"},{"title":"Google Search Console","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/google-search-console/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Google Search Console","url":"/reference/Connectors/capture-connectors/google-search-console/#supported-data-resources","content":" The following data resources are supported:  Search analytics: all fields This resource contains all data in for your search analytics, and can be large. The following five collections come from queries applied to this dataset. Search analytics by countrySearch analytics by dateSearch analytics by deviceSearch analytics by pageSearch analytics by querySitemapsSites  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Custom reports​","type":1,"pageTitle":"Google Search Console","url":"/reference/Connectors/capture-connectors/google-search-console/#custom-reports","content":" In addition to the resources listed above, you can add custom reports created with the Google Analytics Search Console integration. You add these to the endpoint configuration in the format {&quot;name&quot;: &quot;&lt;report-name&gt;&quot;, &quot;dimensions&quot;: [&quot;&lt;dimension-name&gt;&quot;, ...]}. Each report is mapped to an additional Flow collection.  caution Custom reports involve an integration with Google Universal Analytics, which Google deprecated in July 2023.  ","version":"Next","tagName":"h3"},{"title":"Prerequisites​","type":1,"pageTitle":"Google Search Console","url":"/reference/Connectors/capture-connectors/google-search-console/#prerequisites","content":" There are two ways to authenticate with Google when capturing data from Google Search Console: using OAuth2, and manually, by generating a service account key. Their prerequisites differ.  OAuth2 is recommended for simplicity in the Flow web app; the service account key method is the only supported method using the command line.  ","version":"Next","tagName":"h2"},{"title":"Using OAuth2 to authenticate with Google in the Flow web app​","type":1,"pageTitle":"Google Search Console","url":"/reference/Connectors/capture-connectors/google-search-console/#using-oauth2-to-authenticate-with-google-in-the-flow-web-app","content":" You'll need:  Google credentials with Owner access on the Google Search Console property. This can be a user account or a service account.  You'll use these credentials to log in to Google in the Flow web app.  ","version":"Next","tagName":"h3"},{"title":"Authenticating manually with a service account key​","type":1,"pageTitle":"Google Search Console","url":"/reference/Connectors/capture-connectors/google-search-console/#authenticating-manually-with-a-service-account-key","content":" You'll need:  A Google service account with: A JSON key generated.Access to the Google Search Console view through the API.  Follow the steps below to meet these prerequisites:  Create a service account and generate a JSON key. You'll copy the contents of the downloaded key file into the Service Account Credentials parameter when you configure the connector. Set up domain-wide delegation for the service account. During this process, grant the https://www.googleapis.com/auth/webmasters.readonly OAuth scope.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Google Search Console","url":"/reference/Connectors/capture-connectors/google-search-console/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Google Search Console source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Google Search Console","url":"/reference/Connectors/capture-connectors/google-search-console/#properties","content":" Endpoint​  The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so many of these properties aren't required.  Property\tTitle\tDescription\tType\tRequired/Default/credentials\tAuthentication object\tRequired /credentials/auth_type\tAuthentication Type\tSet to Service for manual authentication\tstring\tRequired /credentials/service_account_info\tService Account JSON Key\tThe JSON key of the service account to use for authorization.\tRequired /credentials/email\tAdmin Email\tThe email of your Google Workspace administrator. This is likely the account used during setup. /custom_reports\tCustom Reports (Optional)\tA JSON array describing the custom reports you want to sync from Google Search Console.\tstring /end_date\tEnd Date\tUTC date in the format 2017-01-25. Any data after this date will not be replicated. Must be greater or equal to the start date field.\tstring /site_urls\tWebsite URL\tThe URLs of the website properties attached to your GSC account: domain:example.com https://example.com/ This connector supports both URL-prefix and domain property URLs.\tarray\tRequired /start_date\tStart Date\tUTC date in the format 2017-01-25. Any data before this date will not be replicated.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tGoogle Search Console resource from which a collection is captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Google Search Console","url":"/reference/Connectors/capture-connectors/google-search-console/#sample","content":" This sample specification reflects the manual authentication method.  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-google-search-console:dev config: credentials: auth_type: Service service_account_info: &lt;secret&gt; email: admin@yourdomain.com site_urls: https://yourdomain.com start_date: 2022-03-01 bindings: - resource: stream: sites syncMode: full_refresh target: ${PREFIX}/sites {}  ","version":"Next","tagName":"h3"},{"title":"Greenhouse","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/greenhouse/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Greenhouse","url":"/reference/Connectors/capture-connectors/greenhouse/#supported-data-resources","content":" The following data resources are supported through the Greenhouse APIs:  Activity FeedApplicationsApplications InterviewsApprovalsCandidatesClose ReasonsCustom FieldsDegreesDepartmentsDisciplinesEEOCEmail TemplatesInterviewsJob PostsJob StagesJobsJob OpeningsJobs StagesOffersOfficesProspect PoolsRejection ReasonsSchoolsScorecardsSourcesTagsUsersUser PermissionsUser Roles  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Greenhouse","url":"/reference/Connectors/capture-connectors/greenhouse/#prerequisites","content":" To set up the Greenhouse source connector, you'll need the Harvest API key with permissions to the resources Estuary Flow should be able to access.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Greenhouse","url":"/reference/Connectors/capture-connectors/greenhouse/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Greenhouse source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Greenhouse","url":"/reference/Connectors/capture-connectors/greenhouse/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/apikey\tAPI Key\tThe value of the Greenhouse API Key generated.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Greenhouse project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Greenhouse","url":"/reference/Connectors/capture-connectors/greenhouse/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-greenhouse:dev config: apikey: &lt;secret&gt; bindings: - resource: stream: applications syncMode: full_refresh target: ${PREFIX}/applications {...}  ","version":"Next","tagName":"h3"},{"title":"Harvest","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/harvest/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Harvest","url":"/reference/Connectors/capture-connectors/harvest/#supported-data-resources","content":" The following data resources are supported through the Harvest APIs:  Client ContactsClientsCompanyInvoice MessagesInvoice PaymentsInvoicesInvoice Item CategoriesEstimate MessagesEstimatesEstimate Item CategoriesExpensesExpense CategoriesTasksTime EntriesProject User AssignmentsProject Task AssignmentsProjectsRolesUser Billable RatesUser Cost RatesUser Project AssignmentsExpense ReportsUninvoiced ReportTime ReportsProject Budget Report  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Harvest","url":"/reference/Connectors/capture-connectors/harvest/#prerequisites","content":" To set up the Harvest source connector, you'll need the Harvest Account ID and API key.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Harvest","url":"/reference/Connectors/capture-connectors/harvest/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Harvest source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Harvest","url":"/reference/Connectors/capture-connectors/harvest/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/account_id\tAccount ID\tHarvest account ID. Required for all Harvest requests in pair with Personal Access Token.\tstring\tRequired /start_date\tStart Date\tUTC date and time in the format 2021-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired /end_date\tEnd Date\tUTC date and time in the format 2021-01-25T00:00:00Z. Any data after this date will not be replicated.\tstring\tDefault  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Harvest project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Harvest","url":"/reference/Connectors/capture-connectors/harvest/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-harvest:dev config: account_id: &lt;account id&gt; start_date: 2017-01-25T00:00:00Z end_date: 2020-01-25T00:00:00Z bindings: - resource: stream: clients syncMode: incremental target: ${PREFIX}/clients {...}  ","version":"Next","tagName":"h3"},{"title":"HTTP Ingest (Webhook)","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/http-ingest/","content":"","keywords":"","version":"Next"},{"title":"Usage​","type":1,"pageTitle":"HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/http-ingest/#usage","content":" This connector is different from most other capture connectors in that it's not designed to pull data from a specific system or endpoint. It requires no endpoint-specific configuration, and can accept any and all valid JSON objects from any source.  This is especially useful if you want to test out Flow or see how your webhook data will be received.  To begin, use the web app to create and publish a capture. Estuary will create a unique URL for your public endpoint. By default, this will accept webhook requests at https://&lt;your-public-endpoint&gt;/webhook-data, but you can customize the path, or even capture from multiple URL paths if you like.  ","version":"Next","tagName":"h2"},{"title":"Webhook URLs​","type":1,"pageTitle":"HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/http-ingest/#webhook-urls","content":" Some services, such as GitHub, Shopify, and Segment, allow you to send data to a specified URL. Estuary can generate and manage this destination URL. You will then need to add Estuary’s URL to the source service. This will allow the source service to send webhook data directly to your Estuary capture.    To determine the full URL:  Your capture must first be published and enabled. Retrieve the base URL. On the Capture Details page, scroll down to the Endpoints section. The listed link will be the base URL for your webhook. This should be something like https://abc123-8080.us-central1.v1.estuary-data.dev. Add the specific path. This will depend on the capture's paths endpoint configuration field. By default, this is /webhook-data. You can add additional paths to paths, and the connector will accept webhook requests on each of them.  Using this example, the full webhook URL would be: https://abc123-8080.us-central1.v1.estuary-data.dev/webhook-data  Each path will correspond to a separate binding. If you're editing the capture via the UI, click the &quot;refresh&quot; button after editing the URL paths in the endpoint config to see the resulting collections in the bindings editor. For example, if you set the path to /my-webhook.json, then the full URL for that binding would be https://&lt;your-unique-hostname&gt;/my-webhook.json.  Any URL query parameters that are sent on the request will be captured and serialized under /_meta/query/* the in documents. For example, a webhook request that's sent to /webhook-data?testKey=testValue would result in a document like:  { &quot;_meta&quot;: { &quot;webhookId&quot;: &quot;...&quot;, &quot;query&quot;: { &quot;testKey&quot;: &quot;testValue&quot; }, ... } ... }   ","version":"Next","tagName":"h3"},{"title":"Send sample data to Flow​","type":1,"pageTitle":"HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/http-ingest/#send-sample-data-to-flow","content":" After publishing the capture, click the endpoint link from the confirmation dialog to open the Swagger UI page for your capture. Expand POST or PUT and click Try it out to send some example JSON documents using the UI. You can also copy the provided curl commands to send data via the command line. After sending data, go to the Collections page of the Flow web app and find the collection associated with your capture. Click Details to view the data preview.  ","version":"Next","tagName":"h3"},{"title":"Path parameters​","type":1,"pageTitle":"HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/http-ingest/#path-parameters","content":" Paths are allowed to contain parameter placeholders, which will be captured and serialized under /_meta/pathParams/* in the documents. For example, if you configure a path for /foo/{fooId} a webhook request that's sent to /foo/123 would result in a document like:  { &quot;_meta&quot;: { &quot;webhookId&quot;: &quot;...&quot;, &quot;pathParams&quot;: { &quot;fooId&quot;: &quot;123&quot; }, &quot;reqPath&quot;: &quot;/foo/{fooId}&quot;, ... } ... }   Multiple parameters are allowed, for example /foo/{fooId}/bar/{barId}. Each parameter corresponds to exactly one path segment in the request URL. Capturing multiple segments in a single parameter is not supported. The syntax and semantics of the path specification follow the OpenAPI specification (a.k.a Swagger).  Path parameters are automatically added to the collection write schema as required properties, so they can be used as part of the collection key by editing the collection during capture creation.  Care must be taken when specifying multiple paths, to ensure they don't conflict with each other. For example, you may not specify both /{paramA} and /{paramB}, because it would be impossible to determine which path to use for a request to /123.  ","version":"Next","tagName":"h3"},{"title":"Webhook IDs​","type":1,"pageTitle":"HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/http-ingest/#webhook-ids","content":" Webhook delivery is typically &quot;at least once&quot;. This means that webhooks from common services such as Github, Segment, Shopify, etc. may sometimes be sent multiple times. In order to prevent problems due to duplicate processing of webhooks, these services typically provide either an HTTP header or a field within each document that serves as a unique ID for each webhook event. This can be used to deduplicate the events in your webhook-data collection. The key of the discovered webhook-data collection is /_meta/webhookId. By default, this value is generated automatically by the connector, and no-deduplication will be performed. You can set the idFromHeader option in the resource configuration to have the connector automatically assign the value of the given HTTP header to the /_meta/webhookId property. Doing so means that a materialization of the webhook-data collection will automatically deduplicate the webhook events.  Here's a table with some common webhook services and headers that they use:  Service\tValue to use for idFromHeaderGithub\tX-Github-Event Shopify\tX-Shopify-Webhook-Id Zendesk\tx-zendesk-webhook-id Jira\tX-Atlassian-Webhook-Identifier  ","version":"Next","tagName":"h3"},{"title":"Custom collection IDs​","type":1,"pageTitle":"HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/http-ingest/#custom-collection-ids","content":" Some webhooks don't pass a deduplication ID as part of the HTTP headers. That's fine, and you can still easily deduplicate the events. To do so, you'll just need to customize the schema and key of your webhook-data collection, or bind the webhook to an existing collection that already has the correct schema and key. Just set the key to the field(s) within the webhook payload that uniquely identify the event. For example, to capture webhooks from Segment, you'll want to set the key to [&quot;/messageId&quot;], and ensure that the schema requires that property to exist and be a string.  ","version":"Next","tagName":"h3"},{"title":"Authentication​","type":1,"pageTitle":"HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/http-ingest/#authentication","content":" The connector can optionally require each request to present an authentication token as part of an Authorization: Bearer HTTP header. To enable authentication, generate a secret and paste it into the &quot;Require Auth Token&quot; field. We recommend using a password manager to generate these values, but keep in mind that not all systems will be able to send values with certain special characters, so you may want to disable special characters when you generate the secret. If you enable authentication, then each incoming request must have an Authorization header with the value of your token. For example, if you use an auth token value of mySecretToken, then the header on each request must be Authorization: Bearer mySecretToken.  If you don't enable authentication, then anyone who knows the URL will be able to publish data to your collection. We recommend using authentication whenever possible.  ","version":"Next","tagName":"h3"},{"title":"CORS allowed origins​","type":1,"pageTitle":"HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/http-ingest/#cors-allowed-origins","content":" Under Endpoint Config, you can set CORS (Cross-Origin Resource Sharing) allowed origins for your webhook URLs. By default, CORS will be disabled. Enable it by adding at least one allowed request origin to the list. Each value in the list will be permitted by the Access-Control-Allow-Origin header.  ","version":"Next","tagName":"h3"},{"title":"Webhook signature verification​","type":1,"pageTitle":"HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/http-ingest/#webhook-signature-verification","content":" This connector does not yet support webhook signature verification. If this is a requirement for your use case, please contact support@estuary.dev and let us know.  ","version":"Next","tagName":"h3"},{"title":"Endpoint Configuration​","type":1,"pageTitle":"HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/http-ingest/#endpoint-configuration","content":" Property\tTitle\tDescription\tType\tRequired/Default****\tEndpointConfig object\tRequired /require_auth_token\tAuthentication token\tOptional bearer token to authenticate webhook requests. WARNING: If this is empty or unset, then anyone who knows the URL of the connector will be able to write data to your collections.\tnull, string\tnull /paths\tURL Paths\tList of URL paths to accept requests at. Discovery will return a separate collection for each given path. Paths must be provided without any percent encoding, and should not include any query parameters or fragment.\tnull, string\tnull  ","version":"Next","tagName":"h2"},{"title":"Resource configuration​","type":1,"pageTitle":"HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/http-ingest/#resource-configuration","content":" Property\tTitle\tDescription\tType\tRequired/Default****\tResourceConfig object\tRequired /idFromHeader Set the /_meta/webhookId from the given HTTP header in each request. If not set, then a random id will be generated automatically. If set, then each request will be required to have the header, and the header value will be used as the value of `/_meta/webhookId`.\tnull, string /path The URL path to use for adding documents to this binding. Defaults to the name of the collection.\tnull, string /stream The name of the binding, which is used as a merge key when doing Discovers.\tnull, string\t ","version":"Next","tagName":"h2"},{"title":"HTTP File","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/http-file/","content":"","keywords":"","version":"Next"},{"title":"Supported data types​","type":1,"pageTitle":"HTTP File","url":"/reference/Connectors/capture-connectors/http-file/#supported-data-types","content":" This connector automatically captures the data hosted at the specified URL into a single Flow collection.  The following file types are supported:  AvroCSVJSONProtobufW3C Extended Log  The following compression methods are supported:  ZIPGZIPZSTD  By default, Flow automatically detects the file type and compression method. If necessary, you can specify the correct file type, compression, and other properties (CSV only) using the optional parser configuration.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"HTTP File","url":"/reference/Connectors/capture-connectors/http-file/#prerequisites","content":" To use this connector, you'll need the URL to an HTTP endpoint that hosts data of one of the supported types. The HTTP endpoint must support HEAD HTTP requests, and the response to this request must include a Last-Modified header.  tip You can send a test HEAD request using Curl with the -I parameter, for example:curl -I https://my-site.com/my_hosted_dataset.json.zipUse this online tool to easily do so in your browser.  Some HTTP endpoints require credentials for access. If this is the case, have your username and password ready.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"HTTP File","url":"/reference/Connectors/capture-connectors/http-file/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the HTTP file source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"HTTP File","url":"/reference/Connectors/capture-connectors/http-file/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/credentials\tCredentials\tUser credentials, if required to access the data at the HTTP URL.\tobject /credentials/password\tPassword\tPassword, if required to access the HTTP endpoint.\tstring /credentials/user\tUser\tUsername, if required to access the HTTP endpoint.\tstring /headers\tHeaders object /headers/items\tAdditional HTTP Headers\tAdditional HTTP headers when requesting the file. These are uncommon.\tarray /headers/items/-/key\tHeader Key string /headers/items/-/value\tHeader Value string /parser\tParser Configuration\tConfigures how files are parsed\tobject /parser/compression\tCompression\tDetermines how to decompress the contents. The default, 'Auto', will try to determine the compression automatically.\tnull, string\tnull /parser/format\tFormat\tDetermines how to parse the contents. The default, 'Auto', will try to determine the format automatically based on the file extension or MIME type, if available.\tobject\t{&quot;type&quot;:&quot;auto&quot;} /parser/format/type\tType string /url\tHTTP File URL\tA valid HTTP url for downloading the source file.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tName of the dataset\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"HTTP File","url":"/reference/Connectors/capture-connectors/http-file/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-http-file:dev config: url: https://my-site.com/my_hosted_dataset.json.zip parser: compression: zip format: type: csv config: delimiter: &quot;,&quot; encoding: UTF-8 errorThreshold: 5 headers: [ID, username, first_name, last_name] lineEnding: &quot;\\\\r&quot; quote: &quot;\\&quot;&quot; bindings: - resource: stream: my_hosted_dataset.json.zip target: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Advanced: Parsing HTTP-hosted data​","type":1,"pageTitle":"HTTP File","url":"/reference/Connectors/capture-connectors/http-file/#advanced-parsing-http-hosted-data","content":" HTTP endpoints can support a variety of file types. For each file type, Flow must parse and translate data into collections with defined fields and JSON schemas.  By default, the parser will automatically detect the type and shape of the data at the HTTP endpoint, so you won't need to change the parser configuration for most captures.  However, the automatic detection may be incorrect in some cases. To fix or prevent this, you can provide explicit information in the parser configuration, which is part of the endpoint configuration for this connector.  The parser configuration includes:  Compression: Specify how the data is compressed. If no compression type is specified, the connector will try to determine the compression type automatically. Options are: zipgzipzstdnone Format: Specify the data format, which determines how it will be parsed. If no file type is specified, the connector will try to determine the file type automatically Options are: AvroCSVJSONProtobufW3C Extended Log  CSV configuration​  CSV files include several additional properties that are important to the parser. In most cases, Flow is able to automatically determine the correct values, but you may need to specify for unusual datasets. These properties are:  Delimiter. Options are: Comma (&quot;,&quot;)Pipe (&quot;|&quot;)Space (&quot;0x20&quot;)Semicolon (&quot;;&quot;)Tab (&quot;0x09&quot;)Vertical tab (&quot;0x0B&quot;)Unit separator (&quot;0x1F&quot;)SOH (&quot;0x01&quot;)Auto Encoding type, specified by its WHATWG label. Optionally, an Error threshold, as an acceptable percentage of errors. If set to a number greater than zero, malformed rows that fall within the threshold will be excluded from the capture. Escape characters. Options are: Backslash (&quot;\\\\&quot;)Disable escapes (&quot;&quot;)Auto Optionally, a list of column Headers, if not already included in the first row of the CSV file. If any headers are provided, it is assumed that the provided list of headers is complete and authoritative. The first row of your CSV file will be assumed to be data (not headers), and you must provide a header value for every column in the file. Line ending values CRLF (&quot;\\\\r\\\\n&quot;) (Windows)CR (&quot;\\\\r&quot;)LF (&quot;\\\\n&quot;)Record Separator (&quot;0x1E&quot;)Auto Quote character Double Quote (&quot;\\&quot;&quot;)Single Quote (&quot;)Disable Quoting (&quot;&quot;)Auto  The sample specification above includes these fields.  ","version":"Next","tagName":"h3"},{"title":"Advanced: Using HTTP headers​","type":1,"pageTitle":"HTTP File","url":"/reference/Connectors/capture-connectors/http-file/#advanced-using-http-headers","content":" For data accessed through certain APIs, you may need to send headers as part of your HTTP request. This is uncommon, and is supported by the optional Headers configuration.  This configuration section is encrypted with sops, so you can safely include secrets such as API keys.  See the source data's API documentation for headers that may be required for your capture. ","version":"Next","tagName":"h3"},{"title":"Hubspot","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/hubspot/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Hubspot","url":"/reference/Connectors/capture-connectors/hubspot/#supported-data-resources","content":" By default, each resource associated with your Hubspot account is mapped to a Flow collection through a separate binding.  The following data resources are supported for all subscription levels:  CampaignsCompaniesContact ListsContactsContacts List MembershipsDeal PipelinesDealsEmail EventsEngagementsEngagements CallsEngagements EmailsEngagements MeetingsEngagements NotesEngagements TasksFormsForm SubmissionsLine ItemsOwnersProductsProperty HistoryQuotesSubscription ChangesTicketsTicket Pipelines  The following data resources are supported for pro accounts (set Subscription type to pro in the configuration):  Feedback SubmissionsMarketing EmailsWorkflows  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Hubspot","url":"/reference/Connectors/capture-connectors/hubspot/#prerequisites","content":" There are two ways to authenticate with Hubspot when capturing data: using OAuth2 or with a private app access token. Their prerequisites differ.  OAuth is recommended for simplicity in the Flow web app; the access token method is the only supported method using the command line.  ","version":"Next","tagName":"h2"},{"title":"Using OAuth2 to authenticate with Hubspot in the Flow web app​","type":1,"pageTitle":"Hubspot","url":"/reference/Connectors/capture-connectors/hubspot/#using-oauth2-to-authenticate-with-hubspot-in-the-flow-web-app","content":" A Hubspot account  ","version":"Next","tagName":"h3"},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"Hubspot","url":"/reference/Connectors/capture-connectors/hubspot/#configuring-the-connector-specification-manually","content":" A Hubspot account The access token for an appropriately configured private app on the Hubspot account.  Setup​  To create a private app in Hubspot and generate its access token, do the following.  Ensure that your Hubspot user account has super admin privileges. In Hubspot, create a new private app. Name the app &quot;Estuary Flow,&quot; or choose another name that is memorable to you. Grant the new app Read access for all available scopes. Copy the access token for use in the connector configuration.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Hubspot","url":"/reference/Connectors/capture-connectors/hubspot/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Hubspot source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Hubspot","url":"/reference/Connectors/capture-connectors/hubspot/#properties","content":" Endpoint​  The following properties reflect the access token authentication method.  Property\tTitle\tDescription\tType\tRequired/Default/credentials\tPrivate Application\tAuthenticate with a private app access token\tobject\tRequired /credentials/access_token\tAccess Token\tHubSpot Access token.\tstring\tRequired /credentials/credentials_title\tCredentials\tName of the credentials set\tstring\tRequired, &quot;Private App Credentials&quot; /start_date\tStart Date\tUTC date and time in the format 2017-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired /subscription_type\tYour HubSpot account subscription type\tSome streams are only available to certain subscription packages, we use this information to select which streams to pull data from.\tstring\t&quot;starter&quot;  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tData resource\tName of the data resource.\tstring\tRequired /syncMode\tSync Mode\tConnection method\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Hubspot","url":"/reference/Connectors/capture-connectors/hubspot/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-hubspot:dev config: credentials: credentials_title: Private App Credentials access_token: &lt;secret&gt; bindings: - resource: stream: companies syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}   Your configuration will have many more bindings representing all supported resourcesin your Hubspot account.  Learn more about capture definitions. ","version":"Next","tagName":"h3"},{"title":"HubSpot ( Real-Time )","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/hubspot-real-time/","content":"","keywords":"","version":"Next"},{"title":"Supported HubSpot Resources​","type":1,"pageTitle":"HubSpot ( Real-Time )","url":"/reference/Connectors/capture-connectors/hubspot-real-time/#supported-hubspot-resources","content":" The connector automatically discovers bindings for the following HubSpot resources:  CompaniesContactsCustom ObjectsDeal PipelinesDealsEngagementsEmail EventsLine ItemsOwnersProductsPropertiesTickets  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"HubSpot ( Real-Time )","url":"/reference/Connectors/capture-connectors/hubspot-real-time/#prerequisites","content":" There are two ways to authenticate with HubSpot when capturing data: using OAuth2, or with a private app access token. Their prerequisites differ.  OAuth is recommended for simplicity in the Flow web app.  ","version":"Next","tagName":"h2"},{"title":"Using OAuth2 to authenticate with HubSpot in the Flow web app​","type":1,"pageTitle":"HubSpot ( Real-Time )","url":"/reference/Connectors/capture-connectors/hubspot-real-time/#using-oauth2-to-authenticate-with-hubspot-in-the-flow-web-app","content":" A HubSpot account  ","version":"Next","tagName":"h3"},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"HubSpot ( Real-Time )","url":"/reference/Connectors/capture-connectors/hubspot-real-time/#configuring-the-connector-specification-manually","content":" A HubSpot account The access token for an appropriately configured private app on the Hubspot account.  Setup​  To create a private app in HubSpot and generate its access token, do the following.  Ensure that your HubSpot user account has super admin privileges. In HubSpot, create a new private app. Name the app &quot;Estuary Flow,&quot; or choose another name that is memorable to you. Grant the new app Read access for all available scopes. Copy the access token for use in the connector configuration.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"HubSpot ( Real-Time )","url":"/reference/Connectors/capture-connectors/hubspot-real-time/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the HubSpot Real-Time connector.  Endpoint​  The following properties reflect the access token authentication method.  Property\tTitle\tDescription\tType\tRequired/Default/credentials\tPrivate Application\tAuthenticate with a private app access token\tobject\tRequired /credentials/access_token\tAccess Token\tHubSpot Access token.\tstring\tRequired /credentials/credentials_title\tCredentials\tName of the credentials set\tstring\tRequired, &quot;Private App Credentials&quot;  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/name\tData resource\tName of the data resource.\tstring\tRequired /interval\tInterval\tInterval between data syncs\tstring\t  ","version":"Next","tagName":"h2"},{"title":"Sample​","type":1,"pageTitle":"HubSpot ( Real-Time )","url":"/reference/Connectors/capture-connectors/hubspot-real-time/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-hubspot-native:dev config: credentials_title: Private App Credentials access_token: &lt;secret&gt; bindings: - resource: name: companies target: ${PREFIX}/${COLLECTION_NAME}  ","version":"Next","tagName":"h3"},{"title":"Instagram","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/instagram/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Instagram","url":"/reference/Connectors/capture-connectors/instagram/#supported-data-resources","content":" The following data resources are supported through the Instagram APIs:  For more information, see the Instagram Graph API and Instagram Insights API documentation.  UserUser InsightsMediaMedia InsightsStoriesStory Insights  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Instagram","url":"/reference/Connectors/capture-connectors/instagram/#prerequisites","content":" Meta for Developers accountInstagram business account to your Facebook pageInstagram Graph API to your Facebook appFacebook OAuth ReferenceFacebook ad account ID number  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Instagram","url":"/reference/Connectors/capture-connectors/instagram/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Instagram source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Instagram","url":"/reference/Connectors/capture-connectors/instagram/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/client_id\tClient ID\tThe Client ID of your Instagram developer application.\tstring\tRequired /client_secret\tClient Secret\tThe Client Secret of your Instagram developer application.\tstring\tRequired /access_token\tAccess Token\tThe value of the access token generated with instagram_basic, instagram_manage_insights, pages_show_list, pages_read_engagement, Instagram Public Content Access permissions.\tstring\tRequired /start_date\tReplication Start Date\tUTC date and time in the format YYYY-MM-DDT00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Instagram project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Instagram","url":"/reference/Connectors/capture-connectors/instagram/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-instagram:dev config: client_id: &lt;your client ID&gt; client_secret: &lt;secret&gt; access_token: &lt;secret&gt; start_date: 2017-01-25T00:00:00Z bindings: - resource: stream: stories syncMode: full_refresh target: ${PREFIX}/stories {...}  ","version":"Next","tagName":"h3"},{"title":"Intercom HTTP Ingest (Webhook)","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/intercom-ingest/","content":"","keywords":"","version":"Next"},{"title":"Usage​","type":1,"pageTitle":"Intercom HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/intercom-ingest/#usage","content":" This connector is distinct from all other capture connectors in that it's not designed to pull data from a specific system or endpoint. It requires no endpoint-specific configuration, and can accept any and all valid JSON objects from any source.  This is useful primarily if you want to test out Flow or see how your webhook data will come over.  To begin, use the web app to create a capture. Once published, the confirmation dialog displays a unique URL for your public endpoint.  You're now ready to send data to Flow.  ","version":"Next","tagName":"h2"},{"title":"Send sample data to Flow​","type":1,"pageTitle":"Intercom HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/intercom-ingest/#send-sample-data-to-flow","content":" After publishing the capture, click the endpoint link from the confirmation dialog to open the Swagger UI page for your capture. Expand POST or PUT and click Try it out to send some example JSON documents using the UI. You can also copy the provided curl commands to send data via the command line. After sending data, go to the Collections page of the Flow web app and find the collection associated with your capture. Click Details to view the data preview.  ","version":"Next","tagName":"h3"},{"title":"Configure a Intercom webhook​","type":1,"pageTitle":"Intercom HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/intercom-ingest/#configure-a-intercom-webhook","content":" Navigate to your App in your Developer Hub and select the Webhooks from the configuration options Under Endpoint URL enter in the unique URL generated for your Estuary Webhook endpoint in the format https://&lt;your-webhook-url&gt;/webhook-data Configure the Topics section to trigger on your preferred webhook events and click save. Optionally, you can select Send a test request to preview how the data would be ingested into Estuary.  ","version":"Next","tagName":"h3"},{"title":"Webhook URLs​","type":1,"pageTitle":"Intercom HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/intercom-ingest/#webhook-urls","content":" To determine the full URL, start with the base URL from the Flow web app (for example https://abc123-8080.us-central1.v1.estuary-data.dev), and then append the path.  The path will be whatever is in the paths endpoint configuration field (/webhook-data by default). For example, your full webhook URL would be https://&lt;your-unique-hostname&gt;/webhook-data. You can add additional paths to paths, and the connector will accept webhook requests on each of them. Each path will correspond to a separate binding. If you're editing the capture via the UI, click the &quot;re-fresh&quot; button after editing the URL paths in the endpoint config to see the resulting collections in the bindings editor. For example, if you set the path to /my-webhook.json, then the full URL for that binding would be https://&lt;your-unique-hostname&gt;/my-webhook.json.  Any URL query parameters that are sent on the request will be captured and serialized under /_meta/query/* the in documents. For example, a webhook request that's sent to /webhook-data?testKey=testValue would result in a document like:  { &quot;_meta&quot;: { &quot;webhookId&quot;: &quot;...&quot;, &quot;query&quot;: { &quot;testKey&quot;: &quot;testValue&quot; }, ... } ... }   ","version":"Next","tagName":"h3"},{"title":"Authentication​","type":1,"pageTitle":"Intercom HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/intercom-ingest/#authentication","content":" The connector can optionally require each request to present an authentication token as part of an Authorization: Bearer HTTP header. To enable authentication, generate a secret and paste it into the &quot;Require Auth Token&quot; field. We recommend using a password manager to generate these values, but keep in mind that not all systems will be able to send values with certain special characters, so you may want to disable special characters when you generate the secret. If you enable authentication, then each incoming request must have an Authorization header with the value of your token. For example, if you use an auth token value of mySecretToken, then the header on each request must be Authorization: Bearer mySecretToken.  If you don't enable authentication, then anyone who knows the URL will be able to publish data to your collection. We recommend using authentication whenever possible.  ","version":"Next","tagName":"h3"},{"title":"Webhook signature verification​","type":1,"pageTitle":"Intercom HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/intercom-ingest/#webhook-signature-verification","content":" This connector does not yet support webhook signature verification. If this is a requirement for your use case, please contact support@estuary.dev and let us know.  ","version":"Next","tagName":"h3"},{"title":"Endpoint Configuration​","type":1,"pageTitle":"Intercom HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/intercom-ingest/#endpoint-configuration","content":" Property\tTitle\tDescription\tType\tRequired/Default****\tEndpointConfig object\tRequired /require_auth_token Optional bearer token to authenticate webhook requests. WARNING: If this is empty or unset, then anyone who knows the URL of the connector will be able to write data to your collections.\tnull, string\tnull /paths\tURL Paths\tList of URL paths to accept requests at. Discovery will return a separate collection for each given path. Paths must be provided without any percent encoding, and should not include any query parameters or fragment.\tnull, string\tnull  ","version":"Next","tagName":"h2"},{"title":"Resource configuration​","type":1,"pageTitle":"Intercom HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/intercom-ingest/#resource-configuration","content":" Property\tTitle\tDescription\tType\tRequired/Default****\tResourceConfig object\tRequired /idFromHeader Set the /_meta/webhookId from the given HTTP header in each request. If not set, then a random id will be generated automatically. If set, then each request will be required to have the header, and the header value will be used as the value of `/_meta/webhookId`.\tnull, string /path The URL path to use for adding documents to this binding. Defaults to the name of the collection.\tnull, string\t ","version":"Next","tagName":"h2"},{"title":"Intercom","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/intercom-native/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Intercom","url":"/reference/Connectors/capture-connectors/intercom-native/#supported-data-resources","content":" The following data resources are supported through the Intercom API:  AdminsCompaniesCompany attributesCompany segmentsContactsContact attributesConversationsConversation partsSegmentsTagsTeamsTickets  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Intercom","url":"/reference/Connectors/capture-connectors/intercom-native/#prerequisites","content":" There are two ways to authenticate with Intercom: using OAuth2, or with an access token.  OAuth is recommended for simplicity in the Flow web app.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Intercom","url":"/reference/Connectors/capture-connectors/intercom-native/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Intercom source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Intercom","url":"/reference/Connectors/capture-connectors/intercom-native/#properties","content":" Endpoint​  The properties in the table below reflect manual authentication using the CLI. In the Flow web app, you'll sign in directly and won't need the access token.  Property\tTitle\tDescription\tType\tRequired/Default/credentials/access_token\tAccess Token\tIntercom Access token.\tstring\tRequired /credentials/credentials_title\tCredentials\tName of the credentials set\tstring\tRequired, &quot;Private App Credentials&quot; /start_date\tStart date\tUTC date and time in the format 2017-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\t30 days before the present date /advanced/use_companies_list_endpoint\tUse /companies/list endpoint\tIf TRUE, the /companies/list endpoint is used instead of the /companies/scroll endpoint for the Companies and Company Segments bindings. Typically left as the default unless the connector indicates a different setting is needed.\tboolean\tFalse /advanced/window_size\tWindow size\tWindow size in days for incrementals streams. Typically left as the default unless more frequent checkpoints are desired.\tinteger\t5  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/name\tData resource\tName of the data resource.\tstring\tRequired /interval\tInterval\tInterval between data syncs\tstring\t  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Intercom","url":"/reference/Connectors/capture-connectors/intercom-native/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-intercom-native:dev config: advanced: use_companies_list_endpoint: false window_size: 5 credentials: credentials_title: Private App Credentials access_token: &lt;secret&gt; start_date: &quot;2024-12-13T12:00:00Z&quot; bindings: - resource: name: admins target: ${PREFIX}/admins - resource: name: companies target: ${PREFIX}/companies - resource: name: company_segments target: ${PREFIX}/companysegments - resource: name: conversations target: ${PREFIX}/conversations - resource: name: conversation_parts target: ${PREFIX}/conversationparts - resource: name: contacts target: ${PREFIX}/contacts - resource: name: company_attributes target: ${PREFIX}/companyattributes - resource: name: contact_attributes target: ${PREFIX}/contactattributes - resource: name: segments target: ${PREFIX}/segments - resource: name: tags target: ${PREFIX}/tags - resource: name: teams target: ${PREFIX}/teams - resource: name: tickets target: ${PREFIX}/tickets  ","version":"Next","tagName":"h3"},{"title":"Intercom (Deprecated)","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/intercom/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Intercom (Deprecated)","url":"/reference/Connectors/capture-connectors/intercom/#supported-data-resources","content":" The following data resources are supported through the Intercom API:  AdminsCompaniesCompany attributesCompany segmentsContactsContact attributesConversationsConversation partsSegmentsTagsTeams  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Intercom (Deprecated)","url":"/reference/Connectors/capture-connectors/intercom/#prerequisites","content":" There are two ways to authenticate with Intercom:  In the Flow web app, you sign in directly. You'll need the username and password associated with a user with full permissions on your Intercom workspace. Using the flowctl CLI, you configure authentication manually. You'll need the access token for you Intercom account.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Intercom (Deprecated)","url":"/reference/Connectors/capture-connectors/intercom/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Intercom source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Intercom (Deprecated)","url":"/reference/Connectors/capture-connectors/intercom/#properties","content":" Endpoint​  The properties in the table below reflect manual authentication using the CLI. In the Flow web app, you'll sign in directly and won't need the access token.  Property\tTitle\tDescription\tType\tRequired/Default/access_token\tAccess token\tAccess token for making authenticated requests.\tstring\tRequired /start_date\tStart date\tUTC date and time in the format 2017-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource from Intercom from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Intercom (Deprecated)","url":"/reference/Connectors/capture-connectors/intercom/#sample","content":" The sample below reflects manual authentication in the CLI.  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-intercom:dev config: access_token: &lt;secret&gt; start_date: 2022-06-18T00:00:00Z bindings: - resource: stream: admins syncMode: full_refresh target: ${PREFIX}/admins - resource: stream: companies syncMode: incremental target: ${PREFIX}/companies - resource: stream: company_segments syncMode: incremental target: ${PREFIX}/companysegments - resource: stream: conversations syncMode: incremental target: ${PREFIX}/conversations - resource: stream: conversation_parts syncMode: incremental target: ${PREFIX}/conversationparts - resource: stream: contacts syncMode: incremental target: ${PREFIX}/contacts - resource: stream: company_attributes syncMode: full_refresh target: ${PREFIX}/companyattributes - resource: stream: contact_attributes syncMode: full_refresh target: ${PREFIX}/contactattributes - resource: stream: segments syncMode: incremental target: ${PREFIX}/segments - resource: stream: tags syncMode: full_refresh target: ${PREFIX}/tags - resource: stream: teams syncMode: full_refresh target: ${PREFIX}/teams  ","version":"Next","tagName":"h3"},{"title":"Iterable","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/iterable/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Iterable","url":"/reference/Connectors/capture-connectors/iterable/#supported-data-resources","content":" The following data resources are supported through the Iterable APIs:  CampaignsCampaign MetricsChannelsEmail BounceEmail ClickEmail ComplaintEmail OpenEmail SendEmail Send SkipEmail SubscribeEmail UnsubscribeEventsListsList UsersMessage TypesMetadataTemplatesUsersPushSendPushSendSkipPushOpenPushUninstallPushBounceWebPushSendWebPushClickWebPushSendSkipInAppSendInAppOpenInAppClickInAppCloseInAppDeleteInAppDeliveryInAppSendSkipInboxSessionInboxMessageImpressionSmsSendSmsBounceSmsClickSmsReceivedSmsSendSkipSmsUsageInfoPurchaseCustomEventHostedUnsubscribeClick  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Iterable","url":"/reference/Connectors/capture-connectors/iterable/#prerequisites","content":" To set up the Iterable source connector, you'll need the Iterable Server-side API Key with standard permissions.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Iterable","url":"/reference/Connectors/capture-connectors/iterable/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Iterable source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Iterable","url":"/reference/Connectors/capture-connectors/iterable/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/apikey\tAPI Key\tThe value of the Iterable API Key generated.\tstring\tRequired /start_date\tStart Date\tUTC date and time in the format 2021-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Iterable project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Iterable","url":"/reference/Connectors/capture-connectors/iterable/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-iterable:dev config: apikey: &lt;secret&gt; start_date: 2017-01-25T00:00:00Z bindings: - resource: stream: purchase syncMode: full_refresh target: ${PREFIX}/purchase {...}  ","version":"Next","tagName":"h3"},{"title":"Iterate","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/iterate/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Iterate","url":"/reference/Connectors/capture-connectors/iterate/#supported-data-resources","content":" The following data resources are supported through the Iterate API:  SurveysSurvey Responses  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Iterate","url":"/reference/Connectors/capture-connectors/iterate/#prerequisites","content":" An Iterate API access token  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Iterate","url":"/reference/Connectors/capture-connectors/iterate/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Iterate source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Iterate","url":"/reference/Connectors/capture-connectors/iterate/#properties","content":" Endpoint​  The properties in the table below reflect manual authentication using the CLI. In the Flow web app, you'll sign in directly and won't need the access token.  Property\tTitle\tDescription\tType\tRequired/Default/credentials/access_token\tAccess Token\tIterate Access token.\tstring\tRequired /credentials/credentials_title\tCredentials\tName of the credentials set\tstring\tRequired, &quot;Private App Credentials&quot;  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/name\tData resource\tName of the data resource.\tstring\tRequired /interval\tInterval\tInterval between data syncs\tstring\t  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Iterate","url":"/reference/Connectors/capture-connectors/iterate/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-iterate:dev config: credentials: credentials_title: Private App Credentials access_token: &lt;secret&gt; bindings: - resource: name: surveys target: ${PREFIX}/admins - resource: name: survey_responses target: ${PREFIX}/companies  ","version":"Next","tagName":"h3"},{"title":"Jira HTTP Ingest (Webhook)","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/jira-ingest/","content":"","keywords":"","version":"Next"},{"title":"Usage​","type":1,"pageTitle":"Jira HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/jira-ingest/#usage","content":" This connector is distinct from all other capture connectors in that it's not designed to pull data from a specific system or endpoint. It requires no endpoint-specific configuration, and can accept any and all valid JSON objects from any source.  This is useful primarily if you want to test out Flow or see how your webhook data will come over.  To begin, use the web app to create a capture. Once published, the confirmation dialog displays a unique URL for your public endpoint.  You're now ready to send data to Flow.  ","version":"Next","tagName":"h2"},{"title":"Send sample data to Flow​","type":1,"pageTitle":"Jira HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/jira-ingest/#send-sample-data-to-flow","content":" After publishing the capture, click the endpoint link from the confirmation dialog to open the Swagger UI page for your capture. Expand POST or PUT and click Try it out to send some example JSON documents using the UI. You can also copy the provided curl commands to send data via the command line. After sending data, go to the Collections page of the Flow web app and find the collection associated with your capture. Click Details to view the data preview.  ","version":"Next","tagName":"h3"},{"title":"Configure a Jira webhook​","type":1,"pageTitle":"Jira HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/jira-ingest/#configure-a-jira-webhook","content":" In the Jira Administration console press the . key to bring up Jira's search. Search for Webhooks. Click on a Create a WebHook and in the url section input the url that was generated after publishing a capture in Flow. See the Webhook URLs section below for more information on the structure of your URL.  Review Jira's documentation on configuring a webhook for more information.  ","version":"Next","tagName":"h3"},{"title":"Webhook URLs​","type":1,"pageTitle":"Jira HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/jira-ingest/#webhook-urls","content":" To determine the full URL, start with the base URL from the Flow web app (for example https://abc123-8080.us-central1.v1.estuary-data.dev), and then append the path.  The path will be whatever is in the paths endpoint configuration field (/webhook-data by default). For example, your full webhook URL would be https://&lt;your-unique-hostname&gt;/webhook-data. You can add additional paths to paths, and the connector will accept webhook requests on each of them. Each path will correspond to a separate binding. If you're editing the capture via the UI, click the &quot;re-fresh&quot; button after editing the URL paths in the endpoint config to see the resulting collections in the bindings editor. For example, if you set the path to /my-webhook.json, then the full URL for that binding would be https://&lt;your-unique-hostname&gt;/my-webhook.json.  Any URL query parameters that are sent on the request will be captured and serialized under /_meta/query/* the in documents. For example, a webhook request that's sent to /webhook-data?testKey=testValue would result in a document like:  { &quot;_meta&quot;: { &quot;webhookId&quot;: &quot;...&quot;, &quot;query&quot;: { &quot;testKey&quot;: &quot;testValue&quot; }, ... } ... }   ","version":"Next","tagName":"h3"},{"title":"Authentication​","type":1,"pageTitle":"Jira HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/jira-ingest/#authentication","content":" The connector can optionally require each request to present an authentication token as part of an Authorization: Bearer HTTP header. To enable authentication, generate a secret and paste it into the &quot;Require Auth Token&quot; field. We recommend using a password manager to generate these values, but keep in mind that not all systems will be able to send values with certain special characters, so you may want to disable special characters when you generate the secret. If you enable authentication, then each incoming request must have an Authorization header with the value of your token. For example, if you use an auth token value of mySecretToken, then the header on each request must be Authorization: Bearer mySecretToken.  If you don't enable authentication, then anyone who knows the URL will be able to publish data to your collection. We recommend using authentication whenever possible.  ","version":"Next","tagName":"h3"},{"title":"Webhook signature verification​","type":1,"pageTitle":"Jira HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/jira-ingest/#webhook-signature-verification","content":" This connector does not yet support webhook signature verification. If this is a requirement for your use case, please contact support@estuary.dev and let us know.  ","version":"Next","tagName":"h3"},{"title":"Endpoint Configuration​","type":1,"pageTitle":"Jira HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/jira-ingest/#endpoint-configuration","content":" Property\tTitle\tDescription\tType\tRequired/Default****\tEndpointConfig object\tRequired /require_auth_token Optional bearer token to authenticate webhook requests. WARNING: If this is empty or unset, then anyone who knows the URL of the connector will be able to write data to your collections.\tnull, string\tnull /paths\tURL Paths\tList of URL paths to accept requests at. Discovery will return a separate collection for each given path. Paths must be provided without any percent encoding, and should not include any query parameters or fragment.\tnull, string\tnull  ","version":"Next","tagName":"h2"},{"title":"Resource configuration​","type":1,"pageTitle":"Jira HTTP Ingest (Webhook)","url":"/reference/Connectors/capture-connectors/jira-ingest/#resource-configuration","content":" Property\tTitle\tDescription\tType\tRequired/Default****\tResourceConfig object\tRequired /idFromHeader Set the /_meta/webhookId from the given HTTP header in each request. If not set, then a random id will be generated automatically. If set, then each request will be required to have the header, and the header value will be used as the value of `/_meta/webhookId`.\tnull, string /path The URL path to use for adding documents to this binding. Defaults to the name of the collection.\tnull, string /stream The name of the binding, which is used as a merge key when doing Discovers.\tnull, string\t ","version":"Next","tagName":"h2"},{"title":"Jira","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/jira-native/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Jira","url":"/reference/Connectors/capture-connectors/jira-native/#supported-data-resources","content":" The following data resources are supported through the Jira APIs:  Application rolesAvatarsBoardsBoard issuesDashboardsFiltersFilter sharingGroupsIssuesIssue commentsIssue fieldsIssue field configurationsIssue custom field contextsIssue custom field optionsIssue link typesIssue navigator settingsIssue notification schemesIssue prioritiesIssue propertiesIssue remote linksIssue resolutionsIssue security schemesIssue transitionsIssue typesIssue type schemesIssue type screen schemesIssue votesIssue watchersIssue worklogsJira settingsLabelsPermissionsPermission schemesProjectsProject avatarsProject categoriesProject componentsProject emailProject permission schemesProject rolesProject typesProject versionsScreensScreen tabsScreen tab fieldsScreen schemesSprintsSprint issuesTime trackingUsersUsersGroupsDetailedWorkflowsWorkflow schemesWorkflow statusesWorkflow status categories  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Experimental streams​","type":1,"pageTitle":"Jira","url":"/reference/Connectors/capture-connectors/jira-native/#experimental-streams","content":" These resources are not documented by Jira and must specifically be enabled in the connector configuration.  Pull Requests (GitHub pull requests linked to issues)  ","version":"Next","tagName":"h3"},{"title":"Prerequisites​","type":1,"pageTitle":"Jira","url":"/reference/Connectors/capture-connectors/jira-native/#prerequisites","content":" API Token: You can create an API token following these steps from JiraDomainEmail  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Jira","url":"/reference/Connectors/capture-connectors/jira-native/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Jira source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Jira","url":"/reference/Connectors/capture-connectors/jira-native/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/api_token\tAPI Token\tJira API Token is used for Authorization to your account by BasicAuth.\tstring\tRequired /domain\tDomain\tThe Domain for your Jira account, e.g. estuary.atlassian.net, estuary.jira.com, jira.your-domain.com\tstring\tRequired /email\tEmail\tThe user email for your Jira account which you used to generate the API token. This field is used for Authorization to your account by BasicAuth.\tstring\tRequired /projects\tProjects\tList of Jira project keys to replicate data for. Leave it empty to replicate data for all projects.\tstring[] /start_date\tStart Date\tUTC date-time in the format YYYY-MM-DDT00:00:00Z. Data generated before this date will not be replicated. Note that this field only applies to certain streams.\tstring /lookback_window_minutes\tLookback Window\tWhen set to N, the connector will always refresh resources created within the past N minutes.\tinteger\t0 /enable_experimental_streams\tEnable Experimental Streams\tAllow the use of experimental streams which rely on undocumented Jira API endpoints. See experimental streams above.\tboolean\tfalse /expand_issue_changelog\tExpand Issue Changelog\t(Deprecated) Expand the changelog when replicating issues. See issues_stream_expand_with for a newer implementation.\tboolean\tfalse /render_fields\tRender Issue Fields\t(Deprecated) Render issue fields in HTML format in addition to Jira JSON-like format. See issues_stream_expand_with for a newer implementation.\tboolean\tfalse /expand_issue_transition\tExpand Issue Transitions\t(Deprecated) Expand the transitions when replicating issues. See issues_stream_expand_with for a newer implementation.\tboolean\tfalse /issues_stream_expand_with\tExpand Issues Stream\tArray that compiles issue expansion options. The array can include these enum values: renderedFields, transitions, changelog.\tstring[]\t[]  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Jira project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Jira","url":"/reference/Connectors/capture-connectors/jira-native/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-jira-native:dev config: api_token: &lt;token&gt; domain: &lt;domain&gt; email: &lt;email&gt; start_date: 2025-01-01T00:00:00Z lookback_window_minutes: 60 enable_experimental_streams: false issues_stream_expand_with: - renderedFields - transitions bindings: - resource: stream: issues syncMode: incremental target: ${PREFIX}/issues {...}  ","version":"Next","tagName":"h3"},{"title":"Jira (Deprecated)","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/jira/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Jira (Deprecated)","url":"/reference/Connectors/capture-connectors/jira/#supported-data-resources","content":" The following data resources are supported through the Jira APIs:  Application rolesAvatarsBoardsDashboardsFiltersFilter sharingGroupsIssue fieldsIssue field configurationsIssue custom field contextsIssue link typesIssue navigator settingsIssue notification schemesIssue prioritiesIssue propertiesIssue remote linksIssue resolutionsIssue security schemesIssue type schemesIssue type screen schemesIssue votesIssue watchersJira settingsLabelsPermissionsPermission schemesProjectsProject avatarsProject categoriesProject componentsProject emailProject permission schemesProject typesProject versionsScreensScreen tabsScreen tab fieldsScreen schemesSprintsTime trackingUsersUsersGroupsDetailedWorkflowsWorkflow schemesWorkflow statusesWorkflow status categoriesBoard issuesIssue commentsIssue worklogsIssuesSprint issues  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Jira (Deprecated)","url":"/reference/Connectors/capture-connectors/jira/#prerequisites","content":" API Token: You can create an API token following these steps from JiraDomainEmail  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Jira (Deprecated)","url":"/reference/Connectors/capture-connectors/jira/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Jira source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Jira (Deprecated)","url":"/reference/Connectors/capture-connectors/jira/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/api_token\tAPI Token\tJira API Token is used for Authorization to your account by BasicAuth.\tstring\tRequired /domain\tDomain\tThe Domain for your Jira account, e.g. estuary.atlassian.net, estuary.jira.com, jira.your-domain.com\tstring\tRequired /email\tEmail\tThe user email for your Jira account which you used to generate the API token. This field is used for Authorization to your account by BasicAuth.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Jira project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Jira (Deprecated)","url":"/reference/Connectors/capture-connectors/jira/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-jira:dev config: api_token: &lt;token&gt; domain: &lt;domain&gt; email: &lt;email&gt; bindings: - resource: stream: issues syncMode: incremental target: ${PREFIX}/issues {...}  ","version":"Next","tagName":"h3"},{"title":"Klaviyo","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/klaviyo/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Klaviyo","url":"/reference/Connectors/capture-connectors/klaviyo/#supported-data-resources","content":" This connector can be used to sync the following tables from Klaviyo:  CampaignsEventsGlobalExclusionsListsMetricsFlowsProfiles  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Klaviyo","url":"/reference/Connectors/capture-connectors/klaviyo/#prerequisites","content":" To set up the Klaviyo source connector, you'll need the Klaviyo Private API key.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Klaviyo","url":"/reference/Connectors/capture-connectors/klaviyo/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Klaviyo source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Klaviyo","url":"/reference/Connectors/capture-connectors/klaviyo/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/api_key\tAPI Key\tThe value of the Klaviyo API Key generated.\tstring\tRequired /start_date\tStart Date\tUTC date and time in the format 2021-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Klaviyo project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Klaviyo","url":"/reference/Connectors/capture-connectors/klaviyo/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-klaviyo:dev config: api_key: &lt;secret&gt; start_date: 2017-01-25T00:00:00Z bindings: - resource: stream: lists syncMode: full_refresh target: ${PREFIX}/lists {...}  ","version":"Next","tagName":"h3"},{"title":"LinkedIn Pages","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/linkedin-pages/","content":"","keywords":"","version":"Next"},{"title":"Supported Streams​","type":1,"pageTitle":"LinkedIn Pages","url":"/reference/Connectors/capture-connectors/linkedin-pages/#supported-streams","content":" Organization LookupFollower StatisticsShare StatisticsTotal Follower Count  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"LinkedIn Pages","url":"/reference/Connectors/capture-connectors/linkedin-pages/#prerequisites","content":" An existing LinkedIn Account  The API user account should be assigned the ADMIN role and the following permissions for the API endpoints. Endpoints such as: Organization Lookup API, Follower Statistics and Share Statistics require these permissions:  r_organization_social: Read the organization's posts, comments, reactions, etc.rw_organization_admin: Write and read the organization's pages and read reporting data.  ","version":"Next","tagName":"h2"},{"title":"Authentication​","type":1,"pageTitle":"LinkedIn Pages","url":"/reference/Connectors/capture-connectors/linkedin-pages/#authentication","content":" This connector's authentication can be configured by either passing a LinkedIn access token or using Oauth to connect to your source. Oauth requires the additional setup of verifying your application with LinkedIn in order to use the required scopes.  You can see more details about the Community Management App Review in LinkedIn's Docs.  info LinkedIn access tokens expire in 60 days. You must manually update your capture configuration to continue to capture data from LinkedIn. Refresh tokens expire after 365 days from their creation date. If you receive a 401 invalid token response error, it means that the access token has expired and you need to generate a new token. You can see more details about it in LinkedIn's Docs.  ","version":"Next","tagName":"h3"},{"title":"Access Token Authentication​","type":1,"pageTitle":"LinkedIn Pages","url":"/reference/Connectors/capture-connectors/linkedin-pages/#access-token-authentication","content":" Go to the LinkedIn Developers' OAuth Token Tools and click Create tokenYour app will need the r_organization_social and rw_organization_admin scopes:Click &quot;Request access token&quot; and save the token.  ","version":"Next","tagName":"h3"},{"title":"Oauth Authentication​","type":1,"pageTitle":"LinkedIn Pages","url":"/reference/Connectors/capture-connectors/linkedin-pages/#oauth-authentication","content":" Create a LinkedIn OAuth App​  Create a LinkedIn Page if you don't have one.Create a developer application in LinkedIn's Developer Portal.Ensure your application complies with the Restricted Uses of LinkedIn Marketing APIs and Data.Apply to the Community Management API under the Products tab of your LinkedIn app and complete the form.Save your client_id and client_secret from the Auth tab.  Create a Refresh Token​  Go to the LinkedIn Developers' OAuth Token Tools and click Create tokenYour app will need the r_organization_social and rw_organization_admin scopes:Click &quot;Request access token&quot; and save the token.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"LinkedIn Pages","url":"/reference/Connectors/capture-connectors/linkedin-pages/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the LinkedIn Pages source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"LinkedIn Pages","url":"/reference/Connectors/capture-connectors/linkedin-pages/#properties","content":" Property\tTitle\tDescription\tType\tRequired/Default/organization_id\tOrganization ID\tYour unique organization's id, found in the url of your bussiness' Organization Page\tstring\tRequired /client_id\tClient ID\tYour Oauth app's client id.\tstring\tRequired /client_secret\tClient Secret\tYour Oauth app's client secret.\tstring\tRequired /refresh_token\tRefresh Token\tThe token value generated using the LinkedIn Developers OAuth Token Tools.\tstring\tRequired /access_token\tAccess Token\tThe token value generated using the LinkedIn Developers OAuth Token Tools.\tstring\tRequired ","version":"Next","tagName":"h3"},{"title":"LinkedIn Ads","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/linkedin-ads/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"LinkedIn Ads","url":"/reference/Connectors/capture-connectors/linkedin-ads/#supported-data-resources","content":" The following data resources are supported:  AccountsAccount usersCampaign groupsCampaignsCreativesAdDirectSponsoredContents (Video ads)Ad analytics by campaignAd analytics by creative  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"LinkedIn Ads","url":"/reference/Connectors/capture-connectors/linkedin-ads/#prerequisites","content":" There are two ways to authenticate with LinkedIn when capturing data into Flow: using OAuth2, and manually, by creating a developer application. Their prerequisites differ.  OAuth is recommended for simplicity in the Flow web app; the developer application method is the only supported method using the command line.  ","version":"Next","tagName":"h2"},{"title":"Using OAuth2 to authenticate with LinkedIn in the Flow web app​","type":1,"pageTitle":"LinkedIn Ads","url":"/reference/Connectors/capture-connectors/linkedin-ads/#using-oauth2-to-authenticate-with-linkedin-in-the-flow-web-app","content":" One or more LinkedIn Ad Accounts with active campaigns. A LinkedIn user with access to the Ad Accounts from which you want to capture data.  ","version":"Next","tagName":"h3"},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"LinkedIn Ads","url":"/reference/Connectors/capture-connectors/linkedin-ads/#configuring-the-connector-specification-manually","content":" To configure without using OAuth, you'll need to create an application using the LinkedIn Marketing API, and generate its access token.  Setup​  Create a marketing application on LinkedIn Developers.Apply to the LinkedIn Developer Program.Generate your access token.  caution LinkedIn access tokens expire in 60 days. You must manually update your capture configuration to continue to capture data from LinkedIn.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"LinkedIn Ads","url":"/reference/Connectors/capture-connectors/linkedin-ads/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the capture specification. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the LinkedIn Ads source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"LinkedIn Ads","url":"/reference/Connectors/capture-connectors/linkedin-ads/#properties","content":" Endpoint​  The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so some of these properties aren't required.  Property\tTitle\tDescription\tType\tRequired/Default/account_ids\tAccount IDs (Optional)\tA space-separated list of the account IDs from which to capture data. Leave empty if you want to capture data from all linked accounts.\tarray\t[] /credentials\tAuthentication object /credentials/auth_method\tAuthentication method\tSet to access_token to authenticate manually.\tstring /credentials/access_token\tAccess token\tAccess token generated from your LinkedIn Developers app.\tstring /start_date\tStart date\tUTC date in the format 2020-09-17. Any data before this date will not be replicated.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tLinkedIn Ads stream from which a collection is captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"LinkedIn Ads","url":"/reference/Connectors/capture-connectors/linkedin-ads/#sample","content":" This sample specification reflects the manual authentication method.  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-linkedin-ads:dev config: account_ids: - 000000000 - 111111111 credentials: auth_method: access_token access_token: {secret} start_date: 2022-01-01 bindings: - resource: stream: campaigns syncMode: incremental target: ${PREFIX}/campaign {...}  ","version":"Next","tagName":"h3"},{"title":"Mailchimp","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/mailchimp/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Mailchimp","url":"/reference/Connectors/capture-connectors/mailchimp/#prerequisites","content":" There are two ways to authenticate with MailChimp when capturing data: using OAuth2, and manually, with an API key. Their prerequisites differ.  OAuth is recommended for simplicity in the Flow web app; the API key method is the only supported method using the command line.  ","version":"Next","tagName":"h2"},{"title":"Using OAuth2 to authenticate with Mailchimp in the Flow web app​","type":1,"pageTitle":"Mailchimp","url":"/reference/Connectors/capture-connectors/mailchimp/#using-oauth2-to-authenticate-with-mailchimp-in-the-flow-web-app","content":" A Mailchimp account  ","version":"Next","tagName":"h3"},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"Mailchimp","url":"/reference/Connectors/capture-connectors/mailchimp/#configuring-the-connector-specification-manually","content":" A Mailchimp account A Mailchimp API key  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Mailchimp","url":"/reference/Connectors/capture-connectors/mailchimp/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Mailchimp source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Mailchimp","url":"/reference/Connectors/capture-connectors/mailchimp/#properties","content":" Endpoint​  The following properties reflect the API Key authentication method.  Property\tTitle\tDescription\tType\tRequired/Default/credentials\tAuthentication\tAuthentication Type and Details\tobject\tRequired /credentials/auth_type\tAuthentication Type\tAuthentication type. Set to apikey.\tstring\tRequired /credentials/apikey\tAPI Key\tYour Mailchimp API key\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tResource\tMailchimp lists, campaigns, or email_activity\tstring\tRequired /syncMode\tSync Mode\tConnection method. Always set to incremental.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Mailchimp","url":"/reference/Connectors/capture-connectors/mailchimp/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-mailchimp:dev config: credentials: auth_type: apikey apikey: &lt;secret&gt; bindings: - resource: stream: lists syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: campaigns syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: email_activity syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}   Learn more about capture definitions. ","version":"Next","tagName":"h3"},{"title":"MariaDB","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/MariaDB/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/#prerequisites","content":" To use this connector, you'll need a MariaDB database setup with the following.  The binlog_formatsystem variable must be set to ROW.The binary log expiration period should be at least 7 days. This value may be set lower if necessary, but we discourage doing so as this may increase the likelihood of unrecoverable failures. A database user with appropriate permissions: REPLICATION CLIENT and REPLICATION SLAVE privileges.Permission to read the tables being captured.Permission to read from information_schema tables, if automatic discovery is used. If the table(s) to be captured include columns of type DATETIME, the time_zone system variable must be set to an IANA zone name or numerical offset or the capture configured with a timezone to use by default.  Configuration Tip To configure this connector to capture data from databases hosted on your internal network, you must set up SSH tunneling. For more specific instructions on setup, see configure connections with SSH tunneling.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/#setup","content":" ","version":"Next","tagName":"h2"},{"title":"Self Hosted MariaDB​","type":1,"pageTitle":"MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/#self-hosted-mariadb","content":" To meet these requirements, do the following:  Create the flow_capture user with replication permission, and the ability to read all tables.  The SELECT permission can be restricted to just the tables that need to be captured, but automatic discovery requires information_schema access as well.  CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture';   Configure the binary log to retain data for at least 7 days. We recommend 30 days where possible.  SET PERSIST binlog_expire_logs_seconds = 2592000;   Configure the database's time zone. See below for more information.  SET PERSIST time_zone = '-05:00'   ","version":"Next","tagName":"h3"},{"title":"Azure Database for MariaDB​","type":1,"pageTitle":"MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/#azure-database-for-mariadb","content":" You can use this connector for MariaDB instances on Azure Database for MariaDB using the following setup instructions.  Allow connections to the database from the Estuary Flow IP address. Create a new firewall rulethat grants access to the Estuary Flow IP addresses. info Alternatively, you can allow secure connections via SSH tunneling. To do so: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. Set the binlog_expire_logs_seconds server perameterto 2592000. Using your preferred MariaDB client, create the flow_capture user with replication permission, and the ability to read all tables. The SELECT permission can be restricted to just the tables that need to be captured, but automatic discovery requires information_schema access as well.  tip Your username must be specified in the format username@servername.  CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture';   Note the instance's host under Server name, and the port under Connection Strings (usually 3306). Together, you'll use the host:port as the address property when you configure the connector.  ","version":"Next","tagName":"h3"},{"title":"Capturing from Read Replicas​","type":1,"pageTitle":"MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/#capturing-from-read-replicas","content":" This connector supports capturing from a read replica of your database, provided that binary logging is enabled on the replica and all other requirements are met.  ","version":"Next","tagName":"h2"},{"title":"Setting the MariaDB time zone​","type":1,"pageTitle":"MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/#setting-the-mariadb-time-zone","content":" MariaDB's time_zone server system variable is set to SYSTEM by default. Flow is not able to detect your time zone when it's set this way, so you must explicitly set the variable for your database.  If you intend to capture tables including columns of the type DATETIME, and time_zone is set to SYSTEM, Flow won't be able to detect the time zone and convert the column to RFC3339 format. To avoid this, you must explicitly set the time zone for your database.  You can:  Specify a numerical offset from UTC. Specify a named timezone in IANA timezone format.  For example, if you're located in New Jersey, USA, you could set time_zone to -05:00 or -04:00, depending on the time of year. Because this region observes daylight savings time, you'd be responsible for changing the offset. Alternatively, you could set time_zone to America/New_York, and time changes would occur automatically.  If using IANA time zones, your database must include time zone tables. Learn more in the MariaDB docs.  Capture Timezone Configuration If you are unable to set the time_zone in the database and need to capture tables with DATETIME columns, the capture can be configured to assume a time zone using the timezone configuration property (see below). The timezone configuration property can be set as a numerical offset or IANA timezone format.  ","version":"Next","tagName":"h2"},{"title":"Backfills and performance considerations​","type":1,"pageTitle":"MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/#backfills-and-performance-considerations","content":" When the MariaDB capture is initiated, by default, the connector first backfills, or captures the targeted tables in their current state. It then transitions to capturing change events on an ongoing basis.  This is desirable in most cases, as it ensures that a complete view of your tables is captured into Flow. However, you may find it appropriate to skip the backfill, especially for extremely large tables.  In this case, you may turn off backfilling on a per-table basis. See properties for details.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the MariaDB source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tServer Address\tThe host or host:port at which the database can be reached.\tstring\tRequired /user\tLogin User\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tLogin Password\tPassword for the specified database user.\tstring\tRequired /timezone\tTimezone\tTimezone to use when capturing datetime columns. Should normally be left blank to use the database's 'time_zone' system variable. Only required if the 'time_zone' system variable cannot be read and columns with type datetime are being captured. Must be a valid IANA time zone name or +HH:MM offset. Takes precedence over the 'time_zone' system variable if both are set.\tstring /advanced/dbname\tDatabase Name\tThe name of database to connect to. In general this shouldn't matter. The connector can discover and capture from all databases it's authorized to access.\tstring\t&quot;mysql&quot; /advanced/node_id\tNode ID\tNode ID for the capture. Each node in a replication cluster must have a unique 32-bit ID. The specific value doesn't matter so long as it is unique. If unset or zero the connector will pick a value.\tinteger /advanced/skip_backfills\tSkip Backfills\tA comma-separated list of fully-qualified table names which should not be backfilled.\tstring /advanced/backfill_chunk_size\tBackfill Chunk Size\tThe number of rows which should be fetched from the database in a single backfill query.\tinteger\t131072 /advanced/skip_binlog_retention_check\tSkip Binlog Retention Sanity Check\tBypasses the 'dangerously short binlog retention' sanity check at startup. Only do this if you understand the danger and have a specific need.\tboolean\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe database in which the table resides.\tstring\tRequired /stream\tStream\tName of the table to be captured from the database.\tstring\tRequired  info When you configure this connector in the web application, the automatic discovery process sets up a binding for most tables it finds in your database, but there are exceptions. Tables in the MariaDB system databases information_schema, mysql, and performance_schema will not be discovered. You can add bindings for such tables manually.  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/#sample","content":" A minimal capture definition will look like the following:  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-mariadb:dev config: address: &quot;127.0.0.1:3306&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; bindings: - resource: namespace: ${TABLE_NAMESPACE} stream: ${TABLE_NAME} target: ${PREFIX}/${COLLECTION_NAME}   Your capture definition will likely be more complex, with additional bindings for each table in the source database.  Learn more about capture definitions.  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting Capture Errors​","type":1,"pageTitle":"MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/#troubleshooting-capture-errors","content":" The source-mariadb connector is designed to halt immediately if something wrong or unexpected happens, instead of continuing on and potentially outputting incorrect data. What follows is a non-exhaustive list of some potential failure modes, and what action should be taken to fix these situations:  ","version":"Next","tagName":"h2"},{"title":"Unsupported Operations​","type":1,"pageTitle":"MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/#unsupported-operations","content":" If your capture is failing with an &quot;unsupported operation {ALTER,DROP,TRUNCATE,etc} TABLE&quot; error, this indicates that such an operation has taken place impacting a table which is currently being captured.  In the case of DROP TABLE and other destructive operations this is not supported, and can only be resolved by removing the offending table(s) from the capture bindings list, after which you may recreate the capture if desired (causing the latest state of the table to be recaptured in its entirety).  In the case of ALTER TABLE we currently support table alterations to add or drop columns from a table. This error indicates that whatever alteration took place is not currently supported. Practically speaking the immediate resolution is the same as for a DROP or TRUNCATE TABLE, but if you frequently perform schema migrations it may be worth reaching out to see if we can add support for whatever table alteration you just did.  ","version":"Next","tagName":"h3"},{"title":"Data Manipulation Queries​","type":1,"pageTitle":"MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/#data-manipulation-queries","content":" If your capture is failing with an &quot;unsupported DML query&quot; error, this means that an INSERT, UPDATE, DELETE or other data manipulation query is present in the binlog. This should generally not happen if binlog_format = 'ROW' as described in the Prerequisites section.  Resolving this error requires fixing the binlog_format system variable, and then either tearing down and recreating the entire capture so that it restarts at a later point in the binlog, or in the case of an INSERT/DELETE query it may suffice to remove the capture binding for the offending table and then re-add it.  ","version":"Next","tagName":"h3"},{"title":"Unhandled Queries​","type":1,"pageTitle":"MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/#unhandled-queries","content":" If your capture is failing with an &quot;unhandled query&quot; error, some SQL query is present in the binlog which the connector does not (currently) understand.  In general, this error suggests that the connector should be modified to at least recognize this type of query, and most likely categorize it as either an unsupported DML Query, an unsupported Table Operation, or something that can safely be ignored. Until such a fix is made the capture cannot proceed, and you will need to backfill all collections to allow the capture to jump ahead to a later point in the binlog.  ","version":"Next","tagName":"h3"},{"title":"Metadata Errors​","type":1,"pageTitle":"MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/#metadata-errors","content":" If your capture is failing with a &quot;metadata error&quot; then something has gone badly wrong with the capture's tracking of table metadata, such as column names or datatypes.  This should never happen, and most likely means that the binlog itself is corrupt in some way. If this occurs, it can be resolved by backfilling all collections from the source.  ","version":"Next","tagName":"h3"},{"title":"Insufficient Binlog Retention​","type":1,"pageTitle":"MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/#insufficient-binlog-retention","content":" If your capture fails with a &quot;binlog retention period is too short&quot; error, it is informing you that the MariaDB binlog retention period is set to a dangerously low value.  The concern is that if a capture is disabled or the server becomes unreachable for longer than the binlog retention period, the database might delete a binlog segment which the capture isn't yet done with. If this happens then change events have been permanently lost, and the only way to get the capture running again is to skip ahead to a portion of the binlog which still exists. For correctness this requires backfilling the current contents of all tables from the source, and so we prefer to avoid it as much as possible. It's much easier to just set up your binlog retention with enough wiggle room to recover from temporary failures.  The &quot;binlog retention period is too short&quot; error should normally be fixed by setting a longer retention period as described in these setup instructions. However, advanced users who understand the risks can use the skip_binlog_retention_check configuration option to disable this safety. ","version":"Next","tagName":"h3"},{"title":"Marketo","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/marketo/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Marketo","url":"/reference/Connectors/capture-connectors/marketo/#supported-data-resources","content":" This connector can be used to sync the following tables from Marketo:  activities_X where X is an activity type. Contains information about lead activities of the type X. For example, activities_send_email contains information about lead activities related to the activity type send_email. See the Marketo docs for a detailed explanation of what each column means.activity_types. Contains metadata about activity types. See the Marketo docs for a detailed explanation of columns.campaigns. Contains info about your Marketo campaigns. Marketo docs.leads. Contains info about your Marketo leads. Marketo docs.lists. Contains info about your Marketo static lists. Marketo docs.programs. Contains info about your Marketo programs. Marketo docs.  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Marketo","url":"/reference/Connectors/capture-connectors/marketo/#prerequisites","content":" (Optional) allowlist the Estuary IP addresses if neededAn API-only Marketo User RoleAn Estuary Marketo API-only userA Marketo API Custom ServiceMarketo Client ID &amp; Client SecretMarketo Base URL  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Marketo","url":"/reference/Connectors/capture-connectors/marketo/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Marketo source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Marketo","url":"/reference/Connectors/capture-connectors/marketo/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/client_id\tClient ID\tThe Client ID of your Marketo developer application.\tstring\tRequired /client_secret\tClient Secret\tThe Client Secret of your Marketo developer application.\tstring\tRequired /start_date\tStart Date\tUTC date and time in the format 2021-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired /domain_url\tDomain URL\tYour Marketo Base URL.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Marketo project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Marketo","url":"/reference/Connectors/capture-connectors/marketo/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-marketo:dev config: client_id: &lt;secret&gt; client_secret: &lt;secret&gt; start_date: 2017-01-25T00:00:00Z domain_url: &lt;your domain URL&gt; bindings: - resource: stream: leads syncMode: full_refresh target: ${PREFIX}/leads {...}  ","version":"Next","tagName":"h3"},{"title":"Amazon RDS for MariaDB","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/MariaDB/amazon-rds-mariadb/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon RDS for MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/amazon-rds-mariadb/#prerequisites","content":" To use this connector, you'll need a MariaDB database setup with the following.  The binlog_formatsystem variable must be set to ROW.The binary log retentionperiod should be set to 168 hours (the maximum allowed by RDS). This value may be set lower if necessary, but we discourage doing so as this may increase the likelihood of unrecoverable failures. A database user with appropriate permissions: REPLICATION CLIENT and REPLICATION SLAVE privileges.Permission to read the tables being captured.Permission to read from information_schema tables, if automatic discovery is used. If the table(s) to be captured include columns of type DATETIME, the time_zone system variable must be set to an IANA zone name or numerical offset or the capture configured with a timezone to use by default.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Amazon RDS for MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/amazon-rds-mariadb/#setup","content":" Allow connections to the database from the Estuary Flow IP address. Modify the database, setting Public accessibility to Yes. Edit the VPC security group associated with your database, or create a new VPC security group and associate it with the database. Refer to the steps in the Amazon documentation. Create a new inbound rule and a new outbound rule that allow all traffic from the Estuary Flow IP addresses. info Alternatively, you can allow secure connections via SSH tunneling. To do so: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. Create a RDS parameter group to enable replication in MariaDB. Create a parameter group. Create a unique name and description and set the following properties: Family: mariadb10.6Type: DB Parameter group Modify the new parameter group and update the following parameters: binlog_format: ROW Associate the parameter groupwith the database and set Backup Retention Period to 7 days. Reboot the database to allow the changes to take effect. Switch to your MariaDB client. Run the following commands to create a new user for the capture with appropriate permissions:  CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture';   Run the following command to set the binary log retention to 7 days, the maximum value which RDS MariaDB permits:  CALL mysql.rds_set_configuration('binlog retention hours', 168);   In the RDS console, note the instance's Endpoint and Port. You'll need these for the address property when you configure the connector.  ","version":"Next","tagName":"h3"},{"title":"Capturing from Read Replicas​","type":1,"pageTitle":"Amazon RDS for MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/amazon-rds-mariadb/#capturing-from-read-replicas","content":" This connector supports capturing from a read replica of your database, provided that binary logging is enabled on the replica and all other requirements are met. To create a read replica:  Follow RDS instructions to create a read replicaof your MariaDB database. Modify the replicaand set the following: DB parameter group: the parameter group you created previouslyBackup retention period: 7 daysPublic access: Publicly accessible Reboot the replica to allow the changes to take effect.  ","version":"Next","tagName":"h2"},{"title":"Backfills and performance considerations​","type":1,"pageTitle":"Amazon RDS for MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/amazon-rds-mariadb/#backfills-and-performance-considerations","content":" When the MariaDB capture is initiated, by default, the connector first backfills, or captures the targeted tables in their current state. It then transitions to capturing change events on an ongoing basis.  This is desirable in most cases, as it ensures that a complete view of your tables is captured into Flow. However, you may find it appropriate to skip the backfill, especially for extremely large tables.  In this case, you may turn off backfilling on a per-table basis. See properties for details.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Amazon RDS for MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/amazon-rds-mariadb/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the MariaDB source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Amazon RDS for MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/amazon-rds-mariadb/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tServer Address\tThe host or host:port at which the database can be reached.\tstring\tRequired /user\tLogin User\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tLogin Password\tPassword for the specified database user.\tstring\tRequired /timezone\tTimezone\tTimezone to use when capturing datetime columns. Should normally be left blank to use the database's 'time_zone' system variable. Only required if the 'time_zone' system variable cannot be read and columns with type datetime are being captured. Must be a valid IANA time zone name or +HH:MM offset. Takes precedence over the 'time_zone' system variable if both are set.\tstring /advanced/dbname\tDatabase Name\tThe name of database to connect to. In general this shouldn't matter. The connector can discover and capture from all databases it's authorized to access.\tstring\t&quot;mysql&quot; /advanced/node_id\tNode ID\tNode ID for the capture. Each node in a replication cluster must have a unique 32-bit ID. The specific value doesn't matter so long as it is unique. If unset or zero the connector will pick a value.\tinteger /advanced/skip_backfills\tSkip Backfills\tA comma-separated list of fully-qualified table names which should not be backfilled.\tstring /advanced/backfill_chunk_size\tBackfill Chunk Size\tThe number of rows which should be fetched from the database in a single backfill query.\tinteger\t131072 /advanced/skip_binlog_retention_check\tSkip Binlog Retention Sanity Check\tBypasses the 'dangerously short binlog retention' sanity check at startup. Only do this if you understand the danger and have a specific need.\tboolean\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe database in which the table resides.\tstring\tRequired /stream\tStream\tName of the table to be captured from the database.\tstring\tRequired  info When you configure this connector in the web application, the automatic discovery process sets up a binding for most tables it finds in your database, but there are exceptions. Tables in the MariaDB system databases information_schema, mysql, and performance_schema will not be discovered. You can add bindings for such tables manually.  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Amazon RDS for MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/amazon-rds-mariadb/#sample","content":" A minimal capture definition will look like the following:  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-mariadb:dev config: address: &quot;127.0.0.1:3306&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; bindings: - resource: namespace: ${TABLE_NAMESPACE} stream: ${TABLE_NAME} target: ${PREFIX}/${COLLECTION_NAME}   Your capture definition will likely be more complex, with additional bindings for each table in the source database.  Learn more about capture definitions.  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting Capture Errors​","type":1,"pageTitle":"Amazon RDS for MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/amazon-rds-mariadb/#troubleshooting-capture-errors","content":" The source-amazon-rds-mariadb connector is designed to halt immediately if something wrong or unexpected happens, instead of continuing on and potentially outputting incorrect data. What follows is a non-exhaustive list of some potential failure modes, and what action should be taken to fix these situations:  ","version":"Next","tagName":"h2"},{"title":"Unsupported Operations​","type":1,"pageTitle":"Amazon RDS for MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/amazon-rds-mariadb/#unsupported-operations","content":" If your capture is failing with an &quot;unsupported operation {ALTER,DROP,TRUNCATE,etc} TABLE&quot; error, this indicates that such an operation has taken place impacting a table which is currently being captured.  In the case of DROP TABLE and other destructive operations this is not supported, and can only be resolved by removing the offending table(s) from the capture bindings list, after which you may recreate the capture if desired (causing the latest state of the table to be recaptured in its entirety).  In the case of ALTER TABLE we currently support table alterations to add or drop columns from a table. This error indicates that whatever alteration took place is not currently supported. Practically speaking the immediate resolution is the same as for a DROP or TRUNCATE TABLE, but if you frequently perform schema migrations it may be worth reaching out to see if we can add support for whatever table alteration you just did.  ","version":"Next","tagName":"h3"},{"title":"Data Manipulation Queries​","type":1,"pageTitle":"Amazon RDS for MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/amazon-rds-mariadb/#data-manipulation-queries","content":" If your capture is failing with an &quot;unsupported DML query&quot; error, this means that an INSERT, UPDATE, DELETE or other data manipulation query is present in the binlog. This should generally not happen if binlog_format = 'ROW' as described in the Prerequisites section.  Resolving this error requires fixing the binlog_format system variable, and then either tearing down and recreating the entire capture so that it restarts at a later point in the binlog, or in the case of an INSERT/DELETE query it may suffice to remove the capture binding for the offending table and then re-add it.  ","version":"Next","tagName":"h3"},{"title":"Unhandled Queries​","type":1,"pageTitle":"Amazon RDS for MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/amazon-rds-mariadb/#unhandled-queries","content":" If your capture is failing with an &quot;unhandled query&quot; error, some SQL query is present in the binlog which the connector does not (currently) understand.  In general, this error suggests that the connector should be modified to at least recognize this type of query, and most likely categorize it as either an unsupported DML Query, an unsupported Table Operation, or something that can safely be ignored. Until such a fix is made the capture cannot proceed, and you will need to backfill all collections to allow the capture to jump ahead to a later point in the binlog.  ","version":"Next","tagName":"h3"},{"title":"Metadata Errors​","type":1,"pageTitle":"Amazon RDS for MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/amazon-rds-mariadb/#metadata-errors","content":" If your capture is failing with a &quot;metadata error&quot; then something has gone badly wrong with the capture's tracking of table metadata, such as column names or datatypes.  This should never happen, and most likely means that the binlog itself is corrupt in some way. If this occurs, it can be resolved by backfilling all collections from the source.  ","version":"Next","tagName":"h3"},{"title":"Insufficient Binlog Retention​","type":1,"pageTitle":"Amazon RDS for MariaDB","url":"/reference/Connectors/capture-connectors/MariaDB/amazon-rds-mariadb/#insufficient-binlog-retention","content":" If your capture fails with a &quot;binlog retention period is too short&quot; error, it is informing you that the MariaDB binlog retention period is set to a dangerously low value.  The concern is that if a capture is disabled or the server becomes unreachable for longer than the binlog retention period, the database might delete a binlog segment which the capture isn't yet done with. If this happens then change events have been permanently lost, and the only way to get the capture running again is to skip ahead to a portion of the binlog which still exists. For correctness this requires backfilling the current contents of all tables from the source, and so we prefer to avoid it as much as possible. It's much easier to just set up your binlog retention with enough wiggle room to recover from temporary failures.  The &quot;binlog retention period is too short&quot; error should normally be fixed by setting a longer retention period as described in these setup instructions. However, advanced users who understand the risks can use the skip_binlog_retention_check configuration option to disable this safety. ","version":"Next","tagName":"h3"},{"title":"MixPanel","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/mixpanel/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"MixPanel","url":"/reference/Connectors/capture-connectors/mixpanel/#supported-data-resources","content":" The following data resources are supported through the MixPanel APIs:  ExportEngageFunnelsRevenueAnnotationsCohortsCohort Members  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"MixPanel","url":"/reference/Connectors/capture-connectors/mixpanel/#prerequisites","content":" To set up the Mixpanel source connector, you'll need a Mixpanel Service Account and it's Project ID, the Project Timezone, and the Project region (US or EU).  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"MixPanel","url":"/reference/Connectors/capture-connectors/mixpanel/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the MixPanel source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"MixPanel","url":"/reference/Connectors/capture-connectors/mixpanel/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/project_id\tProject ID\tYour project ID number. See the docs for more information on how to obtain this.\tinteger\tRequired /attribution_window\tAttribution Window\tA period of time for attributing results to ads and the lookback period after those actions occur during which ad results are counted. Default attribution window is 5 days.\tinteger\tDefault /project_timezone\tProject Timezone\tTime zone in which integer date times are stored. The project timezone may be found in the project settings in the Mixpanel console\tstring\tDefault /start_date\tStart Date\tThe date in the format YYYY-MM-DD. Any data before this date will not be replicated. If this option is not set, the connector will replicate data from up to one year ago by default.\tstring\tRequired /end_date\tEnd Date\tThe date in the format YYYY-MM-DD. Any data after this date will not be replicated. Left empty to always sync to most recent date.\tstring\tDefault /region\tRegion\tThe region of mixpanel domain instance either US or EU.\tstring\tDefault /date_window_size\tDate slicing window\tDefines window size in days, that used to slice through data. You can reduce it, if amount of data in each window is too big for your environment.\tinteger\tDefault /advanced/page_size\tPage size\tPage size used for incremental bindings.\tint\t50,000 /advanced/minimal_cohort_members_properties\tMinimal Cohort Members properties\tWhen true, only the minimal properties needed to identify which cohort a member belongs to are captured, and all other properties already captured by the Engage binding are not capture by Cohort Members.\tboolean\ttrue  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your MixPanel project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"MixPanel","url":"/reference/Connectors/capture-connectors/mixpanel/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-mixpanel:dev config: credentials: auth_type: ServiceAccount secret: {secret} username: {your_username} project_id: 1234567 attribution_window: 5 project_timezone: US/Pacific start_date: 2017-01-25T00:00:00Z end_date: 2019-01-25T00:00:00Z region: US date_window_size: 30 advanced: page_size: 10000 minimal_cohort_members_properties: true bindings: - resource: stream: annotations syncMode: full_refresh target: ${PREFIX}/annotations {...}  ","version":"Next","tagName":"h3"},{"title":"Amazon DocumentDB","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/MongoDB/amazon-documentdb/","content":"","keywords":"","version":"Next"},{"title":"Data model​","type":1,"pageTitle":"Amazon DocumentDB","url":"/reference/Connectors/capture-connectors/MongoDB/amazon-documentdb/#data-model","content":" Amazon DocumentDB is a NoSQL database. It is compatible with MongoDB's data model, which consists ofdocuments (lightweight records that contain mappings of fields and values) organized incollections. MongoDB documents have a mandatory _id field that is used as the key of the collection.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon DocumentDB","url":"/reference/Connectors/capture-connectors/MongoDB/amazon-documentdb/#prerequisites","content":" You'll need:  Credentials for connecting to your Amazon DocumentDB instance and database. Read access to your DocumentDB database(s). See Database access using Role-Based Access Control for more information.  ","version":"Next","tagName":"h2"},{"title":"Capture Modes​","type":1,"pageTitle":"Amazon DocumentDB","url":"/reference/Connectors/capture-connectors/MongoDB/amazon-documentdb/#capture-modes","content":" A &quot;batch&quot; mode of capturing documents can be used. The capture mode is configured on a per-collection level in theBindings configuration and can be one of the following:  Batch Snapshot: Performs a &quot;full refresh&quot; by scanning the entire DocumentDB collection on a set schedule. A cursor field must be configured, which should usually be the _id field.Batch Incremental: Performs a scan on a set schedule where only documents having a higher cursor field value than previously observed are captured. This mode should be used for append-only collections, or where a field value is known to be strictly increasing for all document insertions and updates.  Using Cursor Fields For best performance the selected cursor field should have anindex. This ensures backfill queries are able to be run efficiently, since they require sorting the collection based on the cursor field.  Time Series Collections Time series collections do not have a default index on the _id, but do have an index on the timeField for the collection. This makes the timeField a good choice for an incremental cursor if new documents are only ever added to the collection with strictly increasing values for the timeField. The capture connector will automatically discover time series collections in Batch Incremental mode with the cursor set to the collection's timeField.  Batch Snapshot will capture updates by virtue of it re-capturing the entire source collection periodically. Batch Incremental may capture updates to documents if updated documents have strictly increasing values for the cursor field.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Amazon DocumentDB","url":"/reference/Connectors/capture-connectors/MongoDB/amazon-documentdb/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the Flow specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Amazon DocumentDB source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Amazon DocumentDB","url":"/reference/Connectors/capture-connectors/MongoDB/amazon-documentdb/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tHost and port of the database. Optionally can specify scheme for the URL such as mongodb+srv://host.\tstring\tRequired /user\tUser\tDatabase user to connect as.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /database\tDatabase\tOptional comma-separated list of the databases to discover. If not provided will discover all available databases in the instance.\tstring /batchAndChangeStream\tCapture Batch Collections in Addition to Change Stream Collections\tDiscover collections that can only be batch captured if the deployment supports change streams. Check this box to capture views and time series collections as well as change streams. All collections will be captured in batch mode if the server does not support change streams regardless of this setting.\tboolean /pollSchedule\tDefault Batch Collection Polling Schedule\tWhen and how often to poll batch collections. Accepts a Go duration string like '5m' or '6h' for frequency-based polling or a string like 'daily at 12:34Z' to poll at a specific time (specified in UTC) every day. Defaults to '24h' if unset\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/database\tDatabase\tDatabase name\tstring\tRequired /collection\tStream\tCollection name\tstring\tRequired /captureMode\tCapture Mode\tEither Batch Snapshot, or Batch Incremental\tstring /cursorField\tCursor Field\tThe name of the field to use as a cursor for batch-mode bindings. For best performance this field should be indexed. When used with 'Batch Incremental' mode documents added to the collection are expected to always have the cursor field and for it to be strictly increasing.\tstring /pollSchedule\tPolling Schedule\tWhen and how often to poll batch collections (overrides the connector default setting). Accepts a Go duration string like '5m' or '6h' for frequency-based polling or a string like 'daily at 12:34Z' to poll at a specific time (specified in UTC) every day. Defaults to '24h' if unset.\tstring\t  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Amazon DocumentDB","url":"/reference/Connectors/capture-connectors/MongoDB/amazon-documentdb/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-amazon-documentdb:dev config: address: &quot;mongo:27017&quot; password: &quot;flow&quot; user: &quot;flow&quot; bindings: - resource: collection: users database: test target: ${PREFIX}/users   ","version":"Next","tagName":"h3"},{"title":"SSH Tunneling​","type":1,"pageTitle":"Amazon DocumentDB","url":"/reference/Connectors/capture-connectors/MongoDB/amazon-documentdb/#ssh-tunneling","content":" As an alternative to connecting to your DocumentDB instance directly, you can allow secure connections via SSH tunneling. To do so:  Refer to the guide to configure an SSH server on the cloud platform of your choice. Configure your connector as described in the configuration section above, with the addition of the networkTunnel stanza to enable the SSH tunnel, if using. See Connecting to endpoints on secure networks for additional details and a sample. ","version":"Next","tagName":"h2"},{"title":"Azure CosmosDB","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/MongoDB/azure-cosmosdb/","content":"","keywords":"","version":"Next"},{"title":"Data model​","type":1,"pageTitle":"Azure CosmosDB","url":"/reference/Connectors/capture-connectors/MongoDB/azure-cosmosdb/#data-model","content":" Azure Cosmos DB is a NoSQL database. It is compatible with MongoDB's data model, which consists ofdocuments (lightweight records that contain mappings of fields and values) organized incollections. MongoDB documents have a mandatory _id field that is used as the key of the collection.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Azure CosmosDB","url":"/reference/Connectors/capture-connectors/MongoDB/azure-cosmosdb/#prerequisites","content":" You'll need:  Credentials for connecting to your Cosmos DB instance and database. Read access to your Cosmos DB database(s). See Use control plane role-based access control with Azure Cosmos DB for NoSQL for more information. If your Cosmos DB instance has an IP firewall configured, you need toallowlist the Estuary IP addresses.  ","version":"Next","tagName":"h2"},{"title":"Capture Modes​","type":1,"pageTitle":"Azure CosmosDB","url":"/reference/Connectors/capture-connectors/MongoDB/azure-cosmosdb/#capture-modes","content":" Azure Cosmos DB change feeds are the preferred way to capture on-going changes to collections. Change streams allow capturing real-time events representing new documents in your collections, updates to existing documents, and deletions of documents. If change streams are enabled on the Cosmos DB instance/deployment you are connecting to, they will be used preferentially for capturing changes.  An alternate &quot;batch&quot; mode of capturing documents can be used for deployments that do not support change streams, and for Cosmos DB collection types that do not support change streams. The capture mode is configured on a per-collection level in theBindings configuration and can be one of the following:  Change Stream Incremental: This is the preferred mode and uses change streams to capture change events.Batch Snapshot: Performs a &quot;full refresh&quot; by scanning the entire Cosmos DB collection on a set schedule. A cursor field must be configured, which should usually be the _id field.Batch Incremental: Performs a scan on a set schedule where only documents having a higher cursor field value than previously observed are captured. This mode should be used for append-only collections, or where a field value is known to be strictly increasing for all document insertions and updates.  Using Cursor Fields For best performance the selected cursor field should have anindex. This ensures backfill queries are able to be run efficiently, since they require sorting the collection based on the cursor field.  Time Series Collections Time series collections do not have a default index on the _id, but do have an index on the timeField for the collection. This makes the timeField a good choice for an incremental cursor if new documents are only ever added to the collection with strictly increasing values for the timeField. The capture connector will automatically discover time series collections in Batch Incremental mode with the cursor set to the collection's timeField.  Only the Change Stream Incremental mode is capable of capturing deletion events. Batch Snapshot will capture updates by virtue of it re-capturing the entire source collection periodically. Batch Incremental may capture updates to documents if updated documents have strictly increasing values for the cursor field.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Azure CosmosDB","url":"/reference/Connectors/capture-connectors/MongoDB/azure-cosmosdb/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the Flow specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Azure Cosmos DB source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Azure CosmosDB","url":"/reference/Connectors/capture-connectors/MongoDB/azure-cosmosdb/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tHost and port of the database. Optionally can specify scheme for the URL such as mongodb+srv://host.\tstring\tRequired /user\tUser\tDatabase user to connect as.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /database\tDatabase\tOptional comma-separated list of the databases to discover. If not provided will discover all available databases in the instance.\tstring /batchAndChangeStream\tCapture Batch Collections in Addition to Change Stream Collections\tDiscover collections that can only be batch captured if the deployment supports change streams. Check this box to capture views and time series collections as well as change streams. All collections will be captured in batch mode if the server does not support change streams regardless of this setting.\tboolean /pollSchedule\tDefault Batch Collection Polling Schedule\tWhen and how often to poll batch collections. Accepts a Go duration string like '5m' or '6h' for frequency-based polling or a string like 'daily at 12:34Z' to poll at a specific time (specified in UTC) every day. Defaults to '24h' if unset\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/database\tDatabase\tDatabase name\tstring\tRequired /collection\tStream\tCollection name\tstring\tRequired /captureMode\tCapture Mode\tEither Change Stream Incremental, Batch Snapshot, or Batch Incremental\tstring /cursorField\tCursor Field\tThe name of the field to use as a cursor for batch-mode bindings. For best performance this field should be indexed. When used with 'Batch Incremental' mode documents added to the collection are expected to always have the cursor field and for it to be strictly increasing.\tstring /pollSchedule\tPolling Schedule\tWhen and how often to poll batch collections (overrides the connector default setting). Accepts a Go duration string like '5m' or '6h' for frequency-based polling or a string like 'daily at 12:34Z' to poll at a specific time (specified in UTC) every day. Defaults to '24h' if unset.\tstring\t  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Azure CosmosDB","url":"/reference/Connectors/capture-connectors/MongoDB/azure-cosmosdb/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-cosmosdb-mongodb:dev config: address: &quot;mongo:27017&quot; password: &quot;flow&quot; user: &quot;flow&quot; bindings: - resource: collection: users database: test target: ${PREFIX}/users  ","version":"Next","tagName":"h3"},{"title":"MongoDB","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/MongoDB/","content":"","keywords":"","version":"Next"},{"title":"Supported platforms​","type":1,"pageTitle":"MongoDB","url":"/reference/Connectors/capture-connectors/MongoDB/#supported-platforms","content":" The MongoDB connector has a couple variants to support additional document-based database options. Continue reading this page for standard MongoDB setup or see one of the following:  Amazon DocumentDBAzure Cosmos DB  ","version":"Next","tagName":"h2"},{"title":"Data model​","type":1,"pageTitle":"MongoDB","url":"/reference/Connectors/capture-connectors/MongoDB/#data-model","content":" MongoDB is a NoSQL database. Its data model consists ofdocuments (lightweight records that contain mappings of fields and values) organized incollections. MongoDB documents have a mandatory _id field that is used as the key of the collection.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"MongoDB","url":"/reference/Connectors/capture-connectors/MongoDB/#prerequisites","content":" You'll need:  Credentials for connecting to your MongoDB instance and database. Read access to your MongoDB database(s), see Role-Based Access Control for more information.  Configuration Tip If you are using a user with access to all databases, then in your mongodb address, you must specify?authSource=admin parameter so that authentication is done through your admin database.  If you are using MongoDB Atlas, or your MongoDB provider requires allowlisting of IPs, you need toallowlist the Estuary IP addresses.  ","version":"Next","tagName":"h2"},{"title":"Capture Modes​","type":1,"pageTitle":"MongoDB","url":"/reference/Connectors/capture-connectors/MongoDB/#capture-modes","content":" MongoDB change streams are the preferred way to capture on-going changes to collections. Change streams allow capturing real-time events representing new documents in your collections, updates to existing documents, and deletions of documents. If change streams are enabled on the MongoDB instance/deployment you are connecting to, they will be used preferentially for capturing changes.  An alternate &quot;batch&quot; mode of capturing documents can be used for deployments that do not support change streams, and for MongoDB collection types that do not support change streams (viewsand time seriescollections). The capture mode is configured on a per-collection level in theBindings configuration and can be one of the following:  Change Stream Incremental: This is the preferred mode and uses change streams to capture change events.Batch Snapshot: Performs a &quot;full refresh&quot; by scanning the entire MongoDB collection on a set schedule. A cursor field must be configured, which should usually be the _id field.Batch Incremental: Performs a scan on a set schedule where only documents having a higher cursor field value than previously observed are captured. This mode should be used for append-only collections, or where a field value is known to be strictly increasing for all document insertions and updates.  Using Cursor Fields For best performance the selected cursor field should have anindex. This ensures backfill queries are able to be run efficiently, since they require sorting the collection based on the cursor field.  Time Series Collections Time series collections do not have a default index on the _id, but do have an index on the timeField for the collection. This makes the timeField a good choice for an incremental cursor if new documents are only ever added to the collection with strictly increasing values for the timeField. The capture connector will automatically discover time series collections in Batch Incremental mode with the cursor set to the collection's timeField.  Only the Change Stream Incremental mode is capable of capturing deletion events. Batch Snapshot will capture updates by virtue of it re-capturing the entire source collection periodically. Batch Incremental may capture updates to documents if updated documents have strictly increasing values for the cursor field.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"MongoDB","url":"/reference/Connectors/capture-connectors/MongoDB/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the Flow specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the MongoDB source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"MongoDB","url":"/reference/Connectors/capture-connectors/MongoDB/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tHost and port of the database. Optionally can specify scheme for the URL such as mongodb+srv://host.\tstring\tRequired /user\tUser\tDatabase user to connect as.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /database\tDatabase\tOptional comma-separated list of the databases to discover. If not provided will discover all available databases in the instance.\tstring /batchAndChangeStream\tCapture Batch Collections in Addition to Change Stream Collections\tDiscover collections that can only be batch captured if the deployment supports change streams. Check this box to capture views and time series collections as well as change streams. All collections will be captured in batch mode if the server does not support change streams regardless of this setting.\tboolean /pollSchedule\tDefault Batch Collection Polling Schedule\tWhen and how often to poll batch collections. Accepts a Go duration string like '5m' or '6h' for frequency-based polling or a string like 'daily at 12:34Z' to poll at a specific time (specified in UTC) every day. Defaults to '24h' if unset\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/database\tDatabase\tDatabase name\tstring\tRequired /collection\tStream\tCollection name\tstring\tRequired /captureMode\tCapture Mode\tEither Change Stream Incremental, Batch Snapshot, or Batch Incremental\tstring /cursorField\tCursor Field\tThe name of the field to use as a cursor for batch-mode bindings. For best performance this field should be indexed. When used with 'Batch Incremental' mode documents added to the collection are expected to always have the cursor field and for it to be strictly increasing.\tstring /pollSchedule\tPolling Schedule\tWhen and how often to poll batch collections (overrides the connector default setting). Accepts a Go duration string like '5m' or '6h' for frequency-based polling or a string like 'daily at 12:34Z' to poll at a specific time (specified in UTC) every day. Defaults to '24h' if unset.\tstring\t  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"MongoDB","url":"/reference/Connectors/capture-connectors/MongoDB/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-mongodb:dev config: address: &quot;mongo:27017&quot; password: &quot;flow&quot; user: &quot;flow&quot; bindings: - resource: collection: users database: test target: ${PREFIX}/users   ","version":"Next","tagName":"h3"},{"title":"SSH Tunneling​","type":1,"pageTitle":"MongoDB","url":"/reference/Connectors/capture-connectors/MongoDB/#ssh-tunneling","content":" As an alternative to connecting to your MongoDB instance directly, you can allow secure connections via SSH tunneling. To do so:  Refer to the guide to configure an SSH server on the cloud platform of your choice. Configure your connector as described in the configuration section above, with the addition of the networkTunnel stanza to enable the SSH tunnel, if using. See Connecting to endpoints on secure networks for additional details and a sample.  ","version":"Next","tagName":"h2"},{"title":"Backfill and real-time updates​","type":1,"pageTitle":"MongoDB","url":"/reference/Connectors/capture-connectors/MongoDB/#backfill-and-real-time-updates","content":" When performing the initial database snapshot, the connector continuously reads from change streams to capture change events while executing collection scans to backfill pre-existing documents. After the initial snapshot, the connector continues to read from the change streams indefinitely to capture all changes going forward.  If the connector's process is paused for a while, it will attempt to resume capturing change events from where it left off, however the connector's ability to do this depends on the size of thereplica set oplog, and in certain circumstances, when the pause has been long enough for the oplog to have evicted old change events, the connector will need to re-do the backfill to ensure data consistency. In these cases it is necessary to resize your oplogor set a minimum retention periodfor your oplog to be able to reliably capture data. The recommended minimum retention period is at least 24 hours, but we recommend higher values to improve reliability.  ","version":"Next","tagName":"h2"},{"title":"Change Event Pre- and Post-Images​","type":1,"pageTitle":"MongoDB","url":"/reference/Connectors/capture-connectors/MongoDB/#change-event-pre--and-post-images","content":" Captured documents for change events from update operations will always include a full post-image, since the change stream is configured with the { fullDocument: 'updateLookup' }setting.  Pre-images for update, replace, and delete operations will be captured if they are available. For these pre-images to be captured, the source MongoDB collection must have changeStreamPreAndPostImages enabled. See the official MongoDB documentationfor more information on how to enable this setting. ","version":"Next","tagName":"h2"},{"title":"MySQL","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/MySQL/","content":"","keywords":"","version":"Next"},{"title":"Supported platforms​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/capture-connectors/MySQL/#supported-platforms","content":" This connector supports MySQL on major cloud providers, as well as self-hosted instances.  Setup instructions are provided for the following platforms:  Self-hosted MySQLAmazon RDSAmazon AuroraGoogle Cloud SQLAzure Database for MySQL  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/capture-connectors/MySQL/#prerequisites","content":" To use this connector, you'll need a MySQL database setup with the following.  The binlog_formatsystem variable must be set to ROW (the default value).The binary log expiration period should be at least 7 days. This value may be set lower if necessary, but we discourage doing so as this may increase the likelihood of unrecoverable failures. A database user with appropriate permissions: REPLICATION CLIENT and REPLICATION SLAVE privileges.Permission to read the tables being captured.Permission to read from information_schema tables, if automatic discovery is used. If the table(s) to be captured include columns of type DATETIME, the time_zone system variable must be set to an IANA zone name or numerical offset or the capture configured with a timezone to use by default.  Configuration Tip To configure this connector to capture data from databases hosted on your internal network, you must set up SSH tunneling. For more specific instructions on setup, see configure connections with SSH tunneling.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/capture-connectors/MySQL/#setup","content":" To meet these requirements, follow the steps for your hosting type.  Self-hosted MySQLAmazon RDSAmazon AuroraGoogle Cloud SQLAzure Database for MySQL  ","version":"Next","tagName":"h2"},{"title":"Self-hosted MySQL​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/capture-connectors/MySQL/#self-hosted-mysql","content":" Create the flow_capture user with replication permission, and the ability to read all tables.  The SELECT permission can be restricted to just the tables that need to be captured, but automatic discovery requires information_schema access as well.  CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' COMMENT 'User account for Flow MySQL data capture'; GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture';   Configure the binary log to retain data for at least 7 days. We recommend 30 days where possible.  SET PERSIST binlog_expire_logs_seconds = 2592000;   Configure the database's time zone. See below for more information.  SET PERSIST time_zone = '-05:00'   ","version":"Next","tagName":"h3"},{"title":"Amazon Aurora​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/capture-connectors/MySQL/#amazon-aurora","content":" You must apply some of the settings to the entire Aurora DB cluster, and others to a database instance within the cluster. For each step, take note of which entity you're working with.  Allow connections between the database and Estuary Flow. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel. To allow direct access: Modify the instance, choosing Publicly accessible in the Connectivity settings.Edit the VPC security group associated with your instance, or create a new VPC security group and associate it with the instance as described in the Amazon documentation. Create a new inbound rule and a new outbound rule that allow all traffic from the Estuary Flow IP addresses. To allow secure connections via SSH tunneling: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networks for additional details and a sample. Create a RDS parameter group to enable replication on your Aurora DB cluster. Create a parameter group. Create a unique name and description and set the following properties: Family: aurora-mysql8.0Type: DB ClusterParameter group Modify the new parameter group and update the following parameters: binlog_format: ROWbinlog_row_metadata: FULLread_only: 0 Associate the parameter groupwith the DB cluster. While you're modifying the cluster, also set Backup Retention Period to 7 days. Reboot the cluster to allow the changes to take effect. Switch to your MySQL client. Run the following commands to create a new user for the capture with appropriate permissions:  CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' COMMENT 'User account for Flow MySQL data capture'; GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture';   Run the following command to set the binary log retention to 7 days, the maximum value Aurora permits:  CALL mysql.rds_set_configuration('binlog retention hours', 168);   In the RDS console, note the instance's Endpoint and Port. You'll need these for the address property when you configure the connector.  ","version":"Next","tagName":"h3"},{"title":"Azure Database for MySQL​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/capture-connectors/MySQL/#azure-database-for-mysql","content":" Allow connections between the database and Estuary Flow. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel. To allow direct access: Create a new firewall rule that grants access to the Estuary Flow IP addresses. To allow secure connections via SSH tunneling: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networks for additional details and a sample. Set the binlog_expire_logs_seconds server perameterto 2592000. Using MySQL workbench or your preferred client, create the flow_capture user with replication permission, and the ability to read all tables.  The SELECT permission can be restricted to just the tables that need to be captured, but automatic discovery requires information_schema access as well.  tip Your username must be specified in the format username@servername.  CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' COMMENT 'User account for Flow MySQL data capture'; GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture';   Note the instance's host under Server name, and the port under Connection Strings (usually 3306). Together, you'll use the host:port as the address property when you configure the connector.  ","version":"Next","tagName":"h3"},{"title":"Capturing from Read Replicas​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/capture-connectors/MySQL/#capturing-from-read-replicas","content":" This connector supports capturing from a read replica of your database, provided that binary logging is enabled on the replica and all other requirements are met.  ","version":"Next","tagName":"h2"},{"title":"Setting the MySQL time zone​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/capture-connectors/MySQL/#setting-the-mysql-time-zone","content":" MySQL's time_zone server system variable is set to SYSTEM by default.  If you intend to capture tables including columns of the type DATETIME, and time_zone is set to SYSTEM, Flow won't be able to detect the time zone and convert the column to RFC3339 format. To avoid this, you must explicitly set the time zone for your database.  You can:  Specify a numerical offset from UTC. For MySQL version 8.0.19 or higher, values from -13:59 to +14:00, inclusive, are permitted.Prior to MySQL 8.0.19, values from -12:59 to +13:00, inclusive, are permitted Specify a named timezone in IANA timezone format. If you're using Amazon Aurora, create or modify the DB cluster parameter groupassociated with your MySQL database.Set the time_zone parameter to the correct value.  For example, if you're located in New Jersey, USA, you could set time_zone to -05:00 or -04:00, depending on the time of year. Because this region observes daylight savings time, you'd be responsible for changing the offset. Alternatively, you could set time_zone to America/New_York, and time changes would occur automatically.  If using IANA time zones, your database must include time zone tables. Learn more in the MySQL docs.  Capture Timezone Configuration If you are unable to set the time_zone in the database and need to capture tables with DATETIME columns, the capture can be configured to assume a time zone using the timezone configuration property (see below). The timezone configuration property can be set as a numerical offset or IANA timezone format.  ","version":"Next","tagName":"h2"},{"title":"Backfills and performance considerations​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/capture-connectors/MySQL/#backfills-and-performance-considerations","content":" When the MySQL capture is initiated, by default, the connector first backfills, or captures the targeted tables in their current state. It then transitions to capturing change events on an ongoing basis.  This is desirable in most cases, as it ensures that a complete view of your tables is captured into Flow. However, you may find it appropriate to skip the backfill, especially for extremely large tables.  In this case, you may turn off backfilling on a per-table basis. See properties for details.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/capture-connectors/MySQL/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file.  See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the MySQL source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/capture-connectors/MySQL/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tServer Address\tThe host or host:port at which the database can be reached.\tstring\tRequired /user\tLogin User\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tLogin Password\tPassword for the specified database user.\tstring\tRequired /timezone\tTimezone\tTimezone to use when capturing datetime columns. Should normally be left blank to use the database's 'time_zone' system variable. Only required if the 'time_zone' system variable cannot be read and columns with type datetime are being captured. Must be a valid IANA time zone name or +HH:MM offset. Takes precedence over the 'time_zone' system variable if both are set.\tstring /advanced/dbname\tDatabase Name\tThe name of the database to connect to. In general this shouldn't matter. The connector can discover and capture from all databases it's authorized to access.\tstring\t&quot;mysql&quot; /advanced/node_id\tNode ID\tNode ID for the capture. Each node in a replication cluster must have a unique 32-bit ID. The specific value doesn't matter so long as it is unique. If unset or zero the connector will pick a value.\tinteger /advanced/skip_backfills\tSkip Backfills\tA comma-separated list of fully-qualified table names which should not be backfilled.\tstring /advanced/backfill_chunk_size\tBackfill Chunk Size\tThe number of rows which should be fetched from the database in a single backfill query.\tinteger\t131072 /advanced/skip_binlog_retention_check\tSkip Binlog Retention Sanity Check\tBypasses the 'dangerously short binlog retention' sanity check at startup. Only do this if you understand the danger and have a specific need.\tboolean\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe database/schema in which the table resides.\tstring\tRequired /stream\tStream\tName of the table to be captured from the database.\tstring\tRequired  info When you configure this connector in the web application, the automatic discovery process sets up a binding for most tables it finds in your database, but there are exceptions. Tables in the MySQL system schemas information_schema, mysql, performance_schema, and sys will not be discovered. You can add bindings for such tables manually.  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/capture-connectors/MySQL/#sample","content":" A minimal capture definition will look like the following:  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-mysql:dev config: address: &quot;127.0.0.1:3306&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; bindings: - resource: namespace: ${TABLE_NAMESPACE} stream: ${TABLE_NAME} target: ${PREFIX}/${COLLECTION_NAME}   Your capture definition will likely be more complex, with additional bindings for each table in the source database.  Learn more about capture definitions.  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting Capture Errors​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/capture-connectors/MySQL/#troubleshooting-capture-errors","content":" The source-mysql connector is designed to halt immediately if something wrong or unexpected happens, instead of continuing on and potentially outputting incorrect data. What follows is a non-exhaustive list of some potential failure modes, and what action should be taken to fix these situations:  ","version":"Next","tagName":"h2"},{"title":"Unsupported Operations​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/capture-connectors/MySQL/#unsupported-operations","content":" If your capture is failing with an &quot;unsupported operation {ALTER,DROP,TRUNCATE,etc} TABLE&quot; error, this indicates that such an operation has taken place impacting a table which is currently being captured.  In the case of DROP TABLE and other destructive operations this is not supported, and can only be resolved by removing the offending table(s) from the capture bindings list, after which you may recreate the capture if desired (causing the latest state of the table to be recaptured in its entirety).  In the case of ALTER TABLE we currently support table alterations to add or drop columns from a table. This error indicates that whatever alteration took place is not currently supported. Practically speaking the immediate resolution is the same as for a DROP or TRUNCATE TABLE, but if you frequently perform schema migrations it may be worth reaching out to see if we can add support for whatever table alteration you just did.  ","version":"Next","tagName":"h3"},{"title":"Data Manipulation Queries​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/capture-connectors/MySQL/#data-manipulation-queries","content":" If your capture is failing with an &quot;unsupported DML query&quot; error, this means that an INSERT, UPDATE, DELETE or other data manipulation query is present in the MySQL binlog. This should generally not happen if binlog_format = 'ROW' as described in the Prerequisites section.  Resolving this error requires fixing the binlog_format system variable, and then either tearing down and recreating the entire capture so that it restarts at a later point in the binlog, or in the case of an INSERT/DELETE query it may suffice to remove the capture binding for the offending table and then re-add it.  ","version":"Next","tagName":"h3"},{"title":"Unhandled Queries​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/capture-connectors/MySQL/#unhandled-queries","content":" If your capture is failing with an &quot;unhandled query&quot; error, some SQL query is present in the binlog which the connector does not (currently) understand.  In general, this error suggests that the connector should be modified to at least recognize this type of query, and most likely categorize it as either an unsupported DML Query, an unsupported Table Operation, or something that can safely be ignored. Until such a fix is made the capture cannot proceed, and you will need to backfill all collections to allow the capture to jump ahead to a later point in the binlog.  ","version":"Next","tagName":"h3"},{"title":"Metadata Errors​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/capture-connectors/MySQL/#metadata-errors","content":" If your capture is failing with a &quot;metadata error&quot; then something has gone badly wrong with the capture's tracking of table metadata, such as column names or datatypes.  This should never happen, and most likely means that the MySQL binlog itself is corrupt in some way. If this occurs, it can be resolved by backfilling all collections from the source.  ","version":"Next","tagName":"h3"},{"title":"Insufficient Binlog Retention​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/capture-connectors/MySQL/#insufficient-binlog-retention","content":" If your capture fails with a &quot;binlog retention period is too short&quot; error, it is informing you that the MySQL binlog retention period is set to a dangerously low value.  The concern is that if a capture is disabled or the server becomes unreachable for longer than the binlog retention period, the database might delete a binlog segment which the capture isn't yet done with. If this happens then change events have been permanently lost, and the only way to get the capture running again is to skip ahead to a portion of the binlog which still exists. For correctness this requires backfilling the current contents of all tables from the source, and so we prefer to avoid it as much as possible. It's much easier to just set up your binlog retention with enough wiggle room to recover from temporary failures.  The &quot;binlog retention period is too short&quot; error should normally be fixed by setting a longer retention period as described in these setup instructions. However, advanced users who understand the risks can use the skip_binlog_retention_check configuration option to disable this safety.  ","version":"Next","tagName":"h3"},{"title":"Empty Collection Key​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/capture-connectors/MySQL/#empty-collection-key","content":" Every Flow collection must declare a key which is used to group its documents. When testing your capture, if you encounter an error indicating collection key cannot be empty, you will need to either add a key to the table in your source, or manually edit the generated specification and specify keys for the collection before publishing to the catalog as documented here. ","version":"Next","tagName":"h3"},{"title":"MySQL Batch Query Connector","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/MySQL/mysql-batch/","content":"MySQL Batch Query Connector This connector captures data from MySQL into Flow collections by periodically executing queries and translating the results into JSON documents. We recommend using our MySQL CDC Connectorinstead where possible. Using CDC provides lower latency data capture, delete and update events, and usually has a smaller impact on the source database. However there are some circumstances where this might not be feasible. Perhaps you need to capture from a managed MySQL instance which doesn't support logical replication. Or perhaps you need to capture the contents of a view or the result of an ad-hoc query. That's the sort of situation this connector is intended for. The number one caveat you need to be aware of when using this connector is that it will periodically execute its update query over and over. The default polling interval is set to 24 hours to minimize the impact of this behavior, but even then it could mean a lot of duplicated data being processed depending on the size of your tables. If the dataset has a natural cursor which could be used to identify only new or updated rows, this should be specified by editing the Cursor property of the binding. Common examples of suitable cursors include: Update timestamps, which are usually the best choice if available since they can be used to identify all changed rows, not just updates.Creation timestamps, which can be used to identify newly added rows in append-only datasets but can't be used to identify updates.Serially increasing IDs can also be used to identify newly added rows.","keywords":"","version":"Next"},{"title":"Amazon RDS for MySQL","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/MySQL/amazon-rds-mysql/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/amazon-rds-mysql/#prerequisites","content":" To use this connector, you'll need a MySQL database setup with the following.  The binlog_formatsystem variable must be set to ROW (the default value).The binary log retentionperiod should be set to 168 hours (the maximum allowed by RDS). This value may be set lower if necessary, but we discourage doing so as this may increase the likelihood of unrecoverable failures. A database user with appropriate permissions: REPLICATION CLIENT and REPLICATION SLAVE privileges.Permission to read the tables being captured.Permission to read from information_schema tables, if automatic discovery is used. If the table(s) to be captured include columns of type DATETIME, the time_zone system variable must be set to an IANA zone name or numerical offset or the capture configured with a timezone to use by default.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/amazon-rds-mysql/#setup","content":" Allow connections between the database and Estuary Flow. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel. To allow direct access: Modify the database, setting Public accessibility to Yes.Edit the VPC security group associated with your database, or create a new VPC security group and associate it with the database as described in the Amazon documentation. Create a new inbound rule and a new outbound rule that allow all traffic from the Estuary Flow IP addresses. To allow secure connections via SSH tunneling: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networks for additional details and a sample. Create a RDS parameter group to enable replication in MySQL. Create a parameter group. Create a unique name and description and set the following properties: Family: mysql8.0Type: DB Parameter group Modify the new parameter group and update the following parameters: binlog_format: ROW Associate the parameter groupwith the database and set Backup Retention Period to 7 days. Reboot the database to allow the changes to take effect. Switch to your MySQL client. Run the following commands to create a new user for the capture with appropriate permissions:  CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' COMMENT 'User account for Flow MySQL data capture'; GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture';   Run the following command to set the binary log retention to 7 days, the maximum value which RDS MySQL permits:  CALL mysql.rds_set_configuration('binlog retention hours', 168);   In the RDS console, note the instance's Endpoint and Port. You'll need these for the address property when you configure the connector.  ","version":"Next","tagName":"h2"},{"title":"Capturing from Read Replicas​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/amazon-rds-mysql/#capturing-from-read-replicas","content":" This connector supports capturing from a read replica of your database, provided that binary logging is enabled on the replica and all other requirements are met. To create a read replica:  Follow RDS instructions to create a read replicaof your MySQL database. Modify the replicaand set the following: DB parameter group: the parameter group you created previouslyBackup retention period: 7 daysPublic access: Publicly accessible Reboot the replica to allow the changes to take effect.  ","version":"Next","tagName":"h2"},{"title":"Setting the MySQL time zone​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/amazon-rds-mysql/#setting-the-mysql-time-zone","content":" MySQL's time_zone server system variable is set to SYSTEM by default.  If you intend to capture tables including columns of the type DATETIME, and time_zone is set to SYSTEM, Flow won't be able to detect the time zone and convert the column to RFC3339 format. To avoid this, you must explicitly set the time zone for your database.  You can:  Specify a numerical offset from UTC. For MySQL version 8.0.19 or higher, values from -13:59 to +14:00, inclusive, are permitted.Prior to MySQL 8.0.19, values from -12:59 to +13:00, inclusive, are permitted Specify a named timezone in IANA timezone format.  For example, if you're located in New Jersey, USA, you could set time_zone to -05:00 or -04:00, depending on the time of year. Because this region observes daylight savings time, you'd be responsible for changing the offset. Alternatively, you could set time_zone to America/New_York, and time changes would occur automatically.  If using IANA time zones, your database must include time zone tables. Learn more in the MySQL docs.  Capture Timezone Configuration If you are unable to set the time_zone in the database and need to capture tables with DATETIME columns, the capture can be configured to assume a time zone using the timezone configuration property (see below). The timezone configuration property can be set as a numerical offset or IANA timezone format.  ","version":"Next","tagName":"h2"},{"title":"Backfills and performance considerations​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/amazon-rds-mysql/#backfills-and-performance-considerations","content":" When the MySQL capture is initiated, by default, the connector first backfills, or captures the targeted tables in their current state. It then transitions to capturing change events on an ongoing basis.  This is desirable in most cases, as it ensures that a complete view of your tables is captured into Flow. However, you may find it appropriate to skip the backfill, especially for extremely large tables.  In this case, you may turn off backfilling on a per-table basis. See properties for details.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/amazon-rds-mysql/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the MySQL source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/amazon-rds-mysql/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tServer Address\tThe host or host:port at which the database can be reached.\tstring\tRequired /user\tLogin User\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tLogin Password\tPassword for the specified database user.\tstring\tRequired /timezone\tTimezone\tTimezone to use when capturing datetime columns. Should normally be left blank to use the database's 'time_zone' system variable. Only required if the 'time_zone' system variable cannot be read and columns with type datetime are being captured. Must be a valid IANA time zone name or +HH:MM offset. Takes precedence over the 'time_zone' system variable if both are set.\tstring /advanced/dbname\tDatabase Name\tThe name of database to connect to. In general this shouldn't matter. The connector can discover and capture from all databases it's authorized to access.\tstring\t&quot;mysql&quot; /advanced/node_id\tNode ID\tNode ID for the capture. Each node in a replication cluster must have a unique 32-bit ID. The specific value doesn't matter so long as it is unique. If unset or zero the connector will pick a value.\tinteger /advanced/skip_backfills\tSkip Backfills\tA comma-separated list of fully-qualified table names which should not be backfilled.\tstring /advanced/backfill_chunk_size\tBackfill Chunk Size\tThe number of rows which should be fetched from the database in a single backfill query.\tinteger\t131072 /advanced/skip_binlog_retention_check\tSkip Binlog Retention Sanity Check\tBypasses the 'dangerously short binlog retention' sanity check at startup. Only do this if you understand the danger and have a specific need.\tboolean\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe database/schema in which the table resides.\tstring\tRequired /stream\tStream\tName of the table to be captured from the database.\tstring\tRequired  info When you configure this connector in the web application, the automatic discovery process sets up a binding for most tables it finds in your database, but there are exceptions. Tables in the MySQL system schemas information_schema, mysql, performance_schema, and sys will not be discovered. You can add bindings for such tables manually.  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/amazon-rds-mysql/#sample","content":" A minimal capture definition will look like the following:  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-mysql:dev config: address: &quot;127.0.0.1:3306&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; bindings: - resource: namespace: ${TABLE_NAMESPACE} stream: ${TABLE_NAME} target: ${PREFIX}/${COLLECTION_NAME}   Your capture definition will likely be more complex, with additional bindings for each table in the source database.  Learn more about capture definitions.  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting Capture Errors​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/amazon-rds-mysql/#troubleshooting-capture-errors","content":" The source-mysql connector is designed to halt immediately if something wrong or unexpected happens, instead of continuing on and potentially outputting incorrect data. What follows is a non-exhaustive list of some potential failure modes, and what action should be taken to fix these situations:  ","version":"Next","tagName":"h2"},{"title":"Unsupported Operations​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/amazon-rds-mysql/#unsupported-operations","content":" If your capture is failing with an &quot;unsupported operation {ALTER,DROP,TRUNCATE,etc} TABLE&quot; error, this indicates that such an operation has taken place impacting a table which is currently being captured.  In the case of DROP TABLE and other destructive operations this is not supported, and can only be resolved by removing the offending table(s) from the capture bindings list, after which you may recreate the capture if desired (causing the latest state of the table to be recaptured in its entirety).  In the case of ALTER TABLE we currently support table alterations to add or drop columns from a table. This error indicates that whatever alteration took place is not currently supported. Practically speaking the immediate resolution is the same as for a DROP or TRUNCATE TABLE, but if you frequently perform schema migrations it may be worth reaching out to see if we can add support for whatever table alteration you just did.  ","version":"Next","tagName":"h3"},{"title":"Data Manipulation Queries​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/amazon-rds-mysql/#data-manipulation-queries","content":" If your capture is failing with an &quot;unsupported DML query&quot; error, this means that an INSERT, UPDATE, DELETE or other data manipulation query is present in the MySQL binlog. This should generally not happen if binlog_format = 'ROW' as described in the Prerequisites section.  Resolving this error requires fixing the binlog_format system variable, and then either tearing down and recreating the entire capture so that it restarts at a later point in the binlog, or in the case of an INSERT/DELETE query it may suffice to remove the capture binding for the offending table and then re-add it.  ","version":"Next","tagName":"h3"},{"title":"Unhandled Queries​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/amazon-rds-mysql/#unhandled-queries","content":" If your capture is failing with an &quot;unhandled query&quot; error, some SQL query is present in the binlog which the connector does not (currently) understand.  In general, this error suggests that the connector should be modified to at least recognize this type of query, and most likely categorize it as either an unsupported DML Query, an unsupported Table Operation, or something that can safely be ignored. Until such a fix is made the capture cannot proceed, and you will need to backfill all collections to allow the capture to jump ahead to a later point in the binlog.  ","version":"Next","tagName":"h3"},{"title":"Metadata Errors​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/amazon-rds-mysql/#metadata-errors","content":" If your capture is failing with a &quot;metadata error&quot; then something has gone badly wrong with the capture's tracking of table metadata, such as column names or datatypes.  This should never happen, and most likely means that the MySQL binlog itself is corrupt in some way. If this occurs, it can be resolved by backfilling all collections from the source.  ","version":"Next","tagName":"h3"},{"title":"Insufficient Binlog Retention​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/amazon-rds-mysql/#insufficient-binlog-retention","content":" If your capture fails with a &quot;binlog retention period is too short&quot; error, it is informing you that the MySQL binlog retention period is set to a dangerously low value.  The concern is that if a capture is disabled or the server becomes unreachable for longer than the binlog retention period, the database might delete a binlog segment which the capture isn't yet done with. If this happens then change events have been permanently lost, and the only way to get the capture running again is to skip ahead to a portion of the binlog which still exists. For correctness this requires backfilling the current contents of all tables from the source, and so we prefer to avoid it as much as possible. It's much easier to just set up your binlog retention with enough wiggle room to recover from temporary failures.  The &quot;binlog retention period is too short&quot; error should normally be fixed by setting a longer retention period as described in these setup instructions. However, advanced users who understand the risks can use the skip_binlog_retention_check configuration option to disable this safety.  ","version":"Next","tagName":"h3"},{"title":"Empty Collection Key​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/amazon-rds-mysql/#empty-collection-key","content":" Every Flow collection must declare a key which is used to group its documents. When testing your capture, if you encounter an error indicating collection key cannot be empty, you will need to either add a key to the table in your source, or manually edit the generated specification and specify keys for the collection before publishing to the catalog as documented here. ","version":"Next","tagName":"h3"},{"title":"Google Cloud SQL for MySQL","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/MySQL/google-cloud-sql-mysql/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/google-cloud-sql-mysql/#prerequisites","content":" To use this connector, you'll need a MySQL database setup with the following.  The binlog_formatsystem variable must be set to ROW (the default value).The binary log expiration period should be at least 7 days. This value may be set lower if necessary, but we discourage doing so as this may increase the likelihood of unrecoverable failures. A database user with appropriate permissions: REPLICATION CLIENT and REPLICATION SLAVE privileges.Permission to read the tables being captured.Permission to read from information_schema tables, if automatic discovery is used. If the table(s) to be captured include columns of type DATETIME, the time_zone system variable must be set to an IANA zone name or numerical offset or the capture configured with a timezone to use by default.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/google-cloud-sql-mysql/#setup","content":" Allow connections between the database and Estuary Flow. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel. To allow direct access: Enable public IP on your database and add Estuary Flow IP addresses as authorized IP addresses. To allow secure connections via SSH tunneling: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networks for additional details and a sample. Set the instance's binlog_expire_logs_seconds flagto 2592000 (30 days). Using Google Cloud Shell or your preferred client, create the flow_capture user with replication permission, and the ability to read all tables. The SELECT permission can be restricted to just the tables that need to be captured, but automatic discovery requires information_schema access as well.  CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' COMMENT 'User account for Flow MySQL data capture'; GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture';   In the Cloud Console, note the instance's host under Public IP Address. Its port will always be 3306. Together, you'll use the host:port as the address property when you configure the connector.  ","version":"Next","tagName":"h2"},{"title":"Capturing from Read Replicas​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/google-cloud-sql-mysql/#capturing-from-read-replicas","content":" This connector supports capturing from a read replica of your database, provided that binary logging is enabled on the replica and all other requirements are met.  ","version":"Next","tagName":"h2"},{"title":"Setting the MySQL time zone​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/google-cloud-sql-mysql/#setting-the-mysql-time-zone","content":" MySQL's time_zone server system variable is set to SYSTEM by default.  If you intend to capture tables including columns of the type DATETIME, and time_zone is set to SYSTEM, Flow won't be able to detect the time zone and convert the column to RFC3339 format. To avoid this, you must explicitly set the time zone for your database.  You can:  Specify a numerical offset from UTC. For MySQL version 8.0.19 or higher, values from -13:59 to +14:00, inclusive, are permitted.Prior to MySQL 8.0.19, values from -12:59 to +13:00, inclusive, are permitted Specify a named timezone in IANA timezone format. If you're using Amazon Aurora, create or modify the DB cluster parameter groupassociated with your MySQL database.Set the time_zone parameter to the correct value.  For example, if you're located in New Jersey, USA, you could set time_zone to -05:00 or -04:00, depending on the time of year. Because this region observes daylight savings time, you'd be responsible for changing the offset. Alternatively, you could set time_zone to America/New_York, and time changes would occur automatically.  If using IANA time zones, your database must include time zone tables. Learn more in the MySQL docs.  Capture Timezone Configuration If you are unable to set the time_zone in the database and need to capture tables with DATETIME columns, the capture can be configured to assume a time zone using the timezone configuration property (see below). The timezone configuration property can be set as a numerical offset or IANA timezone format.  ","version":"Next","tagName":"h2"},{"title":"Backfills and performance considerations​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/google-cloud-sql-mysql/#backfills-and-performance-considerations","content":" When the MySQL capture is initiated, by default, the connector first backfills, or captures the targeted tables in their current state. It then transitions to capturing change events on an ongoing basis.  This is desirable in most cases, as it ensures that a complete view of your tables is captured into Flow. However, you may find it appropriate to skip the backfill, especially for extremely large tables.  In this case, you may turn off backfilling on a per-table basis. See properties for details.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/google-cloud-sql-mysql/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the MySQL source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/google-cloud-sql-mysql/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tServer Address\tThe host or host:port at which the database can be reached.\tstring\tRequired /user\tLogin User\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tLogin Password\tPassword for the specified database user.\tstring\tRequired /timezone\tTimezone\tTimezone to use when capturing datetime columns. Should normally be left blank to use the database's 'time_zone' system variable. Only required if the 'time_zone' system variable cannot be read and columns with type datetime are being captured. Must be a valid IANA time zone name or +HH:MM offset. Takes precedence over the 'time_zone' system variable if both are set.\tstring /advanced/dbname\tDatabase Name\tThe name of database to connect to. In general this shouldn't matter. The connector can discover and capture from all databases it's authorized to access.\tstring\t&quot;mysql&quot; /advanced/node_id\tNode ID\tNode ID for the capture. Each node in a replication cluster must have a unique 32-bit ID. The specific value doesn't matter so long as it is unique. If unset or zero the connector will pick a value.\tinteger /advanced/skip_backfills\tSkip Backfills\tA comma-separated list of fully-qualified table names which should not be backfilled.\tstring /advanced/backfill_chunk_size\tBackfill Chunk Size\tThe number of rows which should be fetched from the database in a single backfill query.\tinteger\t131072 /advanced/skip_binlog_retention_check\tSkip Binlog Retention Sanity Check\tBypasses the 'dangerously short binlog retention' sanity check at startup. Only do this if you understand the danger and have a specific need.\tboolean\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe database/schema in which the table resides.\tstring\tRequired /stream\tStream\tName of the table to be captured from the database.\tstring\tRequired  info When you configure this connector in the web application, the automatic discovery process sets up a binding for most tables it finds in your database, but there are exceptions. Tables in the MySQL system schemas information_schema, mysql, performance_schema, and sys will not be discovered. You can add bindings for such tables manually.  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/google-cloud-sql-mysql/#sample","content":" A minimal capture definition will look like the following:  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-mysql:dev config: address: &quot;127.0.0.1:3306&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; bindings: - resource: namespace: ${TABLE_NAMESPACE} stream: ${TABLE_NAME} target: ${PREFIX}/${COLLECTION_NAME}   Your capture definition will likely be more complex, with additional bindings for each table in the source database.  Learn more about capture definitions.  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting Capture Errors​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/google-cloud-sql-mysql/#troubleshooting-capture-errors","content":" The source-mysql connector is designed to halt immediately if something wrong or unexpected happens, instead of continuing on and potentially outputting incorrect data. What follows is a non-exhaustive list of some potential failure modes, and what action should be taken to fix these situations:  ","version":"Next","tagName":"h2"},{"title":"Unsupported Operations​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/google-cloud-sql-mysql/#unsupported-operations","content":" If your capture is failing with an &quot;unsupported operation {ALTER,DROP,TRUNCATE,etc} TABLE&quot; error, this indicates that such an operation has taken place impacting a table which is currently being captured.  In the case of DROP TABLE and other destructive operations this is not supported, and can only be resolved by removing the offending table(s) from the capture bindings list, after which you may recreate the capture if desired (causing the latest state of the table to be recaptured in its entirety).  In the case of ALTER TABLE we currently support table alterations to add or drop columns from a table. This error indicates that whatever alteration took place is not currently supported. Practically speaking the immediate resolution is the same as for a DROP or TRUNCATE TABLE, but if you frequently perform schema migrations it may be worth reaching out to see if we can add support for whatever table alteration you just did.  ","version":"Next","tagName":"h3"},{"title":"Data Manipulation Queries​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/google-cloud-sql-mysql/#data-manipulation-queries","content":" If your capture is failing with an &quot;unsupported DML query&quot; error, this means that an INSERT, UPDATE, DELETE or other data manipulation query is present in the MySQL binlog. This should generally not happen if binlog_format = 'ROW' as described in the Prerequisites section.  Resolving this error requires fixing the binlog_format system variable, and then either tearing down and recreating the entire capture so that it restarts at a later point in the binlog, or in the case of an INSERT/DELETE query it may suffice to remove the capture binding for the offending table and then re-add it.  ","version":"Next","tagName":"h3"},{"title":"Unhandled Queries​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/google-cloud-sql-mysql/#unhandled-queries","content":" If your capture is failing with an &quot;unhandled query&quot; error, some SQL query is present in the binlog which the connector does not (currently) understand.  In general, this error suggests that the connector should be modified to at least recognize this type of query, and most likely categorize it as either an unsupported DML Query, an unsupported Table Operation, or something that can safely be ignored. Until such a fix is made the capture cannot proceed, and you will need to backfill all collections to allow the capture to jump ahead to a later point in the binlog.  ","version":"Next","tagName":"h3"},{"title":"Metadata Errors​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/google-cloud-sql-mysql/#metadata-errors","content":" If your capture is failing with a &quot;metadata error&quot; then something has gone badly wrong with the capture's tracking of table metadata, such as column names or datatypes.  This should never happen, and most likely means that the MySQL binlog itself is corrupt in some way. If this occurs, it can be resolved by backfilling all collections from the source.  ","version":"Next","tagName":"h3"},{"title":"Insufficient Binlog Retention​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/google-cloud-sql-mysql/#insufficient-binlog-retention","content":" If your capture fails with a &quot;binlog retention period is too short&quot; error, it is informing you that the MySQL binlog retention period is set to a dangerously low value.  The concern is that if a capture is disabled or the server becomes unreachable for longer than the binlog retention period, the database might delete a binlog segment which the capture isn't yet done with. If this happens then change events have been permanently lost, and the only way to get the capture running again is to skip ahead to a portion of the binlog which still exists. For correctness this requires backfilling the current contents of all tables from the source, and so we prefer to avoid it as much as possible. It's much easier to just set up your binlog retention with enough wiggle room to recover from temporary failures.  The &quot;binlog retention period is too short&quot; error should normally be fixed by setting a longer retention period as described in these setup instructions. However, advanced users who understand the risks can use the skip_binlog_retention_check configuration option to disable this safety.  ","version":"Next","tagName":"h3"},{"title":"Empty Collection Key​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/capture-connectors/MySQL/google-cloud-sql-mysql/#empty-collection-key","content":" Every Flow collection must declare a key which is used to group its documents. When testing your capture, if you encounter an error indicating collection key cannot be empty, you will need to either add a key to the table in your source, or manually edit the generated specification and specify keys for the collection before publishing to the catalog as documented here. ","version":"Next","tagName":"h3"},{"title":"SingleStore Batch Query Connector","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/MySQL/singlestore-batch/","content":"","keywords":"","version":"Next"},{"title":"Setup​","type":1,"pageTitle":"SingleStore Batch Query Connector","url":"/reference/Connectors/capture-connectors/MySQL/singlestore-batch/#setup","content":" Ensure that Estuary's IP addresses are allowlisted to allow access. You can do by following these stepsGrab the following details from the SingleStore workspace. Workspace URLUsernamePasswordDatabase Configure the Connector with the appropriate values. Make sure to specify the database name under the &quot;Advanced&quot; section. ","version":"Next","tagName":"h2"},{"title":"NetSuite SuiteAnalytics Connect","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/netsuite-suiteanalytics/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"NetSuite SuiteAnalytics Connect","url":"/reference/Connectors/capture-connectors/netsuite-suiteanalytics/#supported-data-resources","content":" Flow discovers all of the tables to which you grant access during setup, including Transactions, Reports, Lists, and Setup.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"NetSuite SuiteAnalytics Connect","url":"/reference/Connectors/capture-connectors/netsuite-suiteanalytics/#prerequisites","content":" Oracle NetSuite accountAllowed access to all Account permissions optionsA new integration with token-based authenticationA custom role with access to objects you want to capture. See setup.A new user assigned to the custom roleAccess token generated for the custom role  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"NetSuite SuiteAnalytics Connect","url":"/reference/Connectors/capture-connectors/netsuite-suiteanalytics/#setup","content":" Create a NetSuite account  Create an account on the Oracle NetSuite portal. Confirm your email address.  Set up your NetSuite account  Find your Realm, or Account ID. You'll use this to connect with Flow. In your NetSuite portal, go to Setup &gt; Company &gt; Company Information. Copy your Account ID. If you have a production account, it will look like 2345678. If you're using a sandbox, it'll look like 2345678_SB2. Create a NetSuite integration to obtain a Consumer Key and Consumer Secret. Navigate to Setup &gt; Integration &gt; Manage Integrations &gt; New. Give the integration a name, for example, estuary-rest-integration. Make sure the State option is enabled. In the Authentication section, check the Token-Based Authentication checkbox. Save your changes. Your Consumer Key and Consumer Secret will be shown once. Copy them to a safe place. Set up a role for use with Flow. Go to Setup &gt; Users/Roles &gt; Manage Roles &gt; New. Give the role a name, for example, estuary-integration-role. Scroll to the Permissions section. (IMPORTANT) Click Transactions and add all the dropdown entities with either full or view access level. (IMPORTANT) Click Reports and add all the dropdown entities with either full or view access level. (IMPORTANT) Click Lists and add all the dropdown entities with either full or view access level. (IMPORTANT) Click Setup an add all the dropdown entities with either full or view access level. To allow your custom role to reflect future changes, be sure to edit these parameters again when you rename or customize any NetSuite object. Set up user for use with Flow. Go to Setup &gt; Users/Roles &gt; Manage Users. Find the user you want to give access to use with Flow. In the Name column, click the user's name. Then, click the Edit button. Find the Access tab. From the dropdown list, select role you created previously; for example, estuary-integration-role. Save your changes. Generate an access token. Go to Setup &gt; Users/Roles &gt; Access Tokens &gt; New. Select an Application Name. Under User, select the user you assigned the role previously. Under Role, select the role you assigned to the user previously. Under Token Name, give a descriptive name to the token you are creating, for example estuary-rest-integration-token. Save your changes. Your Token ID and Token Secret will be shown once. Copy them to a safe place.  You now have a properly configured account with the correct permissions and all the information you need to connect with Flow:  Realm (Account ID)Consumer KeyConsumer SecretToken IDToken Secret  info You can also authenticate with a username and password, but a consumer/token is recommended for security.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"NetSuite SuiteAnalytics Connect","url":"/reference/Connectors/capture-connectors/netsuite-suiteanalytics/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the NetSuite source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"NetSuite SuiteAnalytics Connect","url":"/reference/Connectors/capture-connectors/netsuite-suiteanalytics/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/account\tNetsuite Account ID\tNetsuite realm/Account ID e.g. 2344535, as for production or 2344535_SB1, as for sandbox\tstring\tRequired /role_id\tRole ID\tThe ID of the role you created. Defaults to 3, which is the ID of the administrator role.\tint\t3 /suiteanalytics_data_source\tData Source\tWhich NetSuite data source to use. Options are NetSuite.com, or NetSuite2.com\tstring\tRequired /authentication\tAuthentication Details\tCredentials to access your NetSuite account\tobject\tRequired  Token/Consumer Authentication​  Property\tTitle\tDescription\tType\tRequired/Default/authentication/consumer_key\tConsumer Key\tConsumer key associated with your integration.\tstring\tRequired /authentication/consumer_secret\tConsumer Secret\tConsumer secret associated with your integration.\tstring\tRequired /authentication/token_key\tToken Key\tAccess token key\tstring\tRequired /authentication/token_secret\tToken Secret\tAccess token secret\tstring\tRequired  Username/Password Authentication​  Property\tTitle\tDescription\tType\tRequired/Default/authentication/username\tUsername\tYour NetSuite account's email/username\tstring\tRequired /authentication/password\tPassword\tYour NetSuite account's password.\tstring\tRequired  Advanced Config options​  Property\tTitle\tDescription\tType\tRequired/Default/advanced/connection_limit\tConnection Limit\tThe maximum number of concurrent data streams to attempt at once.\tint\t10 Connections /advanced/task_limit\tTask Limit\tThe maximum number of concurrent tasks to run at once. A task is either a backfill or incremental load. Backfills can load multiple chunks in parallel, so this must be strictly &lt;= /advanced/connection_limit\tint\t5 Tasks /advanced/start_date\tStart Date\tThe date that we should attempt to start backfilling from. If not provided, backfill from the beginning.\tdate\tNot Required  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/name\tName\tThe name of the table this binding refers to\tstring\tRequired /interval\tInterval\tHow frequently to check for incremental changes\tISO8601 Duration\tPT1H (1 Hour) /log_cursor\tLog Cursor\tA date-time column to use for incremental capture of modifications.\tString\tRequired (Automatically Discovered) /page_cursor\tPage Cursor\tAn indexed, non-NULL integer column to use for ordered table backfills. Does not need to be unique, but should have high cardinality.\tString\tRequired (Automatically Discovered) /concurrency\tConcurrency\tMaximum number of concurrent connections to use for backfilling.\tint\t1 Connection /query_limit\tQuery Limit\tMaximum number of rows to fetch in a query. Will be divided between all connections if /concurrency &gt; 1\tint\t100,000 Rows /query_timeout\tQuery Timeout\tTimeout for queries. Typically left as the default as some tables just take a very long time to respond.\tISO8601 Duration\tPT10M (10 Minutes) /associations\tAssociations\tList of associated tables for which related data should be loaded.\tArray[TableAssociation]\t[] /associations/[n]/child_table_name\tForeign Table Name\tThe name of the &quot;foreign&quot; table that should be associated with the &quot;parent&quot; binding containing this association\tString\tRequired /associations/[n]/parent_join_column_name\tParent Join Column\tThe name of the column on the &quot;parent&quot; table to be used as the join key\tString\tRequired /associations/[n]/child_join_column_name\tForeign Join Column\tThe name of the column on the &quot;foreign&quot; table to be used as the join key\tString\tRequired /associations/[n]/load_during_backfill\tLoad During Backfill\tWhether or not to load associated documents during backfill\tBoolean\tFalse /associations/[n]/load_during_incremental\tLoad During Incremental\tWhether or not to load associated documents during incremental loads\tBoolean\tTrue  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"NetSuite SuiteAnalytics Connect","url":"/reference/Connectors/capture-connectors/netsuite-suiteanalytics/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-netsuite-v2:v3 config: account: &quot;12345678&quot; authentication: auth_type: token consumer_key: xxx consumer_secret_sops: xxx token_id: xxx token_secret_sops: xxx connection_type: suiteanalytics role_id: 3 suiteanalytics_data_source: NetSuite2.com advanced: connection_limit: 20 cursor_fields: [] enable_auto_cursor: false resource_tracing: false start_date: null task_limit: 10 bindings: - resource: associations: - child_join_column_name: transaction child_table_name: TransactionAccountingLine parent_join_column_name: id load_during_backfill: false load_during_incremental: true interval: PT5M name: transaction page_cursor: id query_limit: 100000 concurrency: 1 query_timeout: PT10M log_cursor: lastmodifieddate target: ${PREFIX}/${CAPTURE_NAME}/transaction {...}  ","version":"Next","tagName":"h3"},{"title":"NetSuite SuiteTalk REST","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/netsuite-suitetalk/","content":"","keywords":"","version":"Next"},{"title":"SuiteAnalytics vs SuiteQL via REST API​","type":1,"pageTitle":"NetSuite SuiteTalk REST","url":"/reference/Connectors/capture-connectors/netsuite-suitetalk/#suiteanalytics-vs-suiteql-via-rest-api","content":" These two different connection modes have some key differences:  ","version":"Next","tagName":"h2"},{"title":"SuiteAnalytics Connect​","type":1,"pageTitle":"NetSuite SuiteTalk REST","url":"/reference/Connectors/capture-connectors/netsuite-suitetalk/#suiteanalytics-connect","content":" Requires the SuiteAnalytics Connect feature to be purchased on your NetSuite accountCan inspect which tables (standard &amp; custom) exist in your accountCan inspect the exact data types specified on these table columnsThis means you can connect to any table in your account and all fields (booleans, date, and datetimes) are properly formatted in Estuary  ","version":"Next","tagName":"h3"},{"title":"SuiteQL via REST API​","type":1,"pageTitle":"NetSuite SuiteTalk REST","url":"/reference/Connectors/capture-connectors/netsuite-suitetalk/#suiteql-via-rest-api","content":" Custom tables are not supported without manual workSome standard tables may not yet be supported and will require additional work from the Estuary teamDatetime values are represented as dates without the time specification (this is a limitation of the REST API)Data types on custom columns may not be properly representedYou are repsonsible for determining the right set of permissions to grant the connector, which can often be complicated and unintuitive  ","version":"Next","tagName":"h3"},{"title":"Prerequisites​","type":1,"pageTitle":"NetSuite SuiteTalk REST","url":"/reference/Connectors/capture-connectors/netsuite-suitetalk/#prerequisites","content":" Oracle NetSuite accountAllowed access to all Account permissions optionsA new integration with token-based authenticationA custom role with access to objects you want to capture or a purchased SuiteAnalytics Module. See setup.A new user assigned to the custom roleAccess token generated for the custom role  ","version":"Next","tagName":"h2"},{"title":"General Setup​","type":1,"pageTitle":"NetSuite SuiteTalk REST","url":"/reference/Connectors/capture-connectors/netsuite-suitetalk/#general-setup","content":" Set up required features on your NetSuite account  Find your Account ID (also know as the &quot;Realm&quot;). You'll use this to connect with Flow. In your NetSuite portal, go to Setup &gt; Company &gt; Company Information. Copy your Account ID. If you have a production account, it will look like 2345678. If you're using a sandbox, it'll look like 2345678_SB2. Enable the required features. Navigate to Setup &gt; Company &gt; Enable Features. Click the SuiteCloud tab. In the Manage Authentication section, check the checkbox labeled TOKEN-BASED AUTHENTICATION. If you are using the SuiteQL connection, in the SuiteTalk (Web Services) section, check the checkbox labeled REST WEB SERVICES. Save your changes. If you are using SuiteAnalytics Connect, navigate to Setup &gt; Company &gt; Analytics &gt; Connectivity and check the checkbox labeled SuiteAnalytics Connect. Save your changes. Create a NetSuite integration to obtain a Consumer Key and Consumer Secret. Navigate to Setup &gt; Integration &gt; Manage Integrations &gt; New. Give the integration a name, for example, estuary-netsuite-integration. Make sure the State option is enabled. In the Authentication section, check the Token-Based Authentication checkbox. Save your changes. Your Consumer Key and Consumer Secret will be shown once. Copy them to a safe place. They will never show up again and will be key to the integration working properly. If you are using the SuiteQL over REST API connection, Set up a role for use with Flow. Go to Setup &gt; Users/Roles &gt; Manage Roles &gt; New. Give the role a name, for example, estuary-integration-role. The easiest thing to do here is to click &quot;Core Administrative Permissions&quot;. If you want to scope down the permissions given to the connector (which you should) you'll have to determine which permissions are necessary. This is challenging because many different settings and configurations can expand the required permissions. Check out this repository for help with determining exactly which permissions are required in your case. Scroll to the Permissions section. (IMPORTANT) Click Transactions and add all the dropdown entities with either full or view access level. Find Transaction (IMPORTANT) Click Setup an add the following entities with either full or view access level. Log in using Access TokensREST Web ServicesUser Access Tokens To allow your custom role to reflect future changes, be sure to edit these parameters again when you rename or customize any NetSuite object. If you are using SuiteAnalytics Connect you don't need a custom role. Instead, you can use the bundled &quot;Data Warehouse Integrator&quot; Set up user for use with the connector. Go to Setup &gt; Users/Roles &gt; Manage Users. Find the user you want to give access to use with Flow. In the Name column, click the user's name. Then, click the Edit button. Find the Access tab. From the dropdown list, select either role you created previously (e.g. estuary-integration-role) or the Data Warehouse Integrator role if you are using SuiteAnalytics Connect. Save your changes. Generate an access token. Go to Setup &gt; Users/Roles &gt; Access Tokens &gt; New. Select the Application Name you created earlier. Under User, select the user you assigned the role previously. Under Role, select the role you assigned to the user previously. Under Token Name, give a descriptive name to the token you are creating, for example estuary-rest-integration-token. Save your changes. Your Token ID and Token Secret will be shown once. Copy them to a safe place.  You now have a properly configured account with the correct permissions and all the information you need to connect with Flow:  Account ID (Realm)Consumer KeyConsumer SecretToken IDToken Secret  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"NetSuite SuiteTalk REST","url":"/reference/Connectors/capture-connectors/netsuite-suitetalk/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the NetSuite source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"NetSuite SuiteTalk REST","url":"/reference/Connectors/capture-connectors/netsuite-suitetalk/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/account_id\tRealm\tNetsuite realm e.g. 2344535, as for production or 2344535_SB1, as for the sandbox\tstring\tRequired /start_date\tToken Secret\tThe date to start collecting data from\tdate\tRequired /consumer_key\tConsumer Key\tConsumer key associated with your integration.\tstring\tRequired /consumer_secret\tConsumer Secret\tConsumer secret associated with your integration.\tstring\tRequired /token_key\tToken Key\tAccess token key\tstring\tRequired /token_secret\tToken Secret\tAccess token secret\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your NetSuite project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"NetSuite SuiteTalk REST","url":"/reference/Connectors/capture-connectors/netsuite-suitetalk/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-netsuite:dev config: account_id: &lt;your account id&gt; consumer_key: &lt;key&gt; consumer_secret: &lt;secret&gt; token_key: &lt;key&gt; token_secret: &lt;secret&gt; start_date: &quot;2023-11-01T00:00:00Z&quot; bindings: - resource: stream: Transaction target: ${PREFIX}/Transaction {...}  ","version":"Next","tagName":"h3"},{"title":"Notion","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/notion/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Notion","url":"/reference/Connectors/capture-connectors/notion/#supported-data-resources","content":" The following data resources are supported:  BlocksCommentsDatabasesPagesUsers  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Notion","url":"/reference/Connectors/capture-connectors/notion/#prerequisites","content":" To use this connector, you'll need a Notion account with an integration created to connect with Flow.  Before you create your integration, choose how you'll authenticate with Notion. There are two ways: using OAuth to sign in directly in the web app, or manually, using an access token. OAuth is recommended in the web app; only manual configuration is supported when using the CLI.  ","version":"Next","tagName":"h2"},{"title":"Setup for OAuth authentication​","type":1,"pageTitle":"Notion","url":"/reference/Connectors/capture-connectors/notion/#setup-for-oauth-authentication","content":" Go to your integrations page and create a new integration. On the new integration's Secrets page, change the integration type to Public. Fill in the required fields. Redirect URIs: http://dashboard.estuary.devWebsite homepage: http://dashboard.estuary.devPrivacy policy: https://www.estuary.dev/privacy-policy/Terms of use: https://www.estuary.dev/terms/  ","version":"Next","tagName":"h3"},{"title":"Setup for manual authentication​","type":1,"pageTitle":"Notion","url":"/reference/Connectors/capture-connectors/notion/#setup-for-manual-authentication","content":" Go to your integrations page and create a new internal integration. Notion integrations are internal by default. During setup, change User Capabilitiesfrom No user information (the default) to Read user information without email address. Copy the generated token for use in the connector configuration.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Notion","url":"/reference/Connectors/capture-connectors/notion/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Notion source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Notion","url":"/reference/Connectors/capture-connectors/notion/#properties","content":" Endpoint​  The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so many of these properties aren't required.  Property\tTitle\tDescription\tType\tRequired/Default/credentials\tAuthenticate using\tPick an authentication method.\tobject\tRequired /credentials/auth_type\tAuthentication type\tSet to token for manual authentication\tstring\tRequired /credentials/token\tAccess Token\tNotion API access token\tstring /start_date\tStart Date\tUTC date and time in the format YYYY-MM-DDTHH:MM:SS.000Z. Any data generated before this date will not be replicated. If left blank, the start date will be set to 2 years before the present date.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tNotion resource from which a collection is captured.\tstring\tRequired /syncMode\tSync Mode\tSync this resource incrementally, or fully refresh it every run\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Notion","url":"/reference/Connectors/capture-connectors/notion/#sample","content":" This sample specification reflects the manual authentication method.  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-notion:dev config: credentials: auth_type: token token: {secret} start_date: 2021-01-25T00:00:00Z bindings: - resource: stream: blocks syncMode: incremental target: ${PREFIX}/blocks {...}  ","version":"Next","tagName":"h3"},{"title":"Paypal Transaction","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/paypal-transaction/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Paypal Transaction","url":"/reference/Connectors/capture-connectors/paypal-transaction/#supported-data-resources","content":" The following data resources are supported through the Paypal APIs:  TransactionsBalances  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Paypal Transaction","url":"/reference/Connectors/capture-connectors/paypal-transaction/#prerequisites","content":" The Paypal Transaction API is used to get the history of transactions for a PayPal account.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Paypal Transaction","url":"/reference/Connectors/capture-connectors/paypal-transaction/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Paypal Transaction source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Paypal Transaction","url":"/reference/Connectors/capture-connectors/paypal-transaction/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/client_id\tClient ID\tThe Client ID of your Paypal developer application.\tstring\tRequired /client_secret\tClient Secret\tThe Client Secret of your Paypal developer application.\tstring\tRequired /is_sandbox\tSandbox\tCheckbox to indicate whether it is a sandbox environment or not\tboolean\tfalse /refresh_token\tRefresh token\tThe key to refresh the expired access token.\tstring\tRequired /start_date\tStart Date\tUTC date and time in the format 2021-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Paypal Transaction project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Paypal Transaction","url":"/reference/Connectors/capture-connectors/paypal-transaction/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-paypal-transaction:dev config: client_id: &lt;secret&gt; client_secret: &lt;secret&gt; is_sandbox: false refresh_token: &lt;secret&gt; start_date: 2017-01-25T00:00:00Z bindings: - resource: stream: transactions syncMode: full_refresh target: ${PREFIX}/transactions {...}  ","version":"Next","tagName":"h3"},{"title":"OracleDB","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/OracleDB/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"OracleDB","url":"/reference/Connectors/capture-connectors/OracleDB/#prerequisites","content":" Oracle 11g or aboveAllow connections from Estuary Flow to your Oracle database (if they exist in separate VPCs)Create a dedicated read-only Estuary Flow user with access to all tables needed for replication  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"OracleDB","url":"/reference/Connectors/capture-connectors/OracleDB/#setup","content":" Follow the steps below to set up the OracleDB connector.  ","version":"Next","tagName":"h2"},{"title":"Create a Dedicated User​","type":1,"pageTitle":"OracleDB","url":"/reference/Connectors/capture-connectors/OracleDB/#create-a-dedicated-user","content":" Creating a dedicated database user with read-only access is recommended for better permission control and auditing. Depending on whether your database is a container database (also known as CDB) or not, follow the corresponding section below.  ","version":"Next","tagName":"h3"},{"title":"Non-container Databases​","type":1,"pageTitle":"OracleDB","url":"/reference/Connectors/capture-connectors/OracleDB/#non-container-databases","content":" To create the user, run the following commands against your database:  CREATE USER estuary_flow_user IDENTIFIED BY &lt;your_password_here&gt;; GRANT CREATE SESSION TO estuary_flow_user;   Next, grant the user read-only access to the relevant tables. The simplest way is to grant read access to all tables in the schema as follows:  GRANT SELECT ANY TABLE TO estuary_flow_user;   Alternatively, you can be more granular and grant access to specific tables in different schemas:  GRANT SELECT ON &quot;&lt;schema_a&gt;&quot;.&quot;&lt;table_1&gt;&quot; TO estuary_flow_user; GRANT SELECT ON &quot;&lt;schema_b&gt;&quot;.&quot;&lt;table_2&gt;&quot; TO estuary_flow_user;   Create a watermarks table:  CREATE TABLE estuary_flow_user.FLOW_WATERMARKS(SLOT varchar(1000) PRIMARY KEY, WATERMARK varchar(4000));   Grant the user access to use logminer, read metadata from the database and write to the watermarks table:  GRANT SELECT_CATALOG_ROLE TO estuary_flow_user; GRANT EXECUTE_CATALOG_ROLE TO estuary_flow_user; GRANT SELECT ON V$DATABASE TO estuary_flow_user; GRANT SELECT ON V$LOG TO estuary_flow_user; GRANT LOGMINING TO estuary_flow_user; GRANT INSERT, UPDATE ON estuary_flow_user.FLOW_WATERMARKS TO estuary_flow_user;   Enable supplemental logging:  For normal instances use:  ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS;   For Amazon RDS instances use:  BEGIN rdsadmin.rdsadmin_util.alter_supplemental_logging(p_action =&gt; 'ADD', p_type =&gt; 'ALL'); end;   Ensure user has quota on the USERS tablespace:  ALTER USER estuary_flow_user QUOTA UNLIMITED ON USERS;   ","version":"Next","tagName":"h2"},{"title":"Container Databases​","type":1,"pageTitle":"OracleDB","url":"/reference/Connectors/capture-connectors/OracleDB/#container-databases","content":" For working with container databases, access to the root container is necessary. Amazon RDS Oracle databases do not allow access to the root container and so they do not work if configured as a multi-tenant architecture database (whether single-tenant or multi-tenant). If your Amazon RDS instance has containers, try the OracleDB Flashback connector instead.  To create a common user (requires c## prefix in the name of the user), run the following commands against your database:  CREATE USER c##estuary_flow_user IDENTIFIED BY &lt;your_password_here&gt; CONTAINER=ALL; GRANT CREATE SESSION TO c##estuary_flow_user CONTAINER=ALL;   Next, grant the user read-only access to the relevant tables. The simplest way is to grant read access to all tables in the schema as follows:  GRANT SELECT ANY TABLE TO c##estuary_flow_user CONTAINER=ALL;   Alternatively, you can be more granular and grant access to specific tables in different schemas (run in the root container):  GRANT SELECT ON &quot;&lt;schema_a&gt;&quot;.&quot;&lt;table_1&gt;&quot; TO c##estuary_flow_user CONTAINER=ALL; GRANT SELECT ON &quot;&lt;schema_b&gt;&quot;.&quot;&lt;table_2&gt;&quot; TO c##estuary_flow_user CONTAINER=ALL;   Create a watermarks table (the table should be in the PDB)  CREATE TABLE c##estuary_flow_user.FLOW_WATERMARKS(SLOT varchar(1000) PRIMARY KEY, WATERMARK varchar(4000)); GRANT INSERT, UPDATE ON c##estuary_flow_user.FLOW_WATERMARKS TO c##estuary_flow_user CONTAINER=ALL;   Finally you need to grant the user access to use logminer, read metadata from the database and write to the watermarks table:  GRANT SELECT_CATALOG_ROLE TO c##estuary_flow_user CONTAINER=ALL; GRANT EXECUTE_CATALOG_ROLE TO c##estuary_flow_user CONTAINER=ALL; GRANT LOGMINING TO c##estuary_flow_user CONTAINER=ALL; GRANT ALTER SESSION TO c##estuary_flow_user CONTAINER=ALL; GRANT SET CONTAINER TO c##estuary_flow_user CONTAINER=ALL;   Enable supplemental logging:  For normal instances use:  ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS;   Ensure user has quota on the USERS tablespace:  ALTER USER c##estuary_flow_user QUOTA UNLIMITED ON USERS;   ","version":"Next","tagName":"h2"},{"title":"Include Schemas for Discovery​","type":1,"pageTitle":"OracleDB","url":"/reference/Connectors/capture-connectors/OracleDB/#include-schemas-for-discovery","content":" In your Oracle configuration, you can specify the schemas that Flow should look at when discovering tables. The schema names are case-sensitive. If the user does not have access to a certain schema, no tables from that schema will be discovered.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"OracleDB","url":"/reference/Connectors/capture-connectors/OracleDB/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the OracleDB source connector.  To allow secure connections via SSH tunneling:  Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networks for additional details and a sample.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"OracleDB","url":"/reference/Connectors/capture-connectors/OracleDB/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tThe host or host:port at which the database can be reached.\tstring\tRequired /user\tUsername\tThe database user to authenticate as.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /database\tDatabase\tLogical database name to capture from. Defaults to ORCL. In multi-container environments use the PDB name.\tstring\tRequired /historyMode\tHistory Mode\tCapture change events without reducing them to a final state.\tboolean\tfalse /advanced/skip_backfills\tSkip Backfills\tA comma-separated list of fully-qualified table names which should not be backfilled.\tstring /advanced/watermarksTable\tWatermarks Table\tThe name of the table used for watermark writes during backfills. Must be fully-qualified in '&lt;schema&gt;.table' form.\tstring\t&amp;lt;USER&amp;gt;.FLOW_WATERMARKS /advanced/backfill_chunk_size\tBackfill Chunk Size\tThe number of rows which should be fetched from the database in a single backfill query.\tinteger\t50000 /advanced/incremental_chunk_size\tIncremental Chunk Size\tThe number of rows which should be fetched from the database in a single incremental query.\tinteger\t10000 /advanced/incremental_scn_range\tIncremental SCN Range\tThe SCN range captured at every iteration.\tinteger\t50000 /advanced/dictionary_mode\tDictionary Mode\tHow should dictionaries be used in Logminer: one of online or extract. When using online mode schema changes to the table may break the capture but resource usage is limited. When using extract mode schema changes are handled gracefully but more resources of your database (including disk) are used by the process. Defaults to extract.\tstring\textract /advanced/discover_schemas\tDiscover Schemas\tIf this is specified only tables in the selected schema(s) will be automatically discovered. Omit all entries to discover tables from all schemas.\tstring /advanced/node_id\tNode ID\tNode ID for the capture. Each node in a replication cluster must have a unique 32-bit ID. The specific value doesn't matter so long as it is unique. If unset or zero the connector will pick a value.\tinteger\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe owner/schema of the table.\tstring\tRequired /stream\tStream\tTable name.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"OracleDB","url":"/reference/Connectors/capture-connectors/OracleDB/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-oracle:dev config: address: database-1.ukqdmzdqvvsz.us-east-2.rds.amazonaws.com:1521 user: &quot;flow_capture&quot; password: secret database: ORCL historyMode: false advanced: incremental_scn_range: 50000 dictionary_mode: extract networkTunnel: sshForwarding: privateKey: -----BEGIN RSA PRIVATE KEY-----\\n... sshEndpoint: ssh://ec2-user@19.220.21.33:22 bindings: - resource: namespace: ${TABLE_NAMESPACE} stream: ${TABLE_NAME} target: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Dictionary Modes​","type":1,"pageTitle":"OracleDB","url":"/reference/Connectors/capture-connectors/OracleDB/#dictionary-modes","content":" Oracle writes redo log files using triplet object ID, data object ID and object versions to identify different objects in the database, rather than their name. This applies to table names as well as column names. When reading data from the redo log files using Logminer, a &quot;dictionary&quot; is used to translate the object identification data into user-facing names of those objects. When interacting with the database directly an online dictionary, which is essentially the latest dictionary that knows how to translate currently existing table and column names is used by the database and by Logminer, however when capturing historical data, it is possible that the names of these objects or even their identifiers have changed (due to an ALTER TABLE statement for example). In these instances the online dictionary will be insufficient for translating the object identifiers into names and Logminer will complain about a dictionary mismatch.  To resolve this issue, it is possible to extract a dictionary into the redo log files themselves, so that when there are schema changes, Logminer can automatically handle using the appropriate dictionary for the time period an event is from. This operation however uses CPU and RAM, as well as consuming disk over time.  Using Estuary's Oracle connector you get to choose which mode to operate it:  To extract the dictionary into the redo log files, the extract mode can be used (this is the default mode). Be aware that this mode leads to more resource usage (CPU, RAM and disk).To always use the online dictionary, the online mode can be used. This mode is more efficient, but it cannot handle schema changes in tables, so only use this mode with caution and when table schemas are known not to change.  ","version":"Next","tagName":"h3"},{"title":"Incremental SCN Range and Events Rate​","type":1,"pageTitle":"OracleDB","url":"/reference/Connectors/capture-connectors/OracleDB/#incremental-scn-range-and-events-rate","content":" At every iteration, the connector fetches changes in a specific SCN (System Change Number) range, this is roughly equivalent to a specific time range. Depending on how many events happen on the captured tables in a database (by default, a 50,000 range is captured in each iteration), the advanced.incremental_scn_range option can be updated to fit your needs:  If the database processes a large amount of events per unit of time, the connector and/or the database may experience resource shortages while trying to process the data. For example you may see the error PGA memory used by the instance exceeds PGA_AGGREGATE_LIMIT which indicates that the memory usage of the database instance has hit a limit. This can happen if too many events are being processed in one iteration. In these cases we recommend lowering the SCN range until the database and the connector are able to handle the load.If the database does not have many events per time unit, a higher value can help with faster processing, although this is usually not necessary.  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"OracleDB","url":"/reference/Connectors/capture-connectors/OracleDB/#troubleshooting","content":" If you see the following error when trying to connect:  ORA-01950: no privileges on tablespace 'USERS'   The SQL command below may resolve the issue:  ALTER USER estuary_flow_user QUOTA UNLIMITED ON USERS;   ","version":"Next","tagName":"h2"},{"title":"Known Limitations​","type":1,"pageTitle":"OracleDB","url":"/reference/Connectors/capture-connectors/OracleDB/#known-limitations","content":" Table and column names longer than 30 characters are not supported by Logminer, and thus they are also not supported by this connector. ","version":"Next","tagName":"h2"},{"title":"OracleDB (Flashback)","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/OracleDB/flashback/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"OracleDB (Flashback)","url":"/reference/Connectors/capture-connectors/OracleDB/flashback/#prerequisites","content":" Oracle 11g or aboveAllow connections from Estuary Flow to your Oracle database (if they exist in separate VPCs)Create a dedicated read-only Estuary Flow user with access to all tables needed for replication  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"OracleDB (Flashback)","url":"/reference/Connectors/capture-connectors/OracleDB/flashback/#setup","content":" Follow the steps below to set up the OracleDB connector.  ","version":"Next","tagName":"h2"},{"title":"Create a Dedicated User​","type":1,"pageTitle":"OracleDB (Flashback)","url":"/reference/Connectors/capture-connectors/OracleDB/flashback/#create-a-dedicated-user","content":" Creating a dedicated database user with read-only access is recommended for better permission control and auditing.  To create the user, run the following commands against your database:  CREATE USER estuary_flow_user IDENTIFIED BY &lt;your_password_here&gt;; GRANT CREATE SESSION TO estuary_flow_user;   Next, grant the user read-only access to the relevant tables. The simplest way is to grant read access to all tables in the schema as follows:  GRANT SELECT ANY TABLE TO estuary_flow_user; GRANT FLASHBACK ANY TABLE to estuary_flow_user;   Alternatively, you can be more granular and grant access to specific tables in different schemas:  GRANT SELECT ON &quot;&lt;schema_a&gt;&quot;.&quot;&lt;table_1&gt;&quot; TO estuary_flow_user; GRANT FLASHBACK ON &quot;&lt;schema_a&gt;&quot;.&quot;&lt;table_1&gt;&quot; to estuary_flow_user; GRANT SELECT ON &quot;&lt;schema_b&gt;&quot;.&quot;&lt;table_2&gt;&quot; TO estuary_flow_user; GRANT FLASHBACK ON &quot;&lt;schema_b&gt;&quot;.&quot;&lt;table_2&gt;&quot; TO estuary_flow_user; -- In this case you need to also grant access to metadata views GRANT SELECT ON V$DATABASE TO estuary_flow_user;   Finally you need to grant the user access to read metadata from the database:  GRANT SELECT_CATALOG_ROLE TO estuary_flow_user;   Your database user should now be ready for use with Estuary Flow.  ","version":"Next","tagName":"h3"},{"title":"Recommended Database Configuration​","type":1,"pageTitle":"OracleDB (Flashback)","url":"/reference/Connectors/capture-connectors/OracleDB/flashback/#recommended-database-configuration","content":" In order to use Flashback and ensure consistency of data, we recommend setting the UNDO_RETENTION configuration to at least 7 days, or at minimum a couple of days. See UNDO_RETENTION in Oracle docs. Example query to set retention to 2 days:  ALTER SYSTEM SET UNDO_RETENTION = 172800;   ","version":"Next","tagName":"h3"},{"title":"Include Schemas for Discovery​","type":1,"pageTitle":"OracleDB (Flashback)","url":"/reference/Connectors/capture-connectors/OracleDB/flashback/#include-schemas-for-discovery","content":" In your Oracle configuration, you can specify the schemas that Flow should look at when discovering tables. The schema names are case-sensitive and will default to the upper-cased user if empty. If the user does not have access to the configured schemas, no tables will be discovered.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"OracleDB (Flashback)","url":"/reference/Connectors/capture-connectors/OracleDB/flashback/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the OracleDB Flashback source connector.  To allow secure connections via SSH tunneling:  Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networks for additional details and a sample.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"OracleDB (Flashback)","url":"/reference/Connectors/capture-connectors/OracleDB/flashback/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tThe connection string for connecting to the database, either in the format of host:port/SID or a connect descriptor such as (address=(protocol=tcps)(host=...)...)\tstring\tRequired /credentials\tCredentials\tCredentials for authenticating with the database. Wallet and Username &amp; Password authentication are supported.\tobject /credentials/credentials_title\tCredentials Title\tAuthentication method to use, one of &quot;Wallet&quot; or &quot;Username &amp; Password&quot;\tstring\tRequired /credentials/username\tUsername\tThe username which is used to access the database.\tstring\tRequired /credentials/password\tPassword\tThe password associated with the username.\tstring\tRequired /credentials/tnsnames\ttnsnames\tThe tnsnames.ora file from the wallet.\tstring /credentials/ewallet\tewallet\tThe ewallet.pem file from the wallet.\tstring /credentials/wallet_password\tWallet Password\tPassword of the wallet, if protected.\tstring /advanced/backfill_chunk_size\tBackfill Chunk Size\tThe number of rows which should be fetched from the database in a single backfill query.\tinteger\t50000 /advanced/skip_flashback_retention_checks\tSkip Flashback Retention Checks\tSkip Flashback retention checks. Use this cautiously as we cannot guarantee consistency if Flashback retention is not sufficient.\tinteger\tfalse /advanced/default_interval\tDefault Interval\tDefault interval between updates for all resources. Can be overwritten by each resource.\tinteger\tPT5M  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/name\tName\tThe table name\tstring\tRequired /schema\tSchema\tIn Oracle tables reside in a schema that points to the user that owns the table.\tstring\tRequired /interval\tInterval\tInterval between updates for this resource\tstring\tPT5M  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"OracleDB (Flashback)","url":"/reference/Connectors/capture-connectors/OracleDB/flashback/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-oracle-flashback:dev config: address: &quot;database-1.ukqdmzdqvvsz.us-east-2.rds.amazonaws.com:1521/ORCL&quot; user: &quot;flow_capture&quot; credentials: credentials_title: Username &amp; Password username: ADMIN password: secret networkTunnel: sshForwarding: privateKey: -----BEGIN RSA PRIVATE KEY-----\\n... sshEndpoint: ssh://ec2-user@19.220.21.33:22 bindings: - resource: name: ${TABLE_NAME} schema: ${TABLE_NAMESPACE} interval: PT5M target: ${PREFIX}/${COLLECTION_NAME}  ","version":"Next","tagName":"h3"},{"title":"Pendo","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/pendo/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Pendo","url":"/reference/Connectors/capture-connectors/pendo/#supported-data-resources","content":" The following data resources are supported through the Pendo API:  FeatureGuidePageReportPageEventsFeatureEventsTrackEventsGuideEventsPollEvents  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Pendo","url":"/reference/Connectors/capture-connectors/pendo/#prerequisites","content":" A Pendo account with the integration feature enabled.A Pendo API key  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Pendo","url":"/reference/Connectors/capture-connectors/pendo/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification files. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Pendo source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Pendo","url":"/reference/Connectors/capture-connectors/pendo/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/credentials/access_token\tAPI Key\tYour Pendo API key.\tstring\tRequired /startDate\tReplication Start Date\tUTC date and time in the format &quot;YYYY-MM-DDTHH:MM:SSZ&quot;. Data prior to this date will not be replicated.\tstring\t1 hour before the current time  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/name\tData resource\tResource in Pendo from which collections are captured.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Pendo","url":"/reference/Connectors/capture-connectors/pendo/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-pendo:dev config: credentials: access_token: &lt;secret&gt; bindings: - resource: name: Feature target: ${PREFIX}/Feature - resource: name: Guide target: ${PREFIX}/Guide - resource: name: Page target: ${PREFIX}/Page - resource: name: Report target: ${PREFIX}/Report - resource: name: PageEvents target: ${PREFIX}/PageEvents - resource: name: FeatureEvents target: ${PREFIX}/FeatureEvents - resource: name: TrackEvents target: ${PREFIX}/TrackEvents - resource: name: GuideEvents target: ${PREFIX}/GuideEvents - resource: name: PollEvents target: ${PREFIX}/PollEvents  ","version":"Next","tagName":"h3"},{"title":"Pinterest","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/pinterest/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Pinterest","url":"/reference/Connectors/capture-connectors/pinterest/#prerequisites","content":" To set up the Pinterest source connector, you'll need the following prerequisites:  Pinterest App ID and secret keyRefresh Token  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Pinterest","url":"/reference/Connectors/capture-connectors/pinterest/#setup","content":" Follow the steps below to set up the Pinterest source connector.  Log into your Estuary Flow account.Navigate to the &quot;Captures&quot; sectionFor the &quot;Start Date,&quot; provide the date in YYYY-MM-DD format. Data added on and after this date will be replicated.Next, go to &quot;Authorization Method&quot;Authenticate your Pinterest account using OAuth2.0 or an Access Token. The OAuth2.0 authorization method is selected by default. For &quot;Client ID&quot; and &quot;Client Secret,&quot; enter your Pinterest App ID and secret key. For the &quot;Refresh Token,&quot; enter your Pinterest Refresh Token. Click &quot;Set up source.&quot;  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Pinterest","url":"/reference/Connectors/capture-connectors/pinterest/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Pinterest source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Pinterest","url":"/reference/Connectors/capture-connectors/pinterest/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/start_date\tStart Date\tA date in the format YYYY-MM-DD. If you have not set a date, it would be defaulted to latest allowed date by api (89 days from today).\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Pinterest project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Pinterest","url":"/reference/Connectors/capture-connectors/pinterest/#sample","content":" { &quot;required&quot;: [&quot;start_date&quot;, &quot;credentials&quot;], &quot;properties&quot;: { &quot;start_date&quot;: { &quot;pattern_descriptor&quot;: null }, &quot;credentials&quot;: { &quot;discriminator&quot;: { &quot;propertyName&quot;: &quot;auth_method&quot; }, &quot;oneOf&quot;: [ { &quot;title&quot;: &quot;OAuth2.0&quot;, &quot;type&quot;: &quot;object&quot;, &quot;x-oauth2-provider&quot;: &quot;pinterest&quot;, &quot;properties&quot;: { &quot;auth_method&quot;: { &quot;const&quot;: &quot;oauth2.0&quot;, &quot;order&quot;: 0, &quot;type&quot;: &quot;string&quot;, &quot;default&quot;: &quot;oauth2.0&quot; }, &quot;client_id&quot;: { &quot;airbyte_secret&quot;: true, &quot;description&quot;: &quot;The Client ID of your OAuth application&quot;, &quot;title&quot;: &quot;Client ID&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;client_secret&quot;: { &quot;airbyte_secret&quot;: true, &quot;description&quot;: &quot;The Client Secret of your OAuth application.&quot;, &quot;title&quot;: &quot;Client Secret&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;refresh_token&quot;: { &quot;airbyte_secret&quot;: true, &quot;description&quot;: &quot;Refresh Token to obtain new Access Token, when it's expired.&quot;, &quot;title&quot;: &quot;Refresh Token&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;auth_method&quot;, &quot;refresh_token&quot; ] }, { &quot;title&quot;: &quot;Access Token&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;access_token&quot;: { &quot;airbyte_secret&quot;: true, &quot;description&quot;: &quot;The Access Token to make authenticated requests.&quot;, &quot;title&quot;: &quot;Access Token&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;auth_method&quot;: { &quot;const&quot;: &quot;access_token&quot;, &quot;order&quot;: 0, &quot;type&quot;: &quot;string&quot;, &quot;default&quot;: &quot;access_token&quot; } }, &quot;required&quot;: [ &quot;auth_method&quot;, &quot;access_token&quot; ] } ] } } }   ","version":"Next","tagName":"h3"},{"title":"Supported Streams​","type":1,"pageTitle":"Pinterest","url":"/reference/Connectors/capture-connectors/pinterest/#supported-streams","content":" The Pinterest source connector supports the following streams:  Account analytics (Incremental)Boards (Full table)Board sections (Full table)Pins on board section (Full table)Pins on board (Full table)Ad accounts (Full table)Ad account analytics (Incremental)Campaigns (Incremental)Campaign analytics (Incremental)Ad groups (Incremental)Ad group analytics (Incremental)Ads (Incremental)Ad analytics (Incremental)  ","version":"Next","tagName":"h2"},{"title":"Performance Considerations​","type":1,"pageTitle":"Pinterest","url":"/reference/Connectors/capture-connectors/pinterest/#performance-considerations","content":" The Pinterest API imposes certain rate limits for the connector. Please take note of the following limits:  Analytics streams: 300 calls per day per userAd accounts streams (Campaigns, Ad groups, Ads): 1000 calls per minute per user per appBoards streams: 10 calls per second per user per app  note For any additional information or troubleshooting, refer to the official Pinterest API documentation. ","version":"Next","tagName":"h2"},{"title":"Google Cloud SQL for PostgreSQL","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/PostgreSQL/google-cloud-sql-postgres/","content":"","keywords":"","version":"Next"},{"title":"Supported versions and platforms​","type":1,"pageTitle":"Google Cloud SQL for PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/google-cloud-sql-postgres/#supported-versions-and-platforms","content":" This connector supports PostgreSQL versions 10.0 and later.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Google Cloud SQL for PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/google-cloud-sql-postgres/#prerequisites","content":" You'll need a PostgreSQL database setup with the following:  Logical replication enabled — wal_level=logicalUser role with REPLICATION attributeA replication slot. This represents a “cursor” into the PostgreSQL write-ahead log from which change events can be read. Optional; if none exist, one will be created by the connector.If you wish to run multiple captures from the same database, each must have its own slot. You can create these slots yourself, or by specifying a name other than the default in the advanced configuration. A publication. This represents the set of tables for which change events will be reported. In more restricted setups, this must be created manually, but can be created automatically if the connector has suitable permissions. A watermarks table. The watermarks table is a small “scratch space” to which the connector occasionally writes a small amount of data to ensure accuracy when backfilling preexisting table contents. In more restricted setups, this must be created manually, but can be created automatically if the connector has suitable permissions.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Google Cloud SQL for PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/google-cloud-sql-postgres/#setup","content":" Allow connections between the database and Estuary Flow. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel. To allow direct access: Enable public IP on your database and add the Estuary Flow IP addresses as authorized IP addresses. To allow secure connections via SSH tunneling: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networks for additional details and a sample. On Google Cloud, navigate to your instance's Overview page. Click &quot;Edit configuration&quot;. Scroll down to the Flags section. Click &quot;ADD FLAG&quot;. Set the cloudsql.logical_decoding flag to on to enable logical replication on your Cloud SQL PostgreSQL instance. In your PostgreSQL client, connect to your instance and issue the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table and publication.  CREATE USER flow_capture WITH REPLICATION IN ROLE cloudsqlsuperuser LOGIN PASSWORD 'secret'; GRANT SELECT ON ALL TABLES IN SCHEMA public TO flow_capture; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO flow_capture; CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication; ALTER PUBLICATION flow_publication SET (publish_via_partition_root = true); ALTER PUBLICATION flow_publication ADD TABLE public.flow_watermarks, &lt;other_tables&gt;;   where &lt;other_tables&gt; lists all tables that will be captured from. The publish_via_partition_rootsetting is recommended (because most users will want changes to a partitioned table to be captured under the name of the root table) but is not required.  In the Cloud Console, note the instance's host under Public IP Address. Its port will always be 5432. Together, you'll use the host:port as the address property when you configure the connector.  ","version":"Next","tagName":"h2"},{"title":"Backfills and performance considerations​","type":1,"pageTitle":"Google Cloud SQL for PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/google-cloud-sql-postgres/#backfills-and-performance-considerations","content":" When the PostgreSQL capture is initiated, by default, the connector first backfills, or captures the targeted tables in their current state. It then transitions to capturing change events on an ongoing basis.  This is desirable in most cases, as it ensures that a complete view of your tables is captured into Flow. However, you may find it appropriate to skip the backfill, especially for extremely large tables.  In this case, you may turn off backfilling on a per-table basis. See properties for details.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Google Cloud SQL for PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/google-cloud-sql-postgres/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the PostgreSQL source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Google Cloud SQL for PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/google-cloud-sql-postgres/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tThe host or host:port at which the database can be reached.\tstring\tRequired /database\tDatabase\tLogical database name to capture from.\tstring\tRequired, &quot;postgres&quot; /user\tUser\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tPassword\tPassword for the specified database user.\tstring\tRequired /advanced\tAdvanced Options\tOptions for advanced users. You should not typically need to modify these.\tobject /advanced/backfill_chunk_size\tBackfill Chunk Size\tThe number of rows which should be fetched from the database in a single backfill query.\tinteger\t4096 /advanced/publicationName\tPublication Name\tThe name of the PostgreSQL publication to replicate from.\tstring\t&quot;flow_publication&quot; /advanced/skip_backfills\tSkip Backfills\tA comma-separated list of fully-qualified table names which should not be backfilled.\tstring /advanced/slotName\tSlot Name\tThe name of the PostgreSQL replication slot to replicate from.\tstring\t&quot;flow_slot&quot; /advanced/watermarksTable\tWatermarks Table\tThe name of the table used for watermark writes during backfills. Must be fully-qualified in '&lt;schema&gt;.&lt;table&gt;' form.\tstring\t&quot;public.flow_watermarks&quot; /advanced/sslmode\tSSL Mode\tOverrides SSL connection behavior by setting the 'sslmode' parameter.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe namespace/schema of the table.\tstring\tRequired /stream\tStream\tTable name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Google Cloud SQL for PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/google-cloud-sql-postgres/#sample","content":" A minimal capture definition will look like the following:  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: &quot;ghcr.io/estuary/source-postgres:dev&quot; config: address: &quot;localhost:5432&quot; database: &quot;postgres&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; bindings: - resource: stream: ${TABLE_NAME} namespace: ${TABLE_NAMESPACE} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}   Your capture definition will likely be more complex, with additional bindings for each table in the source database.  Learn more about capture definitions.  ","version":"Next","tagName":"h3"},{"title":"TOASTed values​","type":1,"pageTitle":"Google Cloud SQL for PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/google-cloud-sql-postgres/#toasted-values","content":" PostgreSQL has a hard page size limit, usually 8 KB, for performance reasons. If your tables contain values that exceed the limit, those values can't be stored directly. PostgreSQL uses TOAST (The Oversized-Attribute Storage Technique) to store them separately.  TOASTed values can sometimes present a challenge for systems that rely on the PostgreSQL write-ahead log (WAL), like this connector. If a change event occurs on a row that contains a TOASTed value, but the TOASTed value itself is unchanged, it is omitted from the WAL. As a result, the connector emits a row update with the value omitted, which might cause unexpected results in downstream catalog tasks if adjustments are not made.  The PostgreSQL connector handles TOASTed values for you when you follow the standard discovery workflowor use the Flow UI to create your capture. It uses merge reductionsto fill in the previous known TOASTed value in cases when that value is omitted from a row update.  However, due to the event-driven nature of certain tasks in Flow, it's still possible to see unexpected results in your data flow, specifically:  When you materialize the captured data to another system using a connector that requires delta updatesWhen you perform a derivation that uses TOASTed values  ","version":"Next","tagName":"h2"},{"title":"Troubleshooting​","type":1,"pageTitle":"Google Cloud SQL for PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/google-cloud-sql-postgres/#troubleshooting","content":" If you encounter an issue that you suspect is due to TOASTed values, try the following:  Ensure your collection's schema is using the merge reduction strategy.Set REPLICA IDENTITY to FULL for the table. This circumvents the problem by forcing the WAL to record all values regardless of size. However, this can have performance impacts on your database and must be carefully evaluated.Contact Estuary support for assistance.  ","version":"Next","tagName":"h3"},{"title":"Publications​","type":1,"pageTitle":"Google Cloud SQL for PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/google-cloud-sql-postgres/#publications","content":" It is recommended that the publication used by the capture only contain the tables that will be captured. In some cases it may be desirable to create this publication for all tables in the database instead of specific tables, for example using:  CREATE PUBLICATION flow_publication FOR ALL TABLES WITH (publish_via_partition_root = true);   Caution must be used if creating the publication in this way as all existing tables (even those not part of the capture) will be included in it, and if any of them do not have a primary key they will no longer be able to process updates or deletes. ","version":"Next","tagName":"h2"},{"title":"PostgreSQL","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/PostgreSQL/","content":"","keywords":"","version":"Next"},{"title":"Supported versions and platforms​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/#supported-versions-and-platforms","content":" This connector supports PostgreSQL versions 10.0 and later on major cloud platforms, as well as self-hosted instances.  Setup instructions are provided for the following platforms:  Self-hosted PostgreSQLAmazon RDSAmazon AuroraGoogle Cloud SQLAzure Database for PostgreSQL  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/#prerequisites","content":" You'll need a PostgreSQL database setup with the following:  Logical replication enabled — wal_level=logicalUser role with REPLICATION attributeA replication slot. This represents a “cursor” into the PostgreSQL write-ahead log from which change events can be read. Optional; if none exist, one will be created by the connector.If you wish to run multiple captures from the same database, each must have its own slot. You can create these slots yourself, or by specifying a name other than the default in the advanced configuration. A publication. This represents the set of tables for which change events will be reported. In more restricted setups, this must be created manually, but can be created automatically if the connector has suitable permissions. A watermarks table. The watermarks table is a small “scratch space” to which the connector occasionally writes a small amount of data to ensure accuracy when backfilling preexisting table contents. In more restricted setups, this must be created manually, but can be created automatically if the connector has suitable permissions.  Configuration Tip To configure this connector to capture data from databases hosted on your internal network, you must set up SSH tunneling. For more specific instructions on setup, see configure connections with SSH tunneling.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/#setup","content":" To meet these requirements, follow the steps for your hosting type.  Self-hosted PostgreSQLAmazon RDSAmazon AuroraGoogle Cloud SQLAzure Database for PostgreSQLSupabase  ","version":"Next","tagName":"h2"},{"title":"Self-hosted PostgreSQL​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/#self-hosted-postgresql","content":" The simplest way to meet the above prerequisites is to change the WAL level and have the connector use a database superuser role.  For a more restricted setup, create a new user with just the required permissions as detailed in the following steps:  Connect to your instance and create a new user and password:  CREATE USER flow_capture WITH PASSWORD 'secret' REPLICATION;   Assign the appropriate role. If using PostgreSQL v14 or later: GRANT pg_read_all_data TO flow_capture; If using an earlier version: ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES to flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA public, &lt;other_schema&gt; TO flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA information_schema, pg_catalog TO flow_capture; where &lt;other_schema&gt; lists all schemas that will be captured from. info If an even more restricted set of permissions is desired, you can also grant SELECT on just the specific table(s) which should be captured from. The ‘information_schema’ and ‘pg_catalog’ access is required for stream auto-discovery, but not for capturing already configured streams. Create the watermarks table, grant privileges, and create publication:  CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication; ALTER PUBLICATION flow_publication SET (publish_via_partition_root = true); ALTER PUBLICATION flow_publication ADD TABLE public.flow_watermarks, &lt;other_tables&gt;;   where &lt;other_tables&gt; lists all tables that will be captured from. The publish_via_partition_rootsetting is recommended (because most users will want changes to a partitioned table to be captured under the name of the root table) but is not required.  Set WAL level to logical:  ALTER SYSTEM SET wal_level = logical;   Restart PostgreSQL to allow the WAL level change to take effect.  ","version":"Next","tagName":"h3"},{"title":"Amazon Aurora​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/#amazon-aurora","content":" You must apply some of the settings to the entire Aurora DB cluster, and others to a database instance within the cluster. For each step, take note of which entity you're working with.  Allow connections between the database and Estuary Flow. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel. To allow direct access: Modify the instance, choosing Publicly accessible in the Connectivity settings.Edit the VPC security group associated with your instance, or create a new VPC security group and associate it with the instance as described in the Amazon documentation. Create a new inbound rule and a new outbound rule that allow all traffic from the Estuary Flow IP addresses. To allow secure connections via SSH tunneling: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networks for additional details and a sample. Enable logical replication on your Aurora DB cluster. Create a parameter group. Create a unique name and description and set the following properties: Family: aurora-postgresql13, or substitute the version of Aurora PostgreSQL used for your cluster.Type: DB Cluster Parameter group Modify the new parameter group and set rds.logical_replication=1. Associate the parameter group with the DB cluster. Reboot the cluster to allow the new parameter group to take effect. In the PostgreSQL client, connect to your instance and run the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table and publication.  CREATE USER flow_capture WITH PASSWORD 'secret'; GRANT rds_replication TO flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA public TO flow_capture; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO flow_capture; CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication; ALTER PUBLICATION flow_publication SET (publish_via_partition_root = true); ALTER PUBLICATION flow_publication ADD TABLE public.flow_watermarks, &lt;other_tables&gt;;   where &lt;other_tables&gt; lists all tables that will be captured from. The publish_via_partition_rootsetting is recommended (because most users will want changes to a partitioned table to be captured under the name of the root table) but is not required.  In the RDS console, note the instance's Endpoint and Port. You'll need these for the address property when you configure the connector.  ","version":"Next","tagName":"h3"},{"title":"Azure Database for PostgreSQL​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/#azure-database-for-postgresql","content":" Allow connections between the database and Estuary Flow. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel. To allow direct access: Create a new firewall rule that grants access to the Estuary Flow IP addresses. To allow secure connections via SSH tunneling: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networks for additional details and a sample. In your Azure PostgreSQL instance's support parameters, set replication to logical to enable logical replication. In the PostgreSQL client, connect to your instance and run the following commands to create a new user for the capture with appropriate permissions.  CREATE USER flow_capture WITH PASSWORD 'secret' REPLICATION;   If using PostgreSQL v14 or later:  GRANT pg_read_all_data TO flow_capture;   If using an earlier version: ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES to flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA public, &lt;others&gt; TO flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA information_schema, pg_catalog TO flow_capture; where &lt;others&gt; lists all schemas that will be captured from. info If an even more restricted set of permissions is desired, you can also grant SELECT on just the specific table(s) which should be captured from. The ‘information_schema’ and ‘pg_catalog’ access is required for stream auto-discovery, but not for capturing already configured streams.  Set up the watermarks table and publication.  ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES to flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA public, &lt;others&gt; TO flow_capture; GRANT SELECT ON information_schema.columns, information_schema.tables, pg_catalog.pg_attribute, pg_catalog.pg_class, pg_catalog.pg_index, pg_catalog.pg_namespace TO flow_capture; CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication; ALTER PUBLICATION flow_publication SET (publish_via_partition_root = true); ALTER PUBLICATION flow_publication ADD TABLE public.flow_watermarks, &lt;other_tables&gt;;   Note the following important items for configuration: Find the instance's host under Server Name, and the port under Connection Strings (usually 5432). Together, you'll use the host:port as the address property when you configure the connector.Format user as username@databasename; for example, flow_capture@myazuredb.  ","version":"Next","tagName":"h3"},{"title":"Backfills and performance considerations​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/#backfills-and-performance-considerations","content":" When the PostgreSQL capture is initiated, by default, the connector first backfills, or captures the targeted tables in their current state. It then transitions to capturing change events on an ongoing basis.  This is desirable in most cases, as it ensures that a complete view of your tables is captured into Flow. However, you may find it appropriate to skip the backfill, especially for extremely large tables.  In this case, you may turn off backfilling on a per-table basis. See properties for details.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the PostgreSQL source connector.  ","version":"Next","tagName":"h2"},{"title":"WAL Retention and Tuning Parameters​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/#wal-retention-and-tuning-parameters","content":" Postgres logical replication works by reading change events from the writeahead log, reordering WAL events in memory on the server, and sending them to the client in the order that transactions were committed. The replication slot used by the capture is essentially a cursor into that logical sequence of changes.  Because of how Postgres reorders WAL events into atomic transactions, there are two distinct LSNs which matter when it comes to WAL retention. The confirmed_flush_lsnproperty of a replication slot represents the latest event in the WAL which has been sent to and confirmed by the client. However there may be some number of uncommitted changes prior to this point in the WAL which are still relevant and will be sent to the client in later transactions. Thus there is also a restart_lsn property which represents the point in the WAL from which logical decoding must resume in the future if the replication connection is closed and restarted.  The server cannot clean up old WAL files so long as there are active replication slots whose restart_lsn position requires them. There are two ways that restart_lsn might get stuck at a particular point in the WAL:  When a capture is deleted, disabled, or repeatedly failing for other reasons, it is not able to advance the confirmed_flush_lsn and thus restart_lsn cannot advance either.When a long-running transaction is open on the server the restart_lsn of a replication slot may be unable to advance even though confirmed_flush_lsn is.  By default Postgres will retain an unbounded amount of WAL data and fill up the entire disk if a replication slot stops advancing. There are two ways to address this:  When deleting a capture, make sure that the replication slot is also successfully deleted. You can list replication slots with the query SELECT * FROM pg_replication_slots and can drop the replication slot manually with pg_drop_replication_slot('flow_slot'). The database setting max_slot_wal_keep_size can be used to bound the maximum amount of WAL data which a replication slot can force the database to retain. This setting defaults to -1 (unlimited) but should be set on production databases to protect them from unbounded WAL retention filling up the entire disk.Proper sizing of this setting is complex for reasons discussed below, but a value of 50GB should be enough for many databases.  When the max_slot_wal_keep_size limit is exceeded, Postgres will terminate any active replication connections using that slot and invalidate the replication slot so that it can no longer be used. If Postgres invalidates the replication slot, the Flow capture using that slot will fail and manual intervention will be required to restart the capture and re-backfill all tables.  Setting too low of a limit for max_slot_wal_keep_size can cause additional failures in the presence of long-running transactions. Even when a client is actively receiving and acknowledging replication events, a long-running transaction can cause the restart_lsnof the replication slot to remain stuck until that transaction commits. Thus the value ofmax_slot_wal_keep_size needs to be set high enough to avoid this happening. The precise value depends on the overall change rate of your database and worst-case transaction open time, but there is no downside to using a larger value provided you have enough free disk space.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tThe host or host:port at which the database can be reached.\tstring\tRequired /database\tDatabase\tLogical database name to capture from.\tstring\tRequired, &quot;postgres&quot; /user\tUser\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tPassword\tPassword for the specified database user.\tstring\tRequired /advanced\tAdvanced Options\tOptions for advanced users. You should not typically need to modify these.\tobject /advanced/backfill_chunk_size\tBackfill Chunk Size\tThe number of rows which should be fetched from the database in a single backfill query.\tinteger\t4096 /advanced/publicationName\tPublication Name\tThe name of the PostgreSQL publication to replicate from.\tstring\t&quot;flow_publication&quot; /advanced/skip_backfills\tSkip Backfills\tA comma-separated list of fully-qualified table names which should not be backfilled.\tstring /advanced/slotName\tSlot Name\tThe name of the PostgreSQL replication slot to replicate from.\tstring\t&quot;flow_slot&quot; /advanced/watermarksTable\tWatermarks Table\tThe name of the table used for watermark writes during backfills. Must be fully-qualified in '&lt;schema&gt;.&lt;table&gt;' form.\tstring\t&quot;public.flow_watermarks&quot; /advanced/sslmode\tSSL Mode\tOverrides SSL connection behavior by setting the 'sslmode' parameter.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe namespace/schema of the table.\tstring\tRequired /stream\tStream\tTable name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired  SSL Mode​  Certain managed PostgreSQL implementations may require you to explicitly set the SSL Mode to connect with Flow. One example is Neon, which requires the setting verify-full. Check your managed PostgreSQL's documentation for details if you encounter errors related to the SSL mode configuration.  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/#sample","content":" A minimal capture definition will look like the following:  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: &quot;ghcr.io/estuary/source-postgres:dev&quot; config: address: &quot;localhost:5432&quot; database: &quot;postgres&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; bindings: - resource: stream: ${TABLE_NAME} namespace: ${TABLE_NAMESPACE} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}   Your capture definition will likely be more complex, with additional bindings for each table in the source database.  Learn more about capture definitions.  ","version":"Next","tagName":"h3"},{"title":"TOASTed values​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/#toasted-values","content":" PostgreSQL has a hard page size limit, usually 8 KB, for performance reasons. If your tables contain values that exceed the limit, those values can't be stored directly. PostgreSQL uses TOAST (The Oversized-Attribute Storage Technique) to store them separately.  TOASTed values can sometimes present a challenge for systems that rely on the PostgreSQL write-ahead log (WAL), like this connector. If a change event occurs on a row that contains a TOASTed value, but the TOASTed value itself is unchanged, it is omitted from the WAL. As a result, the connector emits a row update with the a value omitted, which might cause unexpected results in downstream catalog tasks if adjustments are not made.  The PostgreSQL connector handles TOASTed values for you when you follow the standard discovery workflowor use the Flow UI to create your capture. It uses merge reductionsto fill in the previous known TOASTed value in cases when that value is omitted from a row update.  However, due to the event-driven nature of certain tasks in Flow, it's still possible to see unexpected results in your data flow, specifically:  When you materialize the captured data to another system using a connector that requires delta updatesWhen you perform a derivation that uses TOASTed values  ","version":"Next","tagName":"h2"},{"title":"Troubleshooting​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/#troubleshooting","content":" If you encounter an issue that you suspect is due to TOASTed values, try the following:  Ensure your collection's schema is using the merge reduction strategy.Set REPLICA IDENTITY to FULL for the table. This circumvents the problem by forcing the WAL to record all values regardless of size. However, this can have performance impacts on your database and must be carefully evaluated.Contact Estuary support for assistance.  ","version":"Next","tagName":"h3"},{"title":"Publications​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/#publications","content":" It is recommended that the publication used by the capture only contain the tables that will be captured. In some cases it may be desirable to create this publication for all tables in the database instead of specific tables, for example using:  CREATE PUBLICATION flow_publication FOR ALL TABLES WITH (publish_via_partition_root = true);   Caution must be used if creating the publication in this way as all existing tables (even those not part of the capture) will be included in it, and if any of them do not have a primary key they will no longer be able to process updates or deletes. ","version":"Next","tagName":"h2"},{"title":"Amazon RDS for PostgreSQL","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/PostgreSQL/amazon-rds-postgres/","content":"","keywords":"","version":"Next"},{"title":"Supported versions and platforms​","type":1,"pageTitle":"Amazon RDS for PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/amazon-rds-postgres/#supported-versions-and-platforms","content":" This connector supports PostgreSQL versions 10.0 and later on major cloud platforms.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon RDS for PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/amazon-rds-postgres/#prerequisites","content":" You'll need a PostgreSQL database setup with the following:  Logical replication enabled — wal_level=logicalUser role with REPLICATION attributeA replication slot. This represents a “cursor” into the PostgreSQL write-ahead log from which change events can be read. Optional; if none exist, one will be created by the connector.If you wish to run multiple captures from the same database, each must have its own slot. You can create these slots yourself, or by specifying a name other than the default in the advanced configuration. A publication. This represents the set of tables for which change events will be reported. In more restricted setups, this must be created manually, but can be created automatically if the connector has suitable permissions. A watermarks table. The watermarks table is a small “scratch space” to which the connector occasionally writes a small amount of data to ensure accuracy when backfilling preexisting table contents. In more restricted setups, this must be created manually, but can be created automatically if the connector has suitable permissions.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Amazon RDS for PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/amazon-rds-postgres/#setup","content":" Allow connections between the database and Estuary Flow. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel. To allow direct access: Modify the database, setting Public accessibility to Yes.Edit the VPC security group associated with your database, or create a new VPC security group and associate it with the database as described in the Amazon documentation.Create a new inbound rule and a new outbound rule that allow all traffic from the Estuary Flow IP addresses. To allow secure connections via SSH tunneling: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networks for additional details and a sample. Enable logical replication on your RDS PostgreSQL instance. Create a parameter group. Create a unique name and description and set the following properties: Family: postgres13Type: DB Parameter group Modify the new parameter group and set rds.logical_replication=1. Associate the parameter group with the database. Reboot the database to allow the new parameter group to take effect. In the PostgreSQL client, connect to your instance and run the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table and publication.  CREATE USER flow_capture WITH PASSWORD 'secret'; GRANT rds_replication TO flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA public TO flow_capture; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO flow_capture; CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication; ALTER PUBLICATION flow_publication SET (publish_via_partition_root = true); ALTER PUBLICATION flow_publication ADD TABLE public.flow_watermarks, &lt;other_tables&gt;;   where &lt;other_tables&gt; lists all tables that will be captured from. The publish_via_partition_rootsetting is recommended (because most users will want changes to a partitioned table to be captured under the name of the root table) but is not required.  In the RDS console, note the instance's Endpoint and Port. You'll need these for the address property when you configure the connector.  ","version":"Next","tagName":"h2"},{"title":"Backfills and performance considerations​","type":1,"pageTitle":"Amazon RDS for PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/amazon-rds-postgres/#backfills-and-performance-considerations","content":" When the PostgreSQL capture is initiated, by default, the connector first backfills, or captures the targeted tables in their current state. It then transitions to capturing change events on an ongoing basis.  This is desirable in most cases, as it ensures that a complete view of your tables is captured into Flow. However, you may find it appropriate to skip the backfill, especially for extremely large tables.  In this case, you may turn off backfilling on a per-table basis. See properties for details.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Amazon RDS for PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/amazon-rds-postgres/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the PostgreSQL source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Amazon RDS for PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/amazon-rds-postgres/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tThe host or host:port at which the database can be reached.\tstring\tRequired /database\tDatabase\tLogical database name to capture from.\tstring\tRequired, &quot;postgres&quot; /user\tUser\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tPassword\tPassword for the specified database user.\tstring\tRequired /advanced\tAdvanced Options\tOptions for advanced users. You should not typically need to modify these.\tobject /advanced/backfill_chunk_size\tBackfill Chunk Size\tThe number of rows which should be fetched from the database in a single backfill query.\tinteger\t4096 /advanced/publicationName\tPublication Name\tThe name of the PostgreSQL publication to replicate from.\tstring\t&quot;flow_publication&quot; /advanced/skip_backfills\tSkip Backfills\tA comma-separated list of fully-qualified table names which should not be backfilled.\tstring /advanced/slotName\tSlot Name\tThe name of the PostgreSQL replication slot to replicate from.\tstring\t&quot;flow_slot&quot; /advanced/watermarksTable\tWatermarks Table\tThe name of the table used for watermark writes during backfills. Must be fully-qualified in '&lt;schema&gt;.&lt;table&gt;' form.\tstring\t&quot;public.flow_watermarks&quot; /advanced/sslmode\tSSL Mode\tOverrides SSL connection behavior by setting the 'sslmode' parameter.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe namespace/schema of the table.\tstring\tRequired /stream\tStream\tTable name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired  SSL Mode​  Certain managed PostgreSQL implementations may require you to explicitly set the SSL Mode to connect with Flow. One example is Neon, which requires the setting verify-full. Check your managed PostgreSQL's documentation for details if you encounter errors related to the SSL mode configuration.  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Amazon RDS for PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/amazon-rds-postgres/#sample","content":" A minimal capture definition will look like the following:  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: &quot;ghcr.io/estuary/source-postgres:dev&quot; config: address: &quot;localhost:5432&quot; database: &quot;postgres&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; bindings: - resource: stream: ${TABLE_NAME} namespace: ${TABLE_NAMESPACE} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}   Your capture definition will likely be more complex, with additional bindings for each table in the source database.  Learn more about capture definitions.  ","version":"Next","tagName":"h3"},{"title":"TOASTed values​","type":1,"pageTitle":"Amazon RDS for PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/amazon-rds-postgres/#toasted-values","content":" PostgreSQL has a hard page size limit, usually 8 KB, for performance reasons. If your tables contain values that exceed the limit, those values can't be stored directly. PostgreSQL uses TOAST (The Oversized-Attribute Storage Technique) to store them separately.  TOASTed values can sometimes present a challenge for systems that rely on the PostgreSQL write-ahead log (WAL), like this connector. If a change event occurs on a row that contains a TOASTed value, but the TOASTed value itself is unchanged, it is omitted from the WAL. As a result, the connector emits a row update with the value omitted, which might cause unexpected results in downstream catalog tasks if adjustments are not made.  The PostgreSQL connector handles TOASTed values for you when you follow the standard discovery workflowor use the Flow UI to create your capture. It uses merge reductionsto fill in the previous known TOASTed value in cases when that value is omitted from a row update.  However, due to the event-driven nature of certain tasks in Flow, it's still possible to see unexpected results in your data flow, specifically:  When you materialize the captured data to another system using a connector that requires delta updatesWhen you perform a derivation that uses TOASTed values  ","version":"Next","tagName":"h2"},{"title":"Troubleshooting​","type":1,"pageTitle":"Amazon RDS for PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/amazon-rds-postgres/#troubleshooting","content":" If you encounter an issue that you suspect is due to TOASTed values, try the following:  Ensure your collection's schema is using the merge reduction strategy.Set REPLICA IDENTITY to FULL for the table. This circumvents the problem by forcing the WAL to record all values regardless of size. However, this can have performance impacts on your database and must be carefully evaluated.Contact Estuary support for assistance.  ","version":"Next","tagName":"h3"},{"title":"Publications​","type":1,"pageTitle":"Amazon RDS for PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/amazon-rds-postgres/#publications","content":" It is recommended that the publication used by the capture only contain the tables that will be captured. In some cases it may be desirable to create this publication for all tables in the database instead of specific tables, for example using:  CREATE PUBLICATION flow_publication FOR ALL TABLES WITH (publish_via_partition_root = true);   Caution must be used if creating the publication in this way as all existing tables (even those not part of the capture) will be included in it, and if any of them do not have a primary key they will no longer be able to process updates or deletes. ","version":"Next","tagName":"h2"},{"title":"Neon PostgreSQL","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/PostgreSQL/neon-postgres/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Neon PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/neon-postgres/#prerequisites","content":" An Estuary Flow account (start free, no credit card required)A Neon account  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Neon PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/neon-postgres/#setup","content":" ","version":"Next","tagName":"h2"},{"title":"1. Enable Logical Replication in Neon​","type":1,"pageTitle":"Neon PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/neon-postgres/#1-enable-logical-replication-in-neon","content":" Enabling logical replication modifies the Postgres wal_level configuration parameter, changing it from replica to logical for all databases in your Neon project. Once the wal_level setting is changed to logical, it cannot be reverted. Enabling logical replication also restarts all computes in your Neon project, meaning active connections will be dropped and have to reconnect.  To enable logical replication in Neon:  Select your project in the Neon Console.On the Neon Dashboard, select Project settings.Select Beta.Click Enable to enable logical replication.  You can verify that logical replication is enabled by running the following query from the Neon SQL Editor:  SHOW wal_level; wal_level ----------- logical   ","version":"Next","tagName":"h3"},{"title":"2. Create a Postgres Role for Replication​","type":1,"pageTitle":"Neon PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/neon-postgres/#2-create-a-postgres-role-for-replication","content":" It is recommended that you create a dedicated Postgres role for replicating data. The role must have the REPLICATION privilege. The default Postgres role created with your Neon project and roles created using the Neon Console, CLI, or API are granted membership in the neon_superuser role, which has the required REPLICATION privilege.  To create a role in the Neon Console:  Navigate to the Neon Console.Select a project.Select Roles.Select the branch where you want to create the role.Click New Role.In the role creation dialog, specify a role name.Click Create. The role is created and you are provided with the password for the role.  Alternatively, the following CLI command creates a role. To view the CLI documentation for this command, see Neon CLI commands — roles.  neon roles create --name &lt;role&gt;   As a third option, the following Neon API method also creates a role. To view the API documentation for this method, refer to the Neon API reference.  curl 'https://console.neon.tech/api/v2/projects/hidden-cell-763301/branches/br-blue-tooth-671580/roles' \\ -H 'Accept: application/json' \\ -H &quot;Authorization: Bearer $NEON_API_KEY&quot; \\ -H 'Content-Type: application/json' \\ -d '{ &quot;role&quot;: { &quot;name&quot;: &quot;cdc_role&quot; } }' | jq   ","version":"Next","tagName":"h3"},{"title":"3. Grant Schema Access to Your Postgres Role​","type":1,"pageTitle":"Neon PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/neon-postgres/#3-grant-schema-access-to-your-postgres-role","content":" If your replication role does not own the schemas and tables you are replicating from, make sure to grant access. Run this commands for each schema:  GRANT pg_read_all_data TO cdc_role;   ","version":"Next","tagName":"h3"},{"title":"4. Create the watermarks table, grant privileges, and create publication:​","type":1,"pageTitle":"Neon PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/neon-postgres/#4-create-the-watermarks-table-grant-privileges-and-create-publication","content":" CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication; ALTER PUBLICATION flow_publication SET (publish_via_partition_root = true); ALTER PUBLICATION flow_publication ADD TABLE public.flow_watermarks, &lt;other_tables&gt;;   The publish_via_partition_rootsetting is recommended (because most users will want changes to a partitioned table to be captured under the name of the root table) but is not required.  Refer to the Postgres docs if you need to add or remove tables from your publication. Alternatively, you also can create a publication FOR ALL TABLES.  Upon start-up, the Estuary Flow connector for Postgres will automatically create the replication slot required for ingesting data change events from Postgres. The slot's name will be prefixed with estuary_, followed by a unique identifier.  To prevent storage bloat, Neon automatically removes inactive replication slots after a period of time if there are other active replication slots. If you have or intend on having more than one replication slot, please see Unused replication slots to learn more.  ","version":"Next","tagName":"h3"},{"title":"Allow Inbound Traffic​","type":1,"pageTitle":"Neon PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/neon-postgres/#allow-inbound-traffic","content":" If you are using Neon's IP Allow feature to limit the IP addresses that can connect to Neon, you will need to allow inbound traffic from Estuary Flow's IP addresses. Refer to the Estuary Flow documentation for the list of IPs that need to be allowlisted for the Estuary Flow region of your account. For information about configuring allowed IPs in Neon, see Configure IP Allow.  ","version":"Next","tagName":"h2"},{"title":"Create a Postgres Source Connector in Estuary Flow​","type":1,"pageTitle":"Neon PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/neon-postgres/#create-a-postgres-source-connector-in-estuary-flow","content":" In the Estuary Flow web UI, select Sources from the left navigation bar and click New Capture. In the connector catalog, choose Neon PostgreSQL and click Connect. Enter the connection details for your Neon database. You can get these details from your Neon connection string, which you'll find in the Connection Details widget on the Dashboard of your Neon project. Your connection string will look like this: postgres://cdc_role:AbC123dEf@ep-cool-darkness-123456.us-east-2.aws.neon.tech/dbname?sslmode=require Enter the details for your connection string into the source connector fields. Based on the sample connection string above, the values would be specified as shown below. Your values will differ. Name: Name of the Capture connectorServer Address: ep-cool-darkness-123456.us-east-2.aws.neon.tech:5432User: cdc_rolePassword: AbC123dEf in the example, or your own value based on the connection string.Database: dbname Click Next. Estuary Flow will now scan the source database for all the tables that can be replicated. Select one or more table(s) by checking the checkbox next to their name. Optionally, you can change the name of the destination name for each table. You can also take a look at the schema of each stream by clicking on the Collection tab. Click Save and Publish to provision the connector and kick off the automated backfill process.  ","version":"Next","tagName":"h2"},{"title":"Backfills and performance considerations​","type":1,"pageTitle":"Neon PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/neon-postgres/#backfills-and-performance-considerations","content":" When the PostgreSQL capture is initiated, by default, the connector first backfills, or captures the targeted tables in their current state. It then transitions to capturing change events on an ongoing basis.  This is desirable in most cases, as it ensures that a complete view of your tables is captured into Flow. However, you may find it appropriate to skip the backfill, especially for extremely large tables.  In this case, you may turn off backfilling on a per-table basis. See properties for details.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Neon PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/neon-postgres/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the PostgreSQL source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Neon PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/neon-postgres/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tThe host or host:port at which the database can be reached.\tstring\tRequired /database\tDatabase\tLogical database name to capture from.\tstring\tRequired, &quot;postgres&quot; /user\tUser\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tPassword\tPassword for the specified database user.\tstring\tRequired /advanced\tAdvanced Options\tOptions for advanced users. You should not typically need to modify these.\tobject /advanced/backfill_chunk_size\tBackfill Chunk Size\tThe number of rows which should be fetched from the database in a single backfill query.\tinteger\t4096 /advanced/publicationName\tPublication Name\tThe name of the PostgreSQL publication to replicate from.\tstring\t&quot;flow_publication&quot; /advanced/skip_backfills\tSkip Backfills\tA comma-separated list of fully-qualified table names which should not be backfilled.\tstring /advanced/slotName\tSlot Name\tThe name of the PostgreSQL replication slot to replicate from.\tstring\t&quot;flow_slot&quot; /advanced/watermarksTable\tWatermarks Table\tThe name of the table used for watermark writes during backfills. Must be fully-qualified in '&lt;schema&gt;.&lt;table&gt;' form.\tstring\t&quot;public.flow_watermarks&quot; /advanced/sslmode\tSSL Mode\tOverrides SSL connection behavior by setting the 'sslmode' parameter.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe namespace/schema of the table.\tstring\tRequired /stream\tStream\tTable name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Neon PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/neon-postgres/#sample","content":" A minimal capture definition will look like the following:  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: &quot;ghcr.io/estuary/source-postgres:dev&quot; config: address: &quot;localhost:5432&quot; database: &quot;postgres&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; bindings: - resource: stream: ${TABLE_NAME} namespace: ${TABLE_NAMESPACE} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}   Your capture definition will likely be more complex, with additional bindings for each table in the source database.  Learn more about capture definitions.  ","version":"Next","tagName":"h3"},{"title":"TOASTed values​","type":1,"pageTitle":"Neon PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/neon-postgres/#toasted-values","content":" PostgreSQL has a hard page size limit, usually 8 KB, for performance reasons. If your tables contain values that exceed the limit, those values can't be stored directly. PostgreSQL uses TOAST (The Oversized-Attribute Storage Technique) to store them separately.  TOASTed values can sometimes present a challenge for systems that rely on the PostgreSQL write-ahead log (WAL), like this connector. If a change event occurs on a row that contains a TOASTed value, but the TOASTed value itself is unchanged, it is omitted from the WAL. As a result, the connector emits a row update with the value omitted, which might cause unexpected results in downstream catalog tasks if adjustments are not made.  The PostgreSQL connector handles TOASTed values for you when you follow the standard discovery workflowor use the Flow UI to create your capture. It uses merge reductionsto fill in the previous known TOASTed value in cases when that value is omitted from a row update.  However, due to the event-driven nature of certain tasks in Flow, it's still possible to see unexpected results in your data flow, specifically:  When you materialize the captured data to another system using a connector that requires delta updatesWhen you perform a derivation that uses TOASTed values  ","version":"Next","tagName":"h2"},{"title":"Troubleshooting​","type":1,"pageTitle":"Neon PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/neon-postgres/#troubleshooting","content":" If you encounter an issue that you suspect is due to TOASTed values, try the following:  Ensure your collection's schema is using the merge reduction strategy.Set REPLICA IDENTITY to FULL for the table. This circumvents the problem by forcing the WAL to record all values regardless of size. However, this can have performance impacts on your database and must be carefully evaluated.Contact Estuary support for assistance.  ","version":"Next","tagName":"h3"},{"title":"Publications​","type":1,"pageTitle":"Neon PostgreSQL","url":"/reference/Connectors/capture-connectors/PostgreSQL/neon-postgres/#publications","content":" It is recommended that the publication used by the capture only contain the tables that will be captured. In some cases it may be desirable to create this publication for all tables in the database instead of specific tables, for example using:  CREATE PUBLICATION flow_publication FOR ALL TABLES WITH (publish_via_partition_root = true);   Caution must be used if creating the publication in this way as all existing tables (even those not part of the capture) will be included in it, and if any of them do not have a primary key they will no longer be able to process updates or deletes. ","version":"Next","tagName":"h2"},{"title":"PostgreSQL Batch Query Connector","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/PostgreSQL/postgres-batch/","content":"PostgreSQL Batch Query Connector This connector captures data from Postgres into Flow collections by periodically executing queries and translating the results into JSON documents. For local development or open-source workflows, ghcr.io/estuary/source-postgres-batch:dev provides the latest version of the connector as a Docker image. You can also follow the link in your browser to see past image versions. We recommend using our PostgreSQL CDC Connector instead if possible. Using CDC provides lower latency data capture, delete and update events, and usually has a smaller impact on the source database. However there are some circumstances where this might not be feasible. Perhaps you need to capture from a managed PostgreSQL instance which doesn't support logical replication. Or perhaps you need to capture the contents of a view or the result of an ad-hoc query. That's the sort of situation this connector is intended for. The number one caveat you need to be aware of when using this connector is that it will periodically execute its update query over and over. At the default polling interval of 5 minutes, a naive SELECT * FROM foo query against a 100 MiB view will produce 30 GiB/day of ingested data, most of it duplicated. This is why the connector's autodiscovery logic only returns ordinary tables of data, because in that particular case we can use the xmin system column as a cursor and ask the database to SELECT xmin, * FROM foo WHERE xmin::text::bigint &gt; $1;. If you start editing these queries or manually adding capture bindings for views or to run ad-hoc queries, you need to either have some way of restricting the query to &quot;just the new rows since last time&quot; or else have your polling interval set high enough that the data rate&lt;DatasetSize&gt; / &lt;PollingInterval&gt; is an amount of data you're willing to deal with.","keywords":"","version":"Next"},{"title":"Supabase","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/PostgreSQL/Supabase/","content":"","keywords":"","version":"Next"},{"title":"Supported versions and platforms​","type":1,"pageTitle":"Supabase","url":"/reference/Connectors/capture-connectors/PostgreSQL/Supabase/#supported-versions-and-platforms","content":" This connector supports all Supabase PostgreSQL instances.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Supabase","url":"/reference/Connectors/capture-connectors/PostgreSQL/Supabase/#prerequisites","content":" You'll need a Supabase PostgreSQL database setup with the following:  A Supabase IPv4 address and direct connection hostname which bypasses the Supabase connection pooler. See Direct Database Connection for details.Logical replication enabled — wal_level=logicalUser role with REPLICATION attributeA replication slot. This represents a “cursor” into the PostgreSQL write-ahead log from which change events can be read. Optional; if none exist, one will be created by the connector.If you wish to run multiple captures from the same database, each must have its own slot. You can create these slots yourself, or by specifying a name other than the default in the advanced configuration. A publication. This represents the set of tables for which change events will be reported. In more restricted setups, this must be created manually, but can be created automatically if the connector has suitable permissions. A watermarks table. The watermarks table is a small “scratch space” to which the connector occasionally writes a small amount of data to ensure accuracy when backfilling preexisting table contents. In more restricted setups, this must be created manually, but can be created automatically if the connector has suitable permissions.  Configuration Tip To configure this connector to capture data from databases hosted on your internal network, you must set up SSH tunneling. For more specific instructions on setup, see configure connections with SSH tunneling.  ","version":"Next","tagName":"h2"},{"title":"Direct Database Connection​","type":1,"pageTitle":"Supabase","url":"/reference/Connectors/capture-connectors/PostgreSQL/Supabase/#direct-database-connection","content":" By default, Supabase guides users into connecting to their database through aConnection Pooler. Connection poolers are helpful for many applications, but unfortunately the pooler does not support the CDC replication features that this connector relies on.  This capture connector requires a direct connection address for your database. This address can be found by navigating to Settings &gt; Database in the Supabase dashboard and then making sure that the Display connection pooler checkbox isunchecked so that the appropriate connection information is shown for a direct connection.  You will also need to configure a dedicated IPv4 addressfor your database, if you have not already done so. This can be configured under Project Settings &gt; Add Ons &gt; Dedicated IPv4 addressin the Supabase dashboard.  ","version":"Next","tagName":"h3"},{"title":"Setup​","type":1,"pageTitle":"Supabase","url":"/reference/Connectors/capture-connectors/PostgreSQL/Supabase/#setup","content":" The simplest way to meet the above prerequisites is to change the WAL level and have the connector use a database superuser role.  For a more restricted setup, create a new user with just the required permissions as detailed in the following steps:  Connect to your instance and create a new user and password:  CREATE USER flow_capture WITH PASSWORD 'secret' REPLICATION;   Assign the appropriate role. If using PostgreSQL v14 or later: GRANT pg_read_all_data TO flow_capture; If using an earlier version: ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES to flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA public, &lt;other_schema&gt; TO flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA information_schema, pg_catalog TO flow_capture; where &lt;other_schema&gt; lists all schemas that will be captured from. info If an even more restricted set of permissions is desired, you can also grant SELECT on just the specific table(s) which should be captured from. The ‘information_schema’ and ‘pg_catalog’ access is required for stream auto-discovery, but not for capturing already configured streams. Create the watermarks table, grant privileges, and create publication:  CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication; ALTER PUBLICATION flow_publication SET (publish_via_partition_root = true); ALTER PUBLICATION flow_publication ADD TABLE public.flow_watermarks, &lt;other_tables&gt;;   where &lt;other_tables&gt; lists all tables that will be captured from. The publish_via_partition_rootsetting is recommended (because most users will want changes to a partitioned table to be captured under the name of the root table) but is not required.  Set WAL level to logical:  ALTER SYSTEM SET wal_level = logical;   Restart PostgreSQL to allow the WAL level change to take effect.  ","version":"Next","tagName":"h2"},{"title":"Backfills and performance considerations​","type":1,"pageTitle":"Supabase","url":"/reference/Connectors/capture-connectors/PostgreSQL/Supabase/#backfills-and-performance-considerations","content":" When the PostgreSQL capture is initiated, by default, the connector first backfills, or captures the targeted tables in their current state. It then transitions to capturing change events on an ongoing basis.  This is desirable in most cases, as it ensures that a complete view of your tables is captured into Flow. However, you may find it appropriate to skip the backfill, especially for extremely large tables.  In this case, you may turn off backfilling on a per-table basis. See properties for details.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Supabase","url":"/reference/Connectors/capture-connectors/PostgreSQL/Supabase/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the PostgreSQL source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Supabase","url":"/reference/Connectors/capture-connectors/PostgreSQL/Supabase/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tThe host or host:port at which the database can be reached.\tstring\tRequired /database\tDatabase\tLogical database name to capture from.\tstring\tRequired, &quot;postgres&quot; /user\tUser\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tPassword\tPassword for the specified database user.\tstring\tRequired /advanced\tAdvanced Options\tOptions for advanced users. You should not typically need to modify these.\tobject /advanced/backfill_chunk_size\tBackfill Chunk Size\tThe number of rows which should be fetched from the database in a single backfill query.\tinteger\t4096 /advanced/publicationName\tPublication Name\tThe name of the PostgreSQL publication to replicate from.\tstring\t&quot;flow_publication&quot; /advanced/skip_backfills\tSkip Backfills\tA comma-separated list of fully-qualified table names which should not be backfilled.\tstring /advanced/slotName\tSlot Name\tThe name of the PostgreSQL replication slot to replicate from.\tstring\t&quot;flow_slot&quot; /advanced/watermarksTable\tWatermarks Table\tThe name of the table used for watermark writes during backfills. Must be fully-qualified in '&lt;schema&gt;.&lt;table&gt;' form.\tstring\t&quot;public.flow_watermarks&quot; /advanced/sslmode\tSSL Mode\tOverrides SSL connection behavior by setting the 'sslmode' parameter.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe namespace/schema of the table.\tstring\tRequired /stream\tStream\tTable name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired  SSL Mode​  Certain managed PostgreSQL implementations may require you to explicitly set the SSL Mode to connect with Flow. One example is Neon, which requires the setting verify-full. Check your managed PostgreSQL's documentation for details if you encounter errors related to the SSL mode configuration.  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Supabase","url":"/reference/Connectors/capture-connectors/PostgreSQL/Supabase/#sample","content":" A minimal capture definition will look like the following:  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: &quot;ghcr.io/estuary/source-postgres:dev&quot; config: address: &quot;localhost:5432&quot; database: &quot;postgres&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; bindings: - resource: stream: ${TABLE_NAME} namespace: ${TABLE_NAMESPACE} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}   Your capture definition will likely be more complex, with additional bindings for each table in the source database.  Learn more about capture definitions.  ","version":"Next","tagName":"h3"},{"title":"TOASTed values​","type":1,"pageTitle":"Supabase","url":"/reference/Connectors/capture-connectors/PostgreSQL/Supabase/#toasted-values","content":" PostgreSQL has a hard page size limit, usually 8 KB, for performance reasons. If your tables contain values that exceed the limit, those values can't be stored directly. PostgreSQL uses TOAST (The Oversized-Attribute Storage Technique) to store them separately.  TOASTed values can sometimes present a challenge for systems that rely on the PostgreSQL write-ahead log (WAL), like this connector. If a change event occurs on a row that contains a TOASTed value, but the TOASTed value itself is unchanged, it is omitted from the WAL. As a result, the connector emits a row update with the value omitted, which might cause unexpected results in downstream catalog tasks if adjustments are not made.  The PostgreSQL connector handles TOASTed values for you when you follow the standard discovery workflowor use the Flow UI to create your capture. It uses merge reductionsto fill in the previous known TOASTed value in cases when that value is omitted from a row update.  However, due to the event-driven nature of certain tasks in Flow, it's still possible to see unexpected results in your data flow, specifically:  When you materialize the captured data to another system using a connector that requires delta updatesWhen you perform a derivation that uses TOASTed values  ","version":"Next","tagName":"h2"},{"title":"Troubleshooting​","type":1,"pageTitle":"Supabase","url":"/reference/Connectors/capture-connectors/PostgreSQL/Supabase/#troubleshooting","content":" If you encounter an issue that you suspect is due to TOASTed values, try the following:  Ensure your collection's schema is using the merge reduction strategy.Set REPLICA IDENTITY to FULL for the table. This circumvents the problem by forcing the WAL to record all values regardless of size. However, this can have performance impacts on your database and must be carefully evaluated.Contact Estuary support for assistance.  ","version":"Next","tagName":"h3"},{"title":"Publications​","type":1,"pageTitle":"Supabase","url":"/reference/Connectors/capture-connectors/PostgreSQL/Supabase/#publications","content":" It is recommended that the publication used by the capture only contain the tables that will be captured. In some cases it may be desirable to create this publication for all tables in the database instead of specific tables, for example using:  CREATE PUBLICATION flow_publication FOR ALL TABLES WITH (publish_via_partition_root = true);   Caution must be used if creating the publication in this way as all existing tables (even those not part of the capture) will be included in it, and if any of them do not have a primary key they will no longer be able to process updates or deletes. ","version":"Next","tagName":"h2"},{"title":"Recharge","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/recharge/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Recharge","url":"/reference/Connectors/capture-connectors/recharge/#supported-data-resources","content":" The following data resources are supported through the Recharge APIs:  AddressesChargesCollectionsCustomersDiscountsMetafieldsOnetimesOrdersProductsShopSubscriptions  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Recharge","url":"/reference/Connectors/capture-connectors/recharge/#prerequisites","content":" Recharge Access Token for authentication.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Recharge","url":"/reference/Connectors/capture-connectors/recharge/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Recharge source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Recharge","url":"/reference/Connectors/capture-connectors/recharge/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/access_token\tAccess Token\tThe value of the Access Token generated.\tstring\tRequired /start_date\tStart Date\tThe date from which you'd like to replicate data for Recharge API, in the format YYYY-MM-DDT00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Recharge project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Recharge","url":"/reference/Connectors/capture-connectors/recharge/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-recharge:dev config: access_token: &lt;secret&gt; start_date: 2017-01-25T00:00:00Z bindings: - resource: stream: addresses syncMode: full_refresh target: ${PREFIX}/addresses {...}  ","version":"Next","tagName":"h3"},{"title":"Salesforce","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/Salesforce/","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Salesforce","url":"/reference/Connectors/capture-connectors/Salesforce/#overview","content":" The Estuary Salesforce Connector facilitates seamless integration between Salesforce and Estuary's data processing framework. With this connector, users can effortlessly sync data from Salesforce objects into Estuary Flow collections. There are two primary types of connectors available: the Realtime Connector and the Historical Connector.  ","version":"Next","tagName":"h2"},{"title":"Salesforce Historical Data​","type":1,"pageTitle":"Salesforce","url":"/reference/Connectors/capture-connectors/Salesforce/#salesforce-historical-data","content":" The Historical Data Connector is designed to capture data from Salesforce objects into Flow collections using batch processing methods. This connector is particularly suited for synchronizing historical Salesforce data. By leveraging batch processing capabilities, it efficiently retrieves and syncs large volumes of historical data, ensuring comprehensive integration with Estuary's data processing workflows.  ","version":"Next","tagName":"h2"},{"title":"Salesforce Real Time Data​","type":1,"pageTitle":"Salesforce","url":"/reference/Connectors/capture-connectors/Salesforce/#salesforce-real-time-data","content":" The Real-time Connector provides a mechanism to capture data from Salesforce objects into Flow collections in real time. It utilizes the Salesforce PushTopic API, which enables the streaming of data changes from Salesforce to Estuary. Leveraging the real-time capabilities of the PushTopic API, this connector ensures that updates and modifications in Salesforce objects are promptly reflected in the corresponding Estuary Flow collections.  ","version":"Next","tagName":"h2"},{"title":"Running Both Connectors in Parallel​","type":1,"pageTitle":"Salesforce","url":"/reference/Connectors/capture-connectors/Salesforce/#running-both-connectors-in-parallel","content":" To combine the capabilities of both connectors, users can create two separate captures: one using the Historical Connector to capture historical data, and the other using the Realtime Connector to capture real-time updates. Both captures can be configured to point to the same Flow collection, effectively merging historical and real-time data within the same destination.  This approach provides a comprehensive solution, allowing users to maintain an up-to-date representation of their Salesforce data while also preserving historical context. By seamlessly integrating historical and real-time data updates, users can leverage the combined power of batch processing and real-time streaming for enhanced data analysis and insights.  For help using both connectors in parallel, contact Estuary's support team. ","version":"Next","tagName":"h2"},{"title":"SendGrid","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/sendgrid/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"SendGrid","url":"/reference/Connectors/capture-connectors/sendgrid/#supported-data-resources","content":" The following data resources are supported through the SendGrid APIs:  CampaignsListsContactsStats automationsSegmentsSingle SendsTemplatesGlobal suppressionSuppression groupsSuppression group membersBlocksBouncesInvalid emailsSpam reports  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"SendGrid","url":"/reference/Connectors/capture-connectors/sendgrid/#prerequisites","content":" SendGrid API Key for authentication.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"SendGrid","url":"/reference/Connectors/capture-connectors/sendgrid/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the SendGrid source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"SendGrid","url":"/reference/Connectors/capture-connectors/sendgrid/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/apikey\tSendgrid API key\tThe value of the SendGrid API Key generated.\tstring\tRequired /start_date\tStart Date\tThe date from which you'd like to replicate data for SendGrid API, in the format YYYY-MM-DDT00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your SendGrid project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"SendGrid","url":"/reference/Connectors/capture-connectors/sendgrid/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-sendgrid:dev config: apikey: &lt;secret&gt; start_date: 2017-01-25T00:00:00Z bindings: - resource: stream: blocks syncMode: incremental target: ${PREFIX}/blocks {...}  ","version":"Next","tagName":"h3"},{"title":"Salesforce — Historical data","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/Salesforce/salesforce-historical-data/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Salesforce — Historical data","url":"/reference/Connectors/capture-connectors/Salesforce/salesforce-historical-data/#supported-data-resources","content":" This connector can capture the following Salesforce standard objects, if present in your account:  AccountContactUserOpportunityFilledHistoryLeadHistoryOpportunityCampaignCaseContactLineItemEntitlementLeadLiveChatTranscriptMessagingSessionQuoteQuoteLineItemServiceAppointmentServiceContractTaskUserServicePresenceWorkOrderWorkOrderLineItem  The Salesforce connector has the ability to capture all standard Salesforce objects as well as custom objects. All available objects will appear after connecting to Salesforce.  Because most Salesforce accounts contain large volumes of data, you may only want to capture a subset of the available objects. There are several ways to control this:  Create a dedicated Salesforce user with access only to the objects you'd like to capture. Apply a filter when you configure the connector. If you don't apply a filter, the connector captures all objects available to the user. During capture creation in the web application, remove the bindings for objects you don't want to capture.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Salesforce — Historical data","url":"/reference/Connectors/capture-connectors/Salesforce/salesforce-historical-data/#prerequisites","content":" ","version":"Next","tagName":"h2"},{"title":"Using OAuth2 to authenticate with Salesforce in the Flow web app​","type":1,"pageTitle":"Salesforce — Historical data","url":"/reference/Connectors/capture-connectors/Salesforce/salesforce-historical-data/#using-oauth2-to-authenticate-with-salesforce-in-the-flow-web-app","content":" If you're using the Flow web app, you'll be prompted to authenticate with Salesforce using OAuth. You'll need the following:  A Salesforce organization on the Enterprise tier, or with an equivalent API request allocation. Salesforce user credentials. We recommend creating a dedicated read-only Salesforce user.  ","version":"Next","tagName":"h3"},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"Salesforce — Historical data","url":"/reference/Connectors/capture-connectors/Salesforce/salesforce-historical-data/#configuring-the-connector-specification-manually","content":" If you're working with flowctl and writing specifications in a local development environment, you'll need to manually supply OAuth credentials. You'll need:  The items required to set up with OAuth2. A Salesforce developer application with a generated client ID, client secret, and refresh token. See setup steps.  ","version":"Next","tagName":"h3"},{"title":"Setup​","type":1,"pageTitle":"Salesforce — Historical data","url":"/reference/Connectors/capture-connectors/Salesforce/salesforce-historical-data/#setup","content":" Create a read-only Salesforce user​  Creating a dedicated read-only Salesforce user is a simple way to specify which objects Flow will capture. This is useful if you have a large amount of data in your Salesforce organization.  While signed in as an administrator, create a new profile by cloning the standard Minimum Access profile. Edit the new profile's permissions. Grant it read access to all the standard and custom objects you'd like to capture with Flow. Create a new user, applying the profile you just created. You'll use this user's email address and password to authenticate Salesforce in Flow.  Create a developer application and generate authorization tokens​  To manually write a capture specification for Salesforce, you need to create and configure a developer application. Through this process, you'll obtain the client ID, client secret, and refresh token.  Create a new developer application. a. When selecting Scopes for your app, select Manage user data via APIs (api), Perform requests at any time (refresh_token, offline_access), and Manage user data via Web browsers (web). Edit the app to ensure that Permitted users is set to All users may self-authorize. Locate the Consumer Key and Consumer Secret. These are equivalent to the client id and client secret, respectively. Follow the Salesforce Web Server Flow. The final POST response will include your refresh token.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Salesforce — Historical data","url":"/reference/Connectors/capture-connectors/Salesforce/salesforce-historical-data/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the Flow specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the batch Salesforce source connector.  ","version":"Next","tagName":"h2"},{"title":"Formula Fields​","type":1,"pageTitle":"Salesforce — Historical data","url":"/reference/Connectors/capture-connectors/Salesforce/salesforce-historical-data/#formula-fields","content":" Estuary is able to capture Salesforce formula fields, however, regular full syncs must be configured in order to ensure up to date formula fields. Given the batch processing of this connector, if formula field values are updated in between syncs, Estuary will not be aware of any changes.  In order to ensure data freshness, it is recommended that you configure your capture to regularly initiate full refreshes of your source. Once a historical backfill is complete, updated formula field values will be reflected within Estuary.  ","version":"Next","tagName":"h3"},{"title":"Slowly Changing Dimensions Type 2​","type":1,"pageTitle":"Salesforce — Historical data","url":"/reference/Connectors/capture-connectors/Salesforce/salesforce-historical-data/#slowly-changing-dimensions-type-2","content":" Estuary is capable of capturing a stream of your Salesforce data as it changes through a feature called Delta Updates. To read more about how Delta Updates works visit our docs.  ","version":"Next","tagName":"h3"},{"title":"Merging Real Time and Batch Data​","type":1,"pageTitle":"Salesforce — Historical data","url":"/reference/Connectors/capture-connectors/Salesforce/salesforce-historical-data/#merging-real-time-and-batch-data","content":" Estuary offers connectors for both batch and real time capture. It is possible to create collections that reflect both batch and real time data capture using the following steps:  Create a new capture using the batch capture connector. Create a new capture using the real time capture connector. Both captures should share the same namespace in order to write to the same collection. For example, you would end up with a batch connector named Tenant/Salesforce/source-salesforce and a real time capture named Tenant/Salesforce/source-salesforce-real-time. If configured correctly, both captures will read data into the same collections that are titled Tenant/Salesforce/your-stream.  Estuary should automatically merge your documents, ensuring that duplicates are not produced when pulling from two captures with the same source. Before continuing, it is important to make sure that a reduction strategy has been implemented for your collections.  This step requires using flowctl, please visit our documentation for more information.  Pull down your active specifications into your local environment using the command flowctl catalog pull-specs --prefix Tenant/Salesforce This command will generate a folder for each subtree of your tenant. Using the above example tenant, you would end up with a top level folder structure named Tenant and a sub folder named Salesforce. Within the sub folder for your Salesforce capture you will find yaml specification files for your each of your collections that follow the naming convention BindingName.schema.yaml. For each newly created collection, make sure that it contains the following reduction strategy:  --- type: object additionalProperties: true properties: Id: type: - string ... required: - Id # Your collection must include this line. If missing, please add below reduce: strategy: merge   If the above line was missing you must also run the command flowctl catalog publish --source flow.yaml at the root level of your local folder structure to publish the changes to Flow.  ","version":"Next","tagName":"h3"},{"title":"Properties​","type":1,"pageTitle":"Salesforce — Historical data","url":"/reference/Connectors/capture-connectors/Salesforce/salesforce-historical-data/#properties","content":" Endpoint​  The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so you won't need the /credentials values listed here.  Property\tTitle\tDescription\tType\tRequired/Default/credentials object\tRequired /credentials/auth_type\tAuthorization type\tSet to Client\tstring /credentials/client_id\tClient ID\tThe Salesforce Client ID, also known as a Consumer Key, for your developer application.\tstring\tRequired /credentials/client_secret\tClient Secret\tThe Salesforce Client Secret, also known as a Consumer Secret, for your developer application.\tstring\tRequired /credentials/refresh_token\tRefresh Token\tThe refresh token generated by your developer application.\tstring\tRequired /is_sandbox\tSandbox\tWhether you're using a Salesforce Sandbox.\tboolean\tfalse /start_date\tStart Date\tStart date in the format YYYY-MM-DD. Data added on and after this date will be captured. If this field is blank, all data will be captured.\tstring /streams_criteria\tFilter Salesforce Objects (Optional)\tFilter Salesforce objects for capture.\tarray /streams_criteria/-/criteria\tSearch criteria\tPossible criteria are &quot;starts with&quot;, &quot;ends with&quot;, &quot;contains&quot;, &quot;exacts&quot;, &quot;starts not with&quot;, &quot;ends not with&quot;, &quot;not contains&quot;, and &quot;not exacts&quot;.\tstring\t&quot;contains&quot; /streams_criteria/-/value\tSearch value\tSearch term used with the selected criterion to filter objects.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/cursorField\tCursor field\tField used as a cursor to track data replication; typically a timestamp field.\tarray, null /stream\tStream\tSalesforce object from which a collection is captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Salesforce — Historical data","url":"/reference/Connectors/capture-connectors/Salesforce/salesforce-historical-data/#sample","content":" This sample specification reflects the manual authentication method.  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-salesforce:dev config: credentials: auth_type: Client client_id: {your_client_id} client_secret: {secret} refresh_token: {XXXXXXXX} is_sandbox: false start_date: 2022-01-01 streams_criteria: - criteria: &quot;starts with&quot; value: &quot;Work&quot; bindings: - resource: cursorField: [SystemModstamp] stream: WorkOrder syncMode: incremental target: ${PREFIX}/WorkOrder - resource: cursorField: [SystemModstamp] stream: WorkOrderLineItem syncMode: incremental target: ${PREFIX}/WorkOrderLineItem  ","version":"Next","tagName":"h3"},{"title":"Salesforce — Real-time data","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/Salesforce/salesforce-real-time/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Salesforce — Real-time data","url":"/reference/Connectors/capture-connectors/Salesforce/salesforce-real-time/#supported-data-resources","content":" This connector can capture the following Salesforce standard objects, if present in your account:  AccountContactUserOpportunityFilledHistoryLeadHistoryOpportunityCampaignCaseContactLineItemEntitlementLeadLiveChatTranscriptMessagingSessionQuoteQuoteLineItemServiceAppointmentServiceContractTaskUserServicePresenceWorkOrderWorkOrderLineItem  Because most Salesforce accounts contain large volumes of data, you may only want to capture a subset of the available objects. There are several ways to control this:  Create a dedicated Salesforce user with access only to the objects you'd like to capture. During capture creation in the web application, remove the bindings for objects you don't want to capture.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Salesforce — Real-time data","url":"/reference/Connectors/capture-connectors/Salesforce/salesforce-real-time/#prerequisites","content":" ","version":"Next","tagName":"h2"},{"title":"Using OAuth2 to authenticate with Salesforce in the Flow web app​","type":1,"pageTitle":"Salesforce — Real-time data","url":"/reference/Connectors/capture-connectors/Salesforce/salesforce-real-time/#using-oauth2-to-authenticate-with-salesforce-in-the-flow-web-app","content":" If you're using the Flow web app, you'll be prompted to authenticate with Salesforce using OAuth. You'll need the following:  A Salesforce organization on the Enterprise tier, or with an equivalent API request allocation. Salesforce user credentials. We recommend creating a dedicated read-only Salesforce user.  ","version":"Next","tagName":"h3"},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"Salesforce — Real-time data","url":"/reference/Connectors/capture-connectors/Salesforce/salesforce-real-time/#configuring-the-connector-specification-manually","content":" If you're working with flowctl and writing specifications in a local development environment, you'll need to manually supply OAuth credentials. You'll need:  The items required to set up with OAuth2. A Salesforce developer application with a generated client ID, client secret, and refresh token. See setup steps.  ","version":"Next","tagName":"h3"},{"title":"Setup​","type":1,"pageTitle":"Salesforce — Real-time data","url":"/reference/Connectors/capture-connectors/Salesforce/salesforce-real-time/#setup","content":" Create a read-only Salesforce user​  Creating a dedicated read-only Salesforce user is a simple way to specify which objects Flow will capture.  While signed in as an administrator, create a new profile by cloning the standard Minimum Access profile. Edit the new profile's permissions. Grant it read access to all the standard and custom objects you'd like to capture with Flow. Create a new user, applying the profile you just created. You'll use this user's email address and password to authenticate Salesforce in Flow.  Create a developer application and generate authorization tokens​  To manually write a capture specification for Salesforce, you need to create and configure a developer application. Through this process, you'll obtain the client ID, client secret, and refresh token.  Create a new developer application. a. When selecting Scopes for your app, select Manage user data via APIs (api), Perform requests at any time (refresh_token, offline_access), and Manage user data via Web browsers (web). Edit the app to ensure that Permitted users is set to All users may self-authorize. Locate the Consumer Key and Consumer Secret. These are equivalent to the client id and client secret, respectively. Follow the Salesforce Web Server Flow. The final POST response will include your refresh token.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Salesforce — Real-time data","url":"/reference/Connectors/capture-connectors/Salesforce/salesforce-real-time/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the real-time Salesforce source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Salesforce — Real-time data","url":"/reference/Connectors/capture-connectors/Salesforce/salesforce-real-time/#properties","content":" Endpoint​  The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so you won't need the /credentials values listed here.  Property\tTitle\tDescription\tType\tRequired/Default/credentials object\tRequired /credentials/client_id\tClient ID\tThe Salesforce Client ID, also known as a Consumer Key, for your developer application.\tstring\tRequired /credentials/client_secret\tClient Secret\tThe Salesforce Client Secret, also known as a Consumer Secret, for your developer application.\tstring\tRequired /credentials/refresh_token\tRefresh Token\tThe refresh token generated by your developer application.\tstring\tRequired /is_sandbox\tSandbox\tWhether you're using a Salesforce Sandbox.\tboolean\tfalse  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tSalesforce object from which a collection is captured.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Salesforce — Real-time data","url":"/reference/Connectors/capture-connectors/Salesforce/salesforce-real-time/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-salesforce-next:dev config: credentials: client_id: {your_client_id} client_secret: {secret} refresh_token: {XXXXXXXX} is_sandbox: false bindings: - resource: stream: WorkOrder target: ${PREFIX}/WorkOrder - resource: stream: WorkOrderLineItem target: ${PREFIX}/WorkOrderLineItem  ","version":"Next","tagName":"h3"},{"title":"Sentry","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/sentry/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Sentry","url":"/reference/Connectors/capture-connectors/sentry/#supported-data-resources","content":" The following data resources are supported through the Sentry APIs:  EventsIssuesProjectsReleases  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Sentry","url":"/reference/Connectors/capture-connectors/sentry/#prerequisites","content":" To set up the Sentry source connector, you'll need the Sentry project name, authentication token, and organization.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Sentry","url":"/reference/Connectors/capture-connectors/sentry/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Sentry source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Sentry","url":"/reference/Connectors/capture-connectors/sentry/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/auth_token\tAuth Token\tAuth Token generated in Sentry\tstring\tRequired /organization\tOrganization\tThe slug of the organization the groups belong to.\tstring\tRequired /project\tProject\tThe name (slug) of the Project you want to sync.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Sentry project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Sentry","url":"/reference/Connectors/capture-connectors/sentry/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-sentry:dev config: auth_token: &lt;secret&gt; organization: &lt;your organization&gt; project: &lt;your project&gt; bindings: - resource: stream: events syncMode: full_refresh target: ${PREFIX}/events {...}  ","version":"Next","tagName":"h3"},{"title":"Shopify (GraphQL)","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/shopify-native/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Shopify (GraphQL)","url":"/reference/Connectors/capture-connectors/shopify-native/#supported-data-resources","content":" The following data resources are supported through the Shopify API:  Products  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Shopify (GraphQL)","url":"/reference/Connectors/capture-connectors/shopify-native/#prerequisites","content":" Store ID of your Shopify account. This is the prefix of your admin URL. For example, https://{store_id}.myshopify.com/admin  You can authenticate your account either via OAuth or with a Shopify access token.  ","version":"Next","tagName":"h2"},{"title":"Access Token Permissions​","type":1,"pageTitle":"Shopify (GraphQL)","url":"/reference/Connectors/capture-connectors/shopify-native/#access-token-permissions","content":" If authenticating with an access token, ensure the following permissions are granted:  read_checkoutsread_customersread_fulfillmentsread_inventoryread_localesread_locationsread_ordersread_productsread_publications  ","version":"Next","tagName":"h3"},{"title":"Bulk Query Operation Limitations​","type":1,"pageTitle":"Shopify (GraphQL)","url":"/reference/Connectors/capture-connectors/shopify-native/#bulk-query-operation-limitations","content":" This connector submits and process the results of bulk query operations to capture data. Shopify only allows a single bulk query operation to run at a given time.To ensure the connector can successfully submit bulk queries, ensure no other applications are submitting bulk query operations for your Shopify store.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Shopify (GraphQL)","url":"/reference/Connectors/capture-connectors/shopify-native/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Shopify source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Shopify (GraphQL)","url":"/reference/Connectors/capture-connectors/shopify-native/#properties","content":" Endpoint​  The properties in the table below reflect manual authentication using the CLI. In the Flow web app, you'll sign in directly and won't need the access token.  Property\tTitle\tDescription\tType\tRequired/Default/store\tStore ID\tYour Shopify Store ID. Use the prefix of your admin URL e.g. https://{store_id}.myshopify.com/admin.\tstring\tRequired /start_date\tStart date\tUTC date and time in the format 2025-01-16T00:00:00Z. Any data before this date will not be replicated.\tstring\t30 days before the present date /credentials/access_token\tAccess Token\tShopify access token.\tstring\tRequired /credentials/credentials_title\tCredentials\tName of the credentials set\tstring\tRequired /advanced/window_size\tWindow size\tWindow size in days for incrementals streams. Typically left as the default unless more frequent checkpoints are desired.\tinteger\t30  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/name\tData resource\tName of the data resource.\tstring\tRequired /interval\tInterval\tInterval between data syncs\tstring\t  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Shopify (GraphQL)","url":"/reference/Connectors/capture-connectors/shopify-native/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-shopify-native:dev config: advanced: window_size: 30 credentials: credentials_title: Private App Credentials access_token: &lt;secret&gt; start_date: &quot;2025-01-16T12:00:00Z&quot; store: &lt;store ID&gt; bindings: - resource: name: products target: ${PREFIX}/products  ","version":"Next","tagName":"h3"},{"title":"SFTP","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/sftp/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"SFTP","url":"/reference/Connectors/capture-connectors/sftp/#prerequisites","content":" You'll need an SFTP server that can accept connections from the Estuary Flow IP addresses using password authentication.  ","version":"Next","tagName":"h2"},{"title":"Subdirectories and Symbolic Links​","type":1,"pageTitle":"SFTP","url":"/reference/Connectors/capture-connectors/sftp/#subdirectories-and-symbolic-links","content":" The connector must be configured with a Directory to capture files from. It will also descend into and capture files in normal subdirectories of the configured Directory.  Symbolic links to files are captured from the referent files. Symbolic links to subdirectories are not captured, although the configured Directory may itself be a symbolic link.  ","version":"Next","tagName":"h2"},{"title":"File Capturing Order​","type":1,"pageTitle":"SFTP","url":"/reference/Connectors/capture-connectors/sftp/#file-capturing-order","content":" The standard mode of operation for the connector is to capture files according to their modification time. All files available on the server will initially be captured, and on an on-going basis new files that are added to the server are captured incrementally. New files added to the server are captured based on their modification time: If the connector finds a file with a more recent modification time than any previously observed, it will be captured. This means that any actions that update the modification time of a file on the server may cause it to be re-captured. For symbolic links to files the modification time of referent file is used, not of the symbolic link.  Alternatively, the advanced option Ascending Keys may be set. In this mode of operation the connector processes files strictly based on their path. New files are captured if they have a path lexically greater than any previously captured file. Lexical ordering considers the full path of the file.  As an example, consider a directory structure like the following with a data file initially in the directory /root/b/:  /root/ a/ b/data.csv c/   In the normal mode of operation (Ascending Keys not set) the path /root/b/data.csv will initially be captured. Any added files will be captured by the connector on an on-going basis as they have increasingly more recent modification times.With Ascending Keys set the path /root/b/data.csv will initially be captured, but after that only added files in a higher lexical order will be captured: Any file added to the directory /root/a/ will not be captured, becuase /root/a/ comes before /root/b/.Any file added to the directory /root/c/ will captured, because /root/c/ comes after /root/b/.A file added to the directory /root/b/ may be captured if its name comes after data.csv.This ordering applies on an on-going basis. If a file is added to /root/c/, after that only files with a higher lexical ordering than that file to be captured.  Setting Ascending Keys is only recommended if you have strict control over the naming of files and can ensure they are added in increasing lexical ordering.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"SFTP","url":"/reference/Connectors/capture-connectors/sftp/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the SFTP source connector.  Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tHost and port of the SFTP server. Example: myserver.com:22\tstring\tRequired /username\tUsername\tUsername for authentication.\tstring\tRequired /password\tPassword\tPassword for authentication. Only one of Password or SSHKey must be provided.\tstring /sshKey\tSSH Key\tSSH Key for authentication. Only one of Password or SSHKey must be provided.\tstring /directory\tDirectory\tDirectory to capture files from. All files in this directory and any subdirectories will be included.\tstring\tRequired /matchFiles\tMatch Files Regex\tFilter applied to all file names in the directory. If provided, only files whose path (relative to the directory) matches this regex will be captured. For example, you can use .*\\.json to only capture json files.\tstring /advanced Options for advanced users. You should not typically need to modify these.\tobject /advanced/ascendingKeys\tAscending Keys\tMay improve sync speeds by listing files from the end of the last sync, rather than listing all files in the configured directory. This requires that you write files in ascending lexicographic order, such as an RFC-3339 timestamp, so that lexical path ordering matches modification time ordering.\tboolean\tfalse /parser\tParser Configuration\tConfigures how files are parsed (optional, see below)\tobject /parser/compression\tCompression\tDetermines how to decompress the contents. The default, 'Auto', will try to determine the compression automatically.\tnull, string\tnull /parser/format\tFormat\tDetermines how to parse the contents. The default, 'Auto', will try to determine the format automatically based on the file extension or MIME type, if available.\tobject\t{&quot;type&quot;:&quot;auto&quot;}  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tPrefix\tPath to the captured directory.\tstring\tRequired  ","version":"Next","tagName":"h2"},{"title":"Sample​","type":1,"pageTitle":"SFTP","url":"/reference/Connectors/capture-connectors/sftp/#sample","content":" captures: ${CAPTURE_NAME}: endpoint: connector: image: &quot;ghcr.io/estuary/source-sftp:dev&quot; config: address: myserver.com:22 username: &lt;SECRET&gt; password: &lt;SECRET&gt; directory: /data parser: compression: zip format: type: csv config: delimiter: &quot;,&quot; encoding: UTF-8 errorThreshold: 5 headers: [ID, username, first_name, last_name] lineEnding: &quot;\\\\r&quot; quote: '&quot;' bindings: - resource: stream: /data target: ${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Advanced: Parsing SFTP Files​","type":1,"pageTitle":"SFTP","url":"/reference/Connectors/capture-connectors/sftp/#advanced-parsing-sftp-files","content":" SFTP servers can support a wider variety of file types than other data source systems. For each of these file types, Flow must parse and translate data into collections with defined fields and JSON schemas.  By default, the parser will automatically detect the type and shape of the data in your bucket, so you won't need to change the parser configuration for most captures.  However, the automatic detection may be incorrect in some cases. To fix or prevent this, you can provide explicit information in the parser configuration, which is part of the endpoint configuration for this connector.  The parser configuration includes:  Compression: Specify how the bucket contents are compressed. If no compression type is specified, the connector will try to determine the compression type automatically. Options are: zipgzipzstdnone Format: Specify the data format, which determines how it will be parsed. Options are: Auto: If no format is specified, the connector will try to determine it automatically.AvroCSVJSONProtobufW3C Extended Log info At this time, Flow only supports SFTP captures with data of a single file type. Support for multiple file types, which can be configured on a per-binding basis, will be added in the future. For now, use a prefix in the endpoint configuration to limit the scope of each capture to data of a single file type.  CSV configuration​  CSV files include several additional properties that are important to the parser. In most cases, Flow is able to automatically determine the correct values, but you may need to specify for unusual datasets. These properties are:  Delimiter. Options are: Comma (&quot;,&quot;)Pipe (&quot;|&quot;)Space (&quot;0x20&quot;)Semicolon (&quot;;&quot;)Tab (&quot;0x09&quot;)Vertical tab (&quot;0x0B&quot;)Unit separator (&quot;0x1F&quot;)SOH (&quot;0x01&quot;)Auto Encoding type, specified by its WHATWG label. Optionally, an Error threshold, as an acceptable percentage of errors. If set to a number greater than zero, malformed rows that fall within the threshold will be excluded from the capture. Escape characters. Options are: Backslash (&quot;\\\\&quot;)Disable escapes (&quot;&quot;)Auto Optionally, a list of column Headers, if not already included in the first row of the CSV file. If any headers are provided, it is assumed that the provided list of headers is complete and authoritative. The first row of your CSV file will be assumed to be data (not headers), and you must provide a header value for every column in the file. Line ending values CRLF (&quot;\\\\r\\\\n&quot;) (Windows)CR (&quot;\\\\r&quot;)LF (&quot;\\\\n&quot;)Record Separator (&quot;0x1E&quot;)Auto Quote character Double Quote (&quot;\\&quot;&quot;)Single Quote (&quot;)Disable Quoting (&quot;&quot;)Auto  The sample specification above includes these fields. ","version":"Next","tagName":"h3"},{"title":"Shopify","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/shopify/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Shopify","url":"/reference/Connectors/capture-connectors/shopify/#supported-data-resources","content":" The following data resources are supported through the Shopify APIs:  ","version":"Next","tagName":"h2"},{"title":"Default Streams​","type":1,"pageTitle":"Shopify","url":"/reference/Connectors/capture-connectors/shopify/#default-streams","content":" Abandoned CheckoutsCollectsCustom CollectionsCustomersInventory ItemInventory LevelsLocationsMetafieldsOrdersProductsTransactions  ","version":"Next","tagName":"h3"},{"title":"Shopify Plus Streams​","type":1,"pageTitle":"Shopify","url":"/reference/Connectors/capture-connectors/shopify/#shopify-plus-streams","content":" User  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h3"},{"title":"Prerequisites​","type":1,"pageTitle":"Shopify","url":"/reference/Connectors/capture-connectors/shopify/#prerequisites","content":" Store ID of your Shopify account. Use the prefix of your admin URL. For example, https://{store_id}.myshopify.com/admin.  You can authenticate your account either via OAuth or using a Shopify access token.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Shopify","url":"/reference/Connectors/capture-connectors/shopify/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Shopify source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Shopify","url":"/reference/Connectors/capture-connectors/shopify/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/credentials\tCredentials object\tRequired /credentials/auth_type\tAuthentication Type\tCan either be oauth or access_token.\tstring\tRequired /credentials/client_id\tClient ID\tThe Client ID for Shopify OAuth.\tstring\tRequired when using the oauth Auth Type /credentials/client_secret\tClient Secret\tThe Client Secret for Shopify OAuth.\tstring\tRequired when using the oauth Auth Type /credentials/access_token\tAccess Token\tThe access token to authenticate with the Shopify API.\tstring\tRequired /store\tStore ID\tShopify Store ID, such as from the prefix in https://{store_id}.myshopify.com/admin.\tstring\tRequired /start_date\tStart Date\tUTC date in the format 2020-01-01. Any data before this date will not be replicated.\tstring\tRequired, 2020-01-01 /admin_url\tAdmin URL\tThe Admin URL for the Shopify store (overrides 'store' property).\tstring /is_plus_account\tIs Plus Account\tEnables Shopify plus account endpoints.\tboolean\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Shopify project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Shopify","url":"/reference/Connectors/capture-connectors/shopify/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-shopify:dev config: credentials: auth_type: access_token access_token: &lt;secret&gt; store: &lt;store ID&gt; is_plus_account: false start_date: 2020-01-01 bindings: - resource: stream: transactions syncMode: full_refresh target: ${PREFIX}/transactions {...}  ","version":"Next","tagName":"h3"},{"title":"Slack","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/slack/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Slack","url":"/reference/Connectors/capture-connectors/slack/#supported-data-resources","content":" The following data resources are supported through the Slack APIs:  Channels (Conversations)Channel Members (Conversation Members)Messages (Conversation History)UsersThreads (Conversation Replies)User GroupsFilesRemote Files  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Slack","url":"/reference/Connectors/capture-connectors/slack/#prerequisites","content":" Slack workspace URL or API token for authentication.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Slack","url":"/reference/Connectors/capture-connectors/slack/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Slack source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Slack","url":"/reference/Connectors/capture-connectors/slack/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/join_channels\tJoin Channels\tWhether to join all channels\tboolean\ttrue /lookback_window\tThreads Lookback window (Days)\tHow far into the past to look for messages in threads.\tinteger\tRequired /start_date\tStart Date\tUTC date and time in the format 2021-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Slack project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Slack","url":"/reference/Connectors/capture-connectors/slack/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-slack:dev config: credentials: auth_type: OAuth access_token: {secret} client_id: {your_client_id} client_secret: {secret} join_channels: true lookback_window: 7 start_date: 2017-01-25T00:00:00Z bindings: - resource: stream: channel_members syncMode: full_refresh target: ${PREFIX}/channel_members {...}  ","version":"Next","tagName":"h3"},{"title":"Snapchat Marketing","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/snapchat/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Snapchat Marketing","url":"/reference/Connectors/capture-connectors/snapchat/#supported-data-resources","content":" This connector can be used to sync the following tables from Snapchat:  AdaccountsAdsAdsquadsCampaignsCreativesMediaOrganizationsSegmentsAdaccountsStatsHourlyAdaccountsStatsDailyAdaccountsStatsLifetimeAdsStatsHourlyAdsStatsDailyAdsStatsLifetimeAdsquadsStatsDailyAdsquadsStatsLifetimeCampaignsStatsHourlyCampaignsStatsDailyCampaignsStatsLifetime  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Snapchat Marketing","url":"/reference/Connectors/capture-connectors/snapchat/#prerequisites","content":" A Snapchat Marketing account with permission to access data from accounts you want to sync.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Snapchat Marketing","url":"/reference/Connectors/capture-connectors/snapchat/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Snapchat source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Snapchat Marketing","url":"/reference/Connectors/capture-connectors/snapchat/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/start_date\tStart Date\tUTC date and time in the format 2021-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tDefault /end_date\tEnd Date\tUTC date and time in the format 2021-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Snapchat project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Snapchat Marketing","url":"/reference/Connectors/capture-connectors/snapchat/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-snapchat:dev config: start_date: 2017-01-25T00:00:00Z end_date: 2018-01-25T00:00:00Z bindings: - resource: stream: lists syncMode: full_refresh target: ${PREFIX}/lists {...}  ","version":"Next","tagName":"h3"},{"title":"Snowflake CDC Connector","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/snowflake/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Snowflake CDC Connector","url":"/reference/Connectors/capture-connectors/snowflake/#prerequisites","content":" To use this connector, you'll need:  A Snowflake account that includes: A target database containing the tables you want to capture from.A virtual warehouse which the connector can use to execute queries.A schema which will hold streams and staging tables managed by the connector. The default name for this schema is ESTUARY_STAGING unless overridden in the capture's advanced configuration.A user with access grants for these resources, as well as authorization to read from the desired source tables, and to create streams and transient tables in the staging schema based on the source tables. The host URL for your Snowflake account. This is formatted using your Snowflake account identifier, and might look something like sg31386.snowflakecomputing.com or df98701.us-central1.gcp.snowflakecomputing.com.  See the script below for details.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Snowflake CDC Connector","url":"/reference/Connectors/capture-connectors/snowflake/#setup","content":" To set up a user account and warehouse for use with the Snowflake CDC connector, copy and paste the following script into the Snowflake SQL editor. Modify the variable declarations in the first few lines to set the password and optionally customize the names involved.  set database_name = 'SOURCE_DB'; -- The database to capture from set warehouse_name = 'ESTUARY_WH'; -- The warehouse to execute queries in set estuary_user = 'ESTUARY_USER'; -- The name of the capture user set estuary_password = 'secret'; -- The password of the capture user set estuary_role = 'ESTUARY_ROLE'; -- A role for the capture user's permissions -- Create a role and user for Estuary create role if not exists identifier($estuary_role); grant role identifier($estuary_role) to role SYSADMIN; create user if not exists identifier($estuary_user) password = $estuary_password default_role = $estuary_role default_warehouse = $warehouse_name; grant role identifier($estuary_role) to user identifier($estuary_user); -- Create a warehouse for Estuary and grant access to it create warehouse if not exists identifier($warehouse_name) warehouse_size = xsmall warehouse_type = standard auto_suspend = 60 auto_resume = true initially_suspended = true; grant USAGE on warehouse identifier($warehouse_name) to role identifier($estuary_role); -- Grant Estuary access to read from all tables in the database and to create a staging schema grant CREATE SCHEMA, MONITOR, USAGE on database identifier($database_name) to role identifier($estuary_role); grant USAGE on future schemas in database identifier($database_name) to role identifier($estuary_role); grant USAGE on all schemas in database identifier($database_name) to role identifier($estuary_role); grant SELECT on future tables in database identifier($database_name) to role identifier($estuary_role); grant SELECT on all tables in database identifier($database_name) to role identifier($estuary_role); commit;   Be sure to run the entire script with the &quot;Run All&quot; option.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Snowflake CDC Connector","url":"/reference/Connectors/capture-connectors/snowflake/#configuration","content":" You can configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Snowflake CDC source connector.  ","version":"Next","tagName":"h2"},{"title":"Endpoint Properties​","type":1,"pageTitle":"Snowflake CDC Connector","url":"/reference/Connectors/capture-connectors/snowflake/#endpoint-properties","content":" Property\tTitle\tDescription\tType\tRequired/Default/host\tHost URL\tThe Snowflake Host used for the connection. Example: orgname-accountname.snowflakecomputing.com (do not include the protocol).\tstring\tRequired /account\tAccount\tThe Snowflake account identifier\tstring\tRequired /database\tDatabase\tThe name of the Snowflake database to capture from\tstring\tRequired /user\tUser\tThe Snowflake user login name\tstring\tRequired /password\tPassword\tThe password for the specified login user\tstring\tRequired /warehouse\tWarehouse\tThe Snowflake virtual warehouse used to execute queries. The default warehouse for the user will be used if this is blank.\tstring /advanced\tAdvanced Options\tOptions for advanced users. You should not typically need to modify these.\tobject /advanced/flowSchema\tFlow Schema\tThe schema in which Flow will create and manage its streams and staging tables.\tstring\tESTUARY_STAGING  ","version":"Next","tagName":"h3"},{"title":"Binding Properties​","type":1,"pageTitle":"Snowflake CDC Connector","url":"/reference/Connectors/capture-connectors/snowflake/#binding-properties","content":" Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tThe name of the table to capture\tstring\tRequired /schema\tSchema\tThe schema in which the table resides\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Polling Interval​","type":1,"pageTitle":"Snowflake CDC Connector","url":"/reference/Connectors/capture-connectors/snowflake/#polling-interval","content":" Keeping a Snowflake compute warehouse active 24/7 can be prohibitively expensive for many users, so the Snowflake CDC connector is designed to poll for changes at a configurable interval, at which time it will capture into Flow all new changes since the previous execution. This polling interval is set to 5 minutes by default, in an attempt to strike a balance between cost savings while still providing &quot;good enough&quot; capture latency for most streaming uses. The interval may be configured by editing the task spec interval property as described here.  Specifying a smaller interval can provide even lower capture latencies but is likely to incur higher costs for Snowflake warehouse usage. A higher interval will reduce Snowflake costs by allowing the warehouse to be idle for longer, in cases where it's okay for the captured data to lag the source dataset by a few hours. Note that regardless of the polling interval the output collections will contain an accurate representation of the source tables up to some moment in time, the interval merely controls how frequent and fine-grained the updates are.  ","version":"Next","tagName":"h3"},{"title":"Sample Configuration​","type":1,"pageTitle":"Snowflake CDC Connector","url":"/reference/Connectors/capture-connectors/snowflake/#sample-configuration","content":" captures: ${prefix}/source-snowflake: endpoint: connector: image: ghcr.io/estuary/source-snowflake:v1 config: host: cf22902.us-central1.gcp.snowflakecomputing.com account: cf22902 database: SOURCE_DB user: ESTUARY_USER password: secret bindings: - resource: schema: ${schema_name} table: ${table_name} target: ${prefix}/collection_name interval: 30m  ","version":"Next","tagName":"h3"},{"title":"Google Cloud SQL for SQL Server","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/SQLServer/google-cloud-sql-sqlserver/","content":"","keywords":"","version":"Next"},{"title":"Supported versions and platforms​","type":1,"pageTitle":"Google Cloud SQL for SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/google-cloud-sql-sqlserver/#supported-versions-and-platforms","content":" This connector is designed for databases using any version of SQL Server which has CDC support, and is regularly tested against SQL Server 2017 and up.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Google Cloud SQL for SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/google-cloud-sql-sqlserver/#prerequisites","content":" To capture change events from SQL Server tables using this connector, you need:  For each table to be captured, a primary key should be specified in the database. If a table doesn't have a primary key, you must manually specify a key in the associated Flow collection definition while creating the capture.See detailed steps. CDC enabledon the database and the individual tables to be captured. (This creates change tables in the database, from which the connector reads.) A watermarks table. The watermarks table is a small “scratch space” to which the connector occasionally writes a small amount of data to ensure accuracy when backfilling preexisting table contents. A user role with: SELECT permissions on the CDC schema and the schemas that contain tables to be captured.Access to the change tables created as part of the SQL Server CDC process.SELECT, INSERT, and UPDATE permissions on the watermarks table  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Google Cloud SQL for SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/google-cloud-sql-sqlserver/#setup","content":" Allow connections between the database and Estuary Flow. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel. To allow direct access: Enable public IP on your database and add the Estuary Flow IP addresses as authorized IP addresses. To allow secure connections via SSH tunneling: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networks for additional details and a sample. In your SQL client, connect to your instance as the default sqlserver user and issue the following commands.  USE &lt;database&gt;; -- Enable CDC for the database. EXEC msdb.dbo.gcloudsql_cdc_enable_db '&lt;database&gt;'; -- Create user and password for use with the connector. CREATE LOGIN flow_capture WITH PASSWORD = 'Secret123!'; CREATE USER flow_capture FOR LOGIN flow_capture; -- Grant the user permissions on the CDC schema and schemas with data. -- This assumes all tables to be captured are in the default schema, `dbo`. -- Add similar queries for any other schemas that contain tables you want to capture. GRANT SELECT ON SCHEMA :: dbo TO flow_capture; GRANT SELECT ON SCHEMA :: cdc TO flow_capture; -- Create the watermarks table and grant permissions. CREATE TABLE dbo.flow_watermarks(slot INTEGER PRIMARY KEY, watermark TEXT); GRANT SELECT, INSERT, UPDATE ON dbo.flow_watermarks TO flow_capture; -- Enable CDC on tables. The below query enables CDC the watermarks table ONLY. -- You should add similar query for all other tables you intend to capture. EXEC sys.sp_cdc_enable_table @source_schema = 'dbo', @source_name = 'flow_watermarks', @role_name = 'flow_capture';   In the Cloud Console, note the instance's host under Public IP Address. Its port will always be 1433. Together, you'll use the host:port as the address property when you configure the connector.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Google Cloud SQL for SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/google-cloud-sql-sqlserver/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the SQL Server source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Google Cloud SQL for SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/google-cloud-sql-sqlserver/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tServer Address\tThe host or host:port at which the database can be reached.\tstring\tRequired /database\tDatabase\tLogical database name to capture from.\tstring\tRequired /user\tUser\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tPassword\tPassword for the specified database user.\tstring\tRequired /advanced\tAdvanced Options\tOptions for advanced users. You should not typically need to modify these.\tobject /advanced/backfill_chunk_size\tBackfill Chunk Size\tThe number of rows which should be fetched from the database in a single backfill query.\tinteger\t4096 /advanced/skip_backfills\tSkip Backfills\tA comma-separated list of fully-qualified table names which should not be backfilled.\tstring /advanced/watermarksTable\tWatermarks Table\tThe name of the table used for watermark writes during backfills. Must be fully-qualified in '&lt;schema&gt;.&lt;table&gt;' form.\tstring\t&quot;dbo.flow_watermarks&quot;  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe namespace/schema of the table.\tstring\tRequired /stream\tStream\tTable name.\tstring\tRequired /primary_key\tPrimary Key Columns\tarray\tThe columns which together form the primary key of the table.\t  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Google Cloud SQL for SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/google-cloud-sql-sqlserver/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: &quot;ghcr.io/estuary/source-sqlserver:dev&quot; config: address: &quot;&lt;host&gt;:1433&quot; database: &quot;my_db&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; bindings: - resource: stream: ${TABLE_NAME} namespace: dbo primary_key: [&quot;id&quot;] target: ${PREFIX}/${COLLECTION_NAME}   Your capture definition will likely be more complex, with additional bindings for each table in the source database.  Learn more about capture definitions.  ","version":"Next","tagName":"h3"},{"title":"Specifying Flow collection keys​","type":1,"pageTitle":"Google Cloud SQL for SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/google-cloud-sql-sqlserver/#specifying-flow-collection-keys","content":" Every Flow collection must have a key. As long as your SQL Server tables have a primary key specified, the connector will set the corresponding collection's key accordingly.  In cases where a SQL Server table you want to capture doesn't have a primary key, you can manually add it to the collection definition during the capture creation workflow.  After you input the endpoint configuration and click Next, the tables in your database have been mapped to Flow collections. Click each collection's Specification tab and identify a collection where &quot;key&quot;: [ ], is empty. Click inside the empty key value in the editor and input the name of column in the table to use as the key, formatted as a JSON pointer. For example &quot;key&quot;: [&quot;/foo&quot;], Make sure the key field is required, not nullable, and of an allowed type. Make any other necessary changes to the collection specification to accommodate this. Repeat with other missing collection keys, if necessary. Save and publish the capture as usual. ","version":"Next","tagName":"h2"},{"title":"Microsoft SQL Server","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/SQLServer/","content":"","keywords":"","version":"Next"},{"title":"Supported versions and platforms​","type":1,"pageTitle":"Microsoft SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/#supported-versions-and-platforms","content":" This connector will work on both hosted deployments and all major cloud providers. It is designed for databases using any version of SQL Server which has CDC support, and is regularly tested against SQL Server 2017 and up.  Setup instructions are provided for the following platforms:  Self-hosted SQL ServerAzure SQL DatabaseAmazon RDS for SQL ServerGoogle Cloud SQL for SQL Server  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Microsoft SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/#prerequisites","content":" To capture change events from SQL Server tables using this connector, you need:  For each table to be captured, a primary key should be specified in the database. If a table doesn't have a primary key, you must manually specify a key in the associated Flow collection definition while creating the capture.See detailed steps. CDC enabledon the database and the individual tables to be captured. (This creates change tables in the database, from which the connector reads.) A watermarks table. The watermarks table is a small “scratch space” to which the connector occasionally writes a small amount of data to ensure accuracy when backfilling preexisting table contents. A user role with: SELECT permissions on the CDC schema and the schemas that contain tables to be captured.Access to the change tables created as part of the SQL Server CDC process.SELECT, INSERT, and UPDATE permissions on the watermarks table.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Microsoft SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/#setup","content":" To meet these requirements, follow the steps for your hosting type.  Self-hosted SQL ServerAzure SQL DatabaseAmazon RDS for SQL ServerGoogle Cloud SQL for SQL Server  ","version":"Next","tagName":"h2"},{"title":"Self-hosted SQL Server​","type":1,"pageTitle":"Microsoft SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/#self-hosted-sql-server","content":" Connect to the server and issue the following commands:  USE &lt;database&gt;; -- Enable CDC for the database. EXEC sys.sp_cdc_enable_db; -- Create user and password for use with the connector. CREATE LOGIN flow_capture WITH PASSWORD = 'secret'; CREATE USER flow_capture FOR LOGIN flow_capture; -- Grant the user permissions on the CDC schema and schemas with data. -- This assumes all tables to be captured are in the default schema, `dbo`. -- Add similar queries for any other schemas that contain tables you want to capture. GRANT SELECT ON SCHEMA :: dbo TO flow_capture; GRANT SELECT ON SCHEMA :: cdc TO flow_capture; -- Create the watermarks table and grant permissions. CREATE TABLE dbo.flow_watermarks(slot INTEGER PRIMARY KEY, watermark TEXT); GRANT SELECT, INSERT, UPDATE ON dbo.flow_watermarks TO flow_capture; -- Enable CDC on tables. The below query enables CDC the watermarks table ONLY. -- You should add similar query for all other tables you intend to capture. EXEC sys.sp_cdc_enable_table @source_schema = 'dbo', @source_name = 'flow_watermarks', @role_name = 'flow_capture';   Allow secure connection to Estuary Flow from your hosting environment. Either: Set up an SSH server for tunneling. When you fill out the endpoint configuration, include the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. Allowlist the Estuary IP addresses in your firewall rules.  ","version":"Next","tagName":"h3"},{"title":"Azure SQL Database​","type":1,"pageTitle":"Microsoft SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/#azure-sql-database","content":" Allow connections between the database and Estuary Flow. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel. To allow direct access: Create a new firewall rule that grants access to the Estuary Flow IP addresses. To allow secure connections via SSH tunneling: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networks for additional details and a sample. In your SQL client, connect to your instance as the default sqlserver user and issue the following commands.  USE &lt;database&gt;; -- Enable CDC for the database. EXEC sys.sp_cdc_enable_db; -- Create user and password for use with the connector. CREATE LOGIN flow_capture WITH PASSWORD = 'secret'; CREATE USER flow_capture FOR LOGIN flow_capture; -- Grant the user permissions on the CDC schema and schemas with data. -- This assumes all tables to be captured are in the default schema, `dbo`. -- Add similar queries for any other schemas that contain tables you want to capture. GRANT SELECT ON SCHEMA :: dbo TO flow_capture; GRANT SELECT ON SCHEMA :: cdc TO flow_capture; -- Create the watermarks table and grant permissions. CREATE TABLE dbo.flow_watermarks(slot INTEGER PRIMARY KEY, watermark TEXT); GRANT SELECT, INSERT, UPDATE ON dbo.flow_watermarks TO flow_capture; -- Enable CDC on tables. The below query enables CDC the watermarks table ONLY. -- You should add similar query for all other tables you intend to capture. EXEC sys.sp_cdc_enable_table @source_schema = 'dbo', @source_name = 'flow_watermarks', @role_name = 'flow_capture';   Note the following important items for configuration: Find the instance's host under Server Name. The port is always 1433. Together, you'll use the host:port as the address property when you configure the connector.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Microsoft SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the SQL Server source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Microsoft SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tServer Address\tThe host or host:port at which the database can be reached.\tstring\tRequired /database\tDatabase\tLogical database name to capture from.\tstring\tRequired /user\tUser\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tPassword\tPassword for the specified database user.\tstring\tRequired /advanced\tAdvanced Options\tOptions for advanced users. You should not typically need to modify these.\tobject /advanced/backfill_chunk_size\tBackfill Chunk Size\tThe number of rows which should be fetched from the database in a single backfill query.\tinteger\t4096 /advanced/skip_backfills\tSkip Backfills\tA comma-separated list of fully-qualified table names which should not be backfilled.\tstring /advanced/watermarksTable\tWatermarks Table\tThe name of the table used for watermark writes during backfills. Must be fully-qualified in '&lt;schema&gt;.&lt;table&gt;' form.\tstring\t&quot;dbo.flow_watermarks&quot;  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe namespace/schema of the table.\tstring\tRequired /stream\tStream\tTable name.\tstring\tRequired /primary_key\tPrimary Key Columns\tarray\tThe columns which together form the primary key of the table.\t  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Microsoft SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: &quot;ghcr.io/estuary/source-sqlserver:dev&quot; config: address: &quot;&lt;host&gt;:1433&quot; database: &quot;my_db&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; bindings: - resource: stream: ${TABLE_NAME} namespace: dbo primary_key: [&quot;id&quot;] target: ${PREFIX}/${COLLECTION_NAME}   Your capture definition will likely be more complex, with additional bindings for each table in the source database.  Learn more about capture definitions.  ","version":"Next","tagName":"h3"},{"title":"Specifying Flow collection keys​","type":1,"pageTitle":"Microsoft SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/#specifying-flow-collection-keys","content":" Every Flow collection must have a key. As long as your SQL Server tables have a primary key specified, the connector will set the corresponding collection's key accordingly.  In cases where a SQL Server table you want to capture doesn't have a primary key, you can manually add it to the collection definition during the capture creation workflow.  After you input the endpoint configuration and click Next, the tables in your database have been mapped to Flow collections. Click each collection's Specification tab and identify a collection where &quot;key&quot;: [ ], is empty. Click inside the empty key value in the editor and input the name of column in the table to use as the key, formatted as a JSON pointer. For example &quot;key&quot;: [&quot;/foo&quot;], Make sure the key field is required, not nullable, and of an allowed type. Make any other necessary changes to the collection specification to accommodate this. Repeat with other missing collection keys, if necessary. Save and publish the capture as usual. ","version":"Next","tagName":"h2"},{"title":"Amazon RDS for SQL Server","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/SQLServer/amazon-rds-sqlserver/","content":"","keywords":"","version":"Next"},{"title":"Supported versions and platforms​","type":1,"pageTitle":"Amazon RDS for SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/amazon-rds-sqlserver/#supported-versions-and-platforms","content":" This connector designed for databases using any version of SQL Server which has CDC support, and is regularly tested against SQL Server 2017 and up.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon RDS for SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/amazon-rds-sqlserver/#prerequisites","content":" To capture change events from SQL Server tables using this connector, you need:  For each table to be captured, a primary key should be specified in the database. If a table doesn't have a primary key, you must manually specify a key in the associated Flow collection definition while creating the capture.See detailed steps. CDC enabledon the database and the individual tables to be captured. (This creates change tables in the database, from which the connector reads.) A watermarks table. The watermarks table is a small “scratch space” to which the connector occasionally writes a small amount of data to ensure accuracy when backfilling preexisting table contents. A user role with: SELECT permissions on the CDC schema and the schemas that contain tables to be captured.Access to the change tables created as part of the SQL Server CDC process.SELECT, INSERT, and UPDATE permissions on the watermarks table  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Amazon RDS for SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/amazon-rds-sqlserver/#setup","content":" Allow connections between the database and Estuary Flow. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel. To allow direct access: Modify the database, setting Public accessibility to Yes.Edit the VPC security group associated with your database, or create a new VPC security group and associate it with the database as described in the Amazon documentation. Create a new inbound rule and a new outbound rule that allow all traffic from the Estuary Flow IP addresses. To allow secure connections via SSH tunneling: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networks for additional details and a sample. In your SQL client, connect to your instance as the default sqlserver user and issue the following commands.  USE &lt;database&gt;; -- Enable CDC for the database. EXEC msdb.dbo.rds_cdc_enable_db; -- Create user and password for use with the connector. CREATE LOGIN flow_capture WITH PASSWORD = 'secret'; CREATE USER flow_capture FOR LOGIN flow_capture; -- Grant the user permissions on the CDC schema and schemas with data. -- This assumes all tables to be captured are in the default schema, `dbo`. -- Add similar queries for any other schemas that contain tables you want to capture. GRANT SELECT ON SCHEMA :: dbo TO flow_capture; GRANT SELECT ON SCHEMA :: cdc TO flow_capture; -- Create the watermarks table and grant permissions. CREATE TABLE dbo.flow_watermarks(slot INTEGER PRIMARY KEY, watermark TEXT); GRANT SELECT, INSERT, UPDATE ON dbo.flow_watermarks TO flow_capture; -- Enable CDC on tables. The below query enables CDC the watermarks table ONLY. -- You should add similar query for all other tables you intend to capture. EXEC sys.sp_cdc_enable_table @source_schema = 'dbo', @source_name = 'flow_watermarks', @role_name = 'flow_capture';   In the RDS console, note the instance's Endpoint and Port. You'll need these for the address property when you configure the connector.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Amazon RDS for SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/amazon-rds-sqlserver/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the SQL Server source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Amazon RDS for SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/amazon-rds-sqlserver/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tServer Address\tThe host or host:port at which the database can be reached.\tstring\tRequired /database\tDatabase\tLogical database name to capture from.\tstring\tRequired /user\tUser\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tPassword\tPassword for the specified database user.\tstring\tRequired /advanced\tAdvanced Options\tOptions for advanced users. You should not typically need to modify these.\tobject /advanced/backfill_chunk_size\tBackfill Chunk Size\tThe number of rows which should be fetched from the database in a single backfill query.\tinteger\t4096 /advanced/skip_backfills\tSkip Backfills\tA comma-separated list of fully-qualified table names which should not be backfilled.\tstring /advanced/watermarksTable\tWatermarks Table\tThe name of the table used for watermark writes during backfills. Must be fully-qualified in '&lt;schema&gt;.&lt;table&gt;' form.\tstring\t&quot;dbo.flow_watermarks&quot;  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe namespace/schema of the table.\tstring\tRequired /stream\tStream\tTable name.\tstring\tRequired /primary_key\tPrimary Key Columns\tarray\tThe columns which together form the primary key of the table.\t  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Amazon RDS for SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/amazon-rds-sqlserver/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: &quot;ghcr.io/estuary/source-sqlserver:dev&quot; config: address: &quot;&lt;host&gt;:1433&quot; database: &quot;my_db&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; bindings: - resource: stream: ${TABLE_NAME} namespace: dbo primary_key: [&quot;id&quot;] target: ${PREFIX}/${COLLECTION_NAME}   Your capture definition will likely be more complex, with additional bindings for each table in the source database.  Learn more about capture definitions.  ","version":"Next","tagName":"h3"},{"title":"Specifying Flow collection keys​","type":1,"pageTitle":"Amazon RDS for SQL Server","url":"/reference/Connectors/capture-connectors/SQLServer/amazon-rds-sqlserver/#specifying-flow-collection-keys","content":" Every Flow collection must have a key. As long as your SQL Server tables have a primary key specified, the connector will set the corresponding collection's key accordingly.  In cases where a SQL Server table you want to capture doesn't have a primary key, you can manually add it to the collection definition during the capture creation workflow.  After you input the endpoint configuration and click Next, the tables in your database have been mapped to Flow collections. Click each collection's Specification tab and identify a collection where &quot;key&quot;: [ ], is empty. Click inside the empty key value in the editor and input the name of column in the table to use as the key, formatted as a JSON pointer. For example &quot;key&quot;: [&quot;/foo&quot;], Make sure the key field is required, not nullable, and of an allowed type. Make any other necessary changes to the collection specification to accommodate this. Repeat with other missing collection keys, if necessary. Save and publish the capture as usual. ","version":"Next","tagName":"h2"},{"title":"Stripe Real-time","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/stripe-realtime/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Stripe Real-time","url":"/reference/Connectors/capture-connectors/stripe-realtime/#supported-data-resources","content":" The following data resources are supported through the Stripe API:  AccountsApplication feesApplication fees refundsBalance transactionsBank accountsCardsChargesCheckout sessionsCheckout sessions line itemsCouponsCredit notesCredit notes line itemsCustomer balance transactionsCustomersDisputesEarly fraud warningExternal account cardsExternal bank accountFilesFile linksInvoice itemsInvoice line itemsInvoicesPayment intentsPayment methodsPayoutsPersonsPlansProductsPromotion codesRefundsReviewsSetup attemptsSetup intentsSubscription itemsSubscriptionsSubscription scheduleTop upsTransfer reversalsTransfersUsage records  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Stripe Real-time","url":"/reference/Connectors/capture-connectors/stripe-realtime/#prerequisites","content":" An API Key for your Stripe account. This usually starts with sk_live_ or sk_test_ depending on your environment. Manage your Stripe keys in their developer dashboard.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Stripe Real-time","url":"/reference/Connectors/capture-connectors/stripe-realtime/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Stripe source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Stripe Real-time","url":"/reference/Connectors/capture-connectors/stripe-realtime/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/credentials\tCredentials object\tRequired /credentials/credentials_title\tCredentials Title\tThe type of authentication. Currently only accepts Private App Credentials.\tstring\tPrivate App Credentials /credentials/access_token\tAccess Token\tStripe API key. Usually starts with sk_live_.\tstring\tRequired /start_date\tStart Date\tUTC date and time in the format YYYY-MM-DDTHH:MM:SSZ. Only data generated after this date will be replicated.\tstring\t30 days before the present date  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource from Stripe from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Stripe Real-time","url":"/reference/Connectors/capture-connectors/stripe-realtime/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-stripe-native:dev config: credentials: credentials_title: Private App Credentials access_token: &lt;secret&gt; start_date: 2025-01-01T00:00:00Z bindings: - resource: stream: charges syncMode: incremental target: ${PREFIX}/charges - resource: stream: customer_balance_transactions syncMode: full_refresh target: ${PREFIX}/customerbalancetransactions {...}  ","version":"Next","tagName":"h3"},{"title":"Survey Monkey","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/survey-monkey/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Survey Monkey","url":"/reference/Connectors/capture-connectors/survey-monkey/#supported-data-resources","content":" The following data resources are supported:  SurveysSurvey pagesSurvey questionsSurvey responses  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Survey Monkey","url":"/reference/Connectors/capture-connectors/survey-monkey/#prerequisites","content":" You'll need to configure a SurveyMonkey private app to integrate with Flow.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Survey Monkey","url":"/reference/Connectors/capture-connectors/survey-monkey/#setup","content":" Go to your your SurveyMonkey apps page and create a new private app.Set the following required scopes: View surveysView responses Deploy the app. This requires a paid SurveyMonkey plan; otherwise, the app will be deleted in 90 days.  Once the app is set up, there are two ways to authenticate SurveyMonkey in Flow: using OAuth in the web app, or using an access token with the flowctl CLI.  OAuth authentication in the web app​  You'll need the username and password of a SurveyMonkey user that is part of the teamfor which the private app was created.  Manual authentication with flowctl​  Note the client ID, secret, and access token for the private app you created. You'll use these in the connector configuration.  ","version":"Next","tagName":"h3"},{"title":"Performance considerations​","type":1,"pageTitle":"Survey Monkey","url":"/reference/Connectors/capture-connectors/survey-monkey/#performance-considerations","content":" The SurveyMonkey API imposes call limits of 500 per day and 120 per minute.  This connector uses caching to avoid exceeding these limits.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Survey Monkey","url":"/reference/Connectors/capture-connectors/survey-monkey/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the SurveyMonkey source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Survey Monkey","url":"/reference/Connectors/capture-connectors/survey-monkey/#properties","content":" Endpoint​  The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so many of these properties aren't required.  Property\tTitle\tDescription\tType\tRequired/Default/credentials\tCredentials\tCredentials for the service\tobject\tRequired /credentials/access_token\tAccess Token\tAccess Token for your SurveyMonkey private app.\tstring\tRequired /credentials/client_id\tClient ID\tClient ID associated with your SurveyMonkey private app.\tstring\tRequired /credentials/client_secret\tClient Secret\tClient secret associated with your SurveyMonkey private app.\tstring\tRequired /start_date\tStart Date\tUTC date and time in the format 2017-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired /survey_ids\tSurvey Monkey survey IDs\tIDs of the surveys from which you'd like to replicate data. If left empty, data from all boards to which you have access will be replicated.\tarray\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tSurveyMonkey resource from which a collection is captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Survey Monkey","url":"/reference/Connectors/capture-connectors/survey-monkey/#sample","content":" This sample specification reflects the manual authentication method.  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-surveymonkey:dev config: credentials: access_token: {secret} client_id: XXXXXXXXXXXXXXXX client_secret: {secret} start_date: 2021-01-25T00:00:00Z bindings: - resource: stream: surveys syncMode: incremental target: ${PREFIX}/surveys {...}  ","version":"Next","tagName":"h3"},{"title":"Twilio","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/twilio/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Twilio","url":"/reference/Connectors/capture-connectors/twilio/#supported-data-resources","content":" The following data resources are supported through the Twilio APIs:  AccountsAddressesAlertsApplicationsAvailable Phone Number CountriesAvailable Phone Numbers LocalAvailable Phone Numbers MobileAvailable Phone Numbers Toll FreeCallsConference ParticipantsConferencesConversationsConversation MessagesConversation ParticipantsDependent Phone NumbersExecutionsIncoming Phone NumbersFlowsKeysMessage MediaMessagesOutgoing Caller IdsQueuesRecordingsTranscriptionsUsage RecordsUsage Triggers  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Twilio","url":"/reference/Connectors/capture-connectors/twilio/#prerequisites","content":" Twilio Auth Token for authentication.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Twilio","url":"/reference/Connectors/capture-connectors/twilio/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Twilio source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Twilio","url":"/reference/Connectors/capture-connectors/twilio/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/account_sid\tAccount ID\tTwilio account SID\tstring\tRequired /auth_token\tAuth Token\tTwilio Auth Token.\tstring\tRequired /start_date\tReplication Start Date\tUTC date and time in the format 2021-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired /lookback_window\tLookback window\tHow far into the past to look for records. (in minutes)\tinteger\tDefault  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Twilio project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Twilio","url":"/reference/Connectors/capture-connectors/twilio/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-twilio:dev config: account_sid: &lt;your account ID&gt; auth_token: &lt;secret&gt; start_date: 2017-01-25T00:00:00Z lookback_window: 7 bindings: - resource: stream: accounts syncMode: full_refresh target: ${PREFIX}/accounts {...}  ","version":"Next","tagName":"h3"},{"title":"Stripe (Deprecated)","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/stripe/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Stripe (Deprecated)","url":"/reference/Connectors/capture-connectors/stripe/#supported-data-resources","content":" The following data resources are supported through the Stripe API:  Balance transactionsBank accountsChargesCheckout sessionsCheckout sessions line itemsCouponsCustomer balance transactionsCustomersDisputesEventsInvoice itemsInvoice line itemsInvoicesPayment intentsPayoutsPlansProductsPromotion codesRefundsSubscription itemsSubscriptionsTransfers  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Stripe (Deprecated)","url":"/reference/Connectors/capture-connectors/stripe/#prerequisites","content":" Account ID of your Stripe account.Secret key for the Stripe API.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Stripe (Deprecated)","url":"/reference/Connectors/capture-connectors/stripe/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Stripe source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Stripe (Deprecated)","url":"/reference/Connectors/capture-connectors/stripe/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/account_id\tAccount ID\tYour Stripe account ID (starts with 'acct_', find yours here https://dashboard.stripe.com/settings/account\tstring\tRequired /client_secret\tSecret Key\tStripe API key (usually starts with 'sk_live_'; find yours here https://dashboard.stripe.com/apikeys\tstring\tRequired /lookback_window_days\tLookback Window in days (Optional)\tWhen set, the connector will always re-export data from the past N days, where N is the value set here. This is useful if your data is frequently updated after creation.\tinteger\t0 /start_date\tReplication start date\tUTC date and time in the format 2017-01-25T00:00:00Z. Only data generated after this date will be replicated.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource from Stripe from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Choosing your start date and lookback window​","type":1,"pageTitle":"Stripe (Deprecated)","url":"/reference/Connectors/capture-connectors/stripe/#choosing-your-start-date-and-lookback-window","content":" The connector will continually capture data beginning on the Replication start date you choose.  However, some data from the Stripe API is mutable; for example, a draft invoice can be completed at a later date than it was created. To account for this, it's useful to set the Lookback Window. When this is set, at a given point in time, the connector will not only look for new data; it will also capture changes made to data within the window.  For example, if you start the connector with the start date of 2022-06-06T00:00:00Z (June 6) and the lookback window of 3, the connector will begin to capture data starting from June 3. As time goes on while the capture remains active, the lookback window rolls forward along with the current timestamp. On June 10, the connector will continue to monitor data starting from June 7 and capture any changes to that data, and so on.  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Stripe (Deprecated)","url":"/reference/Connectors/capture-connectors/stripe/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-stripe:dev config: account_id: 00000000 client_secret: &lt;secret&gt; start_date: 2022-06-18T00:00:00Z bindings: - resource: stream: balance_transactions syncMode: incremental target: ${PREFIX}/balancetransactions - resource: stream: bank_accounts syncMode: full_refresh target: ${PREFIX}/bankaccounts - resource: stream: charges syncMode: incremental target: ${PREFIX}/charges - resource: stream: checkout_sessions syncMode: incremental target: ${PREFIX}/checkoutsessions - resource: stream: checkout_sessions_line_items syncMode: incremental target: ${PREFIX}/checkoutsessionslineitems - resource: stream: coupons syncMode: incremental target: ${PREFIX}/coupons - resource: stream: customer_balance_transactions syncMode: full_refresh target: ${PREFIX}/customerbalancetransactions - resource: stream: customers syncMode: incremental target: ${PREFIX}/customers - resource: stream: disputes syncMode: incremental target: ${PREFIX}/disputes - resource: stream: events syncMode: incremental target: ${PREFIX}/events - resource: stream: invoice_items syncMode: incremental target: ${PREFIX}/invoice_items - resource: stream: invoice_line_items syncMode: full_refresh target: ${PREFIX}/invoicelineitems - resource: stream: invoices syncMode: incremental target: ${PREFIX}/invoices - resource: stream: payment_intents syncMode: incremental target: ${PREFIX}/paymentintents - resource: stream: payouts syncMode: incremental target: ${PREFIX}/payouts - resource: stream: plans syncMode: incremental target: ${PREFIX}/plans - resource: stream: products syncMode: incremental target: ${PREFIX}/products - resource: stream: promotion_codes syncMode: incremental target: ${PREFIX}/promotioncodes - resource: stream: refunds syncMode: incremental target: ${PREFIX}/refunds - resource: stream: subscription_items syncMode: full_refresh target: ${PREFIX}/subscriptionitems - resource: stream: subscriptions syncMode: incremental target: ${PREFIX}/subscriptions - resource: stream: transfers syncMode: incremental target: ${PREFIX}/transfers  ","version":"Next","tagName":"h3"},{"title":"TikTok Marketing","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/tiktok/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"TikTok Marketing","url":"/reference/Connectors/capture-connectors/tiktok/#supported-data-resources","content":" The following data resources are supported:  Resource\tProduction\tSandboxAdvertisers\tX\tX Ad Groups\tX\tX Ads\tX\tX Campaigns\tX\tX Ads Reports Hourly\tX\tX Ads Reports Daily\tX\tX Ads Reports Lifetime\tX\tX Advertisers Reports Hourly\tX Advertisers Reports Daily\tX Advertisers Reports Lifetime\tX Ad Groups Reports Hourly\tX\tX Ad Groups Reports Daily\tX\tX Ad Groups Reports Lifetime\tX\tX Campaigns Reports Hourly\tX\tX Campaigns Reports Daily\tX\tX Campaigns Reports Lifetime\tX\tX Advertisers Audience Reports Hourly\tX Advertisers Audience Reports Daily\tX Advertisers Audience Reports Lifetime\tX Ad Group Audience Reports Hourly\tX\tX Ad Group Audience Reports Daily\tX\tX Ads Audience Reports Hourly\tX\tX Ads Audience Reports Daily\tX\tX Campaigns Audience Reports By Country Hourly\tX\tX Campaigns Audience Reports By Country Daily\tX\tX  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"TikTok Marketing","url":"/reference/Connectors/capture-connectors/tiktok/#prerequisites","content":" Prerequisites differ depending on whether you have a production or sandboxTikTok for Business account, and on whether you'll use the Flow web app or the flowctl CLI.  ","version":"Next","tagName":"h2"},{"title":"OAuth authentication in the web app (production accounts)​","type":1,"pageTitle":"TikTok Marketing","url":"/reference/Connectors/capture-connectors/tiktok/#oauth-authentication-in-the-web-app-production-accounts","content":" If you have a TikTok marketing account in production and will use the Flow web app, you'll be able to quickly log in using OAuth.  You'll need:  A TikTok for Business account with one or more active campaigns. Note the username and password used to sign into this account  ","version":"Next","tagName":"h3"},{"title":"Sandbox access token authentication in the web app or CLI​","type":1,"pageTitle":"TikTok Marketing","url":"/reference/Connectors/capture-connectors/tiktok/#sandbox-access-token-authentication-in-the-web-app-or-cli","content":" If you're working in a Sandbox TikTok for Business account, you'll authenticate with an access token in both the web app and CLI.  You'll need:  A TikTok for Business account. A Sandbox account created under an existingdeveloper application. Generate an access token and note the advertiser ID for the Sandbox.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"TikTok Marketing","url":"/reference/Connectors/capture-connectors/tiktok/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the TikTok Marketing source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"TikTok Marketing","url":"/reference/Connectors/capture-connectors/tiktok/#properties","content":" Endpoint​  The properties in the table below reflect the manual authentication method for Sandbox accounts. If you're using a production account, you'll use OAuth2 to authenticate in the Flow web app, so many of these properties aren't required.  Property\tTitle\tDescription\tType\tRequired/Default/credentials\tAuthentication Method\tAuthentication method\tobject\tRequired /credentials/auth_type\tAuthentication type\tSet to sandbox_access_token to manually authenticate a Sandbox.\tstring\tRequired /credentials/advertiser_id\tAdvertiser ID\tThe Advertiser ID generated for the developer's Sandbox application.\tstring /credentials/access_token\tAccess Token\tThe long-term authorized access token.\tstring /end_date\tEnd Date\tThe date until which you'd like to replicate data for all incremental streams, in the format YYYY-MM-DD. All data generated between start_date and this date will be replicated. Not setting this option will result in always syncing the data till the current date.\tstring /report_granularity\tReport Aggregation Granularity\tThe granularity used for aggregating performance data in reports. Choose DAY, LIFETIME, or HOUR.\tstring /start_date\tReplication Start Date\tThe Start Date in format: YYYY-MM-DD. Any data before this date will not be replicated. If this parameter is not set, all data will be replicated.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tTikTok resource from which a collection is captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"TikTok Marketing","url":"/reference/Connectors/capture-connectors/tiktok/#sample","content":" This sample specification reflects the access token method for Sandbox accounts.  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-tiktok-marketing:dev config: credentials: auth_type: sandbox_access_token access_token: {secret} advertiser_id: {secret} end_date: 2022-01-01 report_granularity: DAY start_date: 2020-01-01 bindings: - resource: stream: campaigns syncMode: incremental target: ${PREFIX}/campaigns {...}   ","version":"Next","tagName":"h3"},{"title":"Report aggregation​","type":1,"pageTitle":"TikTok Marketing","url":"/reference/Connectors/capture-connectors/tiktok/#report-aggregation","content":" Many of the resources this connector supports are reports. Data in these reports is aggregated into rows based on the granularity you select in the configuration.  You can choose hourly, daily, or lifetime granularity. For example, if you choose daily granularity, the report will contain one row for each day. ","version":"Next","tagName":"h2"},{"title":"WooCommerce","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/woocommerce/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"WooCommerce","url":"/reference/Connectors/capture-connectors/woocommerce/#prerequisites","content":" To set up the WooCommerce source connector you need:  WooCommerce 3.5+WordPress 4.4+Pretty permalinks in Settings &gt; Permalinks so that the custom endpoints are supported. e.g. /%year%/%monthnum%/%day%/%postname%/A new API key with read permissions and access to Customer key and Customer Secret.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"WooCommerce","url":"/reference/Connectors/capture-connectors/woocommerce/#setup","content":" Follow the steps below to set up the WooCommerce source connector.  ","version":"Next","tagName":"h2"},{"title":"Set up WooCommerce​","type":1,"pageTitle":"WooCommerce","url":"/reference/Connectors/capture-connectors/woocommerce/#set-up-woocommerce","content":" Generate a new Rest API key.Obtain Customer key and Customer Secret.  ","version":"Next","tagName":"h3"},{"title":"Set up the WooCommerce connector in Estuary Flow​","type":1,"pageTitle":"WooCommerce","url":"/reference/Connectors/capture-connectors/woocommerce/#set-up-the-woocommerce-connector-in-estuary-flow","content":" Log into your Estuary Flow account.In the left navigation bar, click on &quot;Captures&quot;. In the top-left corner, click &quot;Connector Search&quot;.Enter the name for the WooCommerce connector and select &quot;WooCommerce&quot; from the dropdown.Fill in &quot;Customer key&quot; and &quot;Customer Secret&quot; with the data from Step 1 of this guide.Fill in &quot;Shop Name&quot;. For example, if your shop URL is https://EXAMPLE.com, the shop name is 'EXAMPLE.com'.Choose the start date you want to start syncing data from.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"WooCommerce","url":"/reference/Connectors/capture-connectors/woocommerce/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the WooCommerce source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"WooCommerce","url":"/reference/Connectors/capture-connectors/woocommerce/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/customer_key\tCustomer Key\tCustomer Key for API in WooCommerce shop\tstring\tRequired /customer_secret\tCustomer Secret\tCustomer Secret for API in WooCommerce shop\tstring\tRequired /shop_name\tShop Name\tThe name of the store.\tstring\tRequired /start_date\tStart Date\tThe date you would like to replicate data from. Format: YYYY-MM-DD\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your WooCommerce project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"WooCommerce","url":"/reference/Connectors/capture-connectors/woocommerce/#sample","content":" { &quot;properties&quot;: { } }   ","version":"Next","tagName":"h3"},{"title":"Supported Streams​","type":1,"pageTitle":"WooCommerce","url":"/reference/Connectors/capture-connectors/woocommerce/#supported-streams","content":" The WooCommerce source connector in Estuary Flow supports the following streams:  Coupons (Incremental)Customers (Incremental)Orders (Incremental)Order notesPayment gatewaysProduct attribute termsProduct attributesProduct categoriesProduct reviews (Incremental)Product shipping classesProduct tagsProduct variationsProducts (Incremental)RefundsShipping methodsShipping zone locationsShipping zone methodsShipping zonesSystem status toolsTax classesTax rates  ","version":"Next","tagName":"h2"},{"title":"Connector-Specific Features & Highlights​","type":1,"pageTitle":"WooCommerce","url":"/reference/Connectors/capture-connectors/woocommerce/#connector-specific-features--highlights","content":" Useful links:  WooCommerce Rest API Docs. ","version":"Next","tagName":"h2"},{"title":"YouTube Analytics","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/youtube-analytics/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"YouTube Analytics","url":"/reference/Connectors/capture-connectors/youtube-analytics/#supported-data-resources","content":" The following data resources are supported through the YouTube Analytics APIs:  channel_annotations_a1channel_basic_a2channel_cards_a1channel_combined_a2channel_demographics_a1channel_device_os_a2channel_end_screens_a1channel_playback_location_a2channel_province_a2channel_sharing_service_a1channel_subtitles_a2channel_traffic_source_a2playlist_basic_a1playlist_combined_a1playlist_device_os_a1playlist_playback_location_a1playlist_province_a1playlist_traffic_source_a1  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"YouTube Analytics","url":"/reference/Connectors/capture-connectors/youtube-analytics/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the YouTube Analytics source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"YouTube Analytics","url":"/reference/Connectors/capture-connectors/youtube-analytics/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/client_id\tClient ID\tYour Client ID\tstring\tRequired /client_secret\tSecret Key\tYour Client Secret\tstring\tRequired /refresh_token\tRefresh Token\tYour Refresh Token\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your YouTube Analytics project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"YouTube Analytics","url":"/reference/Connectors/capture-connectors/youtube-analytics/#sample","content":"  captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-youtube-analytics:dev config: credentials: auth_type: OAuth bindings: - resource: stream: channel_annotations_a1 syncMode: incremental target: ${PREFIX}/channel_annotations_a1 {...}  ","version":"Next","tagName":"h3"},{"title":"Zendesk Chat","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/zendesk-chat/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Zendesk Chat","url":"/reference/Connectors/capture-connectors/zendesk-chat/#supported-data-resources","content":" The following data resources are supported through the Zendesk API:  AccountsAgentsAgent TimelinesChatsShortcutsTriggersBansDepartmentsGoalsSkillsRolesRouting Settings  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Zendesk Chat","url":"/reference/Connectors/capture-connectors/zendesk-chat/#prerequisites","content":" A Zendesk Account with permission to access data from accounts you want to sync.An Access Token. We recommend creating a restricted, read-only key specifically for Estuary access to allow you to control which resources Estuary should be able to access.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Zendesk Chat","url":"/reference/Connectors/capture-connectors/zendesk-chat/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification files. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Zendesk Chat source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Zendesk Chat","url":"/reference/Connectors/capture-connectors/zendesk-chat/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/start_date\tStart Date\tThe date from which you would like to replicate data for Zendesk Support API, in the format YYYY-MM-DDT00:00:00Z. All data generated after this date will be replicated.\tstring\tRequired /subdomain\tSubdomain\tThis is your Zendesk subdomain that can be found in your account URL.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource in Zendesk from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Zendesk Chat","url":"/reference/Connectors/capture-connectors/zendesk-chat/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-zendesk-chat:dev config: credentials: access_token: &lt;secret&gt; credentials: access_token start_date: 2022-03-01T00:00:00Z subdomain: my_subdomain bindings: - resource: stream: accounts syncMode: full_refresh target: ${PREFIX}/accounts  ","version":"Next","tagName":"h3"},{"title":"Dekaf integrations","type":0,"sectionRef":"#","url":"/reference/Connectors/dekaf/","content":"","keywords":"","version":"Next"},{"title":"Available Dekaf integrations​","type":1,"pageTitle":"Dekaf integrations","url":"/reference/Connectors/dekaf/#available-dekaf-integrations","content":" TinybirdMaterializeStarTreeSingleStoreImplyBytewaxClickhouse ","version":"Next","tagName":"h2"},{"title":"Imply Polaris","type":0,"sectionRef":"#","url":"/reference/Connectors/dekaf/dekaf-imply/","content":"","keywords":"","version":"Next"},{"title":"Connecting Estuary Flow to Imply Polaris​","type":1,"pageTitle":"Imply Polaris","url":"/reference/Connectors/dekaf/dekaf-imply/#connecting-estuary-flow-to-imply-polaris","content":" Generate a refresh token for the Imply Polaris connection from the Estuary Admin Dashboard. Log in to your Imply Polaris account and navigate to your project. In the left sidebar, click on &quot;Tables&quot; and then &quot;Create Table&quot;. Choose &quot;Kafka&quot; as the input source for your new table. In the Kafka configuration section, enter the following details: Bootstrap Servers: dekaf.estuary-data.com:9092Topic: Your Estuary Flow collection name (e.g., /my-organization/my-collection)Security Protocol: SASL_SSLSASL Mechanism: PLAINSASL Username: {}SASL Password: Your generated Estuary Access Token For the &quot;Input Format&quot;, select &quot;avro&quot;. Configure the Schema Registry settings: Schema Registry URL: https://dekaf.estuary-data.comSchema Registry Username: {} (same as SASL Username)Schema Registry Password: The same Estuary Access Token as above In the &quot;Schema&quot; section, Imply Polaris should automatically detect the schema from your Avro data. Review and adjust the column definitions as needed. Review and finalize your table configuration, then click &quot;Create Table&quot;. Your Imply Polaris table should now start ingesting data from Estuary Flow. ","version":"Next","tagName":"h2"},{"title":"Bytewax","type":0,"sectionRef":"#","url":"/reference/Connectors/dekaf/dekaf-bytewax/","content":"","keywords":"","version":"Next"},{"title":"Connecting Estuary Flow to Bytewax​","type":1,"pageTitle":"Bytewax","url":"/reference/Connectors/dekaf/dekaf-bytewax/#connecting-estuary-flow-to-bytewax","content":" Generate a refresh token for the Bytewax connection from the Estuary Admin Dashboard. Install Bytewax and the Kafka Python client: pip install bytewax kafka-python Create a Python script for your Bytewax dataflow, using the following template: import json from datetime import timedelta from bytewax.dataflow import Dataflow from bytewax.inputs import KafkaInputConfig from bytewax.outputs import StdOutputConfig from bytewax.window import TumblingWindowConfig, SystemClockConfig # Estuary Flow Dekaf configuration KAFKA_BOOTSTRAP_SERVERS = &quot;dekaf.estuary-data.com:9092&quot; KAFKA_TOPIC = &quot;/full/nameof/your/collection&quot; # Parse incoming messages def parse_message(msg): data = json.loads(msg) # Process your data here return data # Define your dataflow src = KafkaSource(brokers=KAFKA_BOOTSTRAP_SERVERS, topics=[KAFKA_TOPIC], add_config={ &quot;security.protocol&quot;: &quot;SASL_SSL&quot;, &quot;sasl.mechanism&quot;: &quot;PLAIN&quot;, &quot;sasl.username&quot;: &quot;{}&quot;, &quot;sasl.password&quot;: os.getenv(&quot;DEKAF_TOKEN&quot;), }) flow = Dataflow() flow.input(&quot;input&quot;, src) flow.input(&quot;input&quot;, KafkaInputConfig(KAFKA_BOOTSTRAP_SERVERS, KAFKA_TOPIC)) flow.map(parse_message) # Add more processing steps as needed flow.output(&quot;output&quot;, StdOutputConfig()) if __name__ == &quot;__main__&quot;: from bytewax.execution import run_main run_main(flow) Replace &quot;/full/nameof/your/collection&quot; with your actual collection name from Estuary Flow. Run your Bytewax dataflow: python your_dataflow_script.py Your Bytewax dataflow is now processing data from Estuary Flow in real-time. ","version":"Next","tagName":"h2"},{"title":"Integrating ClickHouse Cloud with Estuary Flow via Dekaf","type":0,"sectionRef":"#","url":"/reference/Connectors/dekaf/dekaf-clickhouse/","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Integrating ClickHouse Cloud with Estuary Flow via Dekaf","url":"/reference/Connectors/dekaf/dekaf-clickhouse/#overview","content":" This guide covers how to integrate ClickHouse Cloud with Estuary Flow using Dekaf, Estuary’s Kafka API compatibility layer, and ClickPipes for real-time analytics. This integration allows ClickHouse Cloud users to stream data from a vast array of sources supported by Estuary Flow directly into ClickHouse, using Dekaf for Kafka compatibility.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Integrating ClickHouse Cloud with Estuary Flow via Dekaf","url":"/reference/Connectors/dekaf/dekaf-clickhouse/#prerequisites","content":" ClickHouse Cloud account with permissions to configure ClickPipes for data ingestion.Estuary Flow account with access to Dekaf and necessary connectors ( e.g., Salesforce, databases).Estuary Flow Refresh Token to authenticate with Dekaf.    ","version":"Next","tagName":"h2"},{"title":"Step 1: Configure Data Source in Estuary Flow​","type":1,"pageTitle":"Integrating ClickHouse Cloud with Estuary Flow via Dekaf","url":"/reference/Connectors/dekaf/dekaf-clickhouse/#step-1-configure-data-source-in-estuary-flow","content":" Generate an Estuary Refresh Token: To access the Kafka-compatible topics, create a refresh token in the Estuary Flow dashboard. This token will act as the password for both the broker and schema registry. Connect to Dekaf: Estuary Flow will automatically expose your collections as Kafka-compatible topics through Dekaf. No additional configuration is required. Dekaf provides the following connection details: Broker Address: dekaf.estuary-data.com:9092 Schema Registry Address: https://dekaf.estuary-data.com Security Protocol: SASL_SSL SASL Mechanism: PLAIN SASL Username: {} SASL Password: &lt;Estuary Refresh Token&gt; Schema Registry Username: {} Schema Registry Password: &lt;Estuary Refresh Token&gt;     ","version":"Next","tagName":"h2"},{"title":"Step 2: Configure ClickPipes in ClickHouse Cloud​","type":1,"pageTitle":"Integrating ClickHouse Cloud with Estuary Flow via Dekaf","url":"/reference/Connectors/dekaf/dekaf-clickhouse/#step-2-configure-clickpipes-in-clickhouse-cloud","content":" Set Up ClickPipes: In ClickHouse Cloud, go to Integrations and select Apache Kafka as the data source. Enter Connection Details: Use the connection parameters from the previous step to configure access to Estuary Flow. Map Data Fields: Ensure that ClickHouse can parse the incoming data properly. Use ClickHouse’s mapping interface to align fields between Estuary Flow collections and ClickHouse tables. Provision the ClickPipe: Kick off the integration and allow ClickPipes to set up the pipeline (should complete within a few seconds). ","version":"Next","tagName":"h2"},{"title":"Zendesk Support","type":0,"sectionRef":"#","url":"/reference/Connectors/capture-connectors/zendesk-support/","content":"","keywords":"","version":"Next"},{"title":"Supported data resources​","type":1,"pageTitle":"Zendesk Support","url":"/reference/Connectors/capture-connectors/zendesk-support/#supported-data-resources","content":" The following data resources are supported through the Zendesk API:  Account attributesAttribute definitionsAudit logsBrandsCustom rolesGroup membershipsGroupsMacrosOrganizationsOrganization membershipsPostsPost commentsPost comment votesPost votesSatisfaction ratingsSchedulesSLA policiesTagsTicket auditsTicket commentsTicket fieldsTicket formsTicket metricsTicket metric eventsTicket skipsTicketsUsers  By default, each resource is mapped to a Flow collection through a separate binding.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Zendesk Support","url":"/reference/Connectors/capture-connectors/zendesk-support/#prerequisites","content":" There are two different ways to authenticate with Zendesk Support when capturing data into Flow: using OAuth2 or providing an API token. The prerequisites for both authentication methods are listed below.  ","version":"Next","tagName":"h2"},{"title":"OAuth2 authentication​","type":1,"pageTitle":"Zendesk Support","url":"/reference/Connectors/capture-connectors/zendesk-support/#oauth2-authentication","content":" Subdomain of your Zendesk URL. In the URL https://MY_SUBDOMAIN.zendesk.com/, MY_SUBDOMAIN is the subdomain.  ","version":"Next","tagName":"h3"},{"title":"API token authentication​","type":1,"pageTitle":"Zendesk Support","url":"/reference/Connectors/capture-connectors/zendesk-support/#api-token-authentication","content":" Subdomain of your Zendesk URL. In the URL https://MY_SUBDOMAIN.zendesk.com/, MY_SUBDOMAIN is the subdomain.Email address associated with your Zendesk account.A Zendesk API token. See the Zendesk docs to enable tokens and generate a new token.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Zendesk Support","url":"/reference/Connectors/capture-connectors/zendesk-support/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification files. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Zendesk Support source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Zendesk Support","url":"/reference/Connectors/capture-connectors/zendesk-support/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/credentials/credentials\tCredentials method\tType of credentials used. Set to api_token or oauth2.0.\tstring\tRequired /credentials/api_token\tAPI Token\tThe value of the API token generated.\tstring\tRequired for API token authentication /credentials/email\tEmail\tThe user email for your Zendesk account.\tstring\tRequired for API token authentication /credentials/client_id\tOAuth Client ID\tThe OAuth app's client ID.\tstring\tRequired for OAuth2 authentication /credentials/client_secret\tOAuth Client Secret\tThe OAuth app's client secret.\tstring\tRequired for OAuth2 authentication /credentials/access_token\tAccess Token\tThe access token received from the OAuth app.\tstring\tRequired for OAuth2 authentication /start_date\tStart Date\tThe date from which you'd like to replicate data for Zendesk Support API, in the format YYYY-MM-DDT00:00:00Z. All data generated after this date will be replicated.\tstring\tRequired /subdomain\tSubdomain\tThis is your Zendesk subdomain that can be found in your account URL. For example, in https://MY_SUBDOMAIN.zendesk.com/, where MY_SUBDOMAIN is the value of your subdomain.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource in Zendesk from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired cursorField\tCursor Field\tField to use as a cursor when paginating through results. Required when syncMode is incremental.\tstring\t  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Zendesk Support","url":"/reference/Connectors/capture-connectors/zendesk-support/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-zendesk-support:dev config: credentials: api_token: &lt;secret&gt; credentials: api_token email: user@domain.com start_date: 2022-03-01T00:00:00Z subdomain: my_subdomain bindings: - resource: stream: account_attributes syncMode: full_refresh target: ${PREFIX}/accountattributes - resource: stream: attribute_definitions syncMode: full_refresh target: ${PREFIX}/attributedefinitions - resource: stream: audit_logs syncMode: incremental cursorField: - created_at target: ${PREFIX}/auditlogs - resource: stream: brands syncMode: full_refresh target: ${PREFIX}/brands - resource: stream: custom_roles syncMode: full_refresh target: ${PREFIX}/customroles - resource: stream: group_memberships syncMode: incremental cursorField: - updated_at target: ${PREFIX}/groupmemberships - resource: stream: groups syncMode: incremental cursorField: - updated_at target: ${PREFIX}/groups - resource: stream: macros syncMode: incremental cursorField: - updated_at target: ${PREFIX}/macros - resource: stream: organizations syncMode: incremental cursorField: - updated_at target: ${PREFIX}/organizations - resource: stream: organization_memberships syncMode: incremental cursorField: - updated_at target: ${PREFIX}/organizationmemberships - resource: stream: posts syncMode: incremental cursorField: - updated_at target: ${PREFIX}/posts - resource: stream: post_comments syncMode: full_refresh target: ${PREFIX}/postcomments - resource: stream: post_comment_votes syncMode: full_refresh target: ${PREFIX}/postcommentvotes - resource: stream: post_votes syncMode: full_refresh target: ${PREFIX}/postvotes - resource: stream: satisfaction_ratings syncMode: incremental cursorField: - updated_at target: ${PREFIX}/satisfactionratings - resource: stream: schedules syncMode: full_refresh target: ${PREFIX}/schedules - resource: stream: sla_policies syncMode: full_refresh target: ${PREFIX}/slapoliciies - resource: stream: tags syncMode: full_refresh target: ${PREFIX}/tags - resource: stream: ticket_audits syncMode: incremental cursorField: - created_at target: ${PREFIX}/ticketaudits - resource: stream: ticket_comments syncMode: incremental cursorField: - created_at target: ${PREFIX}/ticketcomments - resource: stream: ticket_fields syncMode: incremental cursorField: - updated_at target: ${PREFIX}/ticketfields - resource: stream: ticket_forms syncMode: incremental cursorField: - updated_at target: ${PREFIX}/ticketforms - resource: stream: ticket_metrics syncMode: incremental cursorField: - after_cursor target: ${PREFIX}/ticketmetrics - resource: stream: ticket_metric_events syncMode: incremental cursorField: - time target: ${PREFIX}/ticketmetricevents - resource: stream: ticket_skips syncMode: incremental cursorField: - updated_at target: ${PREFIX}/ticketskips - resource: stream: tickets syncMode: incremental cursorField: - after_cursor target: ${PREFIX}/tickets - resource: stream: users syncMode: incremental cursorField: - after_cursor target: ${PREFIX}/users  ","version":"Next","tagName":"h3"},{"title":"Materialize","type":0,"sectionRef":"#","url":"/reference/Connectors/dekaf/dekaf-materialize/","content":"","keywords":"","version":"Next"},{"title":"Connecting Estuary Flow to Materialize​","type":1,"pageTitle":"Materialize","url":"/reference/Connectors/dekaf/dekaf-materialize/#connecting-estuary-flow-to-materialize","content":" Generate a refresh token to use for the Materialize connection. You can generate this token from the Estuary Admin Dashboard. In your Materialize dashboard, use the SQL shell to create a new secret and connection using the Kafka source connector. Use the following SQL commands to configure the connection to Estuary Flow: CREATE SECRET estuary_refresh_token AS 'your_generated_token_here'; CREATE CONNECTION estuary_connection TO KAFKA ( BROKER 'dekaf.estuary-data.com', SECURITY PROTOCOL = 'SASL_SSL', SASL MECHANISMS = 'PLAIN', SASL USERNAME = '{}', SASL PASSWORD = SECRET estuary_refresh_token ); CREATE CONNECTION csr_estuary_connection TO CONFLUENT SCHEMA REGISTRY ( URL 'https://dekaf.estuary-data.com', USERNAME = '{}', PASSWORD = SECRET estuary_refresh_token ); Create a source in Materialize to read from the Kafka topic. Use the following SQL command, replacing &lt;name-of-your-flow-collection&gt; with the name of your collection in Estuary Flow: CREATE SOURCE materialize_source FROM KAFKA CONNECTION estuary_connection (TOPIC '&lt;name-of-your-flow-collection&gt;') FORMAT AVRO USING CONFLUENT SCHEMA REGISTRY CONNECTION csr_estuary_connection ENVELOPE UPSERT;   ","version":"Next","tagName":"h2"},{"title":"Creating Real-Time Views​","type":1,"pageTitle":"Materialize","url":"/reference/Connectors/dekaf/dekaf-materialize/#creating-real-time-views","content":" To begin analyzing the data, create a real-time view using SQL in Materialize. Here is an example query to create a materialized view that tracks data changes:  CREATE MATERIALIZED VIEW my_view AS SELECT * FROM materialize_source;   ","version":"Next","tagName":"h2"},{"title":"Final Steps​","type":1,"pageTitle":"Materialize","url":"/reference/Connectors/dekaf/dekaf-materialize/#final-steps","content":" After configuring your source and creating the necessary views, the connection with Materialize is complete. New data from your Estuary Flow collection will now arrive in your Materialize source in real-time, enabling you to perform real-time analytics on live data streams.  For more detailed information on creating materialized views and other advanced configurations, refer to the Materialize documentation.  By following these steps, you can leverage the full potential of Estuary Flow and Materialize for real-time data processing and analytics. ","version":"Next","tagName":"h2"},{"title":"SingleStore (Cloud)","type":0,"sectionRef":"#","url":"/reference/Connectors/dekaf/dekaf-singlestore/","content":"","keywords":"","version":"Next"},{"title":"Connecting Estuary Flow to SingleStore​","type":1,"pageTitle":"SingleStore (Cloud)","url":"/reference/Connectors/dekaf/dekaf-singlestore/#connecting-estuary-flow-to-singlestore","content":" Generate a refresh token for the SingleStore connection from the Estuary Admin Dashboard. In the SingleStore Cloud Portal, navigate to the SQL Editor section of the Data Studio. Execute the following script to create a table and an ingestion pipeline to hydrate it. This example will ingest data from the demo wikipedia collection in Estuary Flow. CREATE TABLE test_table (id NUMERIC, server_name VARCHAR(255), title VARCHAR(255)); CREATE PIPELINE test AS LOAD DATA KAFKA &quot;dekaf.estuary-data.com:9092/demo/wikipedia/recentchange-sampled&quot; CONFIG '{ &quot;security.protocol&quot;:&quot;SASL_SSL&quot;, &quot;sasl.mechanism&quot;:&quot;PLAIN&quot;, &quot;sasl.username&quot;:&quot;{}&quot;, &quot;broker.address.family&quot;: &quot;v4&quot;, &quot;schema.registry.username&quot;: &quot;{}&quot;, &quot;fetch.wait.max.ms&quot;: &quot;2000&quot; }' CREDENTIALS '{ &quot;sasl.password&quot;: &quot;ESTUARY_ACCESS_TOKEN&quot;, &quot;schema.registry.password&quot;: &quot;ESTUARY_ACCESS_TOKEN&quot; }' INTO table test_table FORMAT AVRO SCHEMA REGISTRY 'https://dekaf.estuary-data.com' ( id &lt;- id, server_name &lt;- server_name, title &lt;- title ); Your pipeline should now start ingesting data from Estuary Flow into SingleStore. ","version":"Next","tagName":"h2"},{"title":"StarTree","type":0,"sectionRef":"#","url":"/reference/Connectors/dekaf/dekaf-startree/","content":"","keywords":"","version":"Next"},{"title":"Connecting Estuary Flow to StarTree​","type":1,"pageTitle":"StarTree","url":"/reference/Connectors/dekaf/dekaf-startree/#connecting-estuary-flow-to-startree","content":" Generate a refresh token to use for the StarTree connection. You can generate this token from the Estuary Admin Dashboard. In the StarTree UI, navigate to the Data Sources section and choose Add New Data Source. Select Kafka as your data source type. Enter the following connection details: Bootstrap Servers: dekaf.estuary-data.comSecurity Protocol: SASL_SSLSASL Mechanism: PLAINSASL Username: {}SASL Password: Your generated Estuary Refresh Token Configure Schema Registry: To decode Avro messages, enable schema registry settings: Schema Registry URL: https://dekaf.estuary-data.comSchema Registry Username: {} (same as SASL Username)Schema Registry Password: The same Estuary Refresh Token as above Click Create Connection to proceed. ","version":"Next","tagName":"h2"},{"title":"Tinybird","type":0,"sectionRef":"#","url":"/reference/Connectors/dekaf/dekaf-tinybird/","content":"","keywords":"","version":"Next"},{"title":"Connecting Estuary Flow to Tinybird​","type":1,"pageTitle":"Tinybird","url":"/reference/Connectors/dekaf/dekaf-tinybird/#connecting-estuary-flow-to-tinybird","content":" Generate a refresh token to use for the Tinybird connection. You can do this from the Estuary Admin Dashboard. In your Tinybird Workspace, create a new Data Source and use the Kafka Connector.  To configure the connection details, use the following settings.  Bootstrap servers: dekaf.estuary-data.comSASL Mechanism: PLAINSASL Username: {}SASL Password: Estuary Refresh Token (Generate your token in the Estuary Admin Dashboard)  Tick the Decode Avro messages with Schema Register box, and use the following settings:  URL: https://dekaf.estuary-data.comUsername: {}Password: The same Estuary Refresh Token as above    Click Next and you will see a list of topics. These topics are the collections you have in Estuary. Select the collection you want to ingest into Tinybird, and click Next.  Configure your consumer group as needed.  Finally, you will see a preview of the Data Source schema. Feel free to make any modifications as required, then click Create Data Source.  This will complete the connection with Tinybird, and new data from the Estuary Flow collection will arrive in your Tinybird Data Source in real-time. ","version":"Next","tagName":"h2"},{"title":"Materialization connectors","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/","content":"","keywords":"","version":"Next"},{"title":"Available materialization connectors​","type":1,"pageTitle":"Materialization connectors","url":"/reference/Connectors/materialization-connectors/#available-materialization-connectors","content":" AlloyDB ConfigurationPackage - ghcr.io/estuary/materialize-alloydb:dev Amazon DynamoDB ConfigurationPackage - ghcr.io/estuary/materialize-dynamodb:dev Amazon MySQL ConfigurationPackage - ghcr.io/estuary/materialize-amazon-rds-mysql:dev Amazon PostgreSQL ConfigurationPackage - ghcr.io/estuary/materialize-amazon-rds-postgres:dev Amazon Redshift ConfigurationPackage - ghcr.io/estuary/materialize-redshift:dev Amazon SQL Server ConfigurationPackage - ghcr.io/estuary/materialize-amazon-rds-sqlserver:dev Apache Iceberg Tables in S3 ConfigurationPackage — ghcr.io/estuary/materialize-s3-iceberg:dev Apache Kafka ConfigurationPackage — ghcr.io/estuary/materialize-kafka:dev Apache Parquet Files in GCS ConfigurationPackage — ghcr.io/estuary/materialize-gcs-parquet:dev Apache Parquet Files in S3 ConfigurationPackage — ghcr.io/estuary/materialize-s3-parquet:dev Azure SQL Server ConfigurationPackage - ghcr.io/estuary/materialize-sqlserver:dev CSV Files in GCS ConfigurationPackage — ghcr.io/estuary/materialize-gcs-csv:dev CSV Files in S3 ConfigurationPackage — ghcr.io/estuary/materialize-s3-csv:dev Databricks ConfigurationPackage — ghcr.io/estuary/materialize-databricks:dev Elasticsearch ConfigurationPackage — ghcr.io/estuary/materialize-elasticsearch:dev Firebolt ConfigurationPackage - ghcr.io/estuary/materialize-firebolt:dev Google BigQuery ConfigurationPackage — ghcr.io/estuary/materialize-bigquery:dev Google Cloud MySQL ConfigurationPackage - ghcr.io/estuary/materialize-google-cloud-sql-mysql:dev Google Cloud PostgreSQL ConfigurationPackage - ghcr.io/estuary/materialize-google-cloud-sql-postgres:dev Google Cloud Pub/Sub ConfigurationPackage - ghcr.io/estuary/materialize-google-pubsub:dev Google Cloud SQL Server ConfigurationPackage - ghcr.io/estuary/materialize-google-cloud-sql-sqlserver:dev Google Sheets ConfigurationPackage - ghcr.io/estuary/materialize-google-sheets:dev HTTP Webhook ConfigurationPackage - ghcr.io/estuary/materialize-webhook:dev MongoDB ConfigurationPackage - ghcr.io/estuary/materialize-mongodb:dev MotherDuck ConfigurationPackage - ghcr.io/estuary/materialize-motherduck:dev MySQL ConfigurationPackage - ghcr.io/estuary/materialize-mysql:dev MySQL Heatwave ConfigurationPackage - ghcr.io/estuary/materialize-mysql-heatwave:dev Pinecone ConfigurationPackage — ghcr.io/estuary/materialize-pinecone:dev PostgreSQL ConfigurationPackage — ghcr.io/estuary/materialize-postgres:dev Rockset ConfigurationPackage — ghcr.io/estuary/materialize-rockset:dev Slack ConfigurationPackage - ghcr.io/estuary/materialize-slack:dev Snowflake ConfigurationPackage — ghcr.io/estuary/materialize-snowflake:dev SQLite ConfigurationPackage — ghcr.io/estuary/materialize-sqlite:dev SQL Server ConfigurationPackage - ghcr.io/estuary/materialize-sqlserver:dev Starburst ConfigurationPackage - ghcr.io/estuary/materialize-starburst:dev TimescaleDB ConfigurationPackage - ghcr.io/estuary/materialize-timescaledb:dev ","version":"Next","tagName":"h2"},{"title":"Amazon DynamoDB","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/amazon-dynamodb/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon DynamoDB","url":"/reference/Connectors/materialization-connectors/amazon-dynamodb/#prerequisites","content":" To use this connector, you'll need:  An IAM user with the followingpermissions: BatchGetItem on all resourcesBatchWriteItem on all resourcesCreateTable on all resources These permissions should be specified with the dynamodb: prefix in an IAM policy document. For more details and examples, see Using identity-based policies with Amazon DynamoDBin the Amazon docs. The AWS access key and secret access key for the user. See the AWS blog for help finding these credentials.  ","version":"Next","tagName":"h2"},{"title":"Collection Requirements​","type":1,"pageTitle":"Amazon DynamoDB","url":"/reference/Connectors/materialization-connectors/amazon-dynamodb/#collection-requirements","content":" Materialized collections can have at most 2 collection keys.  By default, the materialized tables will include the collection keys as the DynamoDB partition key and sort key, and the root document. The root document is materialized as &quot;flow_document&quot; unless an alternate projection is configured for the source collection. Additional fields may be included, but DynamoDB has a 400KB size limit on an individual row so selecting too many fields of a collection with large documents will cause errors if the row size exceeds that.  The root document is materialized as a DynamoDB Map type, and the fields of the document must be valid DynamoDB Map keys.  To resolve issues with collections with more than 2 keys, excessively large documents, or incompatible field names, use a derivation to derive a new collection and materialize that collection instead.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Amazon DynamoDB","url":"/reference/Connectors/materialization-connectors/amazon-dynamodb/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the DynamoDB materialization connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Amazon DynamoDB","url":"/reference/Connectors/materialization-connectors/amazon-dynamodb/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/awsAccessKeyId\tAccess Key ID\tAWS Access Key ID for materializing to DynamoDB.\tstring\tRequired /awsSecretAccessKey\tSecret Access Key\tAWS Secret Access Key for materializing to DynamoDB.\tstring\tRequired /region\tAWS Region\tRegion of the materialized tables.\tstring\tRequired advanced/endpoint\tAWS Endpoint\tThe AWS endpoint URI to connect to. Use if you're materializing to a compatible API that isn't provided by AWS.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/table\tTable Name\tThe name of the table to be materialized to.\tstring\tRequired /delta_updates\tDelta Update\tShould updates to this table be done via delta updates. Default is false.\tboolean\tfalse  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Amazon DynamoDB","url":"/reference/Connectors/materialization-connectors/amazon-dynamodb/#sample","content":" materializations: ${PREFIX}/${MATERIALIZATION_NAME}: endpoint: connector: image: ghcr.io/estuary/materialize-dynamodb:dev config: awsAccessKeyId: &quot;example-aws-access-key-id&quot; awsSecretAccessKey: &quot;example-aws-secret-access-key&quot; region: &quot;us-east-1&quot; bindings: - resource: table: ${TABLE_NAME} source: ${PREFIX}/${COLLECTION_NAME}  ","version":"Next","tagName":"h3"},{"title":"Amazon Redshift","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/amazon-redshift/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon Redshift","url":"/reference/Connectors/materialization-connectors/amazon-redshift/#prerequisites","content":" To use this connector, you'll need:  A Redshift cluster accessible either directly or using an SSH tunnel. The user configured to connect to Redshift must have at least &quot;create table&quot; permissions for the configured schema. The connector will create new tables in the database per your specification. Tables created manually in advance are not supported. See setup for more information.An S3 bucket for staging temporary files. For best performance the bucket should be in the same region as your Redshift cluster. See this guide for instructions on setting up a new S3 bucket.An AWS root or IAM user with read and write accessto the S3 bucket. For this user, you'll need the access key and secret access key. See theAWS blog for help finding these credentials.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Amazon Redshift","url":"/reference/Connectors/materialization-connectors/amazon-redshift/#configuration","content":" Use the below properties to configure an Amazon Redshift materialization, which will direct one or more of your Flow collections to your desired tables in the database.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Amazon Redshift","url":"/reference/Connectors/materialization-connectors/amazon-redshift/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tHost and port of the database. Example: red-shift-cluster-name.account.us-east-2.redshift.amazonaws.com:5439\tstring\tRequired /user\tUser\tDatabase user to connect as.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /database\tDatabase\tName of the logical database to materialize to. The materialization will attempt to connect to the default database for the provided user if omitted.\tstring /schema\tDatabase Schema\tDatabase schema for bound collection tables (unless overridden within the binding resource configuration) as well as associated materialization metadata tables.\tstring\t&quot;public&quot; /awsAccessKeyId\tAccess Key ID\tAWS Access Key ID for reading and writing data to the S3 staging bucket.\tstring\tRequired /awsSecretAccessKey\tSecret Access Key\tAWS Secret Access Key for reading and writing data to the S3 staging bucket.\tstring\tRequired /bucket\tS3 Staging Bucket\tName of the S3 bucket to use for staging data loads.\tstring\tRequired /region\tRegion\tRegion of the S3 staging bucket. For optimal performance this should be in the same region as the Redshift database cluster.\tstring\tRequired /bucketPath\tBucket Path\tA prefix that will be used to store objects in S3.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tName of the database table.\tstring\tRequired /delta_updates\tDelta Update\tShould updates to this table be done via delta updates. Default is false.\tboolean\tfalse /schema\tAlternative Schema\tAlternative schema for this table (optional).\tstring\t  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Amazon Redshift","url":"/reference/Connectors/materialization-connectors/amazon-redshift/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: &quot;ghcr.io/estuary/materialize-redshift:dev&quot; config: address: &quot;redshift-cluster.account.us-east-2.redshift.amazonaws.com:5439&quot; user: user password: password database: db awsAccessKeyId: access_key_id awsSecretAccessKey: secret_access_key bucket: my-bucket region: us-east-2 bindings: - resource: table: ${TABLE_NAME} source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Sync Schedule​","type":1,"pageTitle":"Amazon Redshift","url":"/reference/Connectors/materialization-connectors/amazon-redshift/#sync-schedule","content":" This connector supports configuring a schedule for sync frequency. You can read about how to configure this here.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Amazon Redshift","url":"/reference/Connectors/materialization-connectors/amazon-redshift/#setup","content":" You must configure your cluster to allow connections from Estuary. This can be accomplished by making your cluster accessible over the internet for theEstuary Flow IP addresses, or using an SSH tunnel. Connecting to the S3 staging bucket does not use the network tunnel and connects over HTTPS only.  Instructions for making a cluster accessible over the internet can be foundhere. When using this option, database connections are made over SSL only.  For allowing secure connections via SSH tunneling:  Refer to the guide to configure an SSH server on using an AWS EC2 instance. Configure your connector as described in the configuration section above, with the additional of the networkTunnel stanza to enable the SSH tunnel, if using. See Connecting to endpoints on secure networks for additional details and a sample.  ","version":"Next","tagName":"h2"},{"title":"Naming Conventions​","type":1,"pageTitle":"Amazon Redshift","url":"/reference/Connectors/materialization-connectors/amazon-redshift/#naming-conventions","content":" Redshift has requirements for names and identifiers and this connector will automatically apply quoting when needed. All table identifiers and column identifiers (corresponding to Flow collection fields) are treated as lowercase, unless theenable_case_sensitive_identifierconfiguration is enabled on the cluster being materialized to. Table names for bindings must be unique on a case-insensitive basis, as well as field names of the source collection. If any names are not unique on a case-insensitive basis (ex: myField vs. MyField) the materialization will fail to apply.  If necessary, you can add projections to your collection specification to change field names.  ","version":"Next","tagName":"h2"},{"title":"Performance considerations​","type":1,"pageTitle":"Amazon Redshift","url":"/reference/Connectors/materialization-connectors/amazon-redshift/#performance-considerations","content":" For best performance there should at most one Redshift materialization active per Redshift schema. Additional collections to be materialized should be added as bindings to this single materialization rather than creating a separate materialization for each collection.  In order to achieve exactly-once processing of collection documents, the materialization creates and uses metadata tables located in the schema configured by the endpoint schema property. To commit a transaction, a table-level lock is acquired on these metadata tables. If there are multiple materializations using the same metadata tables, they will need to take turns acquiring these locks. This locking behavior prevents &quot;serializable isolation violation&quot; errors in the case of multiple materializations sharing the same metadata tables at the expense of allowing only a single materialization to be actively committing a transaction.  ","version":"Next","tagName":"h2"},{"title":"Maximum record size​","type":1,"pageTitle":"Amazon Redshift","url":"/reference/Connectors/materialization-connectors/amazon-redshift/#maximum-record-size","content":" The maximum size of a single input document is 4 MB. Attempting to materialize collections with documents larger than 4 MB will result in an error. To materialize this data you can use aderivation to create a derived collection with smaller documents, or exclude fields containing excessive amounts of data by customizing the materialized fields.  ","version":"Next","tagName":"h2"},{"title":"Delta updates​","type":1,"pageTitle":"Amazon Redshift","url":"/reference/Connectors/materialization-connectors/amazon-redshift/#delta-updates","content":" This connector supports both standard (merge) and delta updates. The default is to use standard updates. ","version":"Next","tagName":"h2"},{"title":"CSV Files in Amazon S3","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/amazon-s3-csv/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"CSV Files in Amazon S3","url":"/reference/Connectors/materialization-connectors/amazon-s3-csv/#prerequisites","content":" To use this connector, you'll need:  An S3 bucket to write files to. See this guide for instructions on setting up a new S3 bucket.An AWS root or IAM user with thes3:PutObject permission for the S3 bucket. For this user, you'll need the access key and secret access key. See the AWS blog for help finding these credentials.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"CSV Files in Amazon S3","url":"/reference/Connectors/materialization-connectors/amazon-s3-csv/#configuration","content":" Use the below properties to configure the materialization, which will direct one or more of your Flow collections to your bucket.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"CSV Files in Amazon S3","url":"/reference/Connectors/materialization-connectors/amazon-s3-csv/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/bucket\tBucket\tBucket to store materialized objects.\tstring\tRequired /awsAccessKeyId\tAWS Access Key ID\tAccess Key ID for writing data to the bucket.\tstring\tRequired /awsSecretAccessKey\tAWS Secret Access key\tSecret Access Key for writing data to the bucket.\tstring\tRequired /region\tRegion\tRegion of the bucket to write to.\tstring\tRequired /uploadInterval\tUpload Interval\tFrequency at which files will be uploaded.\tstring\t5m /prefix\tPrefix\tOptional prefix that will be used to store objects.\tstring /fileSizeLimit\tFile Size Limit\tApproximate maximum size of materialized files in bytes. Defaults to 10737418240 (10 GiB) if blank.\tinteger /endpoint\tCustom S3 Endpoint\tThe S3 endpoint URI to connect to. Use if you're materializing to a compatible API that isn't provided by AWS. Should normally be left blank.\tstring /csvConfig/delimiter\tDelimiter\tCharacter to separate columns within a row. Defaults to a comma if blank. Must be a single character with a byte length of 1.\tinteger /csvConfig/nullString\tNull String\tString to use to represent NULL values. Defaults to an empty string if blank.\tinteger /csvConfig/skipHeaders\tSkip Headers\tDo not write headers to files.\tinteger\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/path\tPath\tThe path that objects will be materialized to.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"CSV Files in Amazon S3","url":"/reference/Connectors/materialization-connectors/amazon-s3-csv/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: &quot;ghcr.io/estuary/materialize-s3-csv:dev&quot; config: bucket: bucket awsAccessKeyId: &lt;access_key_id&gt; awsSecretAccessKey: &lt;secret_access_key&gt; region: us-east-2 uploadInterval: 5m bindings: - resource: path: ${COLLECTION_NAME} source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"File Names​","type":1,"pageTitle":"CSV Files in Amazon S3","url":"/reference/Connectors/materialization-connectors/amazon-s3-csv/#file-names","content":" Materialized files are named with monotonically increasing integer values, padded with leading 0's so they remain lexically sortable. For example, a set of files may be materialized like this for a given collection:  bucket/prefix/path/v0000000000/00000000000000000000.csv bucket/prefix/path/v0000000000/00000000000000000001.csv bucket/prefix/path/v0000000000/00000000000000000002.csv   Here the values for bucket and prefix are from your endpoint configuration. The path is specific to the binding configuration. v0000000000 represents the current backfill counterfor binding and will be increased if the binding is re-backfilled, along with the file names starting back over from 0.  ","version":"Next","tagName":"h2"},{"title":"Eventual Consistency​","type":1,"pageTitle":"CSV Files in Amazon S3","url":"/reference/Connectors/materialization-connectors/amazon-s3-csv/#eventual-consistency","content":" In rare circumstances, recently materialized files may be re-written by files with the same name if the materialization shard is interrupted in the middle of processing a Flow transaction and the transaction must be re-started. Files that were committed as part of a completed transaction will never be re-written. In this way, eventually all collection data will be written to files effectively-once, although inconsistencies are possible when accessing the most recently written data. ","version":"Next","tagName":"h2"},{"title":"AlloyDB","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/alloydb/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"AlloyDB","url":"/reference/Connectors/materialization-connectors/alloydb/#prerequisites","content":" To use this connector, you'll need:  An AlloyDB database to which to materialize, and user credentials. The connector will create new tables in the database per your specification. Tables created manually in advance are not supported.A virtual machine to connect securely to the instance via SSH tunneling. (AlloyDB doesn't support IP allowlisting.) Follow the instructions to create a virtual machine for SSH tunnelingin the same Google Cloud project as your instance.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"AlloyDB","url":"/reference/Connectors/materialization-connectors/alloydb/#configuration","content":" To use this connector, begin with data in one or more Flow collections. Use the below properties to configure an AlloyDB materialization, which will direct one or more of your Flow collections to your desired tables, or views, in the database.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"AlloyDB","url":"/reference/Connectors/materialization-connectors/alloydb/#properties","content":" Endpoint​  The SSH config section is required for this connector. You'll fill in the database address with a localhost IP address, and specify your VM's IP address as the SSH address. See the table below and the sample config.  Property\tTitle\tDescription\tType\tRequired/Default/database\tDatabase\tName of the logical database to materialize to.\tstring /address\tAddress\tHost and port. Set to 127.0.0.1:5432 to enable SSH tunneling.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /schema\tDatabase Schema\tDatabase schema to use for materialized tables (unless overridden within the binding resource configuration) as well as associated materialization metadata tables\tstring\t&quot;public&quot; /user\tUser\tDatabase user to connect as.\tstring\tRequired networkTunnel\tNetwork Tunnel\tConnect to your system through an SSH server that acts as a bastion host for your network.\tObject networkTunnel/sshForwarding\tSSH Forwarding Object networkTunnel/sshForwarding/sshEndpoint\tSSH Endpoint\tEndpoint of the remote SSH server (in this case, your Google Cloud VM) that supports tunneling (in the form of ssh://user@address.\tString networkTunnel/sshForwarding/privateKey\tSSH Private Key\tPrivate key to connect to the remote SSH server.\tString\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/additional_table_create_sql\tAdditional Table Create SQL\tAdditional SQL statement(s) to be run in the same transaction that creates the table.\tstring /delta_updates\tDelta Update\tShould updates to this table be done via delta updates.\tboolean\tfalse /schema\tAlternative Schema\tAlternative schema for this table (optional). Overrides schema set in endpoint configuration.\tstring /table\tTable\tTable name to materialize to. It will be created by the connector, unless the connector has previously created it.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"AlloyDB","url":"/reference/Connectors/materialization-connectors/alloydb/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: ghcr.io/estuary/materialize-alloydb:dev config: database: postgres address: 127.0.0.1:5432 password: flow user: flow networkTunnel: sshForwarding: sshEndpoint: ssh://sshUser@&lt;vm-ip-address&gt; privateKey: |2 -----BEGIN RSA PRIVATE KEY----- MIICXAIBAAKBgQCJO7G6R+kv2MMS8Suw21sk2twHg8Vog0fjimEWJEwyAfFM/Toi EJ6r5RTaSvN++/+MPWUll7sUdOOBZr6ErLKLHEt7uXxusAzOjMxFKZpEARMcjwHY v/tN1A2OYU0qay1DOwknEE0i+/Bvf8lMS7VDjHmwRaBtRed/+iAQHf128QIDAQAB AoGAGoOUBP+byAjDN8esv1DCPU6jsDf/Tf//RbEYrOR6bDb/3fYW4zn+zgtGih5t CR268+dwwWCdXohu5DNrn8qV/Awk7hWp18mlcNyO0skT84zvippe+juQMK4hDQNi ywp8mDvKQwpOuzw6wNEitcGDuACx5U/1JEGGmuIRGx2ST5kCQQDsstfWDcYqbdhr 5KemOPpu80OtBYzlgpN0iVP/6XW1e5FCRp2ofQKZYXVwu5txKIakjYRruUiiZTza QeXRPbp3AkEAlGx6wMe1l9UtAAlkgCFYbuxM+eRD4Gg5qLYFpKNsoINXTnlfDry5 +1NkuyiQDjzOSPiLZ4Abpf+a+myjOuNL1wJBAOwkdM6aCVT1J9BkW5mrCLY+PgtV GT80KTY/d6091fBMKhxL5SheJ4SsRYVFtguL2eA7S5xJSpyxkadRzR0Wj3sCQAvA bxO2fE1SRqbbF4cBnOPjd9DNXwZ0miQejWHUwrQO0inXeExNaxhYKQCcnJNUAy1J 6JfAT/AbxeSQF3iBKK8CQAt5r/LLEM1/8ekGOvBh8MAQpWBW771QzHUN84SiUd/q xR9mfItngPwYJ9d/pTO7u9ZUPHEoat8Ave4waB08DsI= -----END RSA PRIVATE KEY----- bindings: - resource: table: ${TABLE_NAME} source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Reserved words​","type":1,"pageTitle":"AlloyDB","url":"/reference/Connectors/materialization-connectors/alloydb/#reserved-words","content":" PostgreSQL has a list of reserved words that must be quoted in order to be used as an identifier. Flow considers all the reserved words that are marked as &quot;reserved&quot; in any of the columns in the official PostgreSQL documentation.  These reserved words are listed in the table below. Flow automatically quotes fields that are in this list.  Reserved words abs\tcurrent_transform_group_for_type\tindicator\torder\tsqlexception absolute\tcurrent_user\tinitial\tout\tsqlstate acos\tcursor\tinitially\touter\tsqlwarning action\tcycle\tinner\toutput\tsqrt add\tdatalink\tinout\tover\tstart all\tdate\tinput\toverlaps\tstatic allocate\tday\tinsensitive\toverlay\tstddev_pop alter\tdeallocate\tinsert\tpad\tstddev_samp analyse\tdec\tint\tparameter\tsubmultiset analyze\tdecfloat\tinteger\tpartial\tsubset and\tdecimal\tintersect\tpartition\tsubstring any\tdeclare\tintersection\tpattern\tsubstring_regex are\tdefault\tinterval\tper\tsucceeds array\tdeferrable\tinto\tpercent\tsum array_agg\tdeferred\tis\tpercentile_cont\tsymmetric array_max_cardinality\tdefine\tisnull\tpercentile_disc\tsystem as\tdelete\tisolation\tpercent_rank\tsystem_time asc\tdense_rank\tjoin\tperiod\tsystem_user asensitive\tderef\tjson_array\tpermute\ttable asin\tdesc\tjson_arrayagg\tplacing\ttablesample assertion\tdescribe\tjson_exists\tportion\ttan asymmetric\tdescriptor\tjson_object\tposition\ttanh at\tdeterministic\tjson_objectagg\tposition_regex\ttemporary atan\tdiagnostics\tjson_query\tpower\tthen atomic\tdisconnect\tjson_table\tprecedes\ttime authorization\tdistinct\tjson_table_primitive\tprecision\ttimestamp avg\tdlnewcopy\tjson_value\tprepare\ttimezone_hour begin\tdlpreviouscopy\tkey\tpreserve\ttimezone_minute begin_frame\tdlurlcomplete\tlag\tprimary\tto begin_partition\tdlurlcompleteonly\tlanguage\tprior\ttrailing between\tdlurlcompletewrite\tlarge\tprivileges\ttransaction bigint\tdlurlpath\tlast\tprocedure\ttranslate binary\tdlurlpathonly\tlast_value\tptf\ttranslate_regex bit\tdlurlpathwrite\tlateral\tpublic\ttranslation bit_length\tdlurlscheme\tlead\trange\ttreat blob\tdlurlserver\tleading\trank\ttrigger boolean\tdlvalue\tleft\tread\ttrim both\tdo\tlevel\treads\ttrim_array by\tdomain\tlike\treal\ttrue call\tdouble\tlike_regex\trecursive\ttruncate called\tdrop\tlimit\tref\tuescape cardinality\tdynamic\tlistagg\treferences\tunion cascade\teach\tln\treferencing\tunique cascaded\telement\tlocal\tregr_avgx\tunknown case\telse\tlocaltime\tregr_avgy\tunmatched cast\tempty\tlocaltimestamp\tregr_count\tunnest catalog\tend\tlog\tregr_intercept\tupdate ceil\tend-exec\tlog10\tregr_r2\tupper ceiling\tend_frame\tlower\tregr_slope\tusage char\tend_partition\tmatch\tregr_sxx\tuser character\tequals\tmatches\tregr_sxy\tusing character_length\tescape\tmatch_number\tregr_syy\tvalue char_length\tevery\tmatch_recognize\trelative\tvalues check\texcept\tmax\trelease\tvalue_of classifier\texception\tmeasures\trestrict\tvarbinary clob\texec\tmember\tresult\tvarchar close\texecute\tmerge\treturn\tvariadic coalesce\texists\tmethod\treturning\tvarying collate\texp\tmin\treturns\tvar_pop collation\texternal\tminute\trevoke\tvar_samp collect\textract\tmod\tright\tverbose column\tfalse\tmodifies\trollback\tversioning commit\tfetch\tmodule\trollup\tview concurrently\tfilter\tmonth\trow\twhen condition\tfirst\tmultiset\trows\twhenever connect\tfirst_value\tnames\trow_number\twhere connection\tfloat\tnational\trunning\twidth_bucket constraint\tfloor\tnatural\tsavepoint\twindow constraints\tfor\tnchar\tschema\twith contains\tforeign\tnclob\tscope\twithin continue\tfound\tnew\tscroll\twithout convert\tframe_row\tnext\tsearch\twork copy\tfree\tno\tsecond\twrite corr\tfreeze\tnone\tsection\txml corresponding\tfrom\tnormalize\tseek\txmlagg cos\tfull\tnot\tselect\txmlattributes cosh\tfunction\tnotnull\tsensitive\txmlbinary count\tfusion\tnth_value\tsession\txmlcast covar_pop\tget\tntile\tsession_user\txmlcomment covar_samp\tglobal\tnull\tset\txmlconcat create\tgo\tnullif\tshow\txmldocument cross\tgoto\tnumeric\tsimilar\txmlelement cube\tgrant\toccurrences_regex\tsin\txmlexists cume_dist\tgroup\toctet_length\tsinh\txmlforest current\tgrouping\tof\tsize\txmliterate current_catalog\tgroups\toffset\tskip\txmlnamespaces current_date\thaving\told\tsmallint\txmlparse current_default_transform_group\thold\tomit\tsome\txmlpi current_path\thour\ton\tspace\txmlquery current_role\tidentity\tone\tspecific\txmlserialize current_row\tilike\tonly\tspecifictype\txmltable current_schema\timmediate\topen\tsql\txmltext current_time\timport\toption\tsqlcode\txmlvalidate current_timestamp\tin\tor\tsqlerror\tyear ","version":"Next","tagName":"h2"},{"title":"Apache Iceberg Tables in Amazon S3","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/amazon-s3-iceberg/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Apache Iceberg Tables in Amazon S3","url":"/reference/Connectors/materialization-connectors/amazon-s3-iceberg/#prerequisites","content":" To use this connector, you'll need:  An S3 bucket to write files to. See this guide for instructions on setting up a new S3 bucket.  An AWS root or IAM user with read and write accessto the S3 bucket. For this user, you'll need the access key and secret access key. See the AWS blog for help finding these credentials.  If using the AWS Glue Catalog:  The AWS root or IAM user must have access to AWS Glue. See this guide for instructions on setting up IAM permissions for a user to access AWS Glue.  If using the REST Catalog:  The URI for connecting to the catalog.The name of the warehouse to connect to.Credentials for connecting to the catalog.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Apache Iceberg Tables in Amazon S3","url":"/reference/Connectors/materialization-connectors/amazon-s3-iceberg/#configuration","content":" Use the below properties to configure the materialization, which will direct one or more of your Flow collections to your tables.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Apache Iceberg Tables in Amazon S3","url":"/reference/Connectors/materialization-connectors/amazon-s3-iceberg/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/aws_access_key_id\tAWS Access Key ID\tAccess Key ID for accessing AWS services.\tstring\tRequired /aws_secret_access_key\tAWS Secret Access key\tSecret Access Key for accessing AWS services.\tstring\tRequired /bucket\tBucket\tThe S3 bucket to write data files to.\tstring\tRequired /prefix\tPrefix\tOptional prefix that will be used to store objects.\tstring /region\tRegion\tAWS Region.\tstring\tRequired /namespace\tNamespace\tNamespace for bound collection tables (unless overridden within the binding resource configuration).\tstring\tRequired /upload_interval\tUpload Interval\tFrequency at which files will be uploaded. Must be a valid ISO8601 duration string no greater than 4 hours.\tstring\tPT5M /upload_interval\tUpload Interval\tFrequency at which files will be uploaded. Must be a valid ISO8601 duration string no greater than 4 hours.\tstring\tPT5M /catalog/catalog_type\tCatalog Type\tEither &quot;Iceberg REST Server&quot; or &quot;AWS Glue&quot;.\tstring\tRequired /catalog/uri\tURI\tURI identifying the REST catalog, in the format of 'https://yourserver.com/catalog'.\tstring\tRequired /catalog/credential\tCredential\tCredential for connecting to the REST catalog.\tstring /catalog/token\tToken\tToken for connecting to the TEST catalog.\tstring /catalog/warehouse\tWarehouse\tWarehouse to connect to in the REST catalog.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tName of the database table.\tstring\tRequired /namespace\tAlternative Namespace\tAlternative namespace for this table (optional).\tstring /delta_updates\tDelta Updates\tShould updates to this table be done via delta updates. Currently this connector only supports delta updates.\tbool\ttrue  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Apache Iceberg Tables in Amazon S3","url":"/reference/Connectors/materialization-connectors/amazon-s3-iceberg/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: &quot;ghcr.io/estuary/materialize-s3-iceberg:dev&quot; config: aws_access_key_id: &lt;access_key_id&gt; aws_secret_access_key: &lt;secret_access_key&gt; bucket: bucket region: us-east-2 namespace: namespace upload_interval: PT5M bindings: - resource: table: ${COLLECTION_NAME} delta_updates: true source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Iceberg Column Types​","type":1,"pageTitle":"Apache Iceberg Tables in Amazon S3","url":"/reference/Connectors/materialization-connectors/amazon-s3-iceberg/#iceberg-column-types","content":" Flow collection fields are written to Iceberg table columns based on the data type of the field. Iceberg V2 primitive type columns are created for these Flow collection fields:  Collection Field Data Type\tIceberg Column Typearray\tstring object\tstring boolean\tboolean integer\tlong number\tdouble string with {contentEncoding: base64}\tbinary string with {format: date-time}\ttimestamptz (with microsecond precision) string with {format: date}\tdate string with {format: integer}\tlong string with {format: number}\tdouble string (all others)\tstring  Flow collection fields with {type: string, format: time} and {type: string, format: uuid} are materialized as string columns rather than time and uuid columns for compatibility with Apache Spark. Nested types are not currently supported.  ","version":"Next","tagName":"h2"},{"title":"Table Maintenance​","type":1,"pageTitle":"Apache Iceberg Tables in Amazon S3","url":"/reference/Connectors/materialization-connectors/amazon-s3-iceberg/#table-maintenance","content":" To ensure optimal query performance, you should conduct regular maintenance for your materialized tables since the connector will not perform this maintenance automatically (support for automatic table maintenance is planned).  If you're using the AWS Glue catalog, you can enable automatic data file compaction by followingthis guide.  ","version":"Next","tagName":"h2"},{"title":"At-Least-Once Semantics​","type":1,"pageTitle":"Apache Iceberg Tables in Amazon S3","url":"/reference/Connectors/materialization-connectors/amazon-s3-iceberg/#at-least-once-semantics","content":" In rare cases, it may be possible for documents from a source collection to be appended to a target table more than once. Users of materialized tables should take this possibility into consideration when querying these tables. ","version":"Next","tagName":"h2"},{"title":"Apache Kafka","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/apache-kafka/","content":"","keywords":"","version":"Next"},{"title":"Supported message formats​","type":1,"pageTitle":"Apache Kafka","url":"/reference/Connectors/materialization-connectors/apache-kafka/#supported-message-formats","content":" This connectors supports materializing Kafka messages encoded in Avro or JSON format.  For Avro messages, the connector must be configured to use a schema registry.  JSON messages may be materialized without a schema registry.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Apache Kafka","url":"/reference/Connectors/materialization-connectors/apache-kafka/#prerequisites","content":" A Kafka cluster with: bootstrap.serversconfigured so that clients may connect via the desired host and portAn authentication mechanism of choice set upConnection security enabled with TLS If using Avro message format with schema registry: The endpoint to use for connecting to the schema registryUsername for authenticationPassword for authentication  tip If you are using the Confluent Cloud Schema Registry, your schema registry username and password will be the key and secret from your schema registry API key. See the Confluent Cloud Schema Registry Documentationfor help setting up a schema registry API key.  ","version":"Next","tagName":"h2"},{"title":"Authentication and connection security​","type":1,"pageTitle":"Apache Kafka","url":"/reference/Connectors/materialization-connectors/apache-kafka/#authentication-and-connection-security","content":" A wide variety of authentication methods are available for Kafka clusters. Flow supports SASL/SCRAM-SHA-256, SASL/SCRAM-SHA-512, and SASL/PLAIN. When authentication details are not provided, the client connection will attempt to use PLAINTEXT (insecure) protocol.  If you don't already have authentication enabled on your cluster, Estuary recommends either of the listedSASL/SCRAMmethods. With SCRAM, you set up a username and password, making it analogous to the traditional authentication mechanisms you use in other applications.  tip If you are connecting to Kafka hosted on Confluent Cloud, select the PLAINSASL mechanism.  For connection security, Estuary recommends that you enable TLS encryption for your SASL mechanism of choice, as well as all other components of your cluster. Note that because TLS replaced now-deprecated SSL encryption, Kafka still uses the acronym &quot;SSL&quot; to refer to TLS encryption. See Confluent's documentationfor details.  Beta TLS encryption is currently the only supported connection security mechanism for this connector. Other connection security methods may be enabled in the future.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Apache Kafka","url":"/reference/Connectors/materialization-connectors/apache-kafka/#configuration","content":" Use the below properties to configure the Apache Kafka materialization, which will direct one or more of your Flow collections to your desired topics.  Note that, by default, all top-level fields are recommended for materialization. You can also de-select unnecessary top-level fields or include additional nested fields in the &quot;Field Selection&quot; section of the resource configuration.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Apache Kafka","url":"/reference/Connectors/materialization-connectors/apache-kafka/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/bootstrap_servers\tBootstrap servers\tThe initial servers in the Kafka cluster to connect to, separated by commas.\tstring\tRequired /message_format\tMessage Format\tFormat for materialized messages. Avro format requires a schema registry configuration. Messages in JSON format do not use a schema registry.\tstring\tRequired /topic_partitions\tTopic Partitions\tThe number of partitions to create new topics with.\tinteger\t6 /topic_replication_factor\tTopic Replication Factor\tThe replication factor to create new topics with.\tinteger\t3 /credentials\tCredentials\tConnection details used to authenticate a client connection to Kafka via SASL.\tobject\tRequired /tls\tTLS\tTLS connection settings.\tstring\t&quot;system_certificates&quot; /credentials/auth_type\tAuthentication type\tThe type of authentication to use. Currently supports UserPassword.\tstring /credentials/mechanism\tSASL Mechanism\tSASL mechanism describing how to exchange and authenticate client servers.\tstring /credentials/username\tUsername\tUsername, if applicable for the authentication mechanism chosen.\tstring /credentials/password\tPassword\tPassword, if applicable for the authentication mechanism chosen.\tstring /schema_registry\tSchema Registry\tConnection details for interacting with a schema registry.\tobject /schema_registry/endpoint\tSchema Registry Endpoint\tSchema registry API endpoint. For example: https://registry-id.us-east-2.aws.confluent.cloud.\tstring /schema_registry/username\tSchema Registry Username\tSchema registry username to use for authentication. If you are using Confluent Cloud, this will be the 'Key' from your schema registry API key.\tstring /schema_registry/password\tSchema Registry Password\tSchema registry password to use for authentication. If you are using Confluent Cloud, this will be the 'Secret' from your schema registry API key.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/topic\tTopic\tName of the Kafka topic to materialize to.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Apache Kafka","url":"/reference/Connectors/materialization-connectors/apache-kafka/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: bootstrap_servers: server1:9092,server2:9092 tls: system_certificates credentials: auth_type: UserPassword mechanism: SCRAM-SHA-512 username: bruce.wayne password: definitely-not-batman schema_registry: endpoint: https://schema.registry.com username: schemaregistry.username password: schemaregistry.password bindings: - resource: topic: ${TOPIC_NAME} source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Delta updates​","type":1,"pageTitle":"Apache Kafka","url":"/reference/Connectors/materialization-connectors/apache-kafka/#delta-updates","content":" This connector supports delta updates for materializing documents. ","version":"Next","tagName":"h2"},{"title":"Apache Parquet Files in Amazon S3","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/amazon-s3-parquet/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Apache Parquet Files in Amazon S3","url":"/reference/Connectors/materialization-connectors/amazon-s3-parquet/#prerequisites","content":" To use this connector, you'll need:  An S3 bucket to write files to. See this guide for instructions on setting up a new S3 bucket.An AWS root or IAM user with thes3:PutObject permission for the S3 bucket. For this user, you'll need the access key and secret access key. See the AWS blog for help finding these credentials.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Apache Parquet Files in Amazon S3","url":"/reference/Connectors/materialization-connectors/amazon-s3-parquet/#configuration","content":" Use the below properties to configure the materialization, which will direct one or more of your Flow collections to your bucket.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Apache Parquet Files in Amazon S3","url":"/reference/Connectors/materialization-connectors/amazon-s3-parquet/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/bucket\tBucket\tBucket to store materialized objects.\tstring\tRequired /awsAccessKeyId\tAWS Access Key ID\tAccess Key ID for writing data to the bucket.\tstring\tRequired /awsSecretAccessKey\tAWS Secret Access key\tSecret Access Key for writing data to the bucket.\tstring\tRequired /region\tRegion\tRegion of the bucket to write to.\tstring\tRequired /uploadInterval\tUpload Interval\tFrequency at which files will be uploaded.\tstring\t5m /prefix\tPrefix\tOptional prefix that will be used to store objects.\tstring /fileSizeLimit\tFile Size Limit\tApproximate maximum size of materialized files in bytes. Defaults to 10737418240 (10 GiB) if blank.\tinteger /endpoint\tCustom S3 Endpoint\tThe S3 endpoint URI to connect to. Use if you're materializing to a compatible API that isn't provided by AWS. Should normally be left blank.\tstring /parquetConfig/rowGroupRowLimit\tRow Group Row Limit\tMaximum number of rows in a row group. Defaults to 1000000 if blank.\tinteger /parquetConfig/rowGroupByteLimit\tRow Group Byte Limit\tApproximate maximum number of bytes in a row group. Defaults to 536870912 (512 MiB) if blank.\tinteger\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/path\tPath\tThe path that objects will be materialized to.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Apache Parquet Files in Amazon S3","url":"/reference/Connectors/materialization-connectors/amazon-s3-parquet/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: &quot;ghcr.io/estuary/materialize-s3-parquet:dev&quot; config: bucket: bucket awsAccessKeyId: &lt;access_key_id&gt; awsSecretAccessKey: &lt;secret_access_key&gt; region: us-east-2 uploadInterval: 5m bindings: - resource: path: ${COLLECTION_NAME} source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Parquet Data Types​","type":1,"pageTitle":"Apache Parquet Files in Amazon S3","url":"/reference/Connectors/materialization-connectors/amazon-s3-parquet/#parquet-data-types","content":" Flow collection fields are written to Parquet files based on the data type of the field. Depending on the field data type, the Parquet data type may be a primitive Parquet type, or a primitive Parquet type extended by alogical Parquet type.  Collection Field Data Type\tParquet Data Type\tarray\tJSON (extends BYTE_ARRAY) object\tJSON (extends BYTE_ARRAY) boolean\tBOOLEAN integer\tINT64 number\tDOUBLE string with {contentEncoding: base64}\tBYTE_ARRAY string with {format: date}\tDATE (extends BYTE_ARRAY) string with {format: date-time}\tTIMESTAMP (extends INT64, UTC adjusted with microsecond precision) string with {format: time}\tTIME (extends INT64, UTC adjusted with microsecond precision) string with {format: date}\tDATE (extends INT32) string with {format: duration}\tINTERVAL (extends FIXED_LEN_BYTE_ARRAY with a length of 12) string with {format: uuid}\tUUID (extends FIXED_LEN_BYTE_ARRAY with a length of 16) string (all others)\tSTRING (extends BYTE_ARRAY)\t  ","version":"Next","tagName":"h2"},{"title":"File Names​","type":1,"pageTitle":"Apache Parquet Files in Amazon S3","url":"/reference/Connectors/materialization-connectors/amazon-s3-parquet/#file-names","content":" Materialized files are named with monotonically increasing integer values, padded with leading 0's so they remain lexically sortable. For example, a set of files may be materialized like this for a given collection:  bucket/prefix/path/v0000000000/00000000000000000000.parquet bucket/prefix/path/v0000000000/00000000000000000001.parquet bucket/prefix/path/v0000000000/00000000000000000002.parquet   Here the values for bucket and prefix are from your endpoint configuration. The path is specific to the binding configuration. v0000000000 represents the current backfill counterfor binding and will be increased if the binding is re-backfilled, along with the file names starting back over from 0.  ","version":"Next","tagName":"h2"},{"title":"Eventual Consistency​","type":1,"pageTitle":"Apache Parquet Files in Amazon S3","url":"/reference/Connectors/materialization-connectors/amazon-s3-parquet/#eventual-consistency","content":" In rare circumstances, recently materialized files may be re-written by files with the same name if the materialization shard is interrupted in the middle of processing a Flow transaction and the transaction must be re-started. Files that were committed as part of a completed transaction will never be re-written. In this way, eventually all collection data will be written to files effectively-once, although inconsistencies are possible when accessing the most recently written data. ","version":"Next","tagName":"h2"},{"title":"Google BigQuery","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/BigQuery/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Google BigQuery","url":"/reference/Connectors/materialization-connectors/BigQuery/#prerequisites","content":" To use this connector, you'll need:  A new Google Cloud Storage bucket in the same region as the BigQuery destination dataset. A Google Cloud service account with a key file generated and the following roles: roles/bigquery.dataEditor on the destination datasetroles/bigquery.jobUser on the project with which the BigQuery destination dataset is associatedroles/bigquery.readSessionUser on the project with which the BigQuery destination dataset is associatedroles/storage.objectAdminon the GCS bucket created above See Setup for detailed steps to set up your service account.  tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Google BigQuery","url":"/reference/Connectors/materialization-connectors/BigQuery/#setup","content":" To configure your service account, complete the following steps.  Log into the Google Cloud console and create a service account. During account creation: Grant the user access to the project.Grant the user roles roles/bigquery.dataEditor, roles/bigquery.jobUser, roles/bigquery.readSessionUser and roles/storage.objectAdmin.Click Done. Select the new service account from the list of service accounts. On the Keys tab, click Add key and create a new JSON key. The key is automatically downloaded. You'll use it to configure the connector.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Google BigQuery","url":"/reference/Connectors/materialization-connectors/BigQuery/#configuration","content":" To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a BigQuery materialization, which will direct one or more of your Flow collections to your desired tables within a BigQuery dataset.  A BigQuery dataset is the top-level container within a project, and comprises multiple tables. You can think of a dataset as somewhat analogous to a schema in a relational database. For a complete introduction to resource organization in Bigquery, see the BigQuery docs.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Google BigQuery","url":"/reference/Connectors/materialization-connectors/BigQuery/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/project_id\tProject ID\tThe project ID for the Google Cloud Storage bucket and BigQuery dataset.\tString\tRequired /credentials_json\tService Account JSON\tThe JSON credentials of the service account to use for authorization.\tString\tRequired /region\tRegion\tThe GCS region.\tString\tRequired /dataset\tDataset\tBigQuery dataset for bound collection tables (unless overridden within the binding resource configuration) as well as associated materialization metadata tables.\tString\tRequired /bucket\tBucket\tName of the GCS bucket.\tString\tRequired /bucket_path\tBucket path\tBase path within the GCS bucket. Also called &quot;Folder&quot; in the GCS console.\tString /billing_project_id\tBilling project ID\tThe project ID to which these operations are billed in BigQuery. Typically, you want this to be the same as project_id (the default).\tString\tSame as project_id  To learn more about project billing, see the BigQuery docs.  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tTable in the BigQuery dataset to store materialized result in.\tstring\tRequired /dataset\tTable\tAlternative dataset for this table. Must be located in the region set in the endpoint configuration.\tstring /delta_updates\tDelta updates.\tWhether to use standard or delta updates\tboolean\tfalse  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Google BigQuery","url":"/reference/Connectors/materialization-connectors/BigQuery/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: project_id: our-bigquery-project dataset: materialized-data region: US bucket: our-gcs-bucket bucket_path: bucket-path/ credentials_json: &lt;secret&gt; image: ghcr.io/estuary/materialize-bigquery:dev bindings: - resource: table: ${table_name} source: ${PREFIX}/${source_collection}   ","version":"Next","tagName":"h3"},{"title":"Sync Schedule​","type":1,"pageTitle":"Google BigQuery","url":"/reference/Connectors/materialization-connectors/BigQuery/#sync-schedule","content":" This connector supports configuring a schedule for sync frequency. You can read about how to configure this here.  ","version":"Next","tagName":"h2"},{"title":"Storage Read API​","type":1,"pageTitle":"Google BigQuery","url":"/reference/Connectors/materialization-connectors/BigQuery/#storage-read-api","content":" This connector is able to use the BigQuery Storage Read API for reading results of queries executed for standard updates bindings. For optimal performance, the BigQuery Read Session User role should be granted to the configured service account to enable using the storage read API.  If the BigQuery Read Session User role is not available, slower mechanisms will be used to read query results.  ","version":"Next","tagName":"h2"},{"title":"Delta updates​","type":1,"pageTitle":"Google BigQuery","url":"/reference/Connectors/materialization-connectors/BigQuery/#delta-updates","content":" This connector supports both standard (merge) and delta updates. The default is to use standard updates.  Enabling delta updates will prevent Flow from querying for documents in your BigQuery table, which can reduce latency and costs for large datasets. If you're certain that all events will have unique keys, enabling delta updates is a simple way to improve performance with no effect on the output. However, enabling delta updates is not suitable for all workflows, as the resulting table in BigQuery won't be fully reduced.  You can enable delta updates on a per-binding basis:   bindings: - resource: table: ${table_name} delta_updates: true source: ${PREFIX}/${source_collection}   ","version":"Next","tagName":"h2"},{"title":"Table Partitioning​","type":1,"pageTitle":"Google BigQuery","url":"/reference/Connectors/materialization-connectors/BigQuery/#table-partitioning","content":" Tables are automatically created withclustering based on the Flow collection primary keys. Tables are not created with any other partitioning, but pre-existing partitioned tables can be materialized to.  It isn't possible to alter the partitioning of an existing table, but you can convert an existing table to one with partitioning by creating a new table and copying the data from the existing table into it. This can be done to tables that the connector is materializing to, as long as the materializing task is temporarily disabled while doing the conversion.  To convert an existing materialized table to one with different partitioning:  Pause your materialization by disabling it from the UI or editing the task specification with the CLI.Create a new table with the partitioning you want from the data in the existing table:  create table &lt;your_dataset&gt;.&lt;your_schema&gt;.&lt;your_table&gt;_copy partition by &lt;your_partitioning&gt; as select * from &lt;your_dataset&gt;.&lt;your_schema&gt;.&lt;your_table&gt;;   Verify that the data in &lt;your_table&gt;_copy looks good, then drop the original table:  drop table &lt;your_dataset&gt;.&lt;your_schema&gt;.&lt;your_table&gt;;   &quot;Rename&quot; &lt;your_table&gt;_copy back to &lt;your_table&gt; by copying it as a new table with the original name of &lt;your_table&gt;:  create table &lt;your_dataset&gt;.&lt;your_schema&gt;.&lt;your_table&gt; copy &lt;your_dataset&gt;.&lt;your_schema&gt;.&lt;your_table&gt;_copy;   Verify that the data in &lt;your_table&gt; looks good, then drop the &lt;your_table&gt;_copy table:  drop table &lt;your_dataset&gt;.&lt;your_schema&gt;.&lt;your_table&gt;_copy;   Re-enable the materialization to continue materializing data to the now partitioned table. ","version":"Next","tagName":"h2"},{"title":"Databricks","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/databricks/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Databricks","url":"/reference/Connectors/materialization-connectors/databricks/#prerequisites","content":" To use this connector, you'll need:  A Databricks account that includes: A unity catalogA SQL WarehouseA schema — a logical grouping of tables in a catalogA user or service principal with a role assigned that grants the appropriate access levels to these resources. At least one Flow collection  tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Databricks","url":"/reference/Connectors/materialization-connectors/databricks/#setup","content":" You need to first create a SQL Warehouse if you don't already have one in your account. See Databricks documentation on configuring a Databricks SQL Warehouse. After creating a SQL Warehouse, you can find the details necessary for connecting to it under the Connection Details tab.  In order to save on costs, we recommend that you set the Auto Stop parameter for your SQL warehouse to the minimum available. Estuary's Databricks connector automatically delays updates to the destination according to the configured Sync Schedule (see configuration details below), with a default delay value of 30 minutes.  You also need an access token for your user or service principal to be used by our connector, see the respective documentation for user personal access tokens and service principal access tokens from Databricks on how to create an access token. Note that as of this writing, only service principals in the &quot;admins&quot; group can use a token.  To create an access token for your service principal:  Make sure that they are part of the admins group by going to Settings -&gt; Identity and access -&gt; Groups -&gt; admins -&gt; Members and adding the service principalFind their &quot;Application ID&quot; by going to Settings -&gt; Identity and access -&gt; Service PrincipalsCreate an access token on behalf of the service principal using the databricks CLI  databricks token-management create-obo-token &lt;application id of service principal&gt;   Copy the token_value value of the resulting JSON from the command above  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Databricks","url":"/reference/Connectors/materialization-connectors/databricks/#configuration","content":" To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Databricks materialization, which will direct one or more of your Flow collections to new Databricks tables.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Databricks","url":"/reference/Connectors/materialization-connectors/databricks/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tHost and port of the SQL warehouse (in the form of host[:port]). Port 443 is used as the default if no specific port is provided.\tstring\tRequired /http_path\tHTTP Path\tHTTP path of your SQL warehouse\tstring\tRequired /catalog_name\tCatalog Name\tName of your Unity Catalog\tstring\tRequired /schema_name\tSchema Name\tDefault schema to materialize to\tstring\tdefault schema is used /credentials\tCredentials\tAuthentication credentials\tobject /credentials/auth_type\tRole\tAuthentication type, set to PAT for personal access token\tstring\tRequired /credentials/personal_access_token\tRole\tAccess Token\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tTable name\tstring\tRequired /schema\tAlternative Schema\tAlternative schema for this table\tstring\tRequired /delta_updates\tDelta updates\tWhether to use standard or delta updates\tboolean\tfalse  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Databricks","url":"/reference/Connectors/materialization-connectors/databricks/#sample","content":"  materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: address: dbc-abcdefgh-a12b.cloud.databricks.com catalog_name: main http_path: /sql/1.0/warehouses/abcd123efgh4567 schema_name: default credentials: auth_type: PAT personal_access_token: secret image: ghcr.io/estuary/materialize-databricks:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: table: ${table_name} schema: default source: ${PREFIX}/${source_collection}   ","version":"Next","tagName":"h3"},{"title":"Sync Schedule​","type":1,"pageTitle":"Databricks","url":"/reference/Connectors/materialization-connectors/databricks/#sync-schedule","content":" This connector supports configuring a schedule for sync frequency. You can read about how to configure this here.  ","version":"Next","tagName":"h2"},{"title":"Delta updates​","type":1,"pageTitle":"Databricks","url":"/reference/Connectors/materialization-connectors/databricks/#delta-updates","content":" This connector supports both standard (merge) and delta updates. The default is to use standard updates.  Enabling delta updates will prevent Flow from querying for documents in your Databricks table, which can reduce latency and costs for large datasets. If you're certain that all events will have unique keys, enabling delta updates is a simple way to improve performance with no effect on the output. However, enabling delta updates is not suitable for all workflows, as the resulting table in Databricks won't be fully reduced.  You can enable delta updates on a per-binding basis:   bindings: - resource: table: ${table_name} schema: default delta_updates: true source: ${PREFIX}/${source_collection}   ","version":"Next","tagName":"h2"},{"title":"Reserved words​","type":1,"pageTitle":"Databricks","url":"/reference/Connectors/materialization-connectors/databricks/#reserved-words","content":" Databricks has a list of reserved words that must be quoted in order to be used as an identifier. Flow automatically quotes fields that are in the reserved words list. You can find this list in Databricks's documentation here and in the table below.  caution In Databricks, objects created with quoted identifiers must always be referenced exactly as created, including the quotes. Otherwise, SQL statements and queries can result in errors. See the Databricks docs.  Reserved words\tANTI EXCEPT\tFULL INNER\tINTERSECT JOIN\tLATERAL LEFT\tMINUS NATURAL\tON RIGHT\tSEMI SEMI\tUSING NULL\tDEFAULT TRUE\tFALSE CROSS\t ","version":"Next","tagName":"h2"},{"title":"Elasticsearch","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/Elasticsearch/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Elasticsearch","url":"/reference/Connectors/materialization-connectors/Elasticsearch/#prerequisites","content":" To use this connector, you'll need:  An Elastic cluster with a known endpointThe role used to connect to Elasticsearch must have at least the following privileges (see Elastic's documentation on defining roles and security privileges): Cluster privilege of monitorFor each index to be created: read, write, view_index_metadata, and create_index. When creating Index privileges, you can use a wildcard &quot;*&quot; to grant the privileges to all indices. At least one Flow collection  tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Elasticsearch","url":"/reference/Connectors/materialization-connectors/Elasticsearch/#configuration","content":" To use this connector, begin with data in one or more Flow collections. Use the below properties to configure an Elasticsearch materialization, which will direct the contents of these Flow collections into Elasticsearch indices.  Authentication  You can authenticate to Elasticsearch using either a username and password, or using an API key.  The connector will automatically create an Elasticsearch index for each binding of the materialization with index mappings for each selected field of the binding. It uses the last component of the collection name as the name of the index by default. You can customize the name of the index using the index property in the resource configuration for each binding.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Elasticsearch","url":"/reference/Connectors/materialization-connectors/Elasticsearch/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/endpoint\tEndpoint\tEndpoint host or URL. Must start with http:// or https://. If using Elastic Cloud this follows the format https://CLUSTER_ID.REGION.CLOUD_PLATFORM.DOMAIN:PORT\tstring\tRequired /credentials object\tRequired /credentials/username\tUsername\tUsername to use for authenticating with Elasticsearch.\tstring /credentials/password\tPassword\tPassword to use for authenticating with Elasticsearch.\tstring /credentials/apiKey\tAPI Key\tAPI key for authenticating with the Elasticsearch API. Must be the 'encoded' API key credentials, which is the Base64-encoding of the UTF-8 representation of the id and api_key joined by a colon (:).\tstring advanced/number_of_replicas\tIndex Replicas\tThe number of replicas to create new indices with. Leave blank to use the cluster default.\tinteger\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/index\tindex\tName of the Elasticsearch index to store the materialization results.\tstring\tRequired /delta_updates\tDelta updates\tWhether to use standard or delta updates.\tboolean\tfalse /number_of_shards\tNumber of shards\tThe number of shards to create the index with. Leave blank to use the cluster default.\tinteger\t1  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Elasticsearch","url":"/reference/Connectors/materialization-connectors/Elasticsearch/#sample","content":" materializations: PREFIX/mat_name: endpoint: connector: # Path to the latest version of the connector, provided as a Docker image image: ghcr.io/estuary/materialize-elasticsearch:dev config: endpoint: https://ec47fc4d2c53414e1307e85726d4b9bb.us-east-1.aws.found.io:9243 credentials: username: flow_user password: secret # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: index: my-elasticsearch-index source: PREFIX/source_collection   ","version":"Next","tagName":"h3"},{"title":"Setup​","type":1,"pageTitle":"Elasticsearch","url":"/reference/Connectors/materialization-connectors/Elasticsearch/#setup","content":" You must configure your Elasticsearch cluster to allow connections from Estuary. It may be necessary to allowlist the Estuary IP addresses.  Alternatively, you can allow secure connections via SSH tunneling. To do so:  Refer to the guide to configure an SSH server on the cloud platform of your choice. Configure your connector as described in the configuration section above, with the addition of the networkTunnel stanza to enable the SSH tunnel, if using. See Connecting to endpoints on secure networks for additional details and a sample.  ","version":"Next","tagName":"h2"},{"title":"Delta updates​","type":1,"pageTitle":"Elasticsearch","url":"/reference/Connectors/materialization-connectors/Elasticsearch/#delta-updates","content":" This connector supports both standard and delta updates. You must choose an option for each binding.  Learn more about delta updates and the implications of using each update type.  ","version":"Next","tagName":"h2"},{"title":"Keyword Fields​","type":1,"pageTitle":"Elasticsearch","url":"/reference/Connectors/materialization-connectors/Elasticsearch/#keyword-fields","content":" Collection fields with type: string will have keyword index mappings created for them if they are part of the collection key, and text mappings for them if they are not part of the collection key.  To materialize a collection field with type: string as a keyword mapping instead of a textmapping, configure the field selection for the binding to indicate which fields should having keyword mappings created for them using the key and value of &quot;keyword&quot;: true. This can be changed by updating the JSON in the Advanced Specification Editor in the web app or by using flowctl to edit the specification directly, seeedit a materialization for more details.  An example JSON configuration for a binding that materializes stringField as a keyword mapping is shown below:  { &quot;bindings&quot;: [ { &quot;resource&quot;: { &quot;index&quot;: &quot;my-elasticsearch-index&quot; }, &quot;source&quot;: &quot;PREFIX/source_collection&quot;, &quot;fields&quot;: { &quot;include&quot;: { &quot;stringField&quot;: { &quot;keyword&quot;: true } }, &quot;recommended&quot;: true } } ] }   ","version":"Next","tagName":"h2"},{"title":"Changelog​","type":1,"pageTitle":"Elasticsearch","url":"/reference/Connectors/materialization-connectors/Elasticsearch/#changelog","content":" The changelog includes a list of breaking changes made to this connector. Backwards-compatible changes are not listed.  Proceed with caution when editing materializations created with previous versions of this connector; editing always upgrades your materialization to the latest connector version.  V3: 2023-08-21​  Index mappings will now be created based on the selected fields of the materialization. Previously only dynamic runtime mappings were created, and the entire root document was always materialized. Moved &quot;number of replicas&quot; configuration for new indices to an advanced, optional, endpoint-level configuration. The &quot;number of shards&quot; resource configuration is now optional. ","version":"Next","tagName":"h2"},{"title":"Firebolt","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/Firebolt/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Firebolt","url":"/reference/Connectors/materialization-connectors/Firebolt/#prerequisites","content":" To use this connector, you'll need:  A Firebolt database with at least one engineAn S3 bucket where JSON documents will be stored prior to loading The bucket must be in a supported AWS region matching your Firebolt database.The bucket may be public, or may be accessible by an IAM user. To configure your IAM user, see the steps below. At least one Flow collection  tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Firebolt","url":"/reference/Connectors/materialization-connectors/Firebolt/#setup","content":" For non-public buckets, you'll need to configure access in AWS IAM.  Follow the Firebolt documentation to set up an IAM policy and role, and add it to the external table definition. Create a new IAM user. During setup: Choose Programmatic (access key) access. This ensures that an access key ID and secret access key are generated. You'll use these to configure the connector. On the Permissions page, choose Attach existing policies directly and attach the policy you created in step 1. After creating the user, download the IAM credentials file. Take note of the access key ID and secret access key and use them to configure the connector. See the Amazon docs if you lose your credentials.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Firebolt","url":"/reference/Connectors/materialization-connectors/Firebolt/#configuration","content":" To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Firebolt materialization, which will direct Flow data to your desired Firebolt tables via an external table.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Firebolt","url":"/reference/Connectors/materialization-connectors/Firebolt/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/aws_key_id\tAWS key ID\tAWS access key ID for accessing the S3 bucket.\tstring /aws_region\tAWS region\tAWS region the bucket is in.\tstring /aws_secret_key\tAWS secret access key\tAWS secret key for accessing the S3 bucket.\tstring /database\tDatabase\tName of the Firebolt database.\tstring\tRequired /engine_name\tEngine Name\tName of the Firebolt engine to run your queries.\tstring\tRequired /client_secret\tClient Secret\tSecret of your Firebolt service account.\tstring\tRequired /s3_bucket\tS3 bucket\tName of S3 bucket where the intermediate files for external table will be stored.\tstring\tRequired /s3_prefix\tS3 prefix\tA prefix for files stored in the bucket.\tstring /client_id\tClient ID\tID of your Firebolt service account.\tstring\tRequired /account_name\tAccount Name\tName of your account within your Firebolt organization.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tName of the Firebolt table to store materialized results in. The external table will be named after this table with an _external suffix.\tstring\tRequired /table_type\tTable type\tType of the Firebolt table to store materialized results in. See the Firebolt docs for more details.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Firebolt","url":"/reference/Connectors/materialization-connectors/Firebolt/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: database: my-db engine_name: my-engine-name client_secret: secret # For public S3 buckets, only the bucket name is required s3_bucket: my-bucket client_id: firebolt-user account_name: my-account # Path to the latest version of the connector, provided as a Docker image image: ghcr.io/estuary/materialize-firebolt:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: table: table-name table_type: fact source: ${PREFIX}/${source_collection}   ","version":"Next","tagName":"h3"},{"title":"Delta updates​","type":1,"pageTitle":"Firebolt","url":"/reference/Connectors/materialization-connectors/Firebolt/#delta-updates","content":" Firebolt is an insert-only system; it doesn't support updates or deletes. Because of this, the Firebolt connector operates only in delta updates mode. Firebolt stores all deltas — the unmerged collection documents — directly.  In some cases, this will affect how materialized views look in Firebolt compared to other systems that use standard updates.  ","version":"Next","tagName":"h2"},{"title":"Reserved words​","type":1,"pageTitle":"Firebolt","url":"/reference/Connectors/materialization-connectors/Firebolt/#reserved-words","content":" Firebolt has a list of reserved words, which my not be used in identifiers. Collections with field names that include a reserved word will automatically be quoted as part of a Firebolt materialization.  Reserved words all\tfalse\tor alter\tfetch\torder and\tfirst\touter array\tfloat\tover between\tfrom\tpartition bigint\tfull\tprecision bool\tgenerate\tprepare boolean\tgroup\tprimary both\thaving\tquarter case\tif\tright cast\tilike\trow char\tin\trows concat\tinner\tsample copy\tinsert\tselect create\tint\tset cross\tinteger\tshow current_date\tintersect\ttext current_timestamp\tinterval\ttime database\tis\ttimestamp date\tisnull\ttop datetime\tjoin\ttrailing decimal\tjoin_type\ttrim delete\tleading\ttrue describe\tleft\ttruncate distinct\tlike\tunion double\tlimit\tunknown_char doublecolon\tlimit_distinct\tunnest dow\tlocaltimestamp\tunterminated_string doy\tlong\tupdate drop\tnatural\tusing empty_identifier\tnext\tvarchar epoch\tnot\tweek except\tnull\twhen execute\tnumeric\twhere exists\toffset\twith explain\ton extract\tonly\t ","version":"Next","tagName":"h2"},{"title":"CSV Files in Google GCS","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/google-gcs-csv/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"CSV Files in Google GCS","url":"/reference/Connectors/materialization-connectors/google-gcs-csv/#prerequisites","content":" To use this connector, you'll need:  A GCS bucket to write files to. See this guide for instructions on setting up a new GCS bucket.A Google Cloud service accountwith roles/storage.objectCreatorfor the GCS bucket created above.A key file for the service account.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"CSV Files in Google GCS","url":"/reference/Connectors/materialization-connectors/google-gcs-csv/#configuration","content":" Use the below properties to configure the materialization, which will direct one or more of your Flow collections to your bucket.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"CSV Files in Google GCS","url":"/reference/Connectors/materialization-connectors/google-gcs-csv/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/bucket\tBucket\tBucket to store materialized objects.\tstring\tRequired /credentialsJson\tService Account JSON\tThe JSON credentials of the service account to use for authorization.\tstring\tRequired /uploadInterval\tUpload Interval\tFrequency at which files will be uploaded.\tstring\t5m /prefix\tPrefix\tOptional prefix that will be used to store objects.\tstring /fileSizeLimit\tFile Size Limit\tApproximate maximum size of materialized files in bytes. Defaults to 10737418240 (10 GiB) if blank.\tinteger /csvConfig/delimiter\tDelimiter\tCharacter to separate columns within a row. Defaults to a comma if blank. Must be a single character with a byte length of 1.\tinteger /csvConfig/nullString\tNull String\tString to use to represent NULL values. Defaults to an empty string if blank.\tinteger /csvConfig/skipHeaders\tSkip Headers\tDo not write headers to files.\tinteger\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/path\tPath\tThe path that objects will be materialized to.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"CSV Files in Google GCS","url":"/reference/Connectors/materialization-connectors/google-gcs-csv/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: &quot;ghcr.io/estuary/materialize-gcs-csv:dev&quot; config: bucket: bucket credentialsJson: &lt;credentialsJson&gt; uploadInterval: 5m bindings: - resource: path: ${COLLECTION_NAME} source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"File Names​","type":1,"pageTitle":"CSV Files in Google GCS","url":"/reference/Connectors/materialization-connectors/google-gcs-csv/#file-names","content":" Materialized files are named with monotonically increasing integer values, padded with leading 0's so they remain lexically sortable. For example, a set of files may be materialized like this for a given collection:  bucket/prefix/path/v0000000000/00000000000000000000.csv bucket/prefix/path/v0000000000/00000000000000000001.csv bucket/prefix/path/v0000000000/00000000000000000002.csv   Here the values for bucket and prefix are from your endpoint configuration. The path is specific to the binding configuration. v0000000000 represents the current backfill counterfor binding and will be increased if the binding is re-backfilled, along with the file names starting back over from 0.  ","version":"Next","tagName":"h2"},{"title":"Eventual Consistency​","type":1,"pageTitle":"CSV Files in Google GCS","url":"/reference/Connectors/materialization-connectors/google-gcs-csv/#eventual-consistency","content":" In rare circumstances, recently materialized files may be re-written by files with the same name if the materialization shard is interrupted in the middle of processing a Flow transaction and the transaction must be re-started. Files that were committed as part of a completed transaction will never be re-written. In this way, eventually all collection data will be written to files effectively-once, although inconsistencies are possible when accessing the most recently written data. ","version":"Next","tagName":"h2"},{"title":"Apache Parquet Files in Google GCS","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/google-gcs-parquet/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Apache Parquet Files in Google GCS","url":"/reference/Connectors/materialization-connectors/google-gcs-parquet/#prerequisites","content":" To use this connector, you'll need:  A GCS bucket to write files to. See this guide for instructions on setting up a new GCS bucket.A Google Cloud service accountwith roles/storage.objectCreatorfor the GCS bucket created above.A key file for the service account.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Apache Parquet Files in Google GCS","url":"/reference/Connectors/materialization-connectors/google-gcs-parquet/#configuration","content":" Use the below properties to configure the materialization, which will direct one or more of your Flow collections to your bucket.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Apache Parquet Files in Google GCS","url":"/reference/Connectors/materialization-connectors/google-gcs-parquet/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/bucket\tBucket\tBucket to store materialized objects.\tstring\tRequired /credentialsJson\tService Account JSON\tThe JSON credentials of the service account to use for authorization.\tstring\tRequired /uploadInterval\tUpload Interval\tFrequency at which files will be uploaded.\tstring\t5m /prefix\tPrefix\tOptional prefix that will be used to store objects.\tstring /fileSizeLimit\tFile Size Limit\tApproximate maximum size of materialized files in bytes. Defaults to 10737418240 (10 GiB) if blank.\tinteger /parquetConfig/rowGroupRowLimit\tRow Group Row Limit\tMaximum number of rows in a row group. Defaults to 1000000 if blank.\tinteger /parquetConfig/rowGroupByteLimit\tRow Group Byte Limit\tApproximate maximum number of bytes in a row group. Defaults to 536870912 (512 MiB) if blank.\tinteger\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/path\tPath\tThe path that objects will be materialized to.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Apache Parquet Files in Google GCS","url":"/reference/Connectors/materialization-connectors/google-gcs-parquet/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: &quot;ghcr.io/estuary/materialize-gcs-parquet:dev&quot; config: bucket: bucket credentialsJson: &lt;credentialsJson&gt; uploadInterval: 5m bindings: - resource: path: ${COLLECTION_NAME} source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Parquet Data Types​","type":1,"pageTitle":"Apache Parquet Files in Google GCS","url":"/reference/Connectors/materialization-connectors/google-gcs-parquet/#parquet-data-types","content":" Flow collection fields are written to Parquet files based on the data type of the field. Depending on the field data type, the Parquet data type may be a primitive Parquet type, or a primitive Parquet type extended by alogical Parquet type.  Collection Field Data Type\tParquet Data Type\tarray\tJSON (extends BYTE_ARRAY) object\tJSON (extends BYTE_ARRAY) boolean\tBOOLEAN integer\tINT64 number\tDOUBLE string with {contentEncoding: base64}\tBYTE_ARRAY string with {format: date}\tDATE (extends BYTE_ARRAY) string with {format: date-time}\tTIMESTAMP (extends INT64, UTC adjusted with microsecond precision) string with {format: time}\tTIME (extends INT64, UTC adjusted with microsecond precision) string with {format: date}\tDATE (extends INT32) string with {format: duration}\tINTERVAL (extends FIXED_LEN_BYTE_ARRAY with a length of 12) string with {format: uuid}\tUUID (extends FIXED_LEN_BYTE_ARRAY with a length of 16) string (all others)\tSTRING (extends BYTE_ARRAY)\t  ","version":"Next","tagName":"h2"},{"title":"File Names​","type":1,"pageTitle":"Apache Parquet Files in Google GCS","url":"/reference/Connectors/materialization-connectors/google-gcs-parquet/#file-names","content":" Materialized files are named with monotonically increasing integer values, padded with leading 0's so they remain lexically sortable. For example, a set of files may be materialized like this for a given collection:  bucket/prefix/path/v0000000000/00000000000000000000.parquet bucket/prefix/path/v0000000000/00000000000000000001.parquet bucket/prefix/path/v0000000000/00000000000000000002.parquet   Here the values for bucket and prefix are from your endpoint configuration. The path is specific to the binding configuration. v0000000000 represents the current backfill counterfor binding and will be increased if the binding is re-backfilled, along with the file names starting back over from 0.  ","version":"Next","tagName":"h2"},{"title":"Eventual Consistency​","type":1,"pageTitle":"Apache Parquet Files in Google GCS","url":"/reference/Connectors/materialization-connectors/google-gcs-parquet/#eventual-consistency","content":" In rare circumstances, recently materialized files may be re-written by files with the same name if the materialization shard is interrupted in the middle of processing a Flow transaction and the transaction must be re-started. Files that were committed as part of a completed transaction will never be re-written. In this way, eventually all collection data will be written to files effectively-once, although inconsistencies are possible when accessing the most recently written data. ","version":"Next","tagName":"h2"},{"title":"Google Sheets","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/Google-sheets/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Google Sheets","url":"/reference/Connectors/materialization-connectors/Google-sheets/#prerequisites","content":" To use this connector, you'll need:  At least one Flow collection. If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps.  caution For performance reasons, this connector is limited to 1 million cells per materialized sheet. If a bound collection has more than 1 million unique keys, the materialization will fail. If you plan to materialize a collection with an unbounded number of keys, you should first use a derivation to summarize it into a collection with a bounded set of keys.  The URL of a Google spreadsheet that does not contain the output of a prior Flow materialization.  caution Materializing data to a spreadsheet that already contains the output of another Flow materialization can result in an error. Use a new spreadsheet for each materialization, or completely clear the output of prior materializations from the spreadsheet before you continue.  There are two ways to authenticate with Google when using this connector: signing in with Google through OAuth in the web app, and configuring manually with a Google service account key. OAuth is simpler, and is recommended when using the web app. Only manual configuration is supported using the CLI.  Additional prerequisites depend on the authentication method you choose.  ","version":"Next","tagName":"h2"},{"title":"OAuth authentication using the Flow web app​","type":1,"pageTitle":"Google Sheets","url":"/reference/Connectors/materialization-connectors/Google-sheets/#oauth-authentication-using-the-flow-web-app","content":" You'll need:  The username and password of a Google account with edit access to the destination spreadsheet.  ","version":"Next","tagName":"h3"},{"title":"Manual authentication​","type":1,"pageTitle":"Google Sheets","url":"/reference/Connectors/materialization-connectors/Google-sheets/#manual-authentication","content":" You'll need:  Google Sheets and Google Drive APIs enabled on your Google account. A Google service account with: A JSON key generated.Edit access to the destination spreadsheet.  Follow the steps below to meet these prerequisites:  Enable the Google Sheets and Google Drive APIs for the Google project with which your spreadsheet is associated. (Unless you actively develop with Google Cloud, you'll likely just have one option). Create a service account and generate a JSON key. During setup, grant the account the Editor role on your project. You'll copy the contents of the downloaded key file into the Service Account JSON parameter when you configure the connector. Share your Google spreadsheet with the service account, granting edit access.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Google Sheets","url":"/reference/Connectors/materialization-connectors/Google-sheets/#configuration","content":" To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Google Sheets materialization.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Google Sheets","url":"/reference/Connectors/materialization-connectors/Google-sheets/#properties","content":" Endpoint​  The following properties reflect the manual authentication method. If you're working in the Flow web app, you can use OAuth, so some of these properties aren't required.  Property\tTitle\tDescription\tType\tRequired/Default/credentials\tAuthentication\tCredentials used to authenticate with Google.\tarray, boolean, null, number, object, string\tRequired /credentials/auth_type\tAuthentication type\tSet to Service for manual authentication, or use OAuth in the web app.\tstring /credentials/credentials_json\tService Account JSON\tThe JSON key of the service account to use for authorization, when using the Service authentication method.\tstring\tRequired /spreadsheetURL\tSpreadsheet URL\tURL of the spreadsheet to materialize into, which is shared with the service account.\tstring\tRequired  Bindings​  Configure a separate binding for each collection you want to materialize to a sheet. Note that the connector will add an addition column to the beginning of each sheet; this is to track the internal state of the data.  Property\tTitle\tDescription\tType\tRequired/Default/sheet\tSheet Name\tName of the spreadsheet sheet to materialize into\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Google Sheets","url":"/reference/Connectors/materialization-connectors/Google-sheets/#sample","content":" This sample reflects the manual authentication method using the CLI.  materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: credentials: auth_type: Service credentials_json: &lt;secret&gt; spreadsheetURL: `https://docs.google.com/spreadsheets/d/&lt;your_spreadsheet_ID&gt;/edit image: ghcr.io/estuary/materialize-google-sheets:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: sheet: my_sheet source: ${PREFIX}/${source_collection}  ","version":"Next","tagName":"h3"},{"title":"Google Cloud Pub/Sub","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/google-pubsub/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"/reference/Connectors/materialization-connectors/google-pubsub/#prerequisites","content":" To use this connector, you'll need:  A Google Cloud project with the Google Pub/Sub API enabled.Access to the project. Different items are required to configure access using OAuth in the Flow web app (recommended), and configuring manually.At least one Flow collection to materialize.  tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps.  ","version":"Next","tagName":"h2"},{"title":"OAuth authentication using the Flow web app​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"/reference/Connectors/materialization-connectors/google-pubsub/#oauth-authentication-using-the-flow-web-app","content":" OAuth is the simplest authentication method, and is supported in the Flow web app. You'll need:  A Google account with the role roles/pubsub.editoror equivalent for the Google Cloud project. See the Google IAM documentation to learn about granting roles.  You'll supply this account's username and password to authenticate.  ","version":"Next","tagName":"h3"},{"title":"Manual authentication​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"/reference/Connectors/materialization-connectors/google-pubsub/#manual-authentication","content":" Manual authentication is the only method supported when using flowctl to develop locally. You'll need:  A Google service account with the role roles/pubsub.editoror equivalent for the Google Cloud project. See the Google IAM documentation to learn about granting roles. A JSON key for the service account.  See the Google documentation for help creating a new service account and generating its key.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"/reference/Connectors/materialization-connectors/google-pubsub/#configuration","content":" To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Google Cloud Pub/Sub materialization, which will direct one or more of your Flow collections to your desired Pub/Sub topics.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"/reference/Connectors/materialization-connectors/google-pubsub/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/credentials\tAuthentication\tCredentials used to authenticate with Google.\tarray, boolean, null, number, object, string\tRequired /credentials/auth_type\tAuthentication type\tSet to Service for manual authentication, or use OAuth in the web app.\tstring /credentials/credentials_json\tService Account JSON\tThe JSON key of the service account to use for authorization, if configuring manually.\tstring /project_id\tGoogle Cloud Project ID\tName of the project containing the PubSub topics for this materialization.\tstring\tRequired  Bindings​  caution PubSub topics need a default subscription; otherwise, delivered messages will be lost. Leave Create with Default Subscription set to the default, true, unless you have a specific reason not to do so.  Property\tTitle\tDescription\tType\tRequired/Default/create_default_subscription\tCreate with Default Subscription\tCreate a default subscription when creating the topic. Will be created as &quot;&lt;topic&gt;-sub&quot;. Has no effect if the topic already exists.\tboolean\tRequired, true identifier\tResource Binding Identifier\tOptional identifier for the resource binding if creating a multiplex topic. Included as &quot;identifier&quot; attribute in published messages if specified.\tstring /topic\tTopic Name\tName of the topic to publish materialized results to.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"/reference/Connectors/materialization-connectors/google-pubsub/#sample","content":" This sample reflects the manual authentication method using the CLI.  materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: credentials: auth_type: Service credentials_json: {secret} project_id: my_google_cloud_project bindings: - resource: create_default_subscription: true topic: my_new_topic source: ${PREFIX}/${source_collection}   ","version":"Next","tagName":"h3"},{"title":"Multiplex topics​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"/reference/Connectors/materialization-connectors/google-pubsub/#multiplex-topics","content":" You can materialize multiple Flow collections to the same Pub/Sub topic. This is known as a multiplex topic. You do so by adding the optional identifier field to the binding configuration.  When materializing to a multiplex topic, ensure that:  The bindings you want to combine have the same topic name.Each binding pulls from a different Flow collectionEach binding has a unique identifier. It can be anything you'd like.  The binding configuration will look similar to:  bindings: - resource: identifier: one topic: multiplex-topic source: ${PREFIX}/source_collection_one - resource: identifier: two topic: multiplex-topic source: ${PREFIX}/source_collection_two   ","version":"Next","tagName":"h2"},{"title":"Delta updates​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"/reference/Connectors/materialization-connectors/google-pubsub/#delta-updates","content":" Because Google Cloud Pub/Sub is a write-only event-streaming system, this connector uses only delta updates.  ","version":"Next","tagName":"h2"},{"title":"Message ordering​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"/reference/Connectors/materialization-connectors/google-pubsub/#message-ordering","content":" Google Cloud Pub/Sub manages message ordering using ordering keys.  This connector sets the ordering key of published messages using the Flow collection keyof the documents being being published. Messages are published in order, on a per-key basis. ","version":"Next","tagName":"h2"},{"title":"HTTP Webhook","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/http-webhook/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"HTTP Webhook","url":"/reference/Connectors/materialization-connectors/http-webhook/#prerequisites","content":" To use this materialization connector, you’ll need the following:  A server or service that can accept HTTP requests at the target endpoint.At least one Flow collection.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"HTTP Webhook","url":"/reference/Connectors/materialization-connectors/http-webhook/#configuration","content":" The Webhooks connector is available for use in the Flow web application. To learn more about connectors and setting them up, visit our guide on using connectors.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"HTTP Webhook","url":"/reference/Connectors/materialization-connectors/http-webhook/#properties","content":" ","version":"Next","tagName":"h2"},{"title":"Endpoint​","type":1,"pageTitle":"HTTP Webhook","url":"/reference/Connectors/materialization-connectors/http-webhook/#endpoint","content":" Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tThe URL of the endpoint to send data to.\tstring\tRequired /headers/customHeaders\tHeaders\tArray of additional headers to include in the HTTP request.\tobject\t  ","version":"Next","tagName":"h3"},{"title":"Bindings​","type":1,"pageTitle":"HTTP Webhook","url":"/reference/Connectors/materialization-connectors/http-webhook/#bindings","content":" Property\tTitle\tDescription\tType\tRequired/Default/relativePath\tRelative Path\tThe relative path on the server where data will be sent.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"HTTP Webhook","url":"/reference/Connectors/materialization-connectors/http-webhook/#sample","content":" bindings: - source: ProductionData/orders/orderDetails resource: relativePath: webhook/estuary endpoint: connector: image: ghcr.io/estuary/materialize-webhook:v1 config: address: http://192.168.1.100:3000/ headers: customHeaders: - name: my-header value: my-value - name: another-header value: another-value  ","version":"Next","tagName":"h2"},{"title":"MotherDuck","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/motherduck/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"MotherDuck","url":"/reference/Connectors/materialization-connectors/motherduck/#prerequisites","content":" To use this connector, you'll need:  A MotherDuck account and Service Token.An S3 bucket for staging temporary files. See this guide for instructions on setting up a new S3 bucket.An AWS root or IAM user with read and write accessto the S3 bucket. For this user, you'll need the access key and secret access key. See theAWS blog for help finding these credentials.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"MotherDuck","url":"/reference/Connectors/materialization-connectors/motherduck/#configuration","content":" Use the below properties to configure MotherDuck materialization, which will direct one or more of your Flow collections to your desired tables in the database.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"MotherDuck","url":"/reference/Connectors/materialization-connectors/motherduck/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/token\tMotherDuck Service Token\tService token for authenticating with MotherDuck.\tstring\tRequired /database\tDatabase\tThe database to materialize to.\tstring\tRequired /schema\tDatabase Schema\tDatabase schema for bound collection tables (unless overridden within the binding resource configuration) as well as associated materialization metadata tables.\tstring\tRequired /bucket\tS3 Staging Bucket\tName of the S3 bucket to use for staging data loads.\tstring\tRequired /awsAccessKeyId\tAccess Key ID\tAWS Access Key ID for reading and writing data to the S3 staging bucket.\tstring\tRequired /awsSecretAccessKey\tSecret Access Key\tAWS Secret Access Key for reading and writing data to the S3 staging bucket.\tstring\tRequired /bucketPath\tBucket Path\tA prefix that will be used to store objects in S3.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tName of the database table.\tstring\tRequired /delta_updates\tDelta Update\tShould updates to this table be done via delta updates.\tboolean /schema\tAlternative Schema\tAlternative schema for this table (optional).\tstring\t  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"MotherDuck","url":"/reference/Connectors/materialization-connectors/motherduck/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: &quot;ghcr.io/estuary/materialize-motherduck:dev&quot; config: token: &lt;motherduck_service_token&gt; database: my_db schema: main bucket: my_bucket awsAccessKeyId: &lt;access_key_id&gt; awsSecretAccessKey: &lt;secret_access_key&gt; bindings: - resource: table: ${TABLE_NAME} source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Delta updates​","type":1,"pageTitle":"MotherDuck","url":"/reference/Connectors/materialization-connectors/motherduck/#delta-updates","content":" This connector supports both standard (merge) and delta updates. The default is to use standard updates.  Enabling delta updates will prevent Flow from querying for documents in your MotherDuck table, which can reduce latency and costs for large datasets. If you're certain that all events will have unique keys, enabling delta updates is a simple way to improve performance with no effect on the output. However, enabling delta updates is not suitable for all workflows, as the resulting table in MotherDuck won't be fully reduced.  You can enable delta updates on a per-binding basis:   bindings: - resource: table: ${table_name} delta_updates: true source: ${PREFIX}/${COLLECTION_NAME}  ","version":"Next","tagName":"h2"},{"title":"MongoDB","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/mongodb/","content":"","keywords":"","version":"Next"},{"title":"Data model​","type":1,"pageTitle":"MongoDB","url":"/reference/Connectors/materialization-connectors/mongodb/#data-model","content":" MongoDB is a NoSQL database. Its data modelconsists of documents (lightweight records that contain mappings of fields and values) organized in collections. MongoDB documents have a mandatory_id field that is used as the key of the collection. Flow collection documents are materialized as MongoDB documents with an _id field value based on the Flow collection key.  info If your Flow collection already has a field named _id, its value will be present in the materialized MongoDB document as the field _flow_id to prevent conflicts with the required _id field.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"MongoDB","url":"/reference/Connectors/materialization-connectors/mongodb/#prerequisites","content":" You'll need:  Credentials for connecting to your MongoDB instance and database. Read and write access to your MongoDB database and desired collections. See Role-Based Access Control for more information. If you are using MongoDB Atlas, or your MongoDB provider requires allowlisting of IPs, you need to allowlist the Estuary IP addresses.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"MongoDB","url":"/reference/Connectors/materialization-connectors/mongodb/#configuration","content":" You configure connectors either in the Flow web app, or by directly editing the Flow specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Firestore source connector.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"MongoDB","url":"/reference/Connectors/materialization-connectors/mongodb/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tHost and port of the database. Optionally can specify scheme for the URL such as mongodb+srv://host.\tstring\tRequired /database\tDatabase\tName of the database to capture from.\tstring\tRequired /user\tUser\tDatabase user to connect as.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/collection\tStream\tCollection name\tstring\tRequired /delta_updates\tDelta Update\tShould updates to this table be done via delta updates.\tboolean\tfalse  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"MongoDB","url":"/reference/Connectors/materialization-connectors/mongodb/#sample","content":" materializations: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/materialize-mongodb:dev config: address: &quot;mongo:27017&quot; database: &quot;test&quot; password: &quot;flow&quot; user: &quot;flow&quot; bindings: - resource: collection: users database: test source: ${PREFIX}/users   ","version":"Next","tagName":"h3"},{"title":"SSH Tunneling​","type":1,"pageTitle":"MongoDB","url":"/reference/Connectors/materialization-connectors/mongodb/#ssh-tunneling","content":" As an alternative to connecting to your MongoDB instance directly, you can allow secure connections via SSH tunneling. To do so:  Refer to the guide to configure an SSH server on the cloud platform of your choice. Configure your connector as described in the configuration section above, with the addition of the networkTunnel stanza to enable the SSH tunnel, if using. See Connecting to endpoints on secure networks for additional details and a sample.  ","version":"Next","tagName":"h2"},{"title":"Delta updates​","type":1,"pageTitle":"MongoDB","url":"/reference/Connectors/materialization-connectors/mongodb/#delta-updates","content":" This connector supports both standard (merge) and delta updates. The default is to use standard updates. ","version":"Next","tagName":"h2"},{"title":"MySQL HeatWave","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/mysql-heatwave/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"MySQL HeatWave","url":"/reference/Connectors/materialization-connectors/mysql-heatwave/#prerequisites","content":" To use this materialization connector, you’ll need the following:  A MySQL HeatWave database and the appropriate user credentials.At least one Flow collection.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"MySQL HeatWave","url":"/reference/Connectors/materialization-connectors/mysql-heatwave/#configuration","content":" Select one or more of your Flow collections to start using this connector. The configuration properties below will help you to materialize your collections into tables in MySQL HeatWave.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"MySQL HeatWave","url":"/reference/Connectors/materialization-connectors/mysql-heatwave/#properties","content":" ","version":"Next","tagName":"h2"},{"title":"Endpoint​","type":1,"pageTitle":"MySQL HeatWave","url":"/reference/Connectors/materialization-connectors/mysql-heatwave/#endpoint","content":" Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tHost and port of the database. If only the host is specified, the port will default to 3306.\tstring\tRequired /database\tDatabase\tName of the logical database to send data to.\tstring\tRequired /user\tUser\tUsername for authentication.\tstring\tRequired /password\tPassword\tPassword for authentication.\tstring\tRequired /timezone\tTimezone\tTimezone to use when materializing datetime columns. Should normally be left blank to use the database's 'time_zone' system variable. Only required if the 'time_zone' system variable cannot be read.\tstring\t  ","version":"Next","tagName":"h3"},{"title":"Advanced: SSL Mode​","type":1,"pageTitle":"MySQL HeatWave","url":"/reference/Connectors/materialization-connectors/mysql-heatwave/#advanced-ssl-mode","content":" Configuring the SSL mode strengthens security when transferring data to Oracle MySQL HeatWave. Here are the possible values for SSL mode:  disabled: Establishes an unencrypted connection with the server.preferred: Initiates the SSL connection only if prompted by the server.required: Establishes an SSL connection but doesn’t verify the server’s certificate.verify_ca: Connects via SSL connection and verifies the server’s certificate against the provided SSL Server CA, without validating the server's hostname. SSL Server CA is mandatory for this mode.verify_identity: Ensures an SSL connection, and verifies both the server's certificate and hostname. This is the highest level of security. SSL Server CA is required for this mode.  ","version":"Next","tagName":"h3"},{"title":"Bindings​","type":1,"pageTitle":"MySQL HeatWave","url":"/reference/Connectors/materialization-connectors/mysql-heatwave/#bindings","content":" Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tThe name of the table to send data to.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"MySQL HeatWave","url":"/reference/Connectors/materialization-connectors/mysql-heatwave/#sample","content":" materializations: ${PREFIX}/${MAT_NAME}: endpoint: connector: image: ghcr.io/estuary/materialize-mysql-heatwave:dev config: database: flow address: localhost:5432 password: secret user: flow bindings: - resource: table: ${TABLE_NAME} source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h2"},{"title":"MySQL HeatWave on Oracle Cloud Infrastructure​","type":1,"pageTitle":"MySQL HeatWave","url":"/reference/Connectors/materialization-connectors/mysql-heatwave/#mysql-heatwave-on-oracle-cloud-infrastructure","content":" This connector supports cloud-based MySQL HeatWave instances hosted on Oracle Cloud Infrastructure (OCI).  ","version":"Next","tagName":"h2"},{"title":"SSH Tunneling (Required)​","type":1,"pageTitle":"MySQL HeatWave","url":"/reference/Connectors/materialization-connectors/mysql-heatwave/#ssh-tunneling-required","content":" You are also required to configure SSH tunneling by providing the following:  SSH Endpoint: Enter the endpoint of the remote SSH server that supports tunneling (formatted as ssh://user@hostname[:port]).SSH Private Key: Input the full RSA Private Key for SSH connection. ","version":"Next","tagName":"h3"},{"title":"Pinecone","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/pinecone/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Pinecone","url":"/reference/Connectors/materialization-connectors/pinecone/#prerequisites","content":" To use this connector, you'll need:  A Pinecone account with an API Key for authentication.An OpenAI account with an API Key for authentication.A Pinecone Index created to store materialized vector embeddings. When using the embedding model text-embedding-ada-002 (recommended), the index must have Dimensions set to 1536.  ","version":"Next","tagName":"h2"},{"title":"Embedding Input​","type":1,"pageTitle":"Pinecone","url":"/reference/Connectors/materialization-connectors/pinecone/#embedding-input","content":" The materialization creates a vector embedding for each collection document. Its structure is based on the collection fields.  By default, fields of a single scalar type are including in the embedding: strings, integers, numbers, and booleans. You can include additional array or object type fields using projected fields.  The text generated for the embedding has this structure, with field names and their values separated by newlines:  stringField: stringValue intField: 3 numberField: 1.2 boolField: false   ","version":"Next","tagName":"h2"},{"title":"Pinecone Record Metadata​","type":1,"pageTitle":"Pinecone","url":"/reference/Connectors/materialization-connectors/pinecone/#pinecone-record-metadata","content":" Pinecone supports metadata fields associated with stored vectors that can be used when performingvector queries. This materialization will include the materialized document as a JSON string in the metadata field flow_document to enable retrieval of the document from vectors returned by Pinecone queries.  Pinecone indexes all metadata fields by default. To manage memory usage of the index, use selective metadata indexing to exclude the flow_document metadata field.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Pinecone","url":"/reference/Connectors/materialization-connectors/pinecone/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/index\tPinecone Index\tPinecone index for this materialization. Must already exist and have appropriate dimensions for the embedding model used.\tstring\tRequired /environment\tPinecone Environment\tCloud region for your Pinecone project. Example: us-central1-gcp\tstring\tRequired /pineconeApiKey\tPinecone API Key\tPinecone API key used for authentication.\tstring\tRequired /openAiApiKey\tOpenAI API Key\tOpenAI API key used for authentication.\tstring\tRequired /embeddingModel\tEmbedding Model ID\tEmbedding model ID for generating OpenAI bindings. The default text-embedding-ada-002 is recommended.\tstring\t&quot;text-embedding-ada-002&quot; /advanced Options for advanced users. You should not typically need to modify these.\tobject /advaned/openAiOrg\tOpenAI Organization\tOptional organization name for OpenAI requests. Use this if you belong to multiple organizations to specify which organization is used for API requests.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/namespace\tPinecone Namespace\tName of the Pinecone namespace that this collection will materialize vectors into.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Pinecone","url":"/reference/Connectors/materialization-connectors/pinecone/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: &quot;ghcr.io/estuary/materialize-pinecone:dev&quot; config: index: your-index environment: us-central1-gcp pineconeApiKey: &lt;YOUR_PINECONE_API_KEY&gt; openAiApiKey: &lt;YOUR_OPENAI_API_KEY&gt; bindings: - resource: namespace: your-namespace source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Delta Updates​","type":1,"pageTitle":"Pinecone","url":"/reference/Connectors/materialization-connectors/pinecone/#delta-updates","content":" This connector operates only in delta updates mode.  Pinecone upserts vectors based on their id. The idfor materialized vectors is based on the Flow Collection key.  For collections with a a top-level reduction strategy ofmerge and a strategy oflastWriteWins for all nested values (this is also the default), collections will be materialized &quot;effectively once&quot;, with any updated Flow documents replacing vectors in the Pinecone index if they have the same key. ","version":"Next","tagName":"h2"},{"title":"MySQL","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/MySQL/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/#prerequisites","content":" To use this connector, you'll need:  A MySQL database to which to materialize, and user credentials. MySQL versions 5.7 and later are supportedThe connector will create new tables in the database per your specification, so user credentials must have access to create new tables.The local_infile global variable must be enabled. You can enable this setting by running SET GLOBAL local_infile = true in your database. At least one Flow collection  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/#setup","content":" To meet these requirements, follow the steps for your hosting type.  Amazon RDSGoogle Cloud SQLAzure Database for MySQL  In addition to standard MySQL, this connector supports cloud-based MySQL instances. Google Cloud Platform, Amazon Web Service, and Microsoft Azure are currently supported. You may use other cloud platforms, but Estuary doesn't guarantee performance.  To connect securely, you can either enable direct access for Flows's IP or use an SSH tunnel.  ","version":"Next","tagName":"h2"},{"title":"Azure Database for MySQL​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/#azure-database-for-mysql","content":" You must configure your database to allow connections from Estuary. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel.  Connect Directly With Azure Database For MySQL: Create a new firewall rule that grants access to the Estuary Flow IP addresses Connect With SSH Tunneling: Follow the instructions for setting up an SSH connection to Azure Database.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/#configuration","content":" To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a MySQL materialization, which will direct one or more of your Flow collections to your desired tables, or views, in the database.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/database\tDatabase\tName of the logical database to materialize to.\tstring\tRequired /address\tAddress\tHost and port of the database. If only the host is specified, port will default to 3306.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /user\tUser\tDatabase user to connect as.\tstring\tRequired /timezone\tTimezone\tTimezone to use when materializing datetime columns. Should normally be left blank to use the database's 'time_zone' system variable. Only required if the 'time_zone' system variable cannot be read. Must be a valid IANA time zone name or +HH:MM offset. Takes precedence over the 'time_zone' system variable if both are set.\tstring /advanced\tAdvanced Options\tOptions for advanced users. You should not typically need to modify these.\tobject /advanced/sslmode\tSSL Mode\tOverrides SSL connection behavior by setting the 'sslmode' parameter.\tstring /advanced/ssl_server_ca\tSSL Server CA\tOptional server certificate authority to use when connecting with custom SSL mode\tstring /advanced/ssl_client_cert\tSSL Client Certificate\tOptional client certificate to use when connecting with custom SSL mode.\tstring /advanced/ssl_client_key\tSSL Client Key\tOptional client key to use when connecting with custom SSL mode.\tstring\t  ","version":"Next","tagName":"h3"},{"title":"Setting the MySQL time zone​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/#setting-the-mysql-time-zone","content":" MySQL's time_zone server system variable is set to SYSTEM by default.  If you intend to materialize collections including fields of with format: date-time or format: time, and time_zone is set to SYSTEM, Flow won't be able to detect the time zone and convert datetimes to the appropriate timezone when materializing. To avoid this, you must explicitly set the time zone for your database.  You can:  Specify a numerical offset from UTC. For MySQL version 8.0.19 or higher, values from -13:59 to +14:00, inclusive, are permitted.Prior to MySQL 8.0.19, values from -12:59 to +13:00, inclusive, are permitted Specify a named timezone in IANA timezone format. If you're using Amazon Aurora, create or modify the DB cluster parameter groupassociated with your MySQL database.Set the time_zone parameter to the correct value.  For example, if you're located in New Jersey, USA, you could set time_zone to -05:00 or -04:00, depending on the time of year. Because this region observes daylight savings time, you'd be responsible for changing the offset. Alternatively, you could set time_zone to America/New_York, and time changes would occur automatically.  If using IANA time zones, your database must include time zone tables. Learn more in the MySQL docs.  Materialize Timezone Configuration If you are unable to set the time_zone in the database and need to materialize collections with date-time or time fields, the materialization can be configured to assume a time zone using the timezone configuration property (see above). The timezone configuration property can be set as a numerical offset or IANA timezone format.  SSL Mode​  Possible values:  disabled: A plain unencrypted connection is established with the serverpreferred: Only use SSL connection if the server asks for itrequired: Connect using an SSL connection, but do not verify the server's certificate.verify_ca: Connect using an SSL connection, and verify the server's certificate against the given SSL Server CA, but does not verify the server's hostname. This option is most commonly used when connecting to an IP address which does not have a hostname to be verified. When using this mode, SSL Server CA must be provided.verify_identity: Connect using an SSL connection, verify the server's certificate and the server's hostname. This is the most secure option. When using this mode, SSL Server CA must be provided.  Optionally, SSL Client Certificate and Key can be provided if necessary to authorize the client.  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tTable name to materialize to. It will be created by the connector, unless the connector has previously created it.\tstring\tRequired /delta_updates\tDelta Update\tShould updates to this table be done via delta updates.\tboolean\tfalse  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: ghcr.io/estuary/materialize-mysql:dev config: database: flow address: localhost:5432 password: flow user: flow bindings: - resource: table: ${TABLE_NAME} source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"MySQL on managed cloud platforms​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/#mysql-on-managed-cloud-platforms","content":" In addition to standard MySQL, this connector supports cloud-based MySQL instances. To connect securely, you can either enable direct access for Flows's IP or use an SSH tunnel.  Google Cloud Platform, Amazon Web Service, and Microsoft Azure are currently supported. You may use other cloud platforms, but Estuary doesn't guarantee performance.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/#setup-1","content":" You must configure your database to allow connections from Estuary. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel.  Connect directly with Amazon RDS or Amazon Aurora: Edit the VPC security group associated with your database instance, or create a new VPC security group and associate it with the database instance. Modify the instance, choosing Publicly accessible in the Connectivity settings. Per the steps in the Amazon documentation, create a new inbound rule and a new outbound rule that allow all traffic from Estuary's IP addresses. Connect directly with Google Cloud SQL: Enable public IP on your database and add Estuary Flow IP addresses as authorized IP addresses. See the instructions below to use SSH Tunneling instead of enabling public access. Connect with SSH tunneling Refer to the guide to configure an SSH server on the cloud platform of your choice. Configure your connector as described in the configuration section above, with the addition of the networkTunnel stanza to enable the SSH tunnel, if using. See Connecting to endpoints on secure networksfor additional details and a sample.  Configuration Tip To configure the connector, you must specify the database address in the formathost:port. (You can also supply host only; the connector will use the port 3306 by default, which is correct in many cases.) You can find the host and port in the following locations in each platform's console: Amazon RDS and Amazon Aurora: host as Endpoint; port as Port.Google Cloud SQL: host as Private IP Address; port is always 3306. You may need to configure private IP on your database.Azure Database: host as Server Name; port under Connection Strings (usually 3306).  ","version":"Next","tagName":"h3"},{"title":"Delta updates​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/#delta-updates","content":" This connector supports both standard (merge) and delta updates. The default is to use standard updates.  ","version":"Next","tagName":"h2"},{"title":"Date & times​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/#date--times","content":" Date and time fields that are part of collections, which specify a format: date-time for the field, are automatically converted to UTC and persisted as UTC DATETIME in MySQL.  ","version":"Next","tagName":"h2"},{"title":"Reserved words​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/#reserved-words","content":" MySQL has a list of reserved words that must be quoted in order to be used as an identifier. Flow considers all the reserved words in the official MySQL documentation.  These reserved words are listed in the table below. Flow automatically quotes fields that are in this list.  Reserved words accessible\tclone\tdescribe\tfloat\tint account\tclose\tdescription\tfloat4\tint1 action\tcoalesce\tdes_key_file\tfloat8\tint2 active\tcode\tdeterministic\tflush\tint3 add\tcollate\tdiagnostics\tfollowing\tint4 admin\tcollation\tdirectory\tfollows\tint8 after\tcolumn\tdisable\tfor\tinteger against\tcolumns\tdiscard\tforce\tintersect aggregate\tcolumn_format\tdisk\tforeign\tinterval algorithm\tcolumn_name\tdistinct\tformat\tinto all\tcomment\tdistinctrow\tfound\tinvisible alter\tcommit\tdiv\tfrom\tinvoker always\tcommitted\tdo\tfull\tio analyse\tcompact\tdouble\tfulltext\tio_after_gtid analyze\tcompletion\tdrop\tfunction\tio_before_gti and\tcomponent\tdual\tgeneral\tio_thread any\tcompressed\tdumpfile\tgenerate\tipc array\tcompression\tduplicate\tgenerated\tis as\tconcurrent\tdynamic\tgeomcollectio\tisolation asc\tcondition\teach\tgeometry\tissuer ascii\tconnection\telse\tgeometrycolle\titerate asensitive\tconsistent\telseif\tget\tjoin at\tconstraint\tempty\tget_format\tjson attribute\tconstraint_ca\tenable\tget_master_pu\tjson_table authenticatio\tconstraint_na\tenclosed\tget_source_pu\tjson_value autoextend_si\tconstraint_sc\tencryption\tglobal\tkey auto_incremen\tcontains\tend\tgrant\tkeyring avg\tcontext\tends\tgrants\tkeys avg_row_lengt\tcontinue\tenforced\tgroup\tkey_block_siz backup\tconvert\tengine\tgrouping\tkill before\tcpu\tengines\tgroups\tlag begin\tcreate\tengine_attrib\tgroup_replica\tlanguage between\tcross\tenum\tgtid_only\tlast bigint\tcube\terror\thandler\tlast_value binary\tcume_dist\terrors\thash\tlateral binlog\tcurrent\tescape\thaving\tlead bit\tcurrent_date\tescaped\thelp\tleading blob\tcurrent_time\tevent\thigh_priority\tleave block\tcurrent_times\tevents\thistogram\tleaves bool\tcurrent_user\tevery\thistory\tleft boolean\tcursor\texcept\thost\tless both\tcursor_name\texchange\thosts\tlevel btree\tdata\texclude\thour\tlike buckets\tdatabase\texecute\thour_microsec\tlimit bulk\tdatabases\texists\thour_minute\tlinear by\tdatafile\texit\thour_second\tlines byte\tdate\texpansion\tidentified\tlinestring cache\tdatetime\texpire\tif\tlist call\tday\texplain\tignore\tload cascade\tday_hour\texport\tignore_server\tlocal cascaded\tday_microseco\textended\timport\tlocaltime case\tday_minute\textent_size\tin\tlocaltimestam catalog_name\tday_second\tfactor\tinactive\tlock chain\tdeallocate\tfailedlogin\tindex\tlocked challenge_res\tdec\tfalse\tindexes\tlocks change\tdecimal\tfast\tinfile\tlogfile changed\tdeclare\tfaults\tinitial\tlogs channel\tdefault\tfetch\tinitial_size\tlong char\tdefault_auth\tfields\tinitiate\tlongblob character\tdefiner\tfile\tinner\tlongtext charset\tdefinition\tfile_block_si\tinout\tloop check\tdelayed\tfilter\tinsensitive\tlow_priority checksum\tdelay_key_wri\tfinish\tinsert\tmaster cipher\tdelete\tfirst\tinsert_method\tmaster_auto_p class_origin\tdense_rank\tfirst_value\tinstall\tmaster_bind client\tdesc\tfixed\tinstance\tmaster_compre master_connec\tnever\tpreserve\trestrict\tsource_host master_delay\tnew\tprev\tresume\tsource_log_fi master_heartb\tnext\tprimary\tretain\tsource_log_po master_host\tno\tprivileges\treturn\tsource_passwo master_log_fi\tnodegroup\tprivilege_che\treturned_sqls\tsource_port master_log_po\tnone\tprocedure\treturning\tsource_public master_passwo\tnot\tprocess\treturns\tsourceretry master_port\tnowait\tprocesslist\treuse\tsource_ssl master_public\tno_wait\tprofile\treverse\tsource_ssl_ca masterretry\tno_write_to_b\tprofiles\trevoke\tsource_ssl_ca master_server\tnth_value\tproxy\tright\tsource_ssl_ce master_ssl\tntile\tpurge\trlike\tsource_ssl_ci master_ssl_ca\tnull\tquarter\trole\tsource_ssl_cr master_ssl_ca\tnulls\tquery\trollback\tsource_ssl_cr master_ssl_ce\tnumber\tquick\trollup\tsource_ssl_ke master_ssl_ci\tnumeric\trandom\trotate\tsource_ssl_ve master_ssl_cr\tnvarchar\trange\troutine\tsource_tls_ci master_ssl_cr\tof\trank\trow\tsource_tls_ve master_ssl_ke\toff\tread\trows\tsource_user master_ssl_ve\toffset\treads\trow_count\tsource_zstd_c master_tls_ci\toj\tread_only\trow_format\tspatial master_tls_ve\told\tread_write\trow_number\tspecific master_user\ton\treal\trtree\tsql master_zstd_c\tone\trebuild\tsavepoint\tsqlexception match\tonly\trecover\tschedule\tsqlstate maxvalue\topen\trecursive\tschema\tsqlwarning max_connectio\toptimize\tredofile\tschemas\tsql_after_gti max_queries_p\toptimizer_cos\tredo_buffer_s\tschema_name\tsql_after_mts max_rows\toption\tredundant\tsecond\tsql_before_gt max_size\toptional\treference\tsecondary\tsql_big_resul max_updates_p\toptionally\treferences\tsecondary_eng\tsql_buffer_re max_user_conn\toptions\tregexp\tsecondary_eng\tsql_cache medium\tor\tregistration\tsecondary_loa\tsql_calc_foun mediumblob\torder\trelay\tsecondary_unl\tsql_no_cache mediumint\tordinality\trelaylog\tsecond_micros\tsql_small_res mediumtext\torganization\trelay_log_fil\tsecurity\tsql_thread member\tothers\trelay_log_pos\tselect\tsql_tsi_day memory\tout\trelay_thread\tsensitive\tsql_tsi_hour merge\touter\trelease\tseparator\tsql_tsi_minut message_text\toutfile\treload\tserial\tsql_tsi_month microsecond\tover\tremote\tserializable\tsql_tsi_quart middleint\towner\tremove\tserver\tsql_tsi_secon migrate\tpack_keys\trename\tsession\tsql_tsi_week minute\tpage\treorganize\tset\tsql_tsi_year minute_micros\tparser\trepair\tshare\tsrid minute_second\tpartial\trepeat\tshow\tssl min_rows\tpartition\trepeatable\tshutdown\tstacked mod\tpartitioning\treplace\tsignal\tstart mode\tpartitions\treplica\tsigned\tstarting modifies\tpassword\treplicas\tsimple\tstarts modify\tpassword_lock\treplicatedo\tskip\tstats_auto_re month\tpath\treplicatedo\tslave\tstats_persist multilinestri\tpercent_rank\treplicate_ign\tslow\tstatssample multipoint\tpersist\treplicate_ign\tsmallint\tstatus multipolygon\tpersist_only\treplicate_rew\tsnapshot\tstop mutex\tphase\treplicate_wil\tsocket\tstorage mysql_errno\tplugin\treplicate_wil\tsome\tstored name\tplugins\treplication\tsoname\tstraight_join names\tplugin_dir\trequire\tsounds\tstream national\tpoint\trequire_row_f\tsource\tstring natural\tpolygon\treset\tsource_auto_p\tsubclass_orig nchar\tport\tresignal\tsource_bind\tsubject ndb\tprecedes\tresource\tsource_compre\tsubpartition ndbcluster\tpreceding\trespect\tsource_connec\tsubpartitions nested\tprecision\trestart\tsource_delay\tsuper network_names\tprepare\trestore\tsource_heartb\tsuspend swaps\ttimestampdiff\tundo_buffer_s\tutc_date\twhen switches\ttinyblob\tunicode\tutc_time\twhere system\ttinyint\tuninstall\tutc_timestamp\twhile table\ttinytext\tunion\tvalidation\twindow tables\ttls\tunique\tvalue\twith tablespace\tto\tunknown\tvalues\twithout table_checksu\ttrailing\tunlock\tvarbinary\twork table_name\ttransaction\tunregister\tvarchar\twrapper temporary\ttrigger\tunsigned\tvarcharacter\twrite temptable\ttriggers\tuntil\tvariables\tx509 terminated\ttrue\tupdate\tvarying\txa text\ttruncate\tupgrade\tvcpu\txid than\ttype\turl\tview\txml then\ttypes\tusage\tvirtual\txor thread_priori\tunbounded\tuse\tvisible\tyear ties\tuncommitted\tuser\twait\tyear_month time\tundefined\tuser_resource\twarnings\tzerofill timestamp\tundo\tuse_frm\tweek\tzone timestampadd\tundofile\tusing\tweight_string\t  ","version":"Next","tagName":"h2"},{"title":"Changelog​","type":1,"pageTitle":"MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/#changelog","content":" The changelog includes a list of breaking changes made to this connector. Backwards-compatible changes are not listed.  Proceed with caution when editing materializations created with previous versions of this connector; editing always upgrades your materialization to the latest connector version.  V1: 2023-08-21​  First version ","version":"Next","tagName":"h2"},{"title":"Amazon RDS for MySQL","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/MySQL/amazon-rds-mysql/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/amazon-rds-mysql/#prerequisites","content":" To use this connector, you'll need:  A MySQL database to which to materialize, and user credentials. MySQL versions 5.7 and later are supportedThe connector will create new tables in the database per your specification, so user credentials must have access to create new tables.The local_infile global variable must be enabled. You can enable this setting by running SET GLOBAL local_infile = true in your database. At least one Flow collection  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/amazon-rds-mysql/#setup","content":" You must configure your database to allow connections from Estuary. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel.  ","version":"Next","tagName":"h2"},{"title":"Connecting Directly With Amazon RDS​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/amazon-rds-mysql/#connecting-directly-with-amazon-rds","content":" Edit the VPC security group associated with your database instance, or create a new VPC security group and associate it with the database instance. Modify the instance, choosing Publicly accessible in the Connectivity settings. Per the steps in the Amazon documentation, create a new inbound rule and a new outbound rule that allow all traffic from the Estuary Flow IP addresses.  ","version":"Next","tagName":"h3"},{"title":"Connect With SSH Tunneling​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/amazon-rds-mysql/#connect-with-ssh-tunneling","content":" To allow SSH tunneling to a database instance hosted on AWS, you'll need to create a virtual computing environment, or instance, in Amazon EC2.  Begin by finding your public SSH key on your local machine. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key Import your SSH key into AWS. Launch a new instance in EC2. During setup: Configure the security group to allow SSH connection from anywhere.When selecting a key pair, choose the key you just imported. Connect to the instance, setting the user name to ec2-user. Find and note the instance's public DNS. This will be formatted like: ec2-198-21-98-1.compute-1.amazonaws.com.  Connect with SSH tunneling Refer to the guide to configure an SSH server on the cloud platform of your choice. Configure your connector as described in the configuration section above, with the addition of the networkTunnel stanza to enable the SSH tunnel, if using. See Connecting to endpoints on secure networksfor additional details and a sample.  Configuration Tip To configure the connector, you must specify the database address in the formathost:port. (You can also supply host only; the connector will use the port 3306 by default, which is correct in many cases.) You can find the host and port in the following locations in each platform's console: Amazon RDS: host as Endpoint; port as Port.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/amazon-rds-mysql/#configuration","content":" To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a MySQL materialization, which will direct one or more of your Flow collections to your desired tables, or views, in the database.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/amazon-rds-mysql/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/database\tDatabase\tName of the logical database to materialize to.\tstring\tRequired /address\tAddress\tHost and port of the database. If only the host is specified, port will default to 3306.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /user\tUser\tDatabase user to connect as.\tstring\tRequired /timezone\tTimezone\tTimezone to use when materializing datetime columns. Should normally be left blank to use the database's 'time_zone' system variable. Only required if the 'time_zone' system variable cannot be read. Must be a valid IANA time zone name or +HH:MM offset. Takes precedence over the 'time_zone' system variable if both are set.\tstring /advanced\tAdvanced Options\tOptions for advanced users. You should not typically need to modify these.\tobject /advanced/sslmode\tSSL Mode\tOverrides SSL connection behavior by setting the 'sslmode' parameter.\tstring /advanced/ssl_server_ca\tSSL Server CA\tOptional server certificate authority to use when connecting with custom SSL mode\tstring /advanced/ssl_client_cert\tSSL Client Certificate\tOptional client certificate to use when connecting with custom SSL mode.\tstring /advanced/ssl_client_key\tSSL Client Key\tOptional client key to use when connecting with custom SSL mode.\tstring\t  ","version":"Next","tagName":"h3"},{"title":"Setting the MySQL time zone​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/amazon-rds-mysql/#setting-the-mysql-time-zone","content":" MySQL's time_zone server system variable is set to SYSTEM by default.  If you intend to materialize collections including fields of with format: date-time or format: time, and time_zone is set to SYSTEM, Flow won't be able to detect the time zone and convert datetimes to the appropriate timezone when materializing. To avoid this, you must explicitly set the time zone for your database.  You can:  Specify a numerical offset from UTC. For MySQL version 8.0.19 or higher, values from -13:59 to +14:00, inclusive, are permitted.Prior to MySQL 8.0.19, values from -12:59 to +13:00, inclusive, are permitted Specify a named timezone in IANA timezone format. If you're using Amazon Aurora, create or modify the DB cluster parameter groupassociated with your MySQL database.Set the time_zone parameter to the correct value.  For example, if you're located in New Jersey, USA, you could set time_zone to -05:00 or -04:00, depending on the time of year. Because this region observes daylight savings time, you'd be responsible for changing the offset. Alternatively, you could set time_zone to America/New_York, and time changes would occur automatically.  If using IANA time zones, your database must include time zone tables. Learn more in the MySQL docs.  Materialize Timezone Configuration If you are unable to set the time_zone in the database and need to materialize collections with date-time or time fields, the materialization can be configured to assume a time zone using the timezone configuration property (see above). The timezone configuration property can be set as a numerical offset or IANA timezone format.  SSL Mode​  Possible values:  disabled: A plain unencrypted connection is established with the serverpreferred: Only use SSL connection if the server asks for itrequired: Connect using an SSL connection, but do not verify the server's certificate.verify_ca: Connect using an SSL connection, and verify the server's certificate against the given SSL Server CA, but does not verify the server's hostname. This option is most commonly used when connecting to an IP address which does not have a hostname to be verified. When using this mode, SSL Server CA must be provided.verify_identity: Connect using an SSL connection, verify the server's certificate and the server's hostname. This is the most secure option. When using this mode, SSL Server CA must be provided.  Optionally, SSL Client Certificate and Key can be provided if necessary to authorize the client.  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tTable name to materialize to. It will be created by the connector, unless the connector has previously created it.\tstring\tRequired /delta_updates\tDelta Update\tShould updates to this table be done via delta updates.\tboolean\tfalse  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/amazon-rds-mysql/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: ghcr.io/estuary/materialize-mysql:dev config: database: flow address: localhost:5432 password: flow user: flow bindings: - resource: table: ${TABLE_NAME} source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Delta updates​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/amazon-rds-mysql/#delta-updates","content":" This connector supports both standard (merge) and delta updates. The default is to use standard updates.  ","version":"Next","tagName":"h2"},{"title":"Date & times​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/amazon-rds-mysql/#date--times","content":" Date and time fields that are part of collections, which specify a format: date-time for the field, are automatically converted to UTC and persisted as UTC DATETIME in MySQL.  ","version":"Next","tagName":"h2"},{"title":"Reserved words​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/amazon-rds-mysql/#reserved-words","content":" MySQL has a list of reserved words that must be quoted in order to be used as an identifier. Flow considers all the reserved words in the official MySQL documentation.  These reserved words are listed in the table below. Flow automatically quotes fields that are in this list.  Reserved words accessible\tclone\tdescribe\tfloat\tint account\tclose\tdescription\tfloat4\tint1 action\tcoalesce\tdes_key_file\tfloat8\tint2 active\tcode\tdeterministic\tflush\tint3 add\tcollate\tdiagnostics\tfollowing\tint4 admin\tcollation\tdirectory\tfollows\tint8 after\tcolumn\tdisable\tfor\tinteger against\tcolumns\tdiscard\tforce\tintersect aggregate\tcolumn_format\tdisk\tforeign\tinterval algorithm\tcolumn_name\tdistinct\tformat\tinto all\tcomment\tdistinctrow\tfound\tinvisible alter\tcommit\tdiv\tfrom\tinvoker always\tcommitted\tdo\tfull\tio analyse\tcompact\tdouble\tfulltext\tio_after_gtid analyze\tcompletion\tdrop\tfunction\tio_before_gti and\tcomponent\tdual\tgeneral\tio_thread any\tcompressed\tdumpfile\tgenerate\tipc array\tcompression\tduplicate\tgenerated\tis as\tconcurrent\tdynamic\tgeomcollectio\tisolation asc\tcondition\teach\tgeometry\tissuer ascii\tconnection\telse\tgeometrycolle\titerate asensitive\tconsistent\telseif\tget\tjoin at\tconstraint\tempty\tget_format\tjson attribute\tconstraint_ca\tenable\tget_master_pu\tjson_table authenticatio\tconstraint_na\tenclosed\tget_source_pu\tjson_value autoextend_si\tconstraint_sc\tencryption\tglobal\tkey auto_incremen\tcontains\tend\tgrant\tkeyring avg\tcontext\tends\tgrants\tkeys avg_row_lengt\tcontinue\tenforced\tgroup\tkey_block_siz backup\tconvert\tengine\tgrouping\tkill before\tcpu\tengines\tgroups\tlag begin\tcreate\tengine_attrib\tgroup_replica\tlanguage between\tcross\tenum\tgtid_only\tlast bigint\tcube\terror\thandler\tlast_value binary\tcume_dist\terrors\thash\tlateral binlog\tcurrent\tescape\thaving\tlead bit\tcurrent_date\tescaped\thelp\tleading blob\tcurrent_time\tevent\thigh_priority\tleave block\tcurrent_times\tevents\thistogram\tleaves bool\tcurrent_user\tevery\thistory\tleft boolean\tcursor\texcept\thost\tless both\tcursor_name\texchange\thosts\tlevel btree\tdata\texclude\thour\tlike buckets\tdatabase\texecute\thour_microsec\tlimit bulk\tdatabases\texists\thour_minute\tlinear by\tdatafile\texit\thour_second\tlines byte\tdate\texpansion\tidentified\tlinestring cache\tdatetime\texpire\tif\tlist call\tday\texplain\tignore\tload cascade\tday_hour\texport\tignore_server\tlocal cascaded\tday_microseco\textended\timport\tlocaltime case\tday_minute\textent_size\tin\tlocaltimestam catalog_name\tday_second\tfactor\tinactive\tlock chain\tdeallocate\tfailedlogin\tindex\tlocked challenge_res\tdec\tfalse\tindexes\tlocks change\tdecimal\tfast\tinfile\tlogfile changed\tdeclare\tfaults\tinitial\tlogs channel\tdefault\tfetch\tinitial_size\tlong char\tdefault_auth\tfields\tinitiate\tlongblob character\tdefiner\tfile\tinner\tlongtext charset\tdefinition\tfile_block_si\tinout\tloop check\tdelayed\tfilter\tinsensitive\tlow_priority checksum\tdelay_key_wri\tfinish\tinsert\tmaster cipher\tdelete\tfirst\tinsert_method\tmaster_auto_p class_origin\tdense_rank\tfirst_value\tinstall\tmaster_bind client\tdesc\tfixed\tinstance\tmaster_compre master_connec\tnever\tpreserve\trestrict\tsource_host master_delay\tnew\tprev\tresume\tsource_log_fi master_heartb\tnext\tprimary\tretain\tsource_log_po master_host\tno\tprivileges\treturn\tsource_passwo master_log_fi\tnodegroup\tprivilege_che\treturned_sqls\tsource_port master_log_po\tnone\tprocedure\treturning\tsource_public master_passwo\tnot\tprocess\treturns\tsourceretry master_port\tnowait\tprocesslist\treuse\tsource_ssl master_public\tno_wait\tprofile\treverse\tsource_ssl_ca masterretry\tno_write_to_b\tprofiles\trevoke\tsource_ssl_ca master_server\tnth_value\tproxy\tright\tsource_ssl_ce master_ssl\tntile\tpurge\trlike\tsource_ssl_ci master_ssl_ca\tnull\tquarter\trole\tsource_ssl_cr master_ssl_ca\tnulls\tquery\trollback\tsource_ssl_cr master_ssl_ce\tnumber\tquick\trollup\tsource_ssl_ke master_ssl_ci\tnumeric\trandom\trotate\tsource_ssl_ve master_ssl_cr\tnvarchar\trange\troutine\tsource_tls_ci master_ssl_cr\tof\trank\trow\tsource_tls_ve master_ssl_ke\toff\tread\trows\tsource_user master_ssl_ve\toffset\treads\trow_count\tsource_zstd_c master_tls_ci\toj\tread_only\trow_format\tspatial master_tls_ve\told\tread_write\trow_number\tspecific master_user\ton\treal\trtree\tsql master_zstd_c\tone\trebuild\tsavepoint\tsqlexception match\tonly\trecover\tschedule\tsqlstate maxvalue\topen\trecursive\tschema\tsqlwarning max_connectio\toptimize\tredofile\tschemas\tsql_after_gti max_queries_p\toptimizer_cos\tredo_buffer_s\tschema_name\tsql_after_mts max_rows\toption\tredundant\tsecond\tsql_before_gt max_size\toptional\treference\tsecondary\tsql_big_resul max_updates_p\toptionally\treferences\tsecondary_eng\tsql_buffer_re max_user_conn\toptions\tregexp\tsecondary_eng\tsql_cache medium\tor\tregistration\tsecondary_loa\tsql_calc_foun mediumblob\torder\trelay\tsecondary_unl\tsql_no_cache mediumint\tordinality\trelaylog\tsecond_micros\tsql_small_res mediumtext\torganization\trelay_log_fil\tsecurity\tsql_thread member\tothers\trelay_log_pos\tselect\tsql_tsi_day memory\tout\trelay_thread\tsensitive\tsql_tsi_hour merge\touter\trelease\tseparator\tsql_tsi_minut message_text\toutfile\treload\tserial\tsql_tsi_month microsecond\tover\tremote\tserializable\tsql_tsi_quart middleint\towner\tremove\tserver\tsql_tsi_secon migrate\tpack_keys\trename\tsession\tsql_tsi_week minute\tpage\treorganize\tset\tsql_tsi_year minute_micros\tparser\trepair\tshare\tsrid minute_second\tpartial\trepeat\tshow\tssl min_rows\tpartition\trepeatable\tshutdown\tstacked mod\tpartitioning\treplace\tsignal\tstart mode\tpartitions\treplica\tsigned\tstarting modifies\tpassword\treplicas\tsimple\tstarts modify\tpassword_lock\treplicatedo\tskip\tstats_auto_re month\tpath\treplicatedo\tslave\tstats_persist multilinestri\tpercent_rank\treplicate_ign\tslow\tstatssample multipoint\tpersist\treplicate_ign\tsmallint\tstatus multipolygon\tpersist_only\treplicate_rew\tsnapshot\tstop mutex\tphase\treplicate_wil\tsocket\tstorage mysql_errno\tplugin\treplicate_wil\tsome\tstored name\tplugins\treplication\tsoname\tstraight_join names\tplugin_dir\trequire\tsounds\tstream national\tpoint\trequire_row_f\tsource\tstring natural\tpolygon\treset\tsource_auto_p\tsubclass_orig nchar\tport\tresignal\tsource_bind\tsubject ndb\tprecedes\tresource\tsource_compre\tsubpartition ndbcluster\tpreceding\trespect\tsource_connec\tsubpartitions nested\tprecision\trestart\tsource_delay\tsuper network_names\tprepare\trestore\tsource_heartb\tsuspend swaps\ttimestampdiff\tundo_buffer_s\tutc_date\twhen switches\ttinyblob\tunicode\tutc_time\twhere system\ttinyint\tuninstall\tutc_timestamp\twhile table\ttinytext\tunion\tvalidation\twindow tables\ttls\tunique\tvalue\twith tablespace\tto\tunknown\tvalues\twithout table_checksu\ttrailing\tunlock\tvarbinary\twork table_name\ttransaction\tunregister\tvarchar\twrapper temporary\ttrigger\tunsigned\tvarcharacter\twrite temptable\ttriggers\tuntil\tvariables\tx509 terminated\ttrue\tupdate\tvarying\txa text\ttruncate\tupgrade\tvcpu\txid than\ttype\turl\tview\txml then\ttypes\tusage\tvirtual\txor thread_priori\tunbounded\tuse\tvisible\tyear ties\tuncommitted\tuser\twait\tyear_month time\tundefined\tuser_resource\twarnings\tzerofill timestamp\tundo\tuse_frm\tweek\tzone timestampadd\tundofile\tusing\tweight_string\t  ","version":"Next","tagName":"h2"},{"title":"Changelog​","type":1,"pageTitle":"Amazon RDS for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/amazon-rds-mysql/#changelog","content":" The changelog includes a list of breaking changes made to this connector. Backwards-compatible changes are not listed.  Proceed with caution when editing materializations created with previous versions of this connector; editing always upgrades your materialization to the latest connector version.  V1: 2023-08-21​  First version ","version":"Next","tagName":"h2"},{"title":"Google Cloud SQL for MySQL","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/MySQL/google-cloud-sql-mysql/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/google-cloud-sql-mysql/#prerequisites","content":" To use this connector, you'll need:  A MySQL database to which to materialize, and user credentials. MySQL versions 5.7 and later are supportedThe connector will create new tables in the database per your specification, so user credentials must have access to create new tables.The local_infile global variable must be enabled. You can enable this setting by running SET GLOBAL local_infile = true in your database. At least one Flow collection  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/google-cloud-sql-mysql/#setup","content":" ","version":"Next","tagName":"h2"},{"title":"Connecting Directly to Google Cloud SQL​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/google-cloud-sql-mysql/#connecting-directly-to-google-cloud-sql","content":" Enable public IP on your database and add the Estuary Flow IP addresses as authorized IP addresses.  ","version":"Next","tagName":"h3"},{"title":"Connect With SSH Tunneling​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/google-cloud-sql-mysql/#connect-with-ssh-tunneling","content":" To allow SSH tunneling to a database instance hosted on Google Cloud, you must set up a virtual machine (VM).  Begin by finding your public SSH key on your local machine. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key If your Google login differs from your local username, generate a key that includes your Google email address as a comment: ssh-keygen -m PEM -t rsa -C user@domain.com Create and start a new VM in GCP, choosing an image that supports OS Login. Add your public key to the VM. Reserve an external IP address and connect it to the VM during setup. Note the generated address.  Configuration Tip To configure the connector, you must specify the database address in the formathost:port. (You can also supply host only; the connector will use the port 3306 by default, which is correct in many cases.) You can find the host and port in the following locations in each platform's console: Google Cloud SQL: host as Private IP Address; port is always 3306. You may need to configure private IP on your database.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/google-cloud-sql-mysql/#configuration","content":" To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a MySQL materialization, which will direct one or more of your Flow collections to your desired tables, or views, in the database.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/google-cloud-sql-mysql/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/database\tDatabase\tName of the logical database to materialize to.\tstring\tRequired /address\tAddress\tHost and port of the database. If only the host is specified, port will default to 3306.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /user\tUser\tDatabase user to connect as.\tstring\tRequired /timezone\tTimezone\tTimezone to use when materializing datetime columns. Should normally be left blank to use the database's 'time_zone' system variable. Only required if the 'time_zone' system variable cannot be read. Must be a valid IANA time zone name or +HH:MM offset. Takes precedence over the 'time_zone' system variable if both are set.\tstring /advanced\tAdvanced Options\tOptions for advanced users. You should not typically need to modify these.\tobject /advanced/sslmode\tSSL Mode\tOverrides SSL connection behavior by setting the 'sslmode' parameter.\tstring /advanced/ssl_server_ca\tSSL Server CA\tOptional server certificate authority to use when connecting with custom SSL mode\tstring /advanced/ssl_client_cert\tSSL Client Certificate\tOptional client certificate to use when connecting with custom SSL mode.\tstring /advanced/ssl_client_key\tSSL Client Key\tOptional client key to use when connecting with custom SSL mode.\tstring\t  ","version":"Next","tagName":"h3"},{"title":"Setting the MySQL time zone​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/google-cloud-sql-mysql/#setting-the-mysql-time-zone","content":" MySQL's time_zone server system variable is set to SYSTEM by default.  If you intend to materialize collections including fields of with format: date-time or format: time, and time_zone is set to SYSTEM, Flow won't be able to detect the time zone and convert datetimes to the appropriate timezone when materializing. To avoid this, you must explicitly set the time zone for your database.  You can:  Specify a numerical offset from UTC. For MySQL version 8.0.19 or higher, values from -13:59 to +14:00, inclusive, are permitted.Prior to MySQL 8.0.19, values from -12:59 to +13:00, inclusive, are permitted Specify a named timezone in IANA timezone format.  For example, if you're located in New Jersey, USA, you could set time_zone to -05:00 or -04:00, depending on the time of year. Because this region observes daylight savings time, you'd be responsible for changing the offset. Alternatively, you could set time_zone to America/New_York, and time changes would occur automatically.  If using IANA time zones, your database must include time zone tables. Learn more in the MySQL docs.  Materialize Timezone Configuration If you are unable to set the time_zone in the database and need to materialize collections with date-time or time fields, the materialization can be configured to assume a time zone using the timezone configuration property (see above). The timezone configuration property can be set as a numerical offset or IANA timezone format.  SSL Mode​  Possible values:  disabled: A plain unencrypted connection is established with the serverpreferred: Only use SSL connection if the server asks for itrequired: Connect using an SSL connection, but do not verify the server's certificate.verify_ca: Connect using an SSL connection, and verify the server's certificate against the given SSL Server CA, but does not verify the server's hostname. This option is most commonly used when connecting to an IP address which does not have a hostname to be verified. When using this mode, SSL Server CA must be provided.verify_identity: Connect using an SSL connection, verify the server's certificate and the server's hostname. This is the most secure option. When using this mode, SSL Server CA must be provided.  Optionally, SSL Client Certificate and Key can be provided if necessary to authorize the client.  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tTable name to materialize to. It will be created by the connector, unless the connector has previously created it.\tstring\tRequired /delta_updates\tDelta Update\tShould updates to this table be done via delta updates.\tboolean\tfalse  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/google-cloud-sql-mysql/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: ghcr.io/estuary/materialize-mysql:dev config: database: flow address: localhost:5432 password: flow user: flow bindings: - resource: table: ${TABLE_NAME} source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Setup​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/google-cloud-sql-mysql/#setup-1","content":" Allow connections between the database and Estuary Flow. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel. To allow direct access: Enable public IP on your database and add the Estuary Flow IP addresses as authorized IP addresses. To allow secure connections via SSH tunneling: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networks for additional details and a sample. Configure your connector as described in the configuration section above, with the addition of the networkTunnel stanza to enable the SSH tunnel, if using. See Connecting to endpoints on secure networksfor additional details and a sample.  Configuration Tip To configure the connector, you must specify the database address in the formathost:port. (You can also supply host only; the connector will use the port 3306 by default, which is correct in many cases.) You can find the host in the GCP console as &quot;Private IP Address&quot;. The port is always 3306. You may need to configure private IP on your database.  Create the flow_materialize user with All privileges on your database. This user will need the ability to create and update the flow_materializations table.  CREATE USER IF NOT EXISTS flow_materialize IDENTIFIED BY 'secret' COMMENT 'User account for Flow MySQL data materialization'; GRANT ALL PRIVELEGES ON &lt;database&gt;.* TO 'flow_materialize';   In the Cloud Console, note the instance's host under Public IP Address. Its port will always be 3306. Together, you'll use the host:port as the address property when you configure the connector.  ","version":"Next","tagName":"h3"},{"title":"Delta updates​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/google-cloud-sql-mysql/#delta-updates","content":" This connector supports both standard (merge) and delta updates. The default is to use standard updates.  ","version":"Next","tagName":"h2"},{"title":"Date & times​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/google-cloud-sql-mysql/#date--times","content":" Date and time fields that are part of collections, which specify a format: date-time for the field, are automatically converted to UTC and persisted as UTC DATETIME in MySQL.  ","version":"Next","tagName":"h2"},{"title":"Reserved words​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/google-cloud-sql-mysql/#reserved-words","content":" MySQL has a list of reserved words that must be quoted in order to be used as an identifier. Flow considers all the reserved words in the official MySQL documentation.  These reserved words are listed in the table below. Flow automatically quotes fields that are in this list.  Reserved words accessible\tclone\tdescribe\tfloat\tint account\tclose\tdescription\tfloat4\tint1 action\tcoalesce\tdes_key_file\tfloat8\tint2 active\tcode\tdeterministic\tflush\tint3 add\tcollate\tdiagnostics\tfollowing\tint4 admin\tcollation\tdirectory\tfollows\tint8 after\tcolumn\tdisable\tfor\tinteger against\tcolumns\tdiscard\tforce\tintersect aggregate\tcolumn_format\tdisk\tforeign\tinterval algorithm\tcolumn_name\tdistinct\tformat\tinto all\tcomment\tdistinctrow\tfound\tinvisible alter\tcommit\tdiv\tfrom\tinvoker always\tcommitted\tdo\tfull\tio analyse\tcompact\tdouble\tfulltext\tio_after_gtid analyze\tcompletion\tdrop\tfunction\tio_before_gti and\tcomponent\tdual\tgeneral\tio_thread any\tcompressed\tdumpfile\tgenerate\tipc array\tcompression\tduplicate\tgenerated\tis as\tconcurrent\tdynamic\tgeomcollectio\tisolation asc\tcondition\teach\tgeometry\tissuer ascii\tconnection\telse\tgeometrycolle\titerate asensitive\tconsistent\telseif\tget\tjoin at\tconstraint\tempty\tget_format\tjson attribute\tconstraint_ca\tenable\tget_master_pu\tjson_table authenticatio\tconstraint_na\tenclosed\tget_source_pu\tjson_value autoextend_si\tconstraint_sc\tencryption\tglobal\tkey auto_incremen\tcontains\tend\tgrant\tkeyring avg\tcontext\tends\tgrants\tkeys avg_row_lengt\tcontinue\tenforced\tgroup\tkey_block_siz backup\tconvert\tengine\tgrouping\tkill before\tcpu\tengines\tgroups\tlag begin\tcreate\tengine_attrib\tgroup_replica\tlanguage between\tcross\tenum\tgtid_only\tlast bigint\tcube\terror\thandler\tlast_value binary\tcume_dist\terrors\thash\tlateral binlog\tcurrent\tescape\thaving\tlead bit\tcurrent_date\tescaped\thelp\tleading blob\tcurrent_time\tevent\thigh_priority\tleave block\tcurrent_times\tevents\thistogram\tleaves bool\tcurrent_user\tevery\thistory\tleft boolean\tcursor\texcept\thost\tless both\tcursor_name\texchange\thosts\tlevel btree\tdata\texclude\thour\tlike buckets\tdatabase\texecute\thour_microsec\tlimit bulk\tdatabases\texists\thour_minute\tlinear by\tdatafile\texit\thour_second\tlines byte\tdate\texpansion\tidentified\tlinestring cache\tdatetime\texpire\tif\tlist call\tday\texplain\tignore\tload cascade\tday_hour\texport\tignore_server\tlocal cascaded\tday_microseco\textended\timport\tlocaltime case\tday_minute\textent_size\tin\tlocaltimestam catalog_name\tday_second\tfactor\tinactive\tlock chain\tdeallocate\tfailedlogin\tindex\tlocked challenge_res\tdec\tfalse\tindexes\tlocks change\tdecimal\tfast\tinfile\tlogfile changed\tdeclare\tfaults\tinitial\tlogs channel\tdefault\tfetch\tinitial_size\tlong char\tdefault_auth\tfields\tinitiate\tlongblob character\tdefiner\tfile\tinner\tlongtext charset\tdefinition\tfile_block_si\tinout\tloop check\tdelayed\tfilter\tinsensitive\tlow_priority checksum\tdelay_key_wri\tfinish\tinsert\tmaster cipher\tdelete\tfirst\tinsert_method\tmaster_auto_p class_origin\tdense_rank\tfirst_value\tinstall\tmaster_bind client\tdesc\tfixed\tinstance\tmaster_compre master_connec\tnever\tpreserve\trestrict\tsource_host master_delay\tnew\tprev\tresume\tsource_log_fi master_heartb\tnext\tprimary\tretain\tsource_log_po master_host\tno\tprivileges\treturn\tsource_passwo master_log_fi\tnodegroup\tprivilege_che\treturned_sqls\tsource_port master_log_po\tnone\tprocedure\treturning\tsource_public master_passwo\tnot\tprocess\treturns\tsourceretry master_port\tnowait\tprocesslist\treuse\tsource_ssl master_public\tno_wait\tprofile\treverse\tsource_ssl_ca masterretry\tno_write_to_b\tprofiles\trevoke\tsource_ssl_ca master_server\tnth_value\tproxy\tright\tsource_ssl_ce master_ssl\tntile\tpurge\trlike\tsource_ssl_ci master_ssl_ca\tnull\tquarter\trole\tsource_ssl_cr master_ssl_ca\tnulls\tquery\trollback\tsource_ssl_cr master_ssl_ce\tnumber\tquick\trollup\tsource_ssl_ke master_ssl_ci\tnumeric\trandom\trotate\tsource_ssl_ve master_ssl_cr\tnvarchar\trange\troutine\tsource_tls_ci master_ssl_cr\tof\trank\trow\tsource_tls_ve master_ssl_ke\toff\tread\trows\tsource_user master_ssl_ve\toffset\treads\trow_count\tsource_zstd_c master_tls_ci\toj\tread_only\trow_format\tspatial master_tls_ve\told\tread_write\trow_number\tspecific master_user\ton\treal\trtree\tsql master_zstd_c\tone\trebuild\tsavepoint\tsqlexception match\tonly\trecover\tschedule\tsqlstate maxvalue\topen\trecursive\tschema\tsqlwarning max_connectio\toptimize\tredofile\tschemas\tsql_after_gti max_queries_p\toptimizer_cos\tredo_buffer_s\tschema_name\tsql_after_mts max_rows\toption\tredundant\tsecond\tsql_before_gt max_size\toptional\treference\tsecondary\tsql_big_resul max_updates_p\toptionally\treferences\tsecondary_eng\tsql_buffer_re max_user_conn\toptions\tregexp\tsecondary_eng\tsql_cache medium\tor\tregistration\tsecondary_loa\tsql_calc_foun mediumblob\torder\trelay\tsecondary_unl\tsql_no_cache mediumint\tordinality\trelaylog\tsecond_micros\tsql_small_res mediumtext\torganization\trelay_log_fil\tsecurity\tsql_thread member\tothers\trelay_log_pos\tselect\tsql_tsi_day memory\tout\trelay_thread\tsensitive\tsql_tsi_hour merge\touter\trelease\tseparator\tsql_tsi_minut message_text\toutfile\treload\tserial\tsql_tsi_month microsecond\tover\tremote\tserializable\tsql_tsi_quart middleint\towner\tremove\tserver\tsql_tsi_secon migrate\tpack_keys\trename\tsession\tsql_tsi_week minute\tpage\treorganize\tset\tsql_tsi_year minute_micros\tparser\trepair\tshare\tsrid minute_second\tpartial\trepeat\tshow\tssl min_rows\tpartition\trepeatable\tshutdown\tstacked mod\tpartitioning\treplace\tsignal\tstart mode\tpartitions\treplica\tsigned\tstarting modifies\tpassword\treplicas\tsimple\tstarts modify\tpassword_lock\treplicatedo\tskip\tstats_auto_re month\tpath\treplicatedo\tslave\tstats_persist multilinestri\tpercent_rank\treplicate_ign\tslow\tstatssample multipoint\tpersist\treplicate_ign\tsmallint\tstatus multipolygon\tpersist_only\treplicate_rew\tsnapshot\tstop mutex\tphase\treplicate_wil\tsocket\tstorage mysql_errno\tplugin\treplicate_wil\tsome\tstored name\tplugins\treplication\tsoname\tstraight_join names\tplugin_dir\trequire\tsounds\tstream national\tpoint\trequire_row_f\tsource\tstring natural\tpolygon\treset\tsource_auto_p\tsubclass_orig nchar\tport\tresignal\tsource_bind\tsubject ndb\tprecedes\tresource\tsource_compre\tsubpartition ndbcluster\tpreceding\trespect\tsource_connec\tsubpartitions nested\tprecision\trestart\tsource_delay\tsuper network_names\tprepare\trestore\tsource_heartb\tsuspend swaps\ttimestampdiff\tundo_buffer_s\tutc_date\twhen switches\ttinyblob\tunicode\tutc_time\twhere system\ttinyint\tuninstall\tutc_timestamp\twhile table\ttinytext\tunion\tvalidation\twindow tables\ttls\tunique\tvalue\twith tablespace\tto\tunknown\tvalues\twithout table_checksu\ttrailing\tunlock\tvarbinary\twork table_name\ttransaction\tunregister\tvarchar\twrapper temporary\ttrigger\tunsigned\tvarcharacter\twrite temptable\ttriggers\tuntil\tvariables\tx509 terminated\ttrue\tupdate\tvarying\txa text\ttruncate\tupgrade\tvcpu\txid than\ttype\turl\tview\txml then\ttypes\tusage\tvirtual\txor thread_priori\tunbounded\tuse\tvisible\tyear ties\tuncommitted\tuser\twait\tyear_month time\tundefined\tuser_resource\twarnings\tzerofill timestamp\tundo\tuse_frm\tweek\tzone timestampadd\tundofile\tusing\tweight_string\t  ","version":"Next","tagName":"h2"},{"title":"Changelog​","type":1,"pageTitle":"Google Cloud SQL for MySQL","url":"/reference/Connectors/materialization-connectors/MySQL/google-cloud-sql-mysql/#changelog","content":" The changelog includes a list of breaking changes made to this connector. Backwards-compatible changes are not listed.  Proceed with caution when editing materializations created with previous versions of this connector; editing always upgrades your materialization to the latest connector version.  V1: 2023-08-21​  First version ","version":"Next","tagName":"h2"},{"title":"PostgreSQL","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/PostgreSQL/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/#prerequisites","content":" To use this connector, you'll need:  A Postgres database to which to materialize, and user credentials. The connector will create new tables in the database per your specification. Tables created manually in advance are not supported.At least one Flow collection  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/#setup","content":" To meet these requirements, follow the steps for your hosting type.  Amazon RDSGoogle Cloud SQLAzure Database for PostgreSQL  In addition to standard PostgreSQL, this connector supports cloud-based PostgreSQL instances. Google Cloud Platform, Amazon Web Service, and Microsoft Azure are currently supported. You may use other cloud platforms, but Estuary doesn't guarantee performance.  To connect securely, you can either enable direct access for Flows's IP or use an SSH tunnel.  Configuration Tip To configure the connector, you must specify the database address in the format host:port. (You can also supply host only; the connector will use the port 5432 by default, which is correct in many cases.) You can find the host and port in the following locations in each platform's console: Amazon RDS and Amazon Aurora: host as Endpoint; port as Port.Google Cloud SQL: host as Private IP Address; port is always 5432. You may need to configure private IP on your database.Azure Database: host as Server Name; port under Connection Strings (usually 5432).TimescaleDB: host as Host; port as Port.  ","version":"Next","tagName":"h2"},{"title":"Azure Database for PostgreSQL​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/#azure-database-for-postgresql","content":" Connect Directly With Azure Database For PostgreSQL: Create a new firewall rule that grants access to the Estuary Flow IP addresses. Connect With SSH Tunneling: Follow the instructions for setting up an SSH connection to Azure Database.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/#configuration","content":" To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Postgres materialization, which will direct one or more of your Flow collections to your desired tables, or views, in the database.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/database\tDatabase\tName of the logical database to materialize to.\tstring /address\tAddress\tHost and port of the database. If only the host is specified, port will default to 5432.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /schema\tDatabase Schema\tDatabase schema to use for materialized tables (unless overridden within the binding resource configuration) as well as associated materialization metadata tables\tstring\t&quot;public&quot; /user\tUser\tDatabase user to connect as.\tstring\tRequired /advanced\tAdvanced Options\tOptions for advanced users. You should not typically need to modify these.\tobject /advanced/sslmode\tSSL Mode\tOverrides SSL connection behavior by setting the 'sslmode' parameter.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/additional_table_create_sql\tAdditional Table Create SQL\tAdditional SQL statement(s) to be run in the same transaction that creates the table.\tstring /delta_updates\tDelta Update\tShould updates to this table be done via delta updates.\tboolean\tfalse /schema\tAlternative Schema\tAlternative schema for this table (optional). Overrides schema set in endpoint configuration.\tstring /table\tTable\tTable name to materialize to. It will be created by the connector, unless the connector has previously created it.\tstring\tRequired  SSL Mode​  Certain managed PostgreSQL implementations may require you to explicitly set the SSL Mode to connect with Flow. One example is Neon, which requires the setting verify-full. Check your managed PostgreSQL's documentation for details if you encounter errors related to the SSL mode configuration.  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: ghcr.io/estuary/materialize-postgres:dev config: database: flow address: localhost:5432 password: flow user: flow bindings: - resource: table: ${TABLE_NAME} source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Delta updates​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/#delta-updates","content":" This connector supports both standard (merge) and delta updates.  The default is to use standard updates.  ","version":"Next","tagName":"h2"},{"title":"Reserved words​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/#reserved-words","content":" PostgreSQL has a list of reserved words that must be quoted in order to be used as an identifier. Flow considers all the reserved words that are marked as &quot;reserved&quot; in any of the columns in the official PostgreSQL documentation.  These reserve words are listed in the table below. Flow automatically quotes fields that are in this list.  Reserved words abs\tcurrent_transform_group_for_type\tindicator\torder\tsqlexception absolute\tcurrent_user\tinitial\tout\tsqlstate acos\tcursor\tinitially\touter\tsqlwarning action\tcycle\tinner\toutput\tsqrt add\tdatalink\tinout\tover\tstart all\tdate\tinput\toverlaps\tstatic allocate\tday\tinsensitive\toverlay\tstddev_pop alter\tdeallocate\tinsert\tpad\tstddev_samp analyse\tdec\tint\tparameter\tsubmultiset analyze\tdecfloat\tinteger\tpartial\tsubset and\tdecimal\tintersect\tpartition\tsubstring any\tdeclare\tintersection\tpattern\tsubstring_regex are\tdefault\tinterval\tper\tsucceeds array\tdeferrable\tinto\tpercent\tsum array_agg\tdeferred\tis\tpercentile_cont\tsymmetric array_max_cardinality\tdefine\tisnull\tpercentile_disc\tsystem as\tdelete\tisolation\tpercent_rank\tsystem_time asc\tdense_rank\tjoin\tperiod\tsystem_user asensitive\tderef\tjson_array\tpermute\ttable asin\tdesc\tjson_arrayagg\tplacing\ttablesample assertion\tdescribe\tjson_exists\tportion\ttan asymmetric\tdescriptor\tjson_object\tposition\ttanh at\tdeterministic\tjson_objectagg\tposition_regex\ttemporary atan\tdiagnostics\tjson_query\tpower\tthen atomic\tdisconnect\tjson_table\tprecedes\ttime authorization\tdistinct\tjson_table_primitive\tprecision\ttimestamp avg\tdlnewcopy\tjson_value\tprepare\ttimezone_hour begin\tdlpreviouscopy\tkey\tpreserve\ttimezone_minute begin_frame\tdlurlcomplete\tlag\tprimary\tto begin_partition\tdlurlcompleteonly\tlanguage\tprior\ttrailing between\tdlurlcompletewrite\tlarge\tprivileges\ttransaction bigint\tdlurlpath\tlast\tprocedure\ttranslate binary\tdlurlpathonly\tlast_value\tptf\ttranslate_regex bit\tdlurlpathwrite\tlateral\tpublic\ttranslation bit_length\tdlurlscheme\tlead\trange\ttreat blob\tdlurlserver\tleading\trank\ttrigger boolean\tdlvalue\tleft\tread\ttrim both\tdo\tlevel\treads\ttrim_array by\tdomain\tlike\treal\ttrue call\tdouble\tlike_regex\trecursive\ttruncate called\tdrop\tlimit\tref\tuescape cardinality\tdynamic\tlistagg\treferences\tunion cascade\teach\tln\treferencing\tunique cascaded\telement\tlocal\tregr_avgx\tunknown case\telse\tlocaltime\tregr_avgy\tunmatched cast\tempty\tlocaltimestamp\tregr_count\tunnest catalog\tend\tlog\tregr_intercept\tupdate ceil\tend-exec\tlog10\tregr_r2\tupper ceiling\tend_frame\tlower\tregr_slope\tusage char\tend_partition\tmatch\tregr_sxx\tuser character\tequals\tmatches\tregr_sxy\tusing character_length\tescape\tmatch_number\tregr_syy\tvalue char_length\tevery\tmatch_recognize\trelative\tvalues check\texcept\tmax\trelease\tvalue_of classifier\texception\tmeasures\trestrict\tvarbinary clob\texec\tmember\tresult\tvarchar close\texecute\tmerge\treturn\tvariadic coalesce\texists\tmethod\treturning\tvarying collate\texp\tmin\treturns\tvar_pop collation\texternal\tminute\trevoke\tvar_samp collect\textract\tmod\tright\tverbose column\tfalse\tmodifies\trollback\tversioning commit\tfetch\tmodule\trollup\tview concurrently\tfilter\tmonth\trow\twhen condition\tfirst\tmultiset\trows\twhenever connect\tfirst_value\tnames\trow_number\twhere connection\tfloat\tnational\trunning\twidth_bucket constraint\tfloor\tnatural\tsavepoint\twindow constraints\tfor\tnchar\tschema\twith contains\tforeign\tnclob\tscope\twithin continue\tfound\tnew\tscroll\twithout convert\tframe_row\tnext\tsearch\twork copy\tfree\tno\tsecond\twrite corr\tfreeze\tnone\tsection\txml corresponding\tfrom\tnormalize\tseek\txmlagg cos\tfull\tnot\tselect\txmlattributes cosh\tfunction\tnotnull\tsensitive\txmlbinary count\tfusion\tnth_value\tsession\txmlcast covar_pop\tget\tntile\tsession_user\txmlcomment covar_samp\tglobal\tnull\tset\txmlconcat create\tgo\tnullif\tshow\txmldocument cross\tgoto\tnumeric\tsimilar\txmlelement cube\tgrant\toccurrences_regex\tsin\txmlexists cume_dist\tgroup\toctet_length\tsinh\txmlforest current\tgrouping\tof\tsize\txmliterate current_catalog\tgroups\toffset\tskip\txmlnamespaces current_date\thaving\told\tsmallint\txmlparse current_default_transform_group\thold\tomit\tsome\txmlpi current_path\thour\ton\tspace\txmlquery current_role\tidentity\tone\tspecific\txmlserialize current_row\tilike\tonly\tspecifictype\txmltable current_schema\timmediate\topen\tsql\txmltext current_time\timport\toption\tsqlcode\txmlvalidate current_timestamp\tin\tor\tsqlerror\tyear  ","version":"Next","tagName":"h2"},{"title":"Changelog​","type":1,"pageTitle":"PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/#changelog","content":" The changelog includes a list of breaking changes made to this connector. Backwards-compatible changes are not listed.  Proceed with caution when editing materializations created with previous versions of this connector; editing always upgrades your materialization to the latest connector version.  V4: 2022-11-30​  This version includes breaking changes to materialized table columns. These provide more consistent column names and types, but tables created from previous versions of the connector may not be compatible with this version.  Capitalization is now preserved when fields in Flow are converted to Postgres column names. Previously, fields containing uppercase letters were converted to lowercase. Field names and values of types date, duration, ipv4, ipv6, macaddr, macaddr8, and time are now converted into their corresponding Postgres types. Previously, only date-time was converted, and all others were materialized as strings. ","version":"Next","tagName":"h2"},{"title":"Amazon RDS for PostgreSQL","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/PostgreSQL/amazon-rds-postgres/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon RDS for PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/amazon-rds-postgres/#prerequisites","content":" To use this connector, you'll need:  A Postgres database to which to materialize, and user credentials. The connector will create new tables in the database per your specification. Tables created manually in advance are not supported.At least one Flow collection  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Amazon RDS for PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/amazon-rds-postgres/#setup","content":" You must configure your database to allow connections from Estuary. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel.  ","version":"Next","tagName":"h2"},{"title":"Connect Directly With Amazon RDS or Amazon Aurora​","type":1,"pageTitle":"Amazon RDS for PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/amazon-rds-postgres/#connect-directly-with-amazon-rds-or-amazon-aurora","content":" Edit the VPC security group associated with your database instance, or create a new VPC security group and associate it with the database instance. Modify the instance, choosing Publicly accessible in the Connectivity settings. See the instructions below to use SSH Tunneling instead of enabling public access. Refer to the steps in the Amazon documentation. Create a new inbound rule and a new outbound rule that allow all traffic from the Estuary Flow IP addresses.  ","version":"Next","tagName":"h3"},{"title":"Connect With SSH Tunneling​","type":1,"pageTitle":"Amazon RDS for PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/amazon-rds-postgres/#connect-with-ssh-tunneling","content":" To allow SSH tunneling to a database instance hosted on AWS, you'll need to create a virtual computing environment, or instance, in Amazon EC2.  Begin by finding your public SSH key on your local machine. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key Import your SSH key into AWS. Launch a new instance in EC2. During setup: Configure the security group to allow SSH connection from anywhere.When selecting a key pair, choose the key you just imported. Connect to the instance, setting the user name to ec2-user. Find and note the instance's public DNS. This will be formatted like: ec2-198-21-98-1.compute-1.amazonaws.com.  Configuration Tip To configure the connector, you must specify the database address in the format host:port. (You can also supply host only; the connector will use the port 5432 by default, which is correct in many cases.) You can find the host and port in the following locations in each platform's console: Amazon RDS: host as Endpoint; port as Port.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Amazon RDS for PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/amazon-rds-postgres/#configuration","content":" To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Postgres materialization, which will direct one or more of your Flow collections to your desired tables, or views, in the database.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Amazon RDS for PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/amazon-rds-postgres/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/database\tDatabase\tName of the logical database to materialize to.\tstring /address\tAddress\tHost and port of the database. If only the host is specified, port will default to 5432.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /schema\tDatabase Schema\tDatabase schema to use for materialized tables (unless overridden within the binding resource configuration) as well as associated materialization metadata tables\tstring\t&quot;public&quot; /user\tUser\tDatabase user to connect as.\tstring\tRequired /advanced\tAdvanced Options\tOptions for advanced users. You should not typically need to modify these.\tobject /advanced/sslmode\tSSL Mode\tOverrides SSL connection behavior by setting the 'sslmode' parameter.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/additional_table_create_sql\tAdditional Table Create SQL\tAdditional SQL statement(s) to be run in the same transaction that creates the table.\tstring /delta_updates\tDelta Update\tShould updates to this table be done via delta updates.\tboolean\tfalse /schema\tAlternative Schema\tAlternative schema for this table (optional). Overrides schema set in endpoint configuration.\tstring /table\tTable\tTable name to materialize to. It will be created by the connector, unless the connector has previously created it.\tstring\tRequired  SSL Mode​  Certain managed PostgreSQL implementations may require you to explicitly set the SSL Mode to connect with Flow. One example is Neon, which requires the setting verify-full. Check your managed PostgreSQL's documentation for details if you encounter errors related to the SSL mode configuration.  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Amazon RDS for PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/amazon-rds-postgres/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: ghcr.io/estuary/materialize-postgres:dev config: database: flow address: localhost:5432 password: flow user: flow bindings: - resource: table: ${TABLE_NAME} source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Delta updates​","type":1,"pageTitle":"Amazon RDS for PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/amazon-rds-postgres/#delta-updates","content":" This connector supports both standard (merge) and delta updates. The default is to use standard updates.  ","version":"Next","tagName":"h2"},{"title":"Reserved words​","type":1,"pageTitle":"Amazon RDS for PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/amazon-rds-postgres/#reserved-words","content":" PostgreSQL has a list of reserved words that must be quoted in order to be used as an identifier. Flow considers all the reserved words that are marked as &quot;reserved&quot; in any of the columns in the official PostgreSQL documentation.  These reserve words are listed in the table below. Flow automatically quotes fields that are in this list.  Reserved words abs\tcurrent_transform_group_for_type\tindicator\torder\tsqlexception absolute\tcurrent_user\tinitial\tout\tsqlstate acos\tcursor\tinitially\touter\tsqlwarning action\tcycle\tinner\toutput\tsqrt add\tdatalink\tinout\tover\tstart all\tdate\tinput\toverlaps\tstatic allocate\tday\tinsensitive\toverlay\tstddev_pop alter\tdeallocate\tinsert\tpad\tstddev_samp analyse\tdec\tint\tparameter\tsubmultiset analyze\tdecfloat\tinteger\tpartial\tsubset and\tdecimal\tintersect\tpartition\tsubstring any\tdeclare\tintersection\tpattern\tsubstring_regex are\tdefault\tinterval\tper\tsucceeds array\tdeferrable\tinto\tpercent\tsum array_agg\tdeferred\tis\tpercentile_cont\tsymmetric array_max_cardinality\tdefine\tisnull\tpercentile_disc\tsystem as\tdelete\tisolation\tpercent_rank\tsystem_time asc\tdense_rank\tjoin\tperiod\tsystem_user asensitive\tderef\tjson_array\tpermute\ttable asin\tdesc\tjson_arrayagg\tplacing\ttablesample assertion\tdescribe\tjson_exists\tportion\ttan asymmetric\tdescriptor\tjson_object\tposition\ttanh at\tdeterministic\tjson_objectagg\tposition_regex\ttemporary atan\tdiagnostics\tjson_query\tpower\tthen atomic\tdisconnect\tjson_table\tprecedes\ttime authorization\tdistinct\tjson_table_primitive\tprecision\ttimestamp avg\tdlnewcopy\tjson_value\tprepare\ttimezone_hour begin\tdlpreviouscopy\tkey\tpreserve\ttimezone_minute begin_frame\tdlurlcomplete\tlag\tprimary\tto begin_partition\tdlurlcompleteonly\tlanguage\tprior\ttrailing between\tdlurlcompletewrite\tlarge\tprivileges\ttransaction bigint\tdlurlpath\tlast\tprocedure\ttranslate binary\tdlurlpathonly\tlast_value\tptf\ttranslate_regex bit\tdlurlpathwrite\tlateral\tpublic\ttranslation bit_length\tdlurlscheme\tlead\trange\ttreat blob\tdlurlserver\tleading\trank\ttrigger boolean\tdlvalue\tleft\tread\ttrim both\tdo\tlevel\treads\ttrim_array by\tdomain\tlike\treal\ttrue call\tdouble\tlike_regex\trecursive\ttruncate called\tdrop\tlimit\tref\tuescape cardinality\tdynamic\tlistagg\treferences\tunion cascade\teach\tln\treferencing\tunique cascaded\telement\tlocal\tregr_avgx\tunknown case\telse\tlocaltime\tregr_avgy\tunmatched cast\tempty\tlocaltimestamp\tregr_count\tunnest catalog\tend\tlog\tregr_intercept\tupdate ceil\tend-exec\tlog10\tregr_r2\tupper ceiling\tend_frame\tlower\tregr_slope\tusage char\tend_partition\tmatch\tregr_sxx\tuser character\tequals\tmatches\tregr_sxy\tusing character_length\tescape\tmatch_number\tregr_syy\tvalue char_length\tevery\tmatch_recognize\trelative\tvalues check\texcept\tmax\trelease\tvalue_of classifier\texception\tmeasures\trestrict\tvarbinary clob\texec\tmember\tresult\tvarchar close\texecute\tmerge\treturn\tvariadic coalesce\texists\tmethod\treturning\tvarying collate\texp\tmin\treturns\tvar_pop collation\texternal\tminute\trevoke\tvar_samp collect\textract\tmod\tright\tverbose column\tfalse\tmodifies\trollback\tversioning commit\tfetch\tmodule\trollup\tview concurrently\tfilter\tmonth\trow\twhen condition\tfirst\tmultiset\trows\twhenever connect\tfirst_value\tnames\trow_number\twhere connection\tfloat\tnational\trunning\twidth_bucket constraint\tfloor\tnatural\tsavepoint\twindow constraints\tfor\tnchar\tschema\twith contains\tforeign\tnclob\tscope\twithin continue\tfound\tnew\tscroll\twithout convert\tframe_row\tnext\tsearch\twork copy\tfree\tno\tsecond\twrite corr\tfreeze\tnone\tsection\txml corresponding\tfrom\tnormalize\tseek\txmlagg cos\tfull\tnot\tselect\txmlattributes cosh\tfunction\tnotnull\tsensitive\txmlbinary count\tfusion\tnth_value\tsession\txmlcast covar_pop\tget\tntile\tsession_user\txmlcomment covar_samp\tglobal\tnull\tset\txmlconcat create\tgo\tnullif\tshow\txmldocument cross\tgoto\tnumeric\tsimilar\txmlelement cube\tgrant\toccurrences_regex\tsin\txmlexists cume_dist\tgroup\toctet_length\tsinh\txmlforest current\tgrouping\tof\tsize\txmliterate current_catalog\tgroups\toffset\tskip\txmlnamespaces current_date\thaving\told\tsmallint\txmlparse current_default_transform_group\thold\tomit\tsome\txmlpi current_path\thour\ton\tspace\txmlquery current_role\tidentity\tone\tspecific\txmlserialize current_row\tilike\tonly\tspecifictype\txmltable current_schema\timmediate\topen\tsql\txmltext current_time\timport\toption\tsqlcode\txmlvalidate current_timestamp\tin\tor\tsqlerror\tyear  ","version":"Next","tagName":"h2"},{"title":"Changelog​","type":1,"pageTitle":"Amazon RDS for PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/amazon-rds-postgres/#changelog","content":" The changelog includes a list of breaking changes made to this connector. Backwards-compatible changes are not listed.  Proceed with caution when editing materializations created with previous versions of this connector; editing always upgrades your materialization to the latest connector version.  V4: 2022-11-30​  This version includes breaking changes to materialized table columns. These provide more consistent column names and types, but tables created from previous versions of the connector may not be compatible with this version.  Capitalization is now preserved when fields in Flow are converted to Postgres column names. Previously, fields containing uppercase letters were converted to lowercase. Field names and values of types date, duration, ipv4, ipv6, macaddr, macaddr8, and time are now converted into their corresponding Postgres types. Previously, only date-time was converted, and all others were materialized as strings. ","version":"Next","tagName":"h2"},{"title":"Google Cloud SQL for PostgreSQL","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/PostgreSQL/google-cloud-sql-postgres/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Google Cloud SQL for PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/google-cloud-sql-postgres/#prerequisites","content":" To use this connector, you'll need:  A Postgres database to which to materialize, and user credentials. The connector will create new tables in the database per your specification. Tables created manually in advance are not supported.At least one Flow collection  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Google Cloud SQL for PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/google-cloud-sql-postgres/#setup","content":" You must configure your database to allow connections from Estuary. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel.  ","version":"Next","tagName":"h2"},{"title":"Connecting Directly to Google Cloud SQL​","type":1,"pageTitle":"Google Cloud SQL for PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/google-cloud-sql-postgres/#connecting-directly-to-google-cloud-sql","content":" Enable public IP on your database and add the Estuary Flow IP addresses as authorized IP addresses.  ","version":"Next","tagName":"h3"},{"title":"Connect With SSH Tunneling​","type":1,"pageTitle":"Google Cloud SQL for PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/google-cloud-sql-postgres/#connect-with-ssh-tunneling","content":" To allow SSH tunneling to a database instance hosted on Google Cloud, you must set up a virtual machine (VM).  Begin by finding your public SSH key on your local machine. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key If your Google login differs from your local username, generate a key that includes your Google email address as a comment: ssh-keygen -m PEM -t rsa -C user@domain.com Create and start a new VM in GCP, choosing an image that supports OS Login. Add your public key to the VM. Reserve an external IP address and connect it to the VM during setup. Note the generated address.  Configuration Tip To configure the connector, you must specify the database address in the format host:port. (You can also supply host only; the connector will use the port 5432 by default, which is correct in many cases.) You can find the host and port in the following location: Host as Private IP Address; port is always 5432. You may need to configure private IP on your database.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Google Cloud SQL for PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/google-cloud-sql-postgres/#configuration","content":" To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Postgres materialization, which will direct one or more of your Flow collections to your desired tables, or views, in the database.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Google Cloud SQL for PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/google-cloud-sql-postgres/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/database\tDatabase\tName of the logical database to materialize to.\tstring /address\tAddress\tHost and port of the database. If only the host is specified, port will default to 5432.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /schema\tDatabase Schema\tDatabase schema to use for materialized tables (unless overridden within the binding resource configuration) as well as associated materialization metadata tables\tstring\t&quot;public&quot; /user\tUser\tDatabase user to connect as.\tstring\tRequired /advanced\tAdvanced Options\tOptions for advanced users. You should not typically need to modify these.\tobject /advanced/sslmode\tSSL Mode\tOverrides SSL connection behavior by setting the 'sslmode' parameter.\tstring\t  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/additional_table_create_sql\tAdditional Table Create SQL\tAdditional SQL statement(s) to be run in the same transaction that creates the table.\tstring /delta_updates\tDelta Update\tShould updates to this table be done via delta updates.\tboolean\tfalse /schema\tAlternative Schema\tAlternative schema for this table (optional). Overrides schema set in endpoint configuration.\tstring /table\tTable\tTable name to materialize to. It will be created by the connector, unless the connector has previously created it.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Google Cloud SQL for PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/google-cloud-sql-postgres/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: ghcr.io/estuary/materialize-postgres:dev config: database: flow address: localhost:5432 password: flow user: flow bindings: - resource: table: ${TABLE_NAME} source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Delta updates​","type":1,"pageTitle":"Google Cloud SQL for PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/google-cloud-sql-postgres/#delta-updates","content":" This connector supports both standard (merge) and delta updates. The default is to use standard updates.  ","version":"Next","tagName":"h2"},{"title":"Reserved words​","type":1,"pageTitle":"Google Cloud SQL for PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/google-cloud-sql-postgres/#reserved-words","content":" PostgreSQL has a list of reserved words that must be quoted in order to be used as an identifier. Flow considers all the reserved words that are marked as &quot;reserved&quot; in any of the columns in the official PostgreSQL documentation.  These reserve words are listed in the table below. Flow automatically quotes fields that are in this list.  Reserved words abs\tcurrent_transform_group_for_type\tindicator\torder\tsqlexception absolute\tcurrent_user\tinitial\tout\tsqlstate acos\tcursor\tinitially\touter\tsqlwarning action\tcycle\tinner\toutput\tsqrt add\tdatalink\tinout\tover\tstart all\tdate\tinput\toverlaps\tstatic allocate\tday\tinsensitive\toverlay\tstddev_pop alter\tdeallocate\tinsert\tpad\tstddev_samp analyse\tdec\tint\tparameter\tsubmultiset analyze\tdecfloat\tinteger\tpartial\tsubset and\tdecimal\tintersect\tpartition\tsubstring any\tdeclare\tintersection\tpattern\tsubstring_regex are\tdefault\tinterval\tper\tsucceeds array\tdeferrable\tinto\tpercent\tsum array_agg\tdeferred\tis\tpercentile_cont\tsymmetric array_max_cardinality\tdefine\tisnull\tpercentile_disc\tsystem as\tdelete\tisolation\tpercent_rank\tsystem_time asc\tdense_rank\tjoin\tperiod\tsystem_user asensitive\tderef\tjson_array\tpermute\ttable asin\tdesc\tjson_arrayagg\tplacing\ttablesample assertion\tdescribe\tjson_exists\tportion\ttan asymmetric\tdescriptor\tjson_object\tposition\ttanh at\tdeterministic\tjson_objectagg\tposition_regex\ttemporary atan\tdiagnostics\tjson_query\tpower\tthen atomic\tdisconnect\tjson_table\tprecedes\ttime authorization\tdistinct\tjson_table_primitive\tprecision\ttimestamp avg\tdlnewcopy\tjson_value\tprepare\ttimezone_hour begin\tdlpreviouscopy\tkey\tpreserve\ttimezone_minute begin_frame\tdlurlcomplete\tlag\tprimary\tto begin_partition\tdlurlcompleteonly\tlanguage\tprior\ttrailing between\tdlurlcompletewrite\tlarge\tprivileges\ttransaction bigint\tdlurlpath\tlast\tprocedure\ttranslate binary\tdlurlpathonly\tlast_value\tptf\ttranslate_regex bit\tdlurlpathwrite\tlateral\tpublic\ttranslation bit_length\tdlurlscheme\tlead\trange\ttreat blob\tdlurlserver\tleading\trank\ttrigger boolean\tdlvalue\tleft\tread\ttrim both\tdo\tlevel\treads\ttrim_array by\tdomain\tlike\treal\ttrue call\tdouble\tlike_regex\trecursive\ttruncate called\tdrop\tlimit\tref\tuescape cardinality\tdynamic\tlistagg\treferences\tunion cascade\teach\tln\treferencing\tunique cascaded\telement\tlocal\tregr_avgx\tunknown case\telse\tlocaltime\tregr_avgy\tunmatched cast\tempty\tlocaltimestamp\tregr_count\tunnest catalog\tend\tlog\tregr_intercept\tupdate ceil\tend-exec\tlog10\tregr_r2\tupper ceiling\tend_frame\tlower\tregr_slope\tusage char\tend_partition\tmatch\tregr_sxx\tuser character\tequals\tmatches\tregr_sxy\tusing character_length\tescape\tmatch_number\tregr_syy\tvalue char_length\tevery\tmatch_recognize\trelative\tvalues check\texcept\tmax\trelease\tvalue_of classifier\texception\tmeasures\trestrict\tvarbinary clob\texec\tmember\tresult\tvarchar close\texecute\tmerge\treturn\tvariadic coalesce\texists\tmethod\treturning\tvarying collate\texp\tmin\treturns\tvar_pop collation\texternal\tminute\trevoke\tvar_samp collect\textract\tmod\tright\tverbose column\tfalse\tmodifies\trollback\tversioning commit\tfetch\tmodule\trollup\tview concurrently\tfilter\tmonth\trow\twhen condition\tfirst\tmultiset\trows\twhenever connect\tfirst_value\tnames\trow_number\twhere connection\tfloat\tnational\trunning\twidth_bucket constraint\tfloor\tnatural\tsavepoint\twindow constraints\tfor\tnchar\tschema\twith contains\tforeign\tnclob\tscope\twithin continue\tfound\tnew\tscroll\twithout convert\tframe_row\tnext\tsearch\twork copy\tfree\tno\tsecond\twrite corr\tfreeze\tnone\tsection\txml corresponding\tfrom\tnormalize\tseek\txmlagg cos\tfull\tnot\tselect\txmlattributes cosh\tfunction\tnotnull\tsensitive\txmlbinary count\tfusion\tnth_value\tsession\txmlcast covar_pop\tget\tntile\tsession_user\txmlcomment covar_samp\tglobal\tnull\tset\txmlconcat create\tgo\tnullif\tshow\txmldocument cross\tgoto\tnumeric\tsimilar\txmlelement cube\tgrant\toccurrences_regex\tsin\txmlexists cume_dist\tgroup\toctet_length\tsinh\txmlforest current\tgrouping\tof\tsize\txmliterate current_catalog\tgroups\toffset\tskip\txmlnamespaces current_date\thaving\told\tsmallint\txmlparse current_default_transform_group\thold\tomit\tsome\txmlpi current_path\thour\ton\tspace\txmlquery current_role\tidentity\tone\tspecific\txmlserialize current_row\tilike\tonly\tspecifictype\txmltable current_schema\timmediate\topen\tsql\txmltext current_time\timport\toption\tsqlcode\txmlvalidate current_timestamp\tin\tor\tsqlerror\tyear  ","version":"Next","tagName":"h2"},{"title":"Changelog​","type":1,"pageTitle":"Google Cloud SQL for PostgreSQL","url":"/reference/Connectors/materialization-connectors/PostgreSQL/google-cloud-sql-postgres/#changelog","content":" The changelog includes a list of breaking changes made to this connector. Backwards-compatible changes are not listed.  Proceed with caution when editing materializations created with previous versions of this connector; editing always upgrades your materialization to the latest connector version.  V4: 2022-11-30​  This version includes breaking changes to materialized table columns. These provide more consistent column names and types, but tables created from previous versions of the connector may not be compatible with this version.  Capitalization is now preserved when fields in Flow are converted to Postgres column names. Previously, fields containing uppercase letters were converted to lowercase. Field names and values of types date, duration, ipv4, ipv6, macaddr, macaddr8, and time are now converted into their corresponding Postgres types. Previously, only date-time was converted, and all others were materialized as strings. ","version":"Next","tagName":"h2"},{"title":"Rockset","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/Rockset/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Rockset","url":"/reference/Connectors/materialization-connectors/Rockset/#prerequisites","content":" To use this connector, you'll need:  A Rockset API key generated The API key must have the Member or Admin role. A Rockset workspace Optional; if none exist, one will be created by the connector. A Rockset collection Optional; if none exist, one will be created by the connector. At least one Flow collection  tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Rockset","url":"/reference/Connectors/materialization-connectors/Rockset/#configuration","content":" To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Rockset materialization, which will direct one or more of your Flow collections to your desired Rockset collections.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Rockset","url":"/reference/Connectors/materialization-connectors/Rockset/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/api_key\tRockset API Key\tThe key used to authenticate to the Rockset API. Must have role of admin or member.\tstring\tRequired /region_base_url\tRegion Base URL\tThe base URL to connect to your Rockset deployment. Example: api.usw2a1.rockset.com (do not include the protocol). See supported options and how to find yours.\tstring\tRequired  Bindings​  The binding configuration includes the optional Advanced collection settings section. These settings can help optimize your output Rockset collections:  Clustering fields: You can specify clustering fields for your Rockset collection's columnar index to help optimize specific query patterns. See the Rockset docs for more information.Retention period: Amount of time before data is purged, in seconds. A low value will keep the amount of data indexed in Rockset smaller.  Property\tTitle\tDescription\tType\tRequired/Default/advancedCollectionSettings\tAdvanced Collection Settings object /advancedCollectionSettings/clustering_key\tClustering Key\tList of clustering fields\tarray /advancedCollectionSettings/clustering_key/-/field_name\tField Name\tThe name of a field\tstring /advancedCollectionSettings/retention_secs\tRetention Period\tNumber of seconds after which data is purged based on event time\tinteger /collection\tRockset Collection\tThe name of the Rockset collection (will be created if it does not exist)\tstring\tRequired /workspace\tWorkspace\tThe name of the Rockset workspace (will be created if it does not exist)\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Rockset","url":"/reference/Connectors/materialization-connectors/Rockset/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: region_base_url: api.usw2a1.rockset.com api_key: supersecret # Path to the latest version of the connector, provided as a Docker image image: ghcr.io/estuary/materialize-rockset:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: workspace: ${namespace_name} collection: ${table_name} source: ${PREFIX}/${source_collection}   ","version":"Next","tagName":"h3"},{"title":"Delta updates and reduction strategies​","type":1,"pageTitle":"Rockset","url":"/reference/Connectors/materialization-connectors/Rockset/#delta-updates-and-reduction-strategies","content":" The Rockset connector operates only in delta updates mode. This means that Rockset, rather than Flow, performs the document merge. In some cases, this will affect how materialized views look in Rockset compared to other systems that use standard updates.  Rockset merges documents by the key defined in the Flow collection schema, and always uses the semantics of RFC 7396 - JSON merge. This differs from how Flow would reduce documents, most notably in that Rockset will not honor any reduction strategies defined in your Flow schema. For consistent output of a given collection across Rockset and other materialization endpoints, it's important that that collection's reduction annotations in Flow mirror Rockset's semantics.  To accomplish this, ensure that your collection schema has the following data reductions defined in its schema:  A top-level reduction strategy of mergeA strategy of lastWriteWins for all nested values (this is the default)  ","version":"Next","tagName":"h2"},{"title":"Changelog​","type":1,"pageTitle":"Rockset","url":"/reference/Connectors/materialization-connectors/Rockset/#changelog","content":" The changelog includes a list of breaking changes made to this connector. Backwards-compatible changes are not listed.  Proceed with caution when editing materializations created with previous versions of this connector; editing always upgrades your materialization to the latest connector version.  V2: 2022-12-06​  Region Base URL was added and is now required as part of the endpoint configuration.Event Time fields and the Insert Only option were removed from the advanced collection settings. ","version":"Next","tagName":"h2"},{"title":"Slack","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/slack/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Slack","url":"/reference/Connectors/materialization-connectors/slack/#prerequisites","content":" To use this connector, ensure you have the following:  An active Slack workspace with appropriate permissions.Slack credentials and access token for authentication.At least one Flow collection.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Slack","url":"/reference/Connectors/materialization-connectors/slack/#configuration","content":" The Slack connector is available for use in the Flow web application. To learn more about connectors and how to set them up, read our guide on using connectors.  Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/access_token\tAccess Token\tThe Slack API access token for authentication.\tstring\tRequired /client_id\tClient ID\tClient ID for authentication.\tstring\tRequired /client_secret\tClient Secret\tThe Slack API client secret.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/source\tSource\tSource data in Flow to be sent to Slack.\tstring\tRequired /channel\tChannel\tThe ID of the Slack channel to send messages to.\tstring\tRequired /display_name\tDisplay Name\tThe display name for the sender in Slack.\tstring /logo_emoji\tLogo Emoji\tThe emoji to be used.\tstring\t  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Slack","url":"/reference/Connectors/materialization-connectors/slack/#sample","content":" materializations: ${PREFIX}/${MATERIALIZATION_NAME}: endpoint: connector: image: ghcr.io/estuary/materialize-slack:dev config: credentials: auth_type: OAuth access_token: {secret} client_id: {your_client_id} client_secret: {secret} bindings: - source: ${PREFIX}/source_name resource: channel: &quot;id: C05A95LJHSL&quot; sender_config: display_name: Task Monitor logo_emoji: &quot;:eyes:&quot;  ","version":"Next","tagName":"h3"},{"title":"Microsoft SQLServer","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/SQLServer/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Microsoft SQLServer","url":"/reference/Connectors/materialization-connectors/SQLServer/#prerequisites","content":" To use this connector, you'll need:  A SQLServer database to which to materialize, and user credentials. SQLServer 2017 and later are supportedThe connector will create new tables in the database per your specification, so user credentials must have access to create new tables. At least one Flow collection  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Microsoft SQLServer","url":"/reference/Connectors/materialization-connectors/SQLServer/#setup","content":" To meet these requirements, follow the steps for your hosting type.  Self-hosted SQL ServerAzure SQL DatabaseAmazon RDS for SQL ServerGoogle Cloud SQL for SQL Server  ","version":"Next","tagName":"h2"},{"title":"Self-hosted SQL Server​","type":1,"pageTitle":"Microsoft SQLServer","url":"/reference/Connectors/materialization-connectors/SQLServer/#self-hosted-sql-server","content":" Connect to the server and issue the following commands:  USE &lt;database&gt;; -- Create user and password for use with the connector. CREATE LOGIN flow_materialize WITH PASSWORD = 'secret'; CREATE USER flow_materialize FOR LOGIN flow_materialize; -- Grant control on the database to flow_materialize GRANT CONTROL ON DATABASE::&lt;database&gt; TO flow_materialize;   Allow secure connection to Estuary Flow from your hosting environment. Either: Set up an SSH server for tunneling. When you fill out the endpoint configuration, include the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. Allowlist the Estuary IP addresses in your firewall rules.  ","version":"Next","tagName":"h3"},{"title":"Azure SQL Database​","type":1,"pageTitle":"Microsoft SQLServer","url":"/reference/Connectors/materialization-connectors/SQLServer/#azure-sql-database","content":" Allow connections between the database and Estuary Flow. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel. To allow direct access: Create a new firewall rule that grants access to the Estuary Flow IP addresses. To allow secure connections via SSH tunneling: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networks for additional details and a sample. In your SQL client, connect to your instance as the default sqlserver user and issue the following commands.  USE &lt;database&gt;; -- Create user and password for use with the connector. CREATE LOGIN flow_materialize WITH PASSWORD = 'secret'; CREATE USER flow_materialize FOR LOGIN flow_materialize; -- Grant control on the database to flow_materialize GRANT CONTROL ON DATABASE::&lt;database&gt; TO flow_materialize;   Note the following important items for configuration: Find the instance's host under Server Name. The port is always 1433. Together, you'll use the host:port as the address property when you configure the connector.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Microsoft SQLServer","url":"/reference/Connectors/materialization-connectors/SQLServer/#configuration","content":" To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a SQLServer materialization, which will direct one or more of your Flow collections to your desired tables, or views, in the database.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Microsoft SQLServer","url":"/reference/Connectors/materialization-connectors/SQLServer/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/database\tDatabase\tName of the logical database to materialize to.\tstring\tRequired /address\tAddress\tHost and port of the database. If only the host is specified, port will default to 3306.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /user\tUser\tDatabase user to connect as.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tTable name to materialize to. It will be created by the connector, unless the connector has previously created it.\tstring\tRequired /delta_updates\tDelta Update\tShould updates to this table be done via delta updates.\tboolean\tfalse  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Microsoft SQLServer","url":"/reference/Connectors/materialization-connectors/SQLServer/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: ghcr.io/estuary/materialize-sqlserver:dev config: database: flow address: localhost:5432 password: flow user: flow bindings: - resource: table: ${TABLE_NAME} source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Delta updates​","type":1,"pageTitle":"Microsoft SQLServer","url":"/reference/Connectors/materialization-connectors/SQLServer/#delta-updates","content":" This connector supports both standard (merge) and delta updates. The default is to use standard updates.  ","version":"Next","tagName":"h2"},{"title":"Reserved words​","type":1,"pageTitle":"Microsoft SQLServer","url":"/reference/Connectors/materialization-connectors/SQLServer/#reserved-words","content":" SQLServer has a list of reserved words that must be quoted in order to be used as an identifier. Flow considers all the reserved words in the official SQLServer documentation.  These reserved words are listed in the table below. Flow automatically quotes fields that are in this list.  Reserved words absolute\tconnect\telse\tintersect\ton action\tconnection\tend\tintersection\tonly ada\tconstraint\tend-exec\tinterval\topen add\tconstraints\tequals\tinto\topendatasourc admin\tconstructor\terrlvl\tis\topenquery after\tcontains\tescape\tisolation\topenrowset aggregate\tcontainstable\tevery\titerate\topenxml alias\tcontinue\texcept\tjoin\toperation all\tconvert\texception\tkey\toption allocate\tcorr\texec\tkill\tor alter\tcorresponding\texecute\tlanguage\torder and\tcount\texists\tlarge\tordinality any\tcovar_pop\texit\tlast\tout are\tcovar_samp\texternal\tlateral\touter array\tcreate\textract\tleading\toutput as\tcross\tfalse\tleft\tover asc\tcube\tfetch\tless\toverlaps asensitive\tcume_dist\tfile\tlevel\toverlay assertion\tcurrent\tfillfactor\tlike\tpad asymmetric\tcurrent_catal\tfilter\tlike_regex\tparameter at\tcurrent_date\tfirst\tlimit\tparameters atomic\tcurrent_defau\tfloat\tlineno\tpartial authorization\tcurrent_path\tfor\tln\tpartition avg\tcurrent_role\tforeign\tload\tpascal backup\tcurrent_schem\tfortran\tlocal\tpath before\tcurrent_time\tfound\tlocaltime\tpercent begin\tcurrent_times\tfree\tlocaltimestam\tpercent_rank between\tcurrent_trans\tfreetext\tlocator\tpercentile_co binary\tcurrent_user\tfreetexttable\tlower\tpercentile_di bit\tcursor\tfrom\tmap\tpivot bit_length\tcycle\tfull\tmatch\tplan blob\tdata\tfulltexttable\tmax\tposition boolean\tdatabase\tfunction\tmember\tposition_rege both\tdate\tfusion\tmerge\tpostfix breadth\tday\tgeneral\tmethod\tprecision break\tdbcc\tget\tmin\tprefix browse\tdeallocate\tglobal\tminute\tpreorder bulk\tdec\tgo\tmod\tprepare by\tdecimal\tgoto\tmodifies\tpreserve call\tdeclare\tgrant\tmodify\tprimary called\tdefault\tgroup\tmodule\tprint cardinality\tdeferrable\tgrouping\tmonth\tprior cascade\tdeferred\thaving\tmultiset\tprivileges cascaded\tdelete\thold\tnames\tproc case\tdeny\tholdlock\tnational\tprocedure cast\tdepth\thost\tnatural\tpublic catalog\tderef\thour\tnchar\traiserror char\tdesc\tidentity\tnclob\trange char_length\tdescribe\tidentity_inse\tnew\tread character\tdescriptor\tidentitycol\tnext\treads character_len\tdestroy\tif\tno\treadtext check\tdestructor\tignore\tnocheck\treal checkpoint\tdeterministic\timmediate\tnonclustered\treconfigure class\tdiagnostics\tin\tnone\trecursive clob\tdictionary\tinclude\tnormalize\tref close\tdisconnect\tindex\tnot\treferences clustered\tdisk\tindicator\tnull\treferencing coalesce\tdistinct\tinitialize\tnullif\tregr_avgx collate\tdistributed\tinitially\tnumeric\tregr_avgy collation\tdomain\tinner\tobject\tregr_count collect\tdouble\tinout\toccurrences_r\tregr_intercep column\tdrop\tinput\toctet_length\tregr_r2 commit\tdump\tinsensitive\tof\tregr_slope completion\tdynamic\tinsert\toff\tregr_sxx compute\teach\tint\toffsets\tregr_sxy condition\telement\tinteger\told\tregr_syy relative\tsemanticsimil\tstructure\ttruncate\twindow release\tsemanticsimil\tsubmultiset\ttry_convert\twith replication\tsensitive\tsubstring\ttsequal\twithin restore\tsequence\tsubstring_reg\tuescape\twithin restrict\tsession\tsum\tunder\twithout result\tsession_user\tsymmetric\tunion\twork return\tset\tsystem\tunique\twrite returns\tsets\tsystem_user\tunknown\twritetext revert\tsetuser\ttable\tunnest\txmlagg revoke\tshutdown\ttablesample\tunpivot\txmlattributes right\tsimilar\ttemporary\tupdate\txmlbinary role\tsize\tterminate\tupdatetext\txmlcast rollback\tsmallint\ttextsize\tupper\txmlcomment rollup\tsome\tthan\tusage\txmlconcat routine\tspace\tthen\tuse\txmldocument row\tspecific\ttime\tuser\txmlelement rowcount\tspecifictype\ttimestamp\tusing\txmlexists rowguidcol\tsql\ttimezone_hour\tvalue\txmlforest rows\tsqlca\ttimezone_minu\tvalues\txmliterate rule\tsqlcode\tto\tvar_pop\txmlnamespaces save\tsqlerror\ttop\tvar_samp\txmlparse savepoint\tsqlexception\ttrailing\tvarchar\txmlpi schema\tsqlstate\ttran\tvariable\txmlquery scope\tsqlwarning\ttransaction\tvarying\txmlserialize scroll\tstart\ttranslate\tview\txmltable search\tstate\ttranslate_reg\twaitfor\txmltext second\tstatement\ttranslation\twhen\txmlvalidate section\tstatic\ttreat\twhenever\tyear securityaudit\tstatistics\ttrigger\twhere\tzone select\tstddev_pop\ttrim\twhile semantickeyph\tstddev_samp\ttrue\twidth_bucket\t  ","version":"Next","tagName":"h2"},{"title":"Changelog​","type":1,"pageTitle":"Microsoft SQLServer","url":"/reference/Connectors/materialization-connectors/SQLServer/#changelog","content":" The changelog includes a list of breaking changes made to this connector. Backwards-compatible changes are not listed.  Proceed with caution when editing materializations created with previous versions of this connector; editing always upgrades your materialization to the latest connector version.  V1: 2023-09-01​  First version ","version":"Next","tagName":"h2"},{"title":"Amazon RDS for SQL Server","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/SQLServer/amazon-rds-sqlserver/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon RDS for SQL Server","url":"/reference/Connectors/materialization-connectors/SQLServer/amazon-rds-sqlserver/#prerequisites","content":" To use this connector, you'll need:  A SQLServer database to which to materialize, and user credentials. SQLServer 2017 and later are supportedThe connector will create new tables in the database per your specification, so user credentials must have access to create new tables. At least one Flow collection  ","version":"Next","tagName":"h2"},{"title":"Setup Amazon RDS for SQL Server​","type":1,"pageTitle":"Amazon RDS for SQL Server","url":"/reference/Connectors/materialization-connectors/SQLServer/amazon-rds-sqlserver/#setup-amazon-rds-for-sql-server","content":" Allow connections between the database and Estuary Flow. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel. To allow direct access: Modify the database, setting Public accessibility to Yes.Edit the VPC security group associated with your database, or create a new VPC security group and associate it as described in the Amazon documentation.Create a new inbound rule and a new outbound rule that allow all traffic from the Estuary Flow IP addresses. To allow secure connections via SSH tunneling: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networks for additional details and a sample. In your SQL client, connect to your instance as the default sqlserver user and issue the following commands.  USE &lt;database&gt;; -- Create user and password for use with the connector. CREATE LOGIN flow_materialize WITH PASSWORD = 'secret'; CREATE USER flow_materialize FOR LOGIN flow_materialize; -- Grant control on the database to flow_materialize GRANT CONTROL ON DATABASE::&lt;database&gt; TO flow_materialize;   In the RDS console, note the instance's Endpoint and Port. You'll need these for the address property when you configure the connector.  ","version":"Next","tagName":"h2"},{"title":"Connecting to SQLServer​","type":1,"pageTitle":"Amazon RDS for SQL Server","url":"/reference/Connectors/materialization-connectors/SQLServer/amazon-rds-sqlserver/#connecting-to-sqlserver","content":" Allow connections between the database and Estuary Flow. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel. To allow direct access: Modify the database, setting Public accessibility to Yes.Edit the VPC security group associated with your database, or create a new VPC security group and associate it as described in the Amazon documentation. Create a new inbound rule and a new outbound rule that allow all traffic from the Estuary Flow IP addresses. To allow secure connections via SSH tunneling: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networks for additional details and a sample. In your SQL client, connect to your instance as the default sqlserver user and issue the following commands.  USE &lt;database&gt;; -- Create user and password for use with the connector. CREATE LOGIN flow_materialize WITH PASSWORD = 'Secret123!'; CREATE USER flow_materialize FOR LOGIN flow_materialize; -- Grant control on the database to flow_materialize GRANT CONTROL ON DATABASE::&lt;database&gt; TO flow_materialize;   In the RDS console, note the instance's Endpoint and Port. You'll need these for the address property when you configure the connector.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Amazon RDS for SQL Server","url":"/reference/Connectors/materialization-connectors/SQLServer/amazon-rds-sqlserver/#configuration","content":" To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a SQLServer materialization, which will direct one or more of your Flow collections to your desired tables, or views, in the database.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Amazon RDS for SQL Server","url":"/reference/Connectors/materialization-connectors/SQLServer/amazon-rds-sqlserver/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/database\tDatabase\tName of the logical database to materialize to.\tstring\tRequired /address\tAddress\tHost and port of the database. If only the host is specified, port will default to 3306.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /user\tUser\tDatabase user to connect as.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tTable name to materialize to. It will be created by the connector, unless the connector has previously created it.\tstring\tRequired /delta_updates\tDelta Update\tShould updates to this table be done via delta updates.\tboolean\tfalse  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Amazon RDS for SQL Server","url":"/reference/Connectors/materialization-connectors/SQLServer/amazon-rds-sqlserver/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: ghcr.io/estuary/materialize-sqlserver:dev config: database: flow address: localhost:5432 password: flow user: flow bindings: - resource: table: ${TABLE_NAME} source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Delta updates​","type":1,"pageTitle":"Amazon RDS for SQL Server","url":"/reference/Connectors/materialization-connectors/SQLServer/amazon-rds-sqlserver/#delta-updates","content":" This connector supports both standard (merge) and delta updates. The default is to use standard updates.  ","version":"Next","tagName":"h2"},{"title":"Reserved words​","type":1,"pageTitle":"Amazon RDS for SQL Server","url":"/reference/Connectors/materialization-connectors/SQLServer/amazon-rds-sqlserver/#reserved-words","content":" SQLServer has a list of reserved words that must be quoted in order to be used as an identifier. Flow considers all the reserved words in the official SQLServer documentation.  These reserved words are listed in the table below. Flow automatically quotes fields that are in this list.  Reserved words absolute\tconnect\telse\tintersect\ton action\tconnection\tend\tintersection\tonly ada\tconstraint\tend-exec\tinterval\topen add\tconstraints\tequals\tinto\topendatasourc admin\tconstructor\terrlvl\tis\topenquery after\tcontains\tescape\tisolation\topenrowset aggregate\tcontainstable\tevery\titerate\topenxml alias\tcontinue\texcept\tjoin\toperation all\tconvert\texception\tkey\toption allocate\tcorr\texec\tkill\tor alter\tcorresponding\texecute\tlanguage\torder and\tcount\texists\tlarge\tordinality any\tcovar_pop\texit\tlast\tout are\tcovar_samp\texternal\tlateral\touter array\tcreate\textract\tleading\toutput as\tcross\tfalse\tleft\tover asc\tcube\tfetch\tless\toverlaps asensitive\tcume_dist\tfile\tlevel\toverlay assertion\tcurrent\tfillfactor\tlike\tpad asymmetric\tcurrent_catal\tfilter\tlike_regex\tparameter at\tcurrent_date\tfirst\tlimit\tparameters atomic\tcurrent_defau\tfloat\tlineno\tpartial authorization\tcurrent_path\tfor\tln\tpartition avg\tcurrent_role\tforeign\tload\tpascal backup\tcurrent_schem\tfortran\tlocal\tpath before\tcurrent_time\tfound\tlocaltime\tpercent begin\tcurrent_times\tfree\tlocaltimestam\tpercent_rank between\tcurrent_trans\tfreetext\tlocator\tpercentile_co binary\tcurrent_user\tfreetexttable\tlower\tpercentile_di bit\tcursor\tfrom\tmap\tpivot bit_length\tcycle\tfull\tmatch\tplan blob\tdata\tfulltexttable\tmax\tposition boolean\tdatabase\tfunction\tmember\tposition_rege both\tdate\tfusion\tmerge\tpostfix breadth\tday\tgeneral\tmethod\tprecision break\tdbcc\tget\tmin\tprefix browse\tdeallocate\tglobal\tminute\tpreorder bulk\tdec\tgo\tmod\tprepare by\tdecimal\tgoto\tmodifies\tpreserve call\tdeclare\tgrant\tmodify\tprimary called\tdefault\tgroup\tmodule\tprint cardinality\tdeferrable\tgrouping\tmonth\tprior cascade\tdeferred\thaving\tmultiset\tprivileges cascaded\tdelete\thold\tnames\tproc case\tdeny\tholdlock\tnational\tprocedure cast\tdepth\thost\tnatural\tpublic catalog\tderef\thour\tnchar\traiserror char\tdesc\tidentity\tnclob\trange char_length\tdescribe\tidentity_inse\tnew\tread character\tdescriptor\tidentitycol\tnext\treads character_len\tdestroy\tif\tno\treadtext check\tdestructor\tignore\tnocheck\treal checkpoint\tdeterministic\timmediate\tnonclustered\treconfigure class\tdiagnostics\tin\tnone\trecursive clob\tdictionary\tinclude\tnormalize\tref close\tdisconnect\tindex\tnot\treferences clustered\tdisk\tindicator\tnull\treferencing coalesce\tdistinct\tinitialize\tnullif\tregr_avgx collate\tdistributed\tinitially\tnumeric\tregr_avgy collation\tdomain\tinner\tobject\tregr_count collect\tdouble\tinout\toccurrences_r\tregr_intercep column\tdrop\tinput\toctet_length\tregr_r2 commit\tdump\tinsensitive\tof\tregr_slope completion\tdynamic\tinsert\toff\tregr_sxx compute\teach\tint\toffsets\tregr_sxy condition\telement\tinteger\told\tregr_syy relative\tsemanticsimil\tstructure\ttruncate\twindow release\tsemanticsimil\tsubmultiset\ttry_convert\twith replication\tsensitive\tsubstring\ttsequal\twithin restore\tsequence\tsubstring_reg\tuescape\twithin restrict\tsession\tsum\tunder\twithout result\tsession_user\tsymmetric\tunion\twork return\tset\tsystem\tunique\twrite returns\tsets\tsystem_user\tunknown\twritetext revert\tsetuser\ttable\tunnest\txmlagg revoke\tshutdown\ttablesample\tunpivot\txmlattributes right\tsimilar\ttemporary\tupdate\txmlbinary role\tsize\tterminate\tupdatetext\txmlcast rollback\tsmallint\ttextsize\tupper\txmlcomment rollup\tsome\tthan\tusage\txmlconcat routine\tspace\tthen\tuse\txmldocument row\tspecific\ttime\tuser\txmlelement rowcount\tspecifictype\ttimestamp\tusing\txmlexists rowguidcol\tsql\ttimezone_hour\tvalue\txmlforest rows\tsqlca\ttimezone_minu\tvalues\txmliterate rule\tsqlcode\tto\tvar_pop\txmlnamespaces save\tsqlerror\ttop\tvar_samp\txmlparse savepoint\tsqlexception\ttrailing\tvarchar\txmlpi schema\tsqlstate\ttran\tvariable\txmlquery scope\tsqlwarning\ttransaction\tvarying\txmlserialize scroll\tstart\ttranslate\tview\txmltable search\tstate\ttranslate_reg\twaitfor\txmltext second\tstatement\ttranslation\twhen\txmlvalidate section\tstatic\ttreat\twhenever\tyear securityaudit\tstatistics\ttrigger\twhere\tzone select\tstddev_pop\ttrim\twhile semantickeyph\tstddev_samp\ttrue\twidth_bucket\t  ","version":"Next","tagName":"h2"},{"title":"Changelog​","type":1,"pageTitle":"Amazon RDS for SQL Server","url":"/reference/Connectors/materialization-connectors/SQLServer/amazon-rds-sqlserver/#changelog","content":" The changelog includes a list of breaking changes made to this connector. Backwards-compatible changes are not listed.  Proceed with caution when editing materializations created with previous versions of this connector; editing always upgrades your materialization to the latest connector version.  V1: 2023-09-01​  First version ","version":"Next","tagName":"h2"},{"title":"Snowflake","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/Snowflake/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Snowflake","url":"/reference/Connectors/materialization-connectors/Snowflake/#prerequisites","content":" To use this connector, you'll need:  A Snowflake account that includes: A target database, to which you'll materialize dataA schema — a logical grouping of database objects — within the target databaseA virtual warehouseA user with a role assigned that grants the appropriate access levels to these resources. See the script below for details. Know your Snowflake account's host URL. This is formatted using your Snowflake account identifier, for example, orgname-accountname.snowflakecomputing.com.At least one Flow collection  tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Snowflake","url":"/reference/Connectors/materialization-connectors/Snowflake/#setup","content":" To meet the prerequisites, copy and paste the following script into the Snowflake SQL editor, replacing the variable names in the first six lines.  If you'd like to use an existing database, warehouse, and/or schema, be sure to setdatabase_name, warehouse_name, and estuary_schema accordingly. If you specify a new name, the script will create the item for you. You can set estuary_role, estuary_user, and estuary_password to whatever you'd like.  Check the All Queries check box, and click Run.  set database_name = 'ESTUARY_DB'; set warehouse_name = 'ESTUARY_WH'; set estuary_role = 'ESTUARY_ROLE'; set estuary_user = 'ESTUARY_USER'; set estuary_password = 'secret'; set estuary_schema = 'ESTUARY_SCHEMA'; -- create role and schema for Estuary create role if not exists identifier($estuary_role); grant role identifier($estuary_role) to role SYSADMIN; -- Create snowflake DB create database if not exists identifier($database_name); use database identifier($database_name); create schema if not exists identifier($estuary_schema); -- create a user for Estuary create user if not exists identifier($estuary_user) password = $estuary_password default_role = $estuary_role default_warehouse = $warehouse_name; grant role identifier($estuary_role) to user identifier($estuary_user); grant all on schema identifier($estuary_schema) to identifier($estuary_role); -- create a warehouse for estuary create warehouse if not exists identifier($warehouse_name) warehouse_size = xsmall warehouse_type = standard auto_suspend = 60 auto_resume = true initially_suspended = true; -- grant Estuary role access to warehouse grant USAGE on warehouse identifier($warehouse_name) to role identifier($estuary_role); -- grant Estuary access to database grant CREATE SCHEMA, MONITOR, USAGE on database identifier($database_name) to role identifier($estuary_role); -- change role to ACCOUNTADMIN for STORAGE INTEGRATION support to Estuary (only needed for Snowflake on GCP) use role ACCOUNTADMIN; grant CREATE INTEGRATION on account to role identifier($estuary_role); use role sysadmin; COMMIT;   ","version":"Next","tagName":"h3"},{"title":"Key-pair Authentication & Snowpipe​","type":1,"pageTitle":"Snowflake","url":"/reference/Connectors/materialization-connectors/Snowflake/#key-pair-authentication--snowpipe","content":" In order to enable use of Snowpipe for delta updates bindings, you need to authenticate using key-pair authentication, also known as JWT authentication.  To set up your user for key-pair authentication, first generate a key-pair in your shell:  # generate a private key openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8 -nocrypt # generate a public key openssl rsa -in rsa_key.p8 -pubout -out rsa_key.pub # read the public key and copy it to clipboard cat rsa_key.pub -----BEGIN PUBLIC KEY----- MIIBIj... -----END PUBLIC KEY-----   Then assign the public key with your Snowflake user using these SQL commands:  ALTER USER $estuary_user SET RSA_PUBLIC_KEY='MIIBIjANBgkqh...'   Verify the public key fingerprint in Snowflake matches the one you have locally:  DESC USER $estuary_user; SELECT TRIM((SELECT &quot;value&quot; FROM TABLE(RESULT_SCAN(LAST_QUERY_ID())) WHERE &quot;property&quot; = 'RSA_PUBLIC_KEY_FP'), 'SHA256:');   Then compare with the local version:  openssl rsa -pubin -in rsa_key.pub -outform DER | openssl dgst -sha256 -binary | openssl enc -base64   Now you can use the generated private key when configuring your Snowflake connector. Once you have key-pair authentication enabled, delta updates bindings will use Snowpipe for loading data.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Snowflake","url":"/reference/Connectors/materialization-connectors/Snowflake/#configuration","content":" To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Snowflake materialization, which will direct one or more of your Flow collections to new Snowflake tables.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Snowflake","url":"/reference/Connectors/materialization-connectors/Snowflake/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/host\tHost (Account URL)\tThe Snowflake Host used for the connection. Example: orgname-accountname.snowflakecomputing.com (do not include the protocol).\tstring\tRequired /database\tDatabase\tName of the Snowflake database to which to materialize\tstring\tRequired /schema\tSchema\tDatabase schema for bound collection tables (unless overridden within the binding resource configuration) as well as associated materialization metadata tables\tstring\tRequired /warehouse\tWarehouse\tName of the data warehouse that contains the database\tstring /role\tRole\tRole assigned to the user\tstring /account\tAccount\tThe Snowflake account identifier\tstring /credentials\tCredentials\tCredentials for authentication\tobject\tRequired /credentials/auth_type\tAuthentication type\tOne of user_password or jwt\tstring\tRequired /credentials/user\tUser\tSnowflake username\tstring\tRequired /credentials/password\tPassword\tRequired if using user_password authentication\tstring\tRequired /credentials/privateKey\tPrivate Key\tRequired if using jwt authentication\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tTable name\tstring\tRequired /schema\tAlternative Schema\tAlternative schema for this table\tstring /delta_updates\tDelta updates\tWhether to use standard or delta updates\tboolean\t  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Snowflake","url":"/reference/Connectors/materialization-connectors/Snowflake/#sample","content":" User and password authentication:  materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: database: acmeCo_db host: orgname-accountname.snowflakecomputing.com schema: acmeCo_flow_schema warehouse: acmeCo_warehouse credentials: auth_type: user_pasword user: snowflake_user password: secret image: ghcr.io/estuary/materialize-snowflake:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: table: ${table_name} source: ${PREFIX}/${source_collection}   Key-pair authentication:  materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: database: acmeCo_db host: orgname-accountname.snowflakecomputing.com schema: acmeCo_flow_schema warehouse: acmeCo_warehouse credentials: auth_type: jwt user: snowflake_user privateKey: | -----BEGIN PRIVATE KEY----- MIIEv.... ... ... ... ... ... -----END PRIVATE KEY----- image: ghcr.io/estuary/materialize-snowflake:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: table: ${table_name} source: ${PREFIX}/${source_collection}   ","version":"Next","tagName":"h3"},{"title":"Sync Schedule​","type":1,"pageTitle":"Snowflake","url":"/reference/Connectors/materialization-connectors/Snowflake/#sync-schedule","content":" This connector supports configuring a schedule for sync frequency. You can read about how to configure this here.  Snowflake compute is priced per second of activity, with a minimum of 60 seconds. Inactive warehouses don't incur charges. To keep costs down, you'll want to minimize your warehouse's active time.  To accomplish this, we recommend a two-pronged approach:  Configure your Snowflake warehouse to auto-suspend after 60 seconds. This ensures that after each transaction completes, you'll only be charged for one minute of compute, Snowflake's smallest granularity. Use a query like the one shown below, being sure to substitute your warehouse name: ALTER WAREHOUSE ESTUARY_WH SET auto_suspend = 60; Configure the materialization's Sync Schedule based on your requirements for data freshness.  ","version":"Next","tagName":"h2"},{"title":"Delta updates​","type":1,"pageTitle":"Snowflake","url":"/reference/Connectors/materialization-connectors/Snowflake/#delta-updates","content":" This connector supports both standard (merge) and delta updates. The default is to use standard updates.  Enabling delta updates will prevent Flow from querying for documents in your Snowflake table, which can reduce latency and costs for large datasets. If you're certain that all events will have unique keys, enabling delta updates is a simple way to improve performance with no effect on the output. However, enabling delta updates is not suitable for all workflows, as the resulting table in Snowflake won't be fully reduced.  You can enable delta updates on a per-binding basis:   bindings: - resource: table: ${table_name} delta_updates: true source: ${PREFIX}/${source_collection}   ","version":"Next","tagName":"h2"},{"title":"Performance considerations​","type":1,"pageTitle":"Snowflake","url":"/reference/Connectors/materialization-connectors/Snowflake/#performance-considerations","content":" ","version":"Next","tagName":"h2"},{"title":"Optimizing performance for standard updates​","type":1,"pageTitle":"Snowflake","url":"/reference/Connectors/materialization-connectors/Snowflake/#optimizing-performance-for-standard-updates","content":" When using standard updates for a large dataset, the collection key you choose can have a significant impact on materialization performance and efficiency.  Snowflake uses micro partitions to physically arrange data within tables. Each micro partition includes metadata, such as the minimum and maximum values for each column. If you choose a collection key that takes advantage of this metadata to help Snowflake prune irrelevant micro partitions, you'll see dramatically better performance.  For example, if you materialize a collection with a key of /user_id, it will tend to perform far worse than a materialization of /date, /user_id. This is because most materializations tend to be roughly chronological over time, and that means that data is written to Snowflake in roughly /date order.  This means that updates of keys /date, /user_id will need to physically read far fewer rows as compared to a key like /user_id, because those rows will tend to live in the same micro-partitions, and Snowflake is able to cheaply prune micro-partitions that aren't relevant to the transaction.  ","version":"Next","tagName":"h3"},{"title":"Snowpipe​","type":1,"pageTitle":"Snowflake","url":"/reference/Connectors/materialization-connectors/Snowflake/#snowpipe","content":" Snowpipe allows for loading data into target tables without waking up the warehouse, which can be cheaper and more performant. Snowpipe can be used for delta updates bindings, and it requires configuring your authentication using a private key. Instructions for configuring key-pair authentication can be found in this page: Key-pair Authentication &amp; Snowpipe  ","version":"Next","tagName":"h3"},{"title":"Timestamp Data Type Mapping​","type":1,"pageTitle":"Snowflake","url":"/reference/Connectors/materialization-connectors/Snowflake/#timestamp-data-type-mapping","content":" Flow materializes timestamp data types as either TIMESTAMP_LTZ orTIMESTAMP_TZ columns in Snowflake. TIMESTAMP_LTZ is used unless the Snowflake TIMESTAMP_TYPE_MAPPING configuration is set to TIMESTAMP_TZ, which will cause Flow to use TIMESTAMP_TZ columns. Flow never creates columns as TIMESTAMP_NTZ. See Snowflake documentation on TIMESTAMP_TYPE_MAPPING for more information.  ","version":"Next","tagName":"h2"},{"title":"Reserved words​","type":1,"pageTitle":"Snowflake","url":"/reference/Connectors/materialization-connectors/Snowflake/#reserved-words","content":" Snowflake has a list of reserved words that must be quoted in order to be used as an identifier. Flow automatically quotes fields that are in the reserved words list. You can find this list in Snowflake's documentation here and in the table below.  caution In Snowflake, objects created with quoted identifiers must always be referenced exactly as created, including the quotes. Otherwise, SQL statements and queries can result in errors. See the Snowflake docs.  Reserved words account\tfrom\tqualify all\tfull\tregexp alter\tgrant\trevoke and\tgroup\tright any\tgscluster\trlike as\thaving\trow between\tilike\trows by\tin\tsample case\tincrement\tschema cast\tinner\tselect check\tinsert\tset column\tintersect\tsome connect\tinto\tstart connection\tis\ttable constraint\tissue\ttablesample create\tjoin\tthen cross\tlateral\tto current\tleft\ttrigger current_date\tlike\ttrue current_time\tlocaltime\ttry_cast current_timestamp\tlocaltimestamp\tunion current_user\tminus\tunique database\tnatural\tupdate delete\tnot\tusing distinct\tnull\tvalues drop\tof\tview else\ton\twhen exists\tor\twhenever false\torder\twhere following\torganization\twith for  ","version":"Next","tagName":"h2"},{"title":"SQLite","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/SQLite/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"SQLite","url":"/reference/Connectors/materialization-connectors/SQLite/#prerequisites","content":" To use this connector, you'll need:  At least one Flow collection  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"SQLite","url":"/reference/Connectors/materialization-connectors/SQLite/#configuration","content":" This materialization requires no configuration, and all you need to do is choose a collection to materialize.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"SQLite","url":"/reference/Connectors/materialization-connectors/SQLite/#properties","content":" Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tTable name to materialize to. It will be created by the connector, unless the connector has previously created it.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"SQLite","url":"/reference/Connectors/materialization-connectors/SQLite/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: ghcr.io/estuary/materialize-sqlite:dev config: {} bindings: - resource: table: ${TABLE_NAME} source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Delta updates​","type":1,"pageTitle":"SQLite","url":"/reference/Connectors/materialization-connectors/SQLite/#delta-updates","content":" This connector does not support delta updates at the moment.  ","version":"Next","tagName":"h2"},{"title":"Reserved words​","type":1,"pageTitle":"SQLite","url":"/reference/Connectors/materialization-connectors/SQLite/#reserved-words","content":" SQLite has a list of reserved words that must be quoted in order to be used as an identifier. Flow considers all the reserved words that are marked as &quot;reserved&quot; in any of the columns in the official SQlite documentation.  These reserve words are listed in the table below. Flow automatically quotes fields that are in this list.  Reserved words\tabort\tintersect action\tinto add\tis after\tisnull all\tjoin alter\tkey always\tlast analyze\tleft and\tlike as\tlimit asc\tmatch attach\tmaterialized autoincrement\tnatural before\tno begin\tnot between\tnothing by\tnotnull cascade\tnull case\tnulls cast\tof check\toffset collate\ton column\tor commit\torder conflict\tothers constraint\touter create\tover cross\tpartition current\tplan current_date\tpragma current_time\tpreceding current_timestamp\tprimary database\tquery default\traise deferrable\trange deferred\trecursive delete\treferences desc\tregexp detach\treindex distinct\trelease do\trename drop\treplace each\trestrict else\treturning end\tright escape\trollback except\trow exclude\trows exclusive\tsavepoint exists\tselect explain\tset fail\ttable filter\ttemp first\ttemporary following\tthen for\tties foreign\tto from\ttransaction full\ttrigger generated\tunbounded glob\tunion group\tunique groups\tupdate having\tusing if\tvacuum ignore\tvalues immediate\tview in\tvirtual index\twhen indexed\twhere initially\twindow inner\twith insert\twithout instead\t ","version":"Next","tagName":"h2"},{"title":"Starburst","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/starburst/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Starburst","url":"/reference/Connectors/materialization-connectors/starburst/#prerequisites","content":" To use this connector, you'll need:  A Starburst Galaxy account (To create one see: Staburst Galaxy start) that includes: A running cluster containing an Amazon S3 catalogA schema which is a logical grouping of tablesStorage on S3 for temporary data with awsAccessKeyId and awsSecretAccessKey which should correspond to the chosen catalogA user with a role assigned that grants access to create, modify, and drop tables in the specified Amazon S3 catalog At least one Flow collection  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Starburst","url":"/reference/Connectors/materialization-connectors/starburst/#setup","content":" To get the host, go to your Cluster -&gt; Connection info -&gt; Other clients, as specified in Starburst's Connect clients docs.  You will also need to grant access to temporary storage (Roles and privileges -&gt; Select specific role -&gt; Privileges -&gt; Add privilege -&gt; Location). &quot;Create schema and table in location&quot; should be selected. See the Starburst docs for more.  ","version":"Next","tagName":"h3"},{"title":"Configuration​","type":1,"pageTitle":"Starburst","url":"/reference/Connectors/materialization-connectors/starburst/#configuration","content":" To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Starburst materialization, which will direct one or more of your Flow collections to new Starburst tables.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Starburst","url":"/reference/Connectors/materialization-connectors/starburst/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/host\tHost and optional port string\tRequired /catalog\tCatalog Name\tGalaxy catalog Catalog\tstring\tRequired /schema\tSchema Name\tDefault schema to materialize to\tstring\tRequired /account\tAccount\tGalaxy account name\tstring\tRequired /password\tPassword\tGalaxy account password\tstring\tRequired /awsAccessKeyId\tAWS Access Key ID string\tRequired /awsSecretAccessKey\tAWS Secret Access Key string\tRequired /region\tAWS Region\tRegion of AWS storage\tstring\tRequired /bucket\tBucket name string\tRequired /bucketPath\tBucket path\tA prefix that will be used to store objects in S3.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tTable name\tstring\tRequired /schema\tAlternative Schema\tAlternative schema for this table\tstring\t  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Starburst","url":"/reference/Connectors/materialization-connectors/starburst/#sample","content":"  materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: host: HOST:PORT account: ACCOUNT password: PASSWORD catalog: CATALOG_NAME schema: SCHEMA_NAME awsAccessKeyId: AWS_ACCESS_KEY_ID awsSecretAccessKey: AWS_SECRET_KEY_ID region: REGION bucket: BUCKET bucketPath: BUCKET_PATH image: ghcr.io/estuary/materialize-starburst:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: table: ${table_name} schema: default source: ${PREFIX}/${source_collection}   ","version":"Next","tagName":"h3"},{"title":"Sync Schedule​","type":1,"pageTitle":"Starburst","url":"/reference/Connectors/materialization-connectors/starburst/#sync-schedule","content":" This connector supports configuring a schedule for sync frequency. You can read about how to configure this here.  ","version":"Next","tagName":"h2"},{"title":"Reserved words​","type":1,"pageTitle":"Starburst","url":"/reference/Connectors/materialization-connectors/starburst/#reserved-words","content":" Starburst Galaxy has a list of reserved words that must be quoted in order to be used as an identifier. Flow automatically quotes fields that are in the reserved words list. You can find this list in Trino's documentation here and in the table below.  caution In Starburst Galaxy, objects created with quoted identifiers must always be referenced exactly as created, including the quotes. Otherwise, SQL statements and queries can result in errors. See the Trino docs.  Reserved words CUBE\tINSERT\tTABLE CURRENT_CATALOG\tINTERSECT\tTHEN CURRENT_DATE\tINTO\tTRIM CURRENT_PATH\tIS\tTRUE CURRENT_ROLE\tJOIN\tUESCAPE CURRENT_SCHEMA\tJSON_ARRAY\tUNION CURRENT_TIME\tJSON_EXISTS\tUNNEST CURRENT_TIMESTAMP\tJSON_OBJECT\tUSING CURRENT_USER\tJSON_QUERY\tVALUES DEALLOCATE\tJSON_TABLE\tWHEN DELETE\tJSON_VALUE\tWHERE DESCRIBE\tLEFT\tWITH DISTINCT\tLIKE DROP\tLISTAGG ELSE\tLOCALTIME END\tLOCALTIMESTAMP ESCAPE\tNATURAL EXCEPT\tNORMALIZE EXECUTE\tNOT EXISTS\tNULL\t ","version":"Next","tagName":"h2"},{"title":"Materialization Protocol","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-protocol/","content":"","keywords":"","version":"Next"},{"title":"Sequence Diagram​","type":1,"pageTitle":"Materialization Protocol","url":"/reference/Connectors/materialization-protocol/#sequence-diagram","content":" As a convention and to reduce ambiguity, message types from the Runtime are named in an imperative fashion (Load), while responses from the driver always have a past-tense name (Loaded):  sequenceDiagram Runtime-&gt;&gt;Driver: Open{MaterializationSpec, driverCP} Note right of Driver: Connect to endpoint.&lt;br/&gt;Optionally fetch last-committed&lt;br/&gt;runtime checkpoint. Driver-&gt;&gt;Runtime: Opened{runtimeCP} Note over Runtime, Driver: One-time initialization ☝️.&lt;br/&gt; 👇 Repeats for each transaction. Note left of Runtime: Prior txn commits&lt;br/&gt;to recovery log. Note right of Driver: Prior txn commits to DB&lt;br/&gt;(where applicable). Runtime-&gt;&gt;Driver: Acknowledge Note right of Runtime: Acknowledged MAY be sent&lt;br/&gt;before Acknowledge. Note right of Driver: MAY perform an idempotent&lt;br/&gt;apply of last txn. Note left of Runtime: Runtime does NOT await&lt;br/&gt;Acknowledged before&lt;br/&gt;proceeding to send Load. Driver-&gt;&gt;Runtime: Acknowledged Note left of Runtime: Runtime may now finalize&lt;br/&gt;a pipelined transaction. Note over Runtime, Driver: End of Acknowledge phase. Runtime-&gt;&gt;Driver: Load&lt;A&gt; Note left of Runtime: Load keys may&lt;br/&gt; not exist (yet). Runtime-&gt;&gt;Driver: Load&lt;B&gt; Note right of Driver: MAY evaluate Load immediately,&lt;br/&gt;or stage for deferred retrieval. Driver-&gt;&gt;Runtime: Loaded&lt;A&gt; Runtime-&gt;&gt;Driver: Load&lt;C&gt; Runtime-&gt;&gt;Driver: Flush Driver-&gt;&gt;Runtime: Loaded&lt;C&gt; Note right of Driver: Omits Loaded for keys&lt;br/&gt;that don't exist. Driver-&gt;&gt;Runtime: Flushed Note left of Runtime: All existing keys&lt;br/&gt;have been retrieved. Note over Runtime, Driver: End of Load phase. Runtime-&gt;&gt;Driver: Store&lt;X&gt; Runtime-&gt;&gt;Driver: Store&lt;Y&gt; Runtime-&gt;&gt;Driver: Store&lt;Z&gt; Runtime-&gt;&gt;Driver: StartCommit{runtimeCP} Note right of Driver: * Completes all Store processing.&lt;br/&gt;* MAY include runtimeCP in DB txn. Note right of Driver: Commit to DB&lt;br/&gt;now underway. Driver-&gt;&gt;Runtime: StartedCommit{driverCP} Note left of Runtime: Begins commit to&lt;br/&gt; recovery log. Note over Runtime, Driver: End of Store phase. Loops around&lt;br/&gt;to Acknowledge &lt;=&gt; Acknowledged.  ","version":"Next","tagName":"h2"},{"title":"Exactly-Once Semantics​","type":1,"pageTitle":"Materialization Protocol","url":"/reference/Connectors/materialization-protocol/#exactly-once-semantics","content":" The central tenant of transactional materializations is this: there is a consumption checkpoint, and there is a state of the view. As the materialization progresses, both the checkpoint and the view state will change. Updates to the checkpoint and to the view state MUST always commit together, in the exact same transaction.  Flow materialization tasks have a backing transactional recovery log, which is capable of durable commits that update both the checkpoint and also a (reasonably small) driver-defined state. More on driver states later.  Many interesting endpoint systems are also fully transactional in nature.  When implementing a materialization driver, the first question an implementor must answer is: whose commit is authoritative? Flow's recovery log, or the materialized system? This protocol supports either.  ","version":"Next","tagName":"h2"},{"title":"Common Implementation Patterns​","type":1,"pageTitle":"Materialization Protocol","url":"/reference/Connectors/materialization-protocol/#common-implementation-patterns","content":" There are a few common implementation patterns for materializations. The choice of pattern depends on the transaction capabilities of the remote endpoint.  ","version":"Next","tagName":"h2"},{"title":"Remote Store is Authoritative​","type":1,"pageTitle":"Materialization Protocol","url":"/reference/Connectors/materialization-protocol/#remote-store-is-authoritative","content":" In this pattern, the remote store (for example, a database) persists view states and the Flow consumption checkpoints which those views reflect. There are many such checkpoints: one per task split, and in this pattern the Flow recovery log is effectively ignored.  Typically this workflow runs in the context of a synchronous BEGIN/COMMITtransaction, which updates table states and a Flow checkpoint together. The transaction need be scoped only to the store phase of this workflow, as the materialization protocol requires only read-committed isolation semantics.  Flow is a distributed system, and an important consideration is the effect of a &quot;zombie&quot; assignment of a materialization task, which can race a newly-promoted assignment of that same task.  Fencing is a technique which uses the transactional capabilities of a store to &quot;fence off&quot; an older zombie assignment, such that it's prevented from committing further transactions. This avoids a failure mode where:  New assignment N recovers a checkpoint at Ti.Zombie assignment Z commits another transaction at Ti+1.N beings processing from Ti, inadvertently duplicating the effects of Ti+1.  When a remote store is authoritative, it must implement fencing behavior. As a sketch, the store can maintain a nonce value alongside the checkpoint of each task split. The nonce is updated on each open of this RPC, and each commit transaction then verifies that the nonce has not been changed.  In the future, if another RPC opens and updates the nonce, it fences off this instance of the task split and prevents it from committing further transactions.  ","version":"Next","tagName":"h3"},{"title":"Recovery Log with Non-Transactional Store​","type":1,"pageTitle":"Materialization Protocol","url":"/reference/Connectors/materialization-protocol/#recovery-log-with-non-transactional-store","content":" In this pattern, the runtime's recovery log persists the Flow checkpoint and handles fencing semantics. During the Load and Store phases, the driver directly manipulates a non-transactional store or API, such as a key/value store.  Note that this pattern is at-least-once. A transaction may fail part-way through and be restarted, causing its effects to be partially or fully replayed.  Care must be taken if the collection's schema has reduction annotations such assum, as those reductions may be applied more than once due to a partially completed, but ultimately failed transaction.  If the collection's schema is last-write-wins, this mode still provides effectively-once behavior. Collections which aren't last-write-wins can be turned into last-write-wins through the use of derivations.  ","version":"Next","tagName":"h3"},{"title":"Recovery Log with Idempotent Apply​","type":1,"pageTitle":"Materialization Protocol","url":"/reference/Connectors/materialization-protocol/#recovery-log-with-idempotent-apply","content":" In this pattern the recovery log is authoritative, but the driver uses external stable storage to stage the effects of a transaction -- rather than directly applying them to the store -- such that those effects can be idempotently applied after the transaction commits.  This allows stores which feature a weaker transaction guarantee to still be used in an exactly-once way, so long as they support an idempotent apply operation.  Driver checkpoints can facilitate this pattern. For example, a driver might generate a unique filename in S3 and reference it in its prepared checkpoint, which is committed to the recovery log. During the &quot;store&quot; phase, it writes to this S3 file. After the transaction commits, it tells the store of the new file to incorporate. The store must handle idempotency, by applying the effects of the unique file just once, even if told of the file multiple times.  A related extension of this pattern is for the driver to embed a Flow checkpoint into its driver checkpoint. Doing so allows the driver to express an intention to restart from an older alternative checkpoint, as compared to the most recent committed checkpoint of the recovery log.  As mentioned above, it's crucial that store states and checkpoints commit together. While seemingly bending that rule, this pattern is consistent with it because, on commit, the semantic contents of the store include BOTH its base state, as well as the staged idempotent update. The store just may not know it yet, but eventually it must because of the retried idempotent apply.  Note the driver must therefore ensure that staged updates are fully applied before returning Loaded responses, in order to provide the correct read-committed semantics required by the Flow runtime.  ","version":"Next","tagName":"h3"},{"title":"Push-only Endpoints & Delta Updates​","type":1,"pageTitle":"Materialization Protocol","url":"/reference/Connectors/materialization-protocol/#push-only-endpoints--delta-updates","content":" Some systems, such as APIs, Webhooks, and Pub/Sub, are push-only in nature. Flow materializations can run in a &quot;delta updates&quot; mode, where loads are always skipped and Flow does not attempt to store fully-reduced documents. Instead, during the store phase, the runtime sends delta updates which reflect the combined roll-up of collection documents processed only within this transaction.  To illustrate the meaning of a delta update, consider documents which are simple counters, having a collection schema that uses a sum reduction strategy.  Without delta updates, Flow would reduce documents -1, 3, and 2 by sum to arrive at document 4, which is stored. The next transaction, document 4 is loaded and reduced with 6, -7, and -1 to arrive at a new stored document 2. This document, 2, represents the full reduction of the collection documents materialized thus far.  Compare to delta updates mode: collection documents -1, 3, and 2 are combined to store a delta-update document of 4. The next transaction starts anew, and 6, -7, and -1 combine to arrive at a delta-update document of -2. These delta updates are a windowed combine over documents seen in the current transaction only, and unlike before are not a full reduction of the document. If delta updates were written to pub/sub, note that a subscriber could further reduce over each delta update to recover the fully reduced document of 2.  Note that many use cases require only lastWriteWins reduction behavior, and for these use cases delta updates does the &quot;right thing&quot; by trivially re-writing each document with its most recent version. This matches the behavior of Kafka Connect, for example.  ","version":"Next","tagName":"h3"},{"title":"Protocol Phases​","type":1,"pageTitle":"Materialization Protocol","url":"/reference/Connectors/materialization-protocol/#protocol-phases","content":" ","version":"Next","tagName":"h2"},{"title":"Acknowledge​","type":1,"pageTitle":"Materialization Protocol","url":"/reference/Connectors/materialization-protocol/#acknowledge","content":" Acknowledge and Acknowledged are always the first messages sent every transaction, including the very first transaction of an RPC. The Runtime sendsAcknowledge to indicate that the last transaction has committed to the recovery log. The Driver sends Acknowledged to indicate that its endpoint transaction has committed.  Acknowledge and Acknowledged are not ordered. Acknowledged may be sent before Acknowledge and vice versa.  The Runtime does not wait for Acknowledged before sending Load messages. In most cases the Driver should simply not read these Load messages until it has completed its own commit and sent its own Acknowledged.  A Driver MAY instead process its commit and acknowledgment in the background while actively reading Load messages. It MUST NOT evaluate Loads yet, as this could otherwise be a violation of read-committed semantics, but it MAY stage them for deferred evaluation. This is recommended for Drivers that have very long commit and/or acknowledgement operations. While a background commit progresses the Flow runtime will optimistically pipeline the next transaction, processing documents and preparing for when the Driver sendsAcknowledged.  Drivers following the &quot;Recovery Log with Idempotent Apply&quot; pattern must take care to properly handle the very first acknowledgement phase of an RPC. At startup, a driver cannot know if the last commit has been acknowledged. For example, a previous RPC invocation may have failed immediately after commit but prior to acknowledgement. The Driver must thus idempotent-ly apply or re-apply changes staged by a prior Driver invocation, and reply with Acknowledged only once done.  Drivers with transactional semantics SHOULD send Acknowledged immediately after a previous, started commit completes.  Drivers with at-least-once semantics SHOULD send Acknowledged immediately after sending StartedCommit.  ","version":"Next","tagName":"h3"},{"title":"Load​","type":1,"pageTitle":"Materialization Protocol","url":"/reference/Connectors/materialization-protocol/#load","content":" Zero or more Load messages are sent by the Runtime with documents to fetch. A given document key will appear at most once in a transaction, and will not be repeated across Load messages.  Drivers may immediately evaluate each Load and respond, or may queue many keys to load and defer their evaluation. The Runtime does not await any individualLoad requests.  After the previous transaction has fully completed, and the driver has sentAcknowledged to the Runtime, the current transaction may begin to close.  The Runtime indicates this by sending a Flush message, which is NEVER sent before Acknowledged is received. Acknowledged is thus an important signal as to when the Runtime may begin to finalize an optimistic, pipelined transaction.  On reading Flush, Drivers must process all remaining Load messages, including any deferred evaluations, and send all Loaded responses prior to sending its own Flushed response.  This signals to the Runtime that all documents which can be loaded have been loaded, and the transaction proceeds to the Store phase.  Materialization bindings which are processing in delta-updates mode will never receive a Load message, but will receive a Flush and must still respond withFlushed.  ","version":"Next","tagName":"h3"},{"title":"Store​","type":1,"pageTitle":"Materialization Protocol","url":"/reference/Connectors/materialization-protocol/#store","content":" Zero or more Store messages are sent by the Runtime to the Driver, indicating keys, documents, and extracted fields to store. No response is required of the Driver for these messages.  Once all documents have been stored, the Runtime sends a StartCommit message which carries its opaque runtime checkpoint.  Drivers implementing the &quot;Remote Store is Authoritative&quot; pattern must include the runtime checkpoint in its current transaction, for retrieval in a future Open of a new transactions RPC. Other driver patterns MAY ignore this checkpoint.  On reading StartCommit the driver ensures that all Store messages have been processed. It begins to commit its own transaction (where applicable), and then responds with StartedCommit which contain an update to the driver's checkpoint.  On the Runtime's receipt of StartedCommit, the Runtime now knows that allStore messages have been fully processed. It preserves the updated Driver checkpoint in its recovery log and begins to commit.  From here, the protocol loops back around to the Acknowledge phase. ","version":"Next","tagName":"h3"},{"title":"Google Cloud SQL for SQLServer","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/SQLServer/google-cloud-sql-sqlserver/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Google Cloud SQL for SQLServer","url":"/reference/Connectors/materialization-connectors/SQLServer/google-cloud-sql-sqlserver/#prerequisites","content":" To use this connector, you'll need:  A SQLServer database to which to materialize, and user credentials. SQLServer 2017 and later are supportedThe connector will create new tables in the database per your specification, so user credentials must have access to create new tables. At least one Flow collection  ","version":"Next","tagName":"h2"},{"title":"Setup Google Cloud SQL for SQL Server​","type":1,"pageTitle":"Google Cloud SQL for SQLServer","url":"/reference/Connectors/materialization-connectors/SQLServer/google-cloud-sql-sqlserver/#setup-google-cloud-sql-for-sql-server","content":" Allow connections between the database and Estuary Flow. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel.  To allow direct access: Enable public IP on your database and add Estuary Flow IP addresses as authorized IP addresses. To allow secure connections via SSH tunneling: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networks for additional details and a sample. In your SQL client, connect to your instance as the default sqlserver user and issue the following commands.  USE &lt;database&gt;; -- Create user and password for use with the connector. CREATE LOGIN flow_materialize WITH PASSWORD = 'secret'; CREATE USER flow_materialize FOR LOGIN flow_materialize; -- Grant control on the database to flow_materialize GRANT CONTROL ON DATABASE::&lt;database&gt; TO flow_materialize;   In the Cloud Console, note the instance's host under Public IP Address. Its port will always be 1433. Together, you'll use the host:port as the address property when you configure the connector.  ","version":"Next","tagName":"h2"},{"title":"Connecting to SQLServer​","type":1,"pageTitle":"Google Cloud SQL for SQLServer","url":"/reference/Connectors/materialization-connectors/SQLServer/google-cloud-sql-sqlserver/#connecting-to-sqlserver","content":" Allow connections between the database and Estuary Flow. There are two ways to do this: by granting direct access to Flow's IP or by creating an SSH tunnel. To allow direct access: Enable public IP on your database and add Estuary Flow IP addresses as authorized IP addresses. To allow secure connections via SSH tunneling: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networks for additional details and a sample. In your SQL client, connect to your instance as the default sqlserver user and issue the following commands.  USE &lt;database&gt;; -- Create user and password for use with the connector. CREATE LOGIN flow_materialize WITH PASSWORD = 'Secret123!'; CREATE USER flow_materialize FOR LOGIN flow_materialize; -- Grant control on the database to flow_materialize GRANT CONTROL ON DATABASE::&lt;database&gt; TO flow_materialize;   In the Cloud Console, note the instance's host under Public IP Address. Its port will always be 1433. Together, you'll use the host:port as the address property when you configure the connector.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"Google Cloud SQL for SQLServer","url":"/reference/Connectors/materialization-connectors/SQLServer/google-cloud-sql-sqlserver/#configuration","content":" To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a SQLServer materialization, which will direct one or more of your Flow collections to your desired tables, or views, in the database.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Google Cloud SQL for SQLServer","url":"/reference/Connectors/materialization-connectors/SQLServer/google-cloud-sql-sqlserver/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/database\tDatabase\tName of the logical database to materialize to.\tstring\tRequired /address\tAddress\tHost and port of the database. If only the host is specified, port will default to 1433.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /user\tUser\tDatabase user to connect as.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tTable name to materialize to. It will be created by the connector, unless the connector has previously created it.\tstring\tRequired /delta_updates\tDelta Update\tShould updates to this table be done via delta updates.\tboolean\tfalse  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"Google Cloud SQL for SQLServer","url":"/reference/Connectors/materialization-connectors/SQLServer/google-cloud-sql-sqlserver/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: ghcr.io/estuary/materialize-sqlserver:dev config: database: flow address: localhost:5432 password: flow user: flow bindings: - resource: table: ${TABLE_NAME} source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Delta updates​","type":1,"pageTitle":"Google Cloud SQL for SQLServer","url":"/reference/Connectors/materialization-connectors/SQLServer/google-cloud-sql-sqlserver/#delta-updates","content":" This connector supports both standard (merge) and delta updates. The default is to use standard updates.  ","version":"Next","tagName":"h2"},{"title":"Reserved words​","type":1,"pageTitle":"Google Cloud SQL for SQLServer","url":"/reference/Connectors/materialization-connectors/SQLServer/google-cloud-sql-sqlserver/#reserved-words","content":" SQLServer has a list of reserved words that must be quoted in order to be used as an identifier. Flow considers all the reserved words in the official SQLServer documentation.  These reserved words are listed in the table below. Flow automatically quotes fields that are in this list.  Reserved words absolute\tconnect\telse\tintersect\ton action\tconnection\tend\tintersection\tonly ada\tconstraint\tend-exec\tinterval\topen add\tconstraints\tequals\tinto\topendatasourc admin\tconstructor\terrlvl\tis\topenquery after\tcontains\tescape\tisolation\topenrowset aggregate\tcontainstable\tevery\titerate\topenxml alias\tcontinue\texcept\tjoin\toperation all\tconvert\texception\tkey\toption allocate\tcorr\texec\tkill\tor alter\tcorresponding\texecute\tlanguage\torder and\tcount\texists\tlarge\tordinality any\tcovar_pop\texit\tlast\tout are\tcovar_samp\texternal\tlateral\touter array\tcreate\textract\tleading\toutput as\tcross\tfalse\tleft\tover asc\tcube\tfetch\tless\toverlaps asensitive\tcume_dist\tfile\tlevel\toverlay assertion\tcurrent\tfillfactor\tlike\tpad asymmetric\tcurrent_catal\tfilter\tlike_regex\tparameter at\tcurrent_date\tfirst\tlimit\tparameters atomic\tcurrent_defau\tfloat\tlineno\tpartial authorization\tcurrent_path\tfor\tln\tpartition avg\tcurrent_role\tforeign\tload\tpascal backup\tcurrent_schem\tfortran\tlocal\tpath before\tcurrent_time\tfound\tlocaltime\tpercent begin\tcurrent_times\tfree\tlocaltimestam\tpercent_rank between\tcurrent_trans\tfreetext\tlocator\tpercentile_co binary\tcurrent_user\tfreetexttable\tlower\tpercentile_di bit\tcursor\tfrom\tmap\tpivot bit_length\tcycle\tfull\tmatch\tplan blob\tdata\tfulltexttable\tmax\tposition boolean\tdatabase\tfunction\tmember\tposition_rege both\tdate\tfusion\tmerge\tpostfix breadth\tday\tgeneral\tmethod\tprecision break\tdbcc\tget\tmin\tprefix browse\tdeallocate\tglobal\tminute\tpreorder bulk\tdec\tgo\tmod\tprepare by\tdecimal\tgoto\tmodifies\tpreserve call\tdeclare\tgrant\tmodify\tprimary called\tdefault\tgroup\tmodule\tprint cardinality\tdeferrable\tgrouping\tmonth\tprior cascade\tdeferred\thaving\tmultiset\tprivileges cascaded\tdelete\thold\tnames\tproc case\tdeny\tholdlock\tnational\tprocedure cast\tdepth\thost\tnatural\tpublic catalog\tderef\thour\tnchar\traiserror char\tdesc\tidentity\tnclob\trange char_length\tdescribe\tidentity_inse\tnew\tread character\tdescriptor\tidentitycol\tnext\treads character_len\tdestroy\tif\tno\treadtext check\tdestructor\tignore\tnocheck\treal checkpoint\tdeterministic\timmediate\tnonclustered\treconfigure class\tdiagnostics\tin\tnone\trecursive clob\tdictionary\tinclude\tnormalize\tref close\tdisconnect\tindex\tnot\treferences clustered\tdisk\tindicator\tnull\treferencing coalesce\tdistinct\tinitialize\tnullif\tregr_avgx collate\tdistributed\tinitially\tnumeric\tregr_avgy collation\tdomain\tinner\tobject\tregr_count collect\tdouble\tinout\toccurrences_r\tregr_intercep column\tdrop\tinput\toctet_length\tregr_r2 commit\tdump\tinsensitive\tof\tregr_slope completion\tdynamic\tinsert\toff\tregr_sxx compute\teach\tint\toffsets\tregr_sxy condition\telement\tinteger\told\tregr_syy relative\tsemanticsimil\tstructure\ttruncate\twindow release\tsemanticsimil\tsubmultiset\ttry_convert\twith replication\tsensitive\tsubstring\ttsequal\twithin restore\tsequence\tsubstring_reg\tuescape\twithin restrict\tsession\tsum\tunder\twithout result\tsession_user\tsymmetric\tunion\twork return\tset\tsystem\tunique\twrite returns\tsets\tsystem_user\tunknown\twritetext revert\tsetuser\ttable\tunnest\txmlagg revoke\tshutdown\ttablesample\tunpivot\txmlattributes right\tsimilar\ttemporary\tupdate\txmlbinary role\tsize\tterminate\tupdatetext\txmlcast rollback\tsmallint\ttextsize\tupper\txmlcomment rollup\tsome\tthan\tusage\txmlconcat routine\tspace\tthen\tuse\txmldocument row\tspecific\ttime\tuser\txmlelement rowcount\tspecifictype\ttimestamp\tusing\txmlexists rowguidcol\tsql\ttimezone_hour\tvalue\txmlforest rows\tsqlca\ttimezone_minu\tvalues\txmliterate rule\tsqlcode\tto\tvar_pop\txmlnamespaces save\tsqlerror\ttop\tvar_samp\txmlparse savepoint\tsqlexception\ttrailing\tvarchar\txmlpi schema\tsqlstate\ttran\tvariable\txmlquery scope\tsqlwarning\ttransaction\tvarying\txmlserialize scroll\tstart\ttranslate\tview\txmltable search\tstate\ttranslate_reg\twaitfor\txmltext second\tstatement\ttranslation\twhen\txmlvalidate section\tstatic\ttreat\twhenever\tyear securityaudit\tstatistics\ttrigger\twhere\tzone select\tstddev_pop\ttrim\twhile semantickeyph\tstddev_samp\ttrue\twidth_bucket\t  ","version":"Next","tagName":"h2"},{"title":"Changelog​","type":1,"pageTitle":"Google Cloud SQL for SQLServer","url":"/reference/Connectors/materialization-connectors/SQLServer/google-cloud-sql-sqlserver/#changelog","content":" The changelog includes a list of breaking changes made to this connector. Backwards-compatible changes are not listed.  Proceed with caution when editing materializations created with previous versions of this connector; editing always upgrades your materialization to the latest connector version.  V1: 2023-09-01​  First version ","version":"Next","tagName":"h2"},{"title":"Handling Deletions in Estuary Flow","type":0,"sectionRef":"#","url":"/reference/deletions/","content":"","keywords":"","version":"Next"},{"title":"Soft Deletes​","type":1,"pageTitle":"Handling Deletions in Estuary Flow","url":"/reference/deletions/#soft-deletes","content":" Soft deletes occur when a document is marked as deleted but is not physically removed from the destination. Instead, it is flagged for deletion with a specific metadata field.  Flagging Soft Deletes: The field _meta/op is set to 'd' to indicate that a document has been marked for deletion.Document Retention: The document remains in the destination even though it is marked as deleted.Filtering Soft Deleted Documents: To exclude soft-deleted documents from queries, you can filter out documents where _meta/op = 'd'. This ensures that soft-deleted documents are ignored without permanently removing them.  ","version":"Next","tagName":"h2"},{"title":"Hard Deletes​","type":1,"pageTitle":"Handling Deletions in Estuary Flow","url":"/reference/deletions/#hard-deletes","content":" Hard deletes go a step further than soft deletes by permanently removing documents from the destination.  Flagging Hard Deletes: Similar to soft deletes, the _meta/op field is set to 'd' for documents that need to be deleted.Document Removal: Once a document is flagged, a query is issued to physically remove any document marked with _meta/op = 'd' from the destination.Supported Materialization Connectors for Hard Deletes: SnowflakeGoogle BigQueryDatabricksAmazon RedshiftElasticPostgreSQLMySQLSQL ServerAlloyDBMongoDBMotherDuckTimescaleDB ","version":"Next","tagName":"h2"},{"title":"TimescaleDB","type":0,"sectionRef":"#","url":"/reference/Connectors/materialization-connectors/timescaledb/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"TimescaleDB","url":"/reference/Connectors/materialization-connectors/timescaledb/#prerequisites","content":" To use this connector, you'll need:  A TimescaleDB database to which to materialize. Know your user credentials, and the host and port. If using Timescale Cloud, this information is available on your console, on the Connection info pane. At least one Flow collection.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"TimescaleDB","url":"/reference/Connectors/materialization-connectors/timescaledb/#configuration","content":" To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a TimescaleDB materialization, which will direct one or more of your Flow collections to your desired tables, or views, in the database.  The connector will create new tables in the database per your specification. Tables created manually in advance are not supported.  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"TimescaleDB","url":"/reference/Connectors/materialization-connectors/timescaledb/#properties","content":" Endpoint​  Property\tTitle\tDescription\tType\tRequired/Default/database\tDatabase\tName of the logical database to materialize to.\tstring /address\tAddress\tHost and port of the database\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /schema\tDatabase Schema\tDatabase schema to use for materialized tables (unless overridden within the binding resource configuration) as well as associated materialization metadata tables\tstring\t&quot;public&quot; /user\tUser\tDatabase user to connect as.\tstring\tRequired  Bindings​  Property\tTitle\tDescription\tType\tRequired/Default/additional_table_create_sql\tAdditional Table Create SQL\tAdditional SQL statement(s) to be run in the same transaction that creates the table. Useful for creating Hypertables.\tstring /delta_updates\tDelta Update\tShould updates to this table be done via delta updates.\tboolean\tfalse /schema\tAlternative Schema\tAlternative schema for this table (optional). Overrides schema set in endpoint configuration.\tstring /table\tTable\tTable name to materialize to. It will be created by the connector, unless the connector has previously created it.\tstring\tRequired  ","version":"Next","tagName":"h3"},{"title":"Sample​","type":1,"pageTitle":"TimescaleDB","url":"/reference/Connectors/materialization-connectors/timescaledb/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: ghcr.io/estuary/materialize-timescaledb:dev config: database: flow address: xxxxxxxxxx.xxxxxxxxxx.tsdb.cloud.timescale.com:01234 password: flow user: flow bindings: - resource: table: ${TABLE_NAME} source: ${PREFIX}/${COLLECTION_NAME}   ","version":"Next","tagName":"h3"},{"title":"Creating TimescaleDB hypertables​","type":1,"pageTitle":"TimescaleDB","url":"/reference/Connectors/materialization-connectors/timescaledb/#creating-timescaledb-hypertables","content":" Hypertables are PostgreSQL tables in TimescaleDB optimized for time-series data. They exist alongside regular PostgreSQL tables.  You can add Hypertables to your materialization on a per-binding basis by adding the optional /additional_table_create_sql field to each binding configuration.  Your SQL statement should take the following format:  SELECT create_hypertable('table', 'timestamp_column');   Where 'table' matches the value for the field /table in that binding, and 'timestamp_column' is the name of the table column containing its time values.  For example, materializing the Flow collection acmeCo/my_time_series would produce a table called 'my_time_series'. Assuming its timestamp value is in the field 'time', the binding configuration would look like:  bindings: - resource: additional_table_create_sql: 'SELECT create_hypertable('my_time_series', 'time');' table: my_time_series source: acmeCo/my_time_series   ","version":"Next","tagName":"h2"},{"title":"Delta updates​","type":1,"pageTitle":"TimescaleDB","url":"/reference/Connectors/materialization-connectors/timescaledb/#delta-updates","content":" This connector supports both standard (merge) and delta updates. The default is to use standard updates.  ","version":"Next","tagName":"h2"},{"title":"Reserved words​","type":1,"pageTitle":"TimescaleDB","url":"/reference/Connectors/materialization-connectors/timescaledb/#reserved-words","content":" PostgreSQL (and thus TimescaleDB) has a list of reserved words that must be quoted in order to be used as an identifier. Flow considers all the reserved words that are marked as &quot;reserved&quot; in any of the columns in the official PostgreSQL documentation.  These reserved words are listed in the table below. Flow automatically quotes fields that are in this list.  Reserved words abs\tcurrent_transform_group_for_type\tindicator\torder\tsqlexception absolute\tcurrent_user\tinitial\tout\tsqlstate acos\tcursor\tinitially\touter\tsqlwarning action\tcycle\tinner\toutput\tsqrt add\tdatalink\tinout\tover\tstart all\tdate\tinput\toverlaps\tstatic allocate\tday\tinsensitive\toverlay\tstddev_pop alter\tdeallocate\tinsert\tpad\tstddev_samp analyse\tdec\tint\tparameter\tsubmultiset analyze\tdecfloat\tinteger\tpartial\tsubset and\tdecimal\tintersect\tpartition\tsubstring any\tdeclare\tintersection\tpattern\tsubstring_regex are\tdefault\tinterval\tper\tsucceeds array\tdeferrable\tinto\tpercent\tsum array_agg\tdeferred\tis\tpercentile_cont\tsymmetric array_max_cardinality\tdefine\tisnull\tpercentile_disc\tsystem as\tdelete\tisolation\tpercent_rank\tsystem_time asc\tdense_rank\tjoin\tperiod\tsystem_user asensitive\tderef\tjson_array\tpermute\ttable asin\tdesc\tjson_arrayagg\tplacing\ttablesample assertion\tdescribe\tjson_exists\tportion\ttan asymmetric\tdescriptor\tjson_object\tposition\ttanh at\tdeterministic\tjson_objectagg\tposition_regex\ttemporary atan\tdiagnostics\tjson_query\tpower\tthen atomic\tdisconnect\tjson_table\tprecedes\ttime authorization\tdistinct\tjson_table_primitive\tprecision\ttimestamp avg\tdlnewcopy\tjson_value\tprepare\ttimezone_hour begin\tdlpreviouscopy\tkey\tpreserve\ttimezone_minute begin_frame\tdlurlcomplete\tlag\tprimary\tto begin_partition\tdlurlcompleteonly\tlanguage\tprior\ttrailing between\tdlurlcompletewrite\tlarge\tprivileges\ttransaction bigint\tdlurlpath\tlast\tprocedure\ttranslate binary\tdlurlpathonly\tlast_value\tptf\ttranslate_regex bit\tdlurlpathwrite\tlateral\tpublic\ttranslation bit_length\tdlurlscheme\tlead\trange\ttreat blob\tdlurlserver\tleading\trank\ttrigger boolean\tdlvalue\tleft\tread\ttrim both\tdo\tlevel\treads\ttrim_array by\tdomain\tlike\treal\ttrue call\tdouble\tlike_regex\trecursive\ttruncate called\tdrop\tlimit\tref\tuescape cardinality\tdynamic\tlistagg\treferences\tunion cascade\teach\tln\treferencing\tunique cascaded\telement\tlocal\tregr_avgx\tunknown case\telse\tlocaltime\tregr_avgy\tunmatched cast\tempty\tlocaltimestamp\tregr_count\tunnest catalog\tend\tlog\tregr_intercept\tupdate ceil\tend-exec\tlog10\tregr_r2\tupper ceiling\tend_frame\tlower\tregr_slope\tusage char\tend_partition\tmatch\tregr_sxx\tuser character\tequals\tmatches\tregr_sxy\tusing character_length\tescape\tmatch_number\tregr_syy\tvalue char_length\tevery\tmatch_recognize\trelative\tvalues check\texcept\tmax\trelease\tvalue_of classifier\texception\tmeasures\trestrict\tvarbinary clob\texec\tmember\tresult\tvarchar close\texecute\tmerge\treturn\tvariadic coalesce\texists\tmethod\treturning\tvarying collate\texp\tmin\treturns\tvar_pop collation\texternal\tminute\trevoke\tvar_samp collect\textract\tmod\tright\tverbose column\tfalse\tmodifies\trollback\tversioning commit\tfetch\tmodule\trollup\tview concurrently\tfilter\tmonth\trow\twhen condition\tfirst\tmultiset\trows\twhenever connect\tfirst_value\tnames\trow_number\twhere connection\tfloat\tnational\trunning\twidth_bucket constraint\tfloor\tnatural\tsavepoint\twindow constraints\tfor\tnchar\tschema\twith contains\tforeign\tnclob\tscope\twithin continue\tfound\tnew\tscroll\twithout convert\tframe_row\tnext\tsearch\twork copy\tfree\tno\tsecond\twrite corr\tfreeze\tnone\tsection\txml corresponding\tfrom\tnormalize\tseek\txmlagg cos\tfull\tnot\tselect\txmlattributes cosh\tfunction\tnotnull\tsensitive\txmlbinary count\tfusion\tnth_value\tsession\txmlcast covar_pop\tget\tntile\tsession_user\txmlcomment covar_samp\tglobal\tnull\tset\txmlconcat create\tgo\tnullif\tshow\txmldocument cross\tgoto\tnumeric\tsimilar\txmlelement cube\tgrant\toccurrences_regex\tsin\txmlexists cume_dist\tgroup\toctet_length\tsinh\txmlforest current\tgrouping\tof\tsize\txmliterate current_catalog\tgroups\toffset\tskip\txmlnamespaces current_date\thaving\told\tsmallint\txmlparse current_default_transform_group\thold\tomit\tsome\txmlpi current_path\thour\ton\tspace\txmlquery current_role\tidentity\tone\tspecific\txmlserialize current_row\tilike\tonly\tspecifictype\txmltable current_schema\timmediate\topen\tsql\txmltext current_time\timport\toption\tsqlcode\txmlvalidate current_timestamp\tin\tor\tsqlerror\tyear  ","version":"Next","tagName":"h2"},{"title":"Changelog​","type":1,"pageTitle":"TimescaleDB","url":"/reference/Connectors/materialization-connectors/timescaledb/#changelog","content":" The changelog includes a list of breaking changes made to this connector. Backwards-compatible changes are not listed.  Proceed with caution when editing materializations created with previous versions of this connector; editing always upgrades your materialization to the latest connector version.  V4: 2022-11-30​  This version includes breaking changes to materialized table columns. These provide more consistent column names and types, but tables created from previous versions of the connector may not be compatible with this version.  Capitalization is now preserved when fields in Flow are converted to Postgres (TimescaleDB) column names. Previously, fields containing uppercase letters were converted to lowercase. Field names and values of types date, duration, ipv4, ipv6, macaddr, macaddr8, and time are now converted into their corresponding Postgres (TimescaleDB) types. Previously, only date-time was converted, and all others were materialized as strings. ","version":"Next","tagName":"h2"},{"title":"Editing considerations","type":0,"sectionRef":"#","url":"/reference/editing/","content":"","keywords":"","version":"Next"},{"title":"How to edit Flow entities​","type":1,"pageTitle":"Editing considerations","url":"/reference/editing/#how-to-edit-flow-entities","content":" In the Flow web app, you can edit captures and materializations, and use the Schema Inference tool to edit collection schemas.  Editing captures and associated collectionsEditing materializations and associated collections  With flowctl, you can edit captures, materializations, collections, derivations, and tests. You do this by pulling the desired specification locally, editing, and re-publishing.  Editing with flowctl  ","version":"Next","tagName":"h2"},{"title":"Endpoint configuration changes​","type":1,"pageTitle":"Editing considerations","url":"/reference/editing/#endpoint-configuration-changes","content":" A common reason to edit a capture or materialization is to fix a broken endpoint configuration: for example, if a database is now accessed through a different port. Changes that prevent Flow from finding the source system immediately cause the capture or materialization to fail.  By contrast, certain credential changes might not cause issues unless you attempt to edit the capture or materialization. Because Flow tasks run continuously, the connector doesn't have to re-authenticate and an outdated credential won't cause failure. Editing, however, requires the task to re-start, so you'll need to provide current credentials to the endpoint configuration. Before editing, take note of any changed credentials, even if the task is still running successfully.  ","version":"Next","tagName":"h2"},{"title":"Managing connector updates​","type":1,"pageTitle":"Editing considerations","url":"/reference/editing/#managing-connector-updates","content":" Connectors are updated periodically. In some cases, required fields are added or removed. When you edit a capture or materialization, you'll need to update the configuration to comply with the current connector version. You may need to change a property's formatting or add a new field.  Additionally, certain updates to capture connectors can affect the way available collections are named. After editing, the connector may map a data resource to new collection with a different name.  For example, say you have capture that writes to a collection called post/fruity_pebbles/nutritionFacts. You begin to edit the capture using the latest version of the connector. The connector detects the same set of nutrition facts data, but maps it to a collection called post/fruity_pebbles/nutrition-facts. If you continue to publish the edited capture, both collections will persist, but new data will be written to the new collection.  Before editing, check if a connector has been updated:  Go to the Admin tab and view the list of connectors. Each tile shows the date it was last updated.Check the connector's documentation. Pertinent updates, if any, are noted in the Changelog section.  ","version":"Next","tagName":"h2"},{"title":"Considerations for name changes​","type":1,"pageTitle":"Editing considerations","url":"/reference/editing/#considerations-for-name-changes","content":" You're not able to change the name of a capture or materialization after you create it. You're also unable to manually change the names of collections; however, connector updates can cause collection names to change, as discussed above.  It is possible to manually change the names of destination resources (tables or analogous data storage units to which collections are written) when editing a materialization. You should avoid doing so unless you want to route future data to a new location.  If you do this, a new resource with that name will be created and the old resource will continue to exist. Historical data may not be backfilled into the new resource, depending on the connector used. ","version":"Next","tagName":"h2"},{"title":"Materialization sync schedule","type":0,"sectionRef":"#","url":"/reference/materialization-sync-schedule/","content":"","keywords":"","version":"Next"},{"title":"How transactions are used to sync data to a destination​","type":1,"pageTitle":"Materialization sync schedule","url":"/reference/materialization-sync-schedule/#how-transactions-are-used-to-sync-data-to-a-destination","content":" Estuary Flow processes data intransactions. Materialization connectors use the materialization protocol to process transactions and sync data to the destination.  When a materialization is caught up to its source collections, it runs frequent small transactions to keep the destination up to date. In this case every new transaction contains the latest data that needs updated. But when a materialization is backfilling its source collections, it runs larger transactions to efficiently load the data in bulk to the destination and catch up to the latest changes.  The sync schedule is configured in terms of these transactions: For less frequent updates, processing of additional transactions is delayed by some amount of time. This extra delay is only applied when the materialization is fully caught up - backfills always run as fast as possible. And while a transaction is delayed, Estuary Flow will continue batching and combining new documents so that the next transaction contains all of the latest data.  You can read about how continuous materialization works for more background information.  ","version":"Next","tagName":"h2"},{"title":"Configuring a sync schedule​","type":1,"pageTitle":"Materialization sync schedule","url":"/reference/materialization-sync-schedule/#configuring-a-sync-schedule","content":" A materialization can be configured to run on a fixed schedule 24/7 or it can have a faster sync schedule during certain times of the day and on certain days of the week. The following options are available for configuring the sync schedule:  Property\tTitle\tDescription\tType/syncFrequency\tSync Frequency\tFrequency at which transactions are executed when the materialization is fully caught up and streaming changes. May be enabled only for certain time periods and days of the week if configured below; otherwise it is effective 24/7. Defaults to 30 minutes if unset.\tstring /timezone\tTimezone\tTimezone applicable to sync time windows and active days. Must be a valid IANA time zone name or +HH:MM offset.\tstring /fastSyncStartTime\tFast Sync Start Time\tTime of day that transactions begin executing at the configured Sync Frequency. Prior to this time transactions will be executed more slowly. Must be in the form of '09:00'.\tstring /fastSyncStopTime\tFast Sync Stop Time\tTime of day that transactions stop executing at the configured Sync Frequency. After this time transactions will be executed more slowly. Must be in the form of '17:00'.\tstring /fastSyncEnabledDays\tFast Sync Enabled Days\tDays of the week that the configured Sync Frequency is active. On days that are not enabled, transactions will be executed more slowly for the entire day. Examples: 'M-F' (Monday through Friday, inclusive), 'M,W,F' (Monday, Wednesday, and Friday), 'Su-T,Th-S' (Sunday through Tuesday, inclusive; Thursday through Saturday, inclusive). All days are enabled if unset.\tstring  warning Changes to a materialization's specification are only applied after the materialization task has completed and acknowledged all of its outstanding transactions. This means that if a task is running with a 4 hour sync frequency, it may take up to 8 hours for a change to the specification to take effect: 4 hours for the &quot;current&quot; transaction to complete and be acknowledged, and another 4 hours for the next &quot;pipelined&quot; commit to complete and be acknowledged. If you are making changes to a materialization with a Sync Scheduleconfigured and would like those changes to take effect immediately, you can disable and then re-enable the materialization.  Example: Sync data on a fixed schedule​  To use the same schedule for syncing data 24/7, set the value of Sync Frequency only and leave the other inputs empty. For example, you might set aSync Frequency of 15m to always have your destination sync every 15 minutes instead of the default 30 minutes.  tip If you want the materialization to always push updated data as fast as possible, use a Sync Frequency of 0s.  Example: Sync data faster during certain times of the day​  If you only care about having the most-up-to-date data possible during certain times of the day, you can set a start and stop time for that time period. The value you set for Sync Frequency will be used during that time period; otherwise syncs will be performed every 4 hours.  The Fast Sync Start Time and Fast Sync Stop Time values must be set as 24-hour times, and you must provide a value for Timezone that this time window should use. Timezones must either be a valid IANA time zone name or a +HH:MMoffset. Providing a time zone name will ensure local factors like daylight savings time are considered for the schedule, whereas an offset timezone is always relative to UTC.  An example configuration data syncs data as fast as possible between the hours of 9:00AM and 5:00PM in the Eastern Time Zone (ET) would use these values:  Sync Frequency: 0sTimezone: America/New_YorkFast Sync Start Time: 09:00Fast Sync Stop Time: 17:00  Example: Sync data faster only on certain days of the week​  You can also set certain days of the week that the fast sync is active. On all other days, data will be sync'd more slowly all day.  To enable this, set values for Sync Frequency, Timezone, Fast Sync Start Time, and Fast Sync Stop Time as you would for syncing data faster during certain times of the day, and also provide a value for Fast Sync Enabled Days.  Fast Sync Enabled Days is a range of days, where the days of the week are abbreviated as (Su)nday, (M)onday, (T)uesday, (W)ednesday, (Th)ursday,(F)riday, (S)aturday.  Here are some examples of valid inputs for Fast Sync Enabled Days:  M-F to enable fast sync on Monday through Friday.Su, T, Th, S to enable fast sync on Sunday, Tuesday, Thursday, and Saturday.Su-M,Th-S to enable fast sync on Thursday through Monday. Note that the days of the week must be listed in order, so Th-M will not work.  ","version":"Next","tagName":"h2"},{"title":"Timing of syncs​","type":1,"pageTitle":"Materialization sync schedule","url":"/reference/materialization-sync-schedule/#timing-of-syncs","content":" In technical terms, timing of syncs is controlled by the materialization connector sending a transaction acknowledgement to the Flow runtime at computed times. Practically this means that at these times the prior transaction will complete and have its statistics recorded, and the next transaction will begin.  This timing is computed so that it occurs at predictable instants in time. As a hypothetical example, if you have set a Sync Frequency of 15m, transaction acknowledgements might be sent at times like 00:00, 00:15, 00:30, 00:45and so on, where each acknowledgement is sent at a multiple of the Sync Frequency relative to the hour. This means that if the materialization task shard restarts and completes its first transaction at 00:13, it will run its next transaction at 00:15 rather than00:28.  In actuality these computed points in time have some amount ofjitter applied to them to avoid overwhelming the system at common intervals, so setting a Sync Frequency to a specific value will ensure that transactions are predictably acknowledged that often, but makes no assumptions about precisely what time instants the acknowledgements will occur.  info The jitter value is deterministic based on the compute resource for the destination system from the materialization's endpoint configuration. How this compute resource is identified varies for different systems, but is usually something like &quot;account_name&quot; + &quot;warehouse_Name&quot;. This means that separate materializations using the same compute resource will synchronize their usage of that compute resource if they have the same Sync Schedule configured. ","version":"Next","tagName":"h2"},{"title":"Notifications","type":0,"sectionRef":"#","url":"/reference/notifications/","content":"","keywords":"","version":"Next"},{"title":"Data Movement Alerts​","type":1,"pageTitle":"Notifications","url":"/reference/notifications/#data-movement-alerts","content":" When navigating to the main view of a capture or a materialization, a user can select an interval for tracking zero data movement. Under the Notification Settings card, select a time interval from the dropdown labeled Interval. There is no need to save, but you must also have already configured notifications in order for the alert to take effect. If you are not yet subscribed to notifications, an info box will appear prompting you to set up a subscription by clicking on CLICK HERE.  If your task does not receive any new documents with the selected timeframe, an email will be sent to any email addresses that are subscribed to this tenant.  ","version":"Next","tagName":"h2"},{"title":"Billing Alerts​","type":1,"pageTitle":"Notifications","url":"/reference/notifications/#billing-alerts","content":" Billing alerts are automatically subscribed to when a user inputs their email into the Organization Notifications table. Alerts will be sent out for the following events:  Free Tier Started: A tenant has transitioned into the free trialFree Trial Ending: Five days are remaining in a tenant's free trialFree Trial Ended: A tenant's free trial has endedProvided Payment Method: A valid payment method has been provided for a tenant  ","version":"Next","tagName":"h2"},{"title":"Properties​","type":1,"pageTitle":"Notifications","url":"/reference/notifications/#properties","content":" Property\tTitle\tDescription\tType/catalogPrefix\tPrefix\tSubscribe to notifications for this tenant\tstring /email\tEmail\tAlert the following email with all notifications\tstring ","version":"Next","tagName":"h2"},{"title":"Reduction strategies","type":0,"sectionRef":"#","url":"/reference/reduction-strategies/","content":"","keywords":"","version":"Next"},{"title":"Reduction guarantees​","type":1,"pageTitle":"Reduction strategies","url":"/reference/reduction-strategies/#reduction-guarantees","content":" In Flow, documents that share the same collection key and are written to the same logical partition have a total order, meaning that one document is universally understood to have been written before the other.  This isn't true of documents of the same key written to different logical partitions. These documents can be considered “mostly” ordered: Flow uses timestamps to understand the relative ordering of these documents, and while this largely produces the desired outcome, small amounts of re-ordering are possible and even likely.  Flow guarantees exactly-once semantics within derived collections and materializations (so long as the target system supports transactions), and a document reduction will be applied exactly one time.  Flow does not guarantee that documents are reduced in sequential order, directly into a base document. For example, documents of a single Flow capture transaction are combined together into one document per collection key at capture time – and that document may be again combined with still others, and so on until a final reduction into the base document occurs.  Taken together, these total-order and exactly-once guarantees mean that reduction strategies must be associative [as in (2 + 3) + 4 = 2 + (3 + 4) ], but need not be commutative [ 2 + 3 = 3 + 2 ] or idempotent [ S u S = S ]. They expand the palette of strategies that can be implemented, and allow for more efficient implementations as compared to, for example CRDTs.  In this documentation, we’ll refer to the “left-hand side” (LHS) as the preceding document and the “right-hand side” (RHS) as the following one. Keep in mind that both the LHS and RHS may themselves represent a combination of still more ordered documents because, for example, reductions are applied associatively. ","version":"Next","tagName":"h3"},{"title":"Organizing a Flow catalog","type":0,"sectionRef":"#","url":"/reference/organizing-catalogs/","content":"","keywords":"","version":"Next"},{"title":"import​","type":1,"pageTitle":"Organizing a Flow catalog","url":"/reference/organizing-catalogs/#import","content":" Flow's import directive can help you easily handle all of these scenarios while keeping your catalogs well organized. Each catalog spec file may import any number of other files, and each import may refer to either relative or an absolute URL.  When you use import in a catalog spec, you're conceptually bringing the entirety of another catalog — as well as the schemas and typescript files it uses — into your catalog. Imports are also transitive, so when you import another catalog, you're also importing everything that other catalog has imported. This allows you to keep your catalogs organized, and is flexible enough to support collaboration between separate teams and organizations.  Perhaps the best way of explaining this is with some examples.  Example: Organizing collections​  Let's look at a relatively simple case in which you want to organize your collections into multiple catalog files. Say you work for Acme Corp on the team that's introducing Flow. You might start with the collections and directory structure below:  acme/customers/customerInfo acme/products/info/manufacturers acme/products/info/skus acme/products/inventory acme/sales/pending acme/sales/complete   acme ├── flow.yaml ├── customers │ ├── flow.ts │ ├── flow.yaml │ └── schemas.yaml ├── products │ ├── flow.yaml │ ├── info │ │ ├── flow.ts │ │ ├── flow.yaml │ │ └── schemas.yaml │ └── inventory │ ├── flow.ts │ ├── flow.yaml │ └── schemas.yaml schemas.yaml └── sales ├── flow.ts ├── flow.yaml └── schemas.yaml   It's immediately clear where each of the given collections is defined, since the directory names match the path segments in the collection names. This is not required by theflowctl CLI, but is strongly recommended, since it makes your catalogs more readable and maintainable. Each directory contains a catalog spec (flow.yaml), which will import all of the catalogs from child directories.  So, the top-level catalog spec, acme/flow.yaml, might look something like this:  import: - customers/flow.yaml - products/flow.yaml - sales/flow.yaml   This type of layout has a number of other advantages. During development, you can easily work with a subset of collections using, for example, flowctl test --source acme/products/flow.yaml to run only the tests for product-related collections. It also allows other imports to be more granular. For example, you might want a derivation under sales to read from acme/products/info. Since info has a separate catalog spec, acme/sales/flow.yaml can import acme/products/info/flow.yaml without creating a dependency on the inventory collection.  Example: Separate environments​  It's common to use separate environments for tiers like development, staging, and production. Flow catalog specs often necessarily include endpoint configuration for external systems that will hold materialized views. Let's say you want your production environment to materialize views to Snowflake, but you want to develop locally on SQLite. We might modify the Acme example slightly to account for this.  acme ├── dev.flow.yaml ├── prod.flow.yaml ... the remainder is the same as above   Each of the top-level catalog specs might import all of the collections and define an endpoint called ourMaterializationEndpoint that points to the desired system. The import block might be the same for each system, but each file may use a different configuration for the endpoint, which is used by any materializations that reference it.  Our configuration for our development environment will look like:  dev.flow.yaml import: - customers/flow.yaml - products/flow.yaml - sales/flow.yaml ourMaterializationEndpoint: # dev.flow.yaml sqlite: path: dev-materializations.db   While production will look like:  prod.flow.yaml import: - customers/flow.yaml - products/flow.yaml - sales/flow.yaml endpoints: snowflake: account: acme_production role: admin schema: snowflake.com/acmeProd user: importantAdmin password: abc123 warehouse: acme_production   When we test the draft locally, we'll work with dev.flow.yaml, but we'll publish prod.flow.yaml.  Everything will continue to work because in our development environment we'll be binding collections to our local SQLite DB and in production we'll use Snowflake.  Example: Cross-team collaboration​  When working across teams, it's common for one team to provide a data product for another to reference and use. Flow is designed for cross-team collaboration, allowing teams and users to reference each other's full catalog or schema.  Again using the Acme example, let's imagine we have two teams. Team Web is responsible for Acme's website, and Team User is responsible for providing a view of Acme customers that's always up to date. Since Acme wants a responsive site that provides a good customer experience, Team Web needs to pull the most up-to-date information from Team User at any point. Let's look at Team User's collections:  teamUser.flow.yaml import: - userProfile.flow.yaml   Which references:  userProfile.flow.yaml collection: userProfile: schema: -&quot;/userProfile/schema&quot; key: [/id]   Team User references files in their directory, which they actively manage in both their import and schema sections. If Team Web wants to access user data (and they have access), they can use a relative path or a URL-based path given that Team User publishes their data to a URL for access:  teamWeb.flow.yaml import: -http://www.acme.com/teamUser#userProfile.flow.yaml -webStuff.flow.yaml   Now Team Web has direct access to collections (referenced by their name) to build derived collections on top of. They can also directly import schemas:  webStuff.flow.yaml collection: webStuff: schema: -http://acme.com/teamUser#userProfile/#schema key: [/id]   ","version":"Next","tagName":"h3"},{"title":"Global namespace​","type":1,"pageTitle":"Organizing a Flow catalog","url":"/reference/organizing-catalogs/#global-namespace","content":" Every Flow collection has a name, and that name must be unique within a running Flow system. Flow collections should be thought of as existing within a global namespace. Keeping names globally unique makes it easy to import catalogs from other teams, or even other organizations, without having naming conflicts or ambiguities.  For example, imagine your catalog for the inside sales team has a collection just named customers. If you later try to import a catalog from the outside sales team that also contains a customers collection, 💥 there's a collision. A better collection name would be acme/inside-sales/customers. This allows a catalog to include customer data from separate teams, and also separate organizations.  Learn more about the Flow namespace. ","version":"Next","tagName":"h3"},{"title":"append","type":0,"sectionRef":"#","url":"/reference/reduction-strategies/append/","content":"append append works with arrays, and extends the left-hand array with items from the right-hand side. collections: - name: example/reductions/append schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Append only works with type &quot;array&quot;. # Others will throw an error at build time. type: array reduce: { strategy: append } required: [key] key: [/key] tests: &quot;Expect we can append arrays&quot;: - ingest: collection: example/reductions/append documents: - { key: &quot;key&quot;, value: [1, 2] } - { key: &quot;key&quot;, value: [3, null, &quot;abc&quot;] } - verify: collection: example/reductions/append documents: - { key: &quot;key&quot;, value: [1, 2, 3, null, &quot;abc&quot;] } The right-hand side must always be an array. The left-hand side may be null, in which case the reduction is treated as a no-op and its result remains null. This can be combined with schema conditionals to toggle whether reduction-reduction should be done or not.","keywords":"","version":"Next"},{"title":"firstWriteWins and lastWriteWins","type":0,"sectionRef":"#","url":"/reference/reduction-strategies/firstwritewins-and-lastwritewins/","content":"firstWriteWins and lastWriteWins firstWriteWins always takes the first value seen at the annotated location. Likewise, lastWriteWins always takes the last. Schemas that don’t have an explicit reduce annotation default to lastWriteWins behavior. collections: - name: example/reductions/fww-lww schema: type: object reduce: { strategy: merge } properties: key: { type: string } fww: { reduce: { strategy: firstWriteWins } } lww: { reduce: { strategy: lastWriteWins } } required: [key] key: [/key] tests: &quot;Expect we can track first- and list-written values&quot;: - ingest: collection: example/reductions/fww-lww documents: - { key: &quot;key&quot;, fww: &quot;one&quot;, lww: &quot;one&quot; } - { key: &quot;key&quot;, fww: &quot;two&quot;, lww: &quot;two&quot; } - verify: collection: example/reductions/fww-lww documents: - { key: &quot;key&quot;, fww: &quot;one&quot;, lww: &quot;two&quot; } ","keywords":"","version":"Next"},{"title":"Composing with conditionals","type":0,"sectionRef":"#","url":"/reference/reduction-strategies/composing-with-conditionals/","content":"Composing with conditionals Reduction strategies are JSON Schema annotations. As such, their applicability at a given document location can be controlled through the use of conditional keywords within the schema, like oneOf or if/then/else. This means Flow’s built-in strategies can be combined with schema conditionals to construct a wider variety of custom reduction behaviors. For example, here’s a reset-able counter: collections: - name: example/reductions/sum-reset schema: type: object properties: key: { type: string } value: { type: number } required: [key] # Use oneOf to express a tagged union over &quot;action&quot;. oneOf: # When action = reset, reduce by taking this document. - properties: { action: { const: reset } } reduce: { strategy: lastWriteWins } # When action = sum, reduce by summing &quot;value&quot;. Keep the LHS &quot;action&quot;, # preserving a LHS &quot;reset&quot;, so that resets are properly associative. - properties: action: const: sum reduce: { strategy: firstWriteWins } value: { reduce: { strategy: sum } } reduce: { strategy: merge } key: [/key] tests: &quot;Expect we can sum or reset numbers&quot;: - ingest: collection: example/reductions/sum-reset documents: - { key: &quot;key&quot;, action: sum, value: 5 } - { key: &quot;key&quot;, action: sum, value: -1.2 } - verify: collection: example/reductions/sum-reset documents: - { key: &quot;key&quot;, value: 3.8 } - ingest: collection: example/reductions/sum-reset documents: - { key: &quot;key&quot;, action: reset, value: 0 } - { key: &quot;key&quot;, action: sum, value: 1.3 } - verify: collection: example/reductions/sum-reset documents: - { key: &quot;key&quot;, value: 1.3 } ","keywords":"","version":"Next"},{"title":"sum","type":0,"sectionRef":"#","url":"/reference/reduction-strategies/sum/","content":"sum sum reduces two numbers or integers by adding their values. collections: - name: example/reductions/sum schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Sum only works with types &quot;number&quot; or &quot;integer&quot;. # Others will throw an error at build time. type: number reduce: { strategy: sum } required: [key] key: [/key] tests: &quot;Expect we can sum two numbers&quot;: - ingest: collection: example/reductions/sum documents: - { key: &quot;key&quot;, value: 5 } - { key: &quot;key&quot;, value: -1.2 } - verify: collection: example/reductions/sum documents: - { key: &quot;key&quot;, value: 3.8 } ","keywords":"","version":"Next"},{"title":"merge","type":0,"sectionRef":"#","url":"/reference/reduction-strategies/merge/","content":"merge merge reduces the left-hand side and right-hand side by recursively reducing shared document locations. The LHS and RHS must either both be objects, or both be arrays. If both sides are objects, merge performs a deep merge of each property. If LHS and RHS are both arrays, items at each index of both sides are merged together, extending the shorter of the two sides by taking items off the longer: collections: - name: example/reductions/merge schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Merge only works with types &quot;array&quot; or &quot;object&quot;. # Others will throw an error at build time. type: [array, object] reduce: { strategy: merge } # Deeply merge sub-locations (items or properties) by summing them. items: type: number reduce: { strategy: sum } additionalProperties: type: number reduce: { strategy: sum } required: [key] key: [/key] tests: &quot;Expect we can merge arrays by index&quot;: - ingest: collection: example/reductions/merge documents: - { key: &quot;key&quot;, value: [1, 1] } - { key: &quot;key&quot;, value: [2, 2, 2] } - verify: collection: example/reductions/merge documents: - { key: &quot;key&quot;, value: [3, 3, 2] } &quot;Expect we can merge objects by property&quot;: - ingest: collection: example/reductions/merge documents: - { key: &quot;key&quot;, value: { &quot;a&quot;: 1, &quot;b&quot;: 1 } } - { key: &quot;key&quot;, value: { &quot;a&quot;: 1, &quot;c&quot;: 1 } } - verify: collection: example/reductions/merge documents: - { key: &quot;key&quot;, value: { &quot;a&quot;: 2, &quot;b&quot;: 1, &quot;c&quot;: 1 } } Merge may also take a key, which is one or more JSON pointers that are relative to the reduced location. If both sides are arrays and a merge key is present, then a deep sorted merge of the respective items is done, as ordered by the key. Arrays must be pre-sorted and de-duplicated by the key, and merge itself always maintains this invariant. Note that you can use a key of [“”] for natural item ordering, such as merging sorted arrays of scalars. collections: - name: example/reductions/merge-key schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: type: array reduce: strategy: merge key: [/k] items: { reduce: { strategy: firstWriteWins } } required: [key] key: [/key] tests: &quot;Expect we can merge sorted arrays&quot;: - ingest: collection: example/reductions/merge-key documents: - { key: &quot;key&quot;, value: [{ k: &quot;a&quot;, v: 1 }, { k: &quot;b&quot;, v: 1 }] } - { key: &quot;key&quot;, value: [{ k: &quot;a&quot;, v: 2 }, { k: &quot;c&quot;, v: 2 }] } - verify: collection: example/reductions/merge-key documents: - { key: &quot;key&quot;, value: [{ k: &quot;a&quot;, v: 1 }, { k: &quot;b&quot;, v: 1 }, { k: &quot;c&quot;, v: 2 }], } As with append, the LHS of merge may be null, in which case the reduction is treated as a no-op and its result remains null.","keywords":"","version":"Next"},{"title":"minimize and maximize","type":0,"sectionRef":"#","url":"/reference/reduction-strategies/minimize-and-maximize/","content":"minimize and maximize minimize and maximize reduce by taking the smallest or largest seen value, respectively. collections: - name: example/reductions/min-max schema: type: object reduce: { strategy: merge } properties: key: { type: string } min: { reduce: { strategy: minimize } } max: { reduce: { strategy: maximize } } required: [key] key: [/key] tests: &quot;Expect we can min/max values&quot;: - ingest: collection: example/reductions/min-max documents: - { key: &quot;key&quot;, min: 32, max: &quot;abc&quot; } - { key: &quot;key&quot;, min: 42, max: &quot;def&quot; } - verify: collection: example/reductions/min-max documents: - { key: &quot;key&quot;, min: 32, max: &quot;def&quot; } minimize and maximize can also take a key, which is one or more JSON pointers that are relative to the reduced location. Keys make it possible to minimize and maximize over complex types by ordering over an extracted composite key. In the event that a right-hand side document key equals the current left-hand side minimum or maximum, the documents are deeply merged. This can be used to, for example, track not just the minimum value but also the number of times it’s been seen: collections: - name: example/reductions/min-max-key schema: type: object reduce: { strategy: merge } properties: key: { type: string } min: $anchor: min-max-value type: array items: - type: string - type: number reduce: { strategy: sum } reduce: strategy: minimize key: [/0] max: $ref: &quot;#min-max-value&quot; reduce: strategy: maximize key: [/0] required: [key] key: [/key] tests: &quot;Expect we can min/max values using a key extractor&quot;: - ingest: collection: example/reductions/min-max-key documents: - { key: &quot;key&quot;, min: [&quot;a&quot;, 1], max: [&quot;a&quot;, 1] } - { key: &quot;key&quot;, min: [&quot;c&quot;, 2], max: [&quot;c&quot;, 2] } - { key: &quot;key&quot;, min: [&quot;b&quot;, 3], max: [&quot;b&quot;, 3] } - { key: &quot;key&quot;, min: [&quot;a&quot;, 4], max: [&quot;a&quot;, 4] } - verify: collection: example/reductions/min-max-key documents: # Min of equal keys [&quot;a&quot;, 1] and [&quot;a&quot;, 4] =&gt; [&quot;a&quot;, 5]. - { key: &quot;key&quot;, min: [&quot;a&quot;, 5], max: [&quot;c&quot;, 2] } ","keywords":"","version":"Next"},{"title":"Time Travel","type":0,"sectionRef":"#","url":"/reference/time-travel/","content":"","keywords":"","version":"Next"},{"title":"How to configure time travel​","type":1,"pageTitle":"Time Travel","url":"/reference/time-travel/#how-to-configure-time-travel","content":" In the Flow web app, either navigate to an existing materialization or create a new one. Under Source Collections scroll to the bottom of a Resource Configuration for a specific collection. If you are working with a new materialization, you must link a collection to the materialization before continuing.  You'll find two optional date-time fields for implementing time travel: notBefore and notAfter. Click on either field to open a date/time picker that you can use to set the values. It's not mandatory to select values for both fields for time travel to take effect. However, selecting values for both fields will ensure that only data meeting both criteria is materialized. In other words, new data must fall before the notAfter date and after the notBefore date to be included in the materialization.  ","version":"Next","tagName":"h2"},{"title":"Specification​","type":1,"pageTitle":"Time Travel","url":"/reference/time-travel/#specification","content":" Alternatively, both fields can be defined in the Flow specification file with the following format:  materializations: # The name of the materialization. acmeCo/example/database-views: # Endpoint defines how to connect to the destination of the materialization. # Required, type: object endpoint: # This endpoint uses a connector provided as a Docker image. connector: # Docker image that implements the materialization connector. image: ghcr.io/estuary/materialize-mysql:dev # File that provides the connector's required configuration. # Configuration may also be presented inline. config: path/to//connector-config.yaml bindings: - # Source collection read by this binding. # Required, type: object or string source: # Name of the collection to be read. # Required. name: acmeCo/example/collection # Lower bound date-time for documents which should be processed. # Source collection documents published before this date-time are filtered. # `notBefore` is *only* a filter. Updating its value will not cause Flow # to re-process documents that have already been read. # Optional. Default is to process all documents. notBefore: 2023-01-23T01:00:00Z # Upper bound date-time for documents which should be processed. # Source collection documents published after this date-time are filtered. # Like `notBefore`, `notAfter` is *only* a filter. Updating its value will # not cause Flow to re-process documents that have already been read. # Optional. Default is to process all documents. notAfter: 2023-01-23T02:00:00Z   ","version":"Next","tagName":"h3"},{"title":"Properties​","type":1,"pageTitle":"Time Travel","url":"/reference/time-travel/#properties","content":" Property\tTitle\tDescription\tType/notBefore\tNot Before\tOnly include date before this time\tdate-time /notAfter\tNot After\tOnly include data after this time\tdate-time ","version":"Next","tagName":"h2"},{"title":"Working with logs and statistics","type":0,"sectionRef":"#","url":"/reference/working-logs-stats/","content":"","keywords":"","version":"Next"},{"title":"Accessing logs and statistics​","type":1,"pageTitle":"Working with logs and statistics","url":"/reference/working-logs-stats/#accessing-logs-and-statistics","content":" You can access logs and statistics in the Flow web app, by materializing them to an external endpoint, or from the command line.  ","version":"Next","tagName":"h2"},{"title":"Logs and statistics in the Flow web app​","type":1,"pageTitle":"Working with logs and statistics","url":"/reference/working-logs-stats/#logs-and-statistics-in-the-flow-web-app","content":" You can view a subset of logs and statistics for individual tasks in the Flow web app.  Logs​  After you publish a new capture or materialization, a pop-up window appears that displays the task's logs. Once you close the window, you can't regain access to the full logs in the web app. For a complete view of logs, use flowctl or materialize the logs collection to an outside system.  However, if a task fails, you can view the logs associated with the error(s) that caused the failure. In the Details view of the published capture or materialization, click the name of its shard to display the logs.  Statistics​  Two statistics are shown for each capture, collection, and materialization:  Bytes Written or Read. This corresponds to the bytesTotal property of the stats collection.Docs Written or Read. This corresponds to the docsTotal property of the stats collection.  These fields have slightly different meanings for each Flow entity type:  For captures, Bytes Written and Docs Written represent the total data written across all of the capture's associated collections.For collections, Bytes Written and Docs Written represent the data written to the collection from its associated capture or derivation.For materializations, Bytes Read and Docs Read represent the total data read from all of the materialization's associated collections.  ","version":"Next","tagName":"h3"},{"title":"Accessing logs and statistics from the command line​","type":1,"pageTitle":"Working with logs and statistics","url":"/reference/working-logs-stats/#accessing-logs-and-statistics-from-the-command-line","content":" The flowctl logs and flowctl stats subcommands allow you to print logs and stats, respectively, from the command line. This method allows more flexibility and is ideal for debugging.  You can retrieve logs and stats for any published Flow task. For example:  flowctl logs --task acmeCo/anvils/capture-one flowctl stats --task acmeCo/anvils/capture-one --uncommitted   Beta The --uncommitted flag is currently required for flowctl stats. This means that all statistics are read, regardless of whether they are about a successfully committed transaction, or a transaction that was rolled back or uncommitted. In the future, committed reads will be the default.  Printing logs or stats since a specific time​  To limit output, you can retrieve logs are stats starting at a specific time in the past. For example:  flowctl stats --task acmeCo/anvils/materialization-one --since 1h   ...will retrieve stats from approximately the last hour. The actual start time will always be at the previous fragment boundary, so it can be significantly before the requested time period.  Additional options for flowctl logs and flowctl stats can be accessed through command-line help.  ","version":"Next","tagName":"h3"},{"title":"Available statistics​","type":1,"pageTitle":"Working with logs and statistics","url":"/reference/working-logs-stats/#available-statistics","content":" Available statistics include information about the amount of data in inputs and outputs of each transaction. They also include temporal information about the transaction. Statistics vary by task type (capture, materialization, or derivation).  A thorough knowledge of Flow's advanced concepts is necessary to effectively leverage these statistics.  stats collection documents include the following properties.  ","version":"Next","tagName":"h2"},{"title":"Shard information​","type":1,"pageTitle":"Working with logs and statistics","url":"/reference/working-logs-stats/#shard-information","content":" A stats document begins with data about the shard processing the transaction. Each processing shard is uniquely identified by the combination of its name, keyBegin, and rClockBegin. This information is important for tasks with multiple shards: it allows you to determine whether data throughput is evenly distributed amongst those shards.  Property\tDescription\tData Type\tApplicable Task Type/shard\tFlow shard information\tobject\tAll /shard/kind\tThe type of catalog task. One of &quot;capture&quot;, &quot;derivation&quot;, or &quot;materialization&quot;\tstring\tAll /shard/name\tThe name of the catalog task (without the task type prefix)\tstring\tAll /shard/keyBegin\tWith rClockBegin, this comprises the shard ID. The inclusive beginning of the shard's assigned key range.\tstring\tAll /shard/rClockBegin\tWith keyBegin, this comprises the shard ID. The inclusive beginning of the shard's assigned rClock range.\tstring\tAll  ","version":"Next","tagName":"h3"},{"title":"Transaction information​","type":1,"pageTitle":"Working with logs and statistics","url":"/reference/working-logs-stats/#transaction-information","content":" stats documents include information about a transaction: its inputs and outputs, the amount of data processed, and the time taken. You can use this information to ensure that your Flow tasks are running efficiently, and that the amount of data processed matches your expectations.  Property\tDescription\tData Type\tApplicable Task Type/ts\tTimestamp corresponding to the start of the transaction, rounded to the nearest minute\tstring\tAll /openSecondsTotal\tTotal time that the transaction was open before starting to commit\tnumber\tAll /txnCount\tTotal number of transactions represented by this stats document. Used for reduction.\tinteger\tAll /capture\tCapture stats, organized by collection\tobject\tCapture /materialize\tMaterialization stats, organized by collection\tobject\tMaterialization /derive\tDerivation statistics\tobject\tDerivation /&lt;task-type&gt;/&lt;collection-name&gt;/right/\tInput documents from a the task's source\tobject\tCapture, materialization /&lt;task-type&gt;/&lt;collection-name&gt;/left/\tInput documents from an external destination; used for reduced updates in materializations\tobject\tMaterialization /&lt;task-type&gt;/&lt;collection-name&gt;/out/\tOutput documents from the transaction\tobject\tAll /&lt;task-type&gt;/{}/docsTotal\tTotal number of documents\tinteger\tAll /&lt;task-type&gt;/{}/bytesTotal\tTotal number of bytes representing the JSON encoded documents\tinteger\tAll /derivations/transforms/transformStats\tStats for a specific transform of a derivation, which will have an update, publish, or both\tobject\tDerivation /derivations/transforms/transformStats/input\tThe input documents that were fed into this transform\tobject\tDerivation /derivations/transforms/transformStats/update\tThe outputs from update lambda invocations, which were combined into registers\tobject\tDerivation /derivations/transforms/transformStats/publish\tThe outputs from publish lambda invocations.\tobject\tDerivation /derivations/registers/createdTotal\tThe total number of new register keys that were created\tinteger\tDerivation ","version":"Next","tagName":"h3"},{"title":"set","type":0,"sectionRef":"#","url":"/reference/reduction-strategies/set/","content":"set set interprets the document location as an update to a set. The location must be an object having only “add&quot;, “intersect&quot;, and “remove” properties. Any single “add&quot;, “intersect&quot;, or “remove” is always allowed. A document with “intersect” and “add” is allowed, and is interpreted as applying the intersection to the left-hand side set, followed by a union with the additions. A document with “remove” and “add” is also allowed, and is interpreted as applying the removals to the base set, followed by a union with the additions. “remove” and “intersect” within the same document are prohibited. Set additions are deeply merged. This makes sets behave like associative maps, where the “value” of a set member can be updated by adding it to the set again, with a reducible update. Sets may be objects, in which case the object property serves as the set item key: collections: - name: example/reductions/set schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Sets are always represented as an object. type: object reduce: { strategy: set } # Schema for &quot;add&quot;, &quot;intersect&quot;, and &quot;remove&quot; properties # (each a map of keys and their associated sums): additionalProperties: type: object additionalProperties: type: number reduce: { strategy: sum } # Flow requires that all parents of locations with a reduce # annotation also have one themselves. # This strategy therefore must (currently) be here, but is ignored. reduce: { strategy: lastWriteWins } required: [key] key: [/key] tests: &quot;Expect we can apply set operations to incrementally build associative maps&quot;: - ingest: collection: example/reductions/set documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 1, &quot;b&quot;: 1, &quot;c&quot;: 1 } } } - { key: &quot;key&quot;, value: { &quot;remove&quot;: { &quot;b&quot;: 0 } } } - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 1, &quot;d&quot;: 1 } } } - verify: collection: example/reductions/set documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 2, &quot;c&quot;: 1, &quot;d&quot;: 1 } } } - ingest: collection: example/reductions/set documents: - { key: &quot;key&quot;, value: { &quot;intersect&quot;: { &quot;a&quot;: 0, &quot;d&quot;: 0 } } } - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 1, &quot;e&quot;: 1 } } } - verify: collection: example/reductions/set documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 3, &quot;d&quot;: 1, &quot;e&quot;: 1 } } } Sets can also be sorted arrays, which are ordered using a provide key extractor. Keys are given as one or more JSON pointers, each relative to the item. As with merge, arrays must be pre-sorted and de-duplicated by the key, and set reductions always maintain this invariant. Use a key extractor of [“”] to apply the natural ordering of scalar values. Whether array or object types are used, the type must always be consistent across the “add” / “intersect” / “remove” terms of both sides of the reduction. collections: - name: example/reductions/set-array schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Sets are always represented as an object. type: object reduce: strategy: set key: [/0] # Schema for &quot;add&quot;, &quot;intersect&quot;, &amp; &quot;remove&quot; properties # (each a sorted array of [key, sum] 2-tuples): additionalProperties: type: array # Flow requires that all parents of locations with a reduce # annotation also have one themselves. # This strategy therefore must (currently) be here, but is ignored. reduce: { strategy: lastWriteWins } # Schema for contained [key, sum] 2-tuples: items: type: array items: - type: string - type: number reduce: { strategy: sum } reduce: { strategy: merge } required: [key] key: [/key] tests: ? &quot;Expect we can apply operations of sorted-array sets to incrementally build associative maps&quot; : - ingest: collection: example/reductions/set-array documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 1], [&quot;b&quot;, 1], [&quot;c&quot;, 1]] } } - { key: &quot;key&quot;, value: { &quot;remove&quot;: [[&quot;b&quot;, 0]] } } - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 1], [&quot;d&quot;, 1]] } } - verify: collection: example/reductions/set-array documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 2], [&quot;c&quot;, 1], [&quot;d&quot;, 1]] } } - ingest: collection: example/reductions/set-array documents: - { key: &quot;key&quot;, value: { &quot;intersect&quot;: [[&quot;a&quot;, 0], [&quot;d&quot;, 0]] } } - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 1], [&quot;e&quot;, 1]] } } - verify: collection: example/reductions/set-array documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 3], [&quot;d&quot;, 1], [&quot;e&quot;, 1]] } } ","keywords":"","version":"Next"}],"options":{"excludeRoutes":["blog/**/*"],"id":"default"}}