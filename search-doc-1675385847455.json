[{"title":"Journals","type":0,"sectionRef":"#","url":"concepts/advanced/journals/","content":"","keywords":""},{"title":"Specification​","type":1,"pageTitle":"Journals","url":"concepts/advanced/journals/#specification","content":"Flow collections can control some aspects of how their contents are mapped into journals through the journals stanza: collections: acmeCo/orders: schema: orders.schema.yaml key: [/id] journals: # Configuration for journal fragments. # Required, type: object. fragments: # Codec used to compress fragment files. # One of ZSTANDARD, SNAPPY, GZIP, or NONE. # Optional. Default is GZIP. compressionCodec: GZIP # Maximum flush delay before in-progress fragment buffers are closed # and persisted. Default uses no flush interval. # Optional. Given as a time duration. flushInterval: 15m # Desired content length of each fragment, in megabytes before compression. # Default is 512MB. # Optional, type: integer. length: 512 # Duration for which historical files of the collection should be kept. # Default is forever. # Optional. Given as a time duration. retention: 720h  Your storage mappings determine which of your cloud storage buckets is used for storage of collection fragment files. "},{"title":"Physical partitions​","type":1,"pageTitle":"Journals","url":"concepts/advanced/journals/#physical-partitions","content":"Every logical partition of a Flow collection is created with a single physical partition. Later and as required, new physical partitions are added in order to increase the write throughput of the collection. Each physical partition is responsible for all new writes covering a range of keys occurring in collection documents. Conceptually, if keys range from [A-Z] then one partition might cover [A-F] while another covers [G-Z]. The pivot of a partition reflects the first key in its covered range. One physical partition is turned into more partitions by subdividing its range of key ownership. For instance, a partition covering [A-F]is split into partitions [A-C] and [D-F]. Physical partitions are journals. The relationship between the journal and its specific collection and logical partition are encoded withinits journal specification. "},{"title":"Fragment files​","type":1,"pageTitle":"Journals","url":"concepts/advanced/journals/#fragment-files","content":"Journal fragment files each hold a slice of your collection's content, stored as a compressed file of newline-delimited JSON documents in your cloud storage bucket. Files are flushed to cloud storage periodically, typically after they reach a desired size threshold. They use a content-addressed naming scheme which allows Flow to understand how each file stitches into the overall journal. Consider a fragment file path like: s3://acmeCo-bucket/acmeCo/orders/category=Anvils/pivot=00/utc_date=2022-01-07/utc_hour=19/0000000000000000-00000000201a3f27-1ec69e2de187b7720fb864a8cd6d50bb69cc7f26.gz This path has the following components: Component\tExampleStorage prefix of physical partition\ts3://acmeCo-bucket/acmeCo/orders/category=Anvils/pivot=00/ Supplemental time pseudo-partitions\tutc_date=2022-01-07/utc_hour=19/ Beginning content offset\t0000000000000000 Ending content offset\t00000000201a3f27 SHA content checksum\t1ec69e2de187b7720fb864a8cd6d50bb69cc7f26 Compression codec\t.gz The supplemental time pseudo-partitions are not logical partitions, but are added to each fragment file path to facilitate integration with external tools that understand Hive layouts. "},{"title":"Hive layouts​","type":1,"pageTitle":"Journals","url":"concepts/advanced/journals/#hive-layouts","content":"As we've seen, collection fragment files are written to cloud storage with path components like/category=Anvils/pivot=00/utc_date=2022-01-07/utc_hour=19/. If you've used tools within the Apache Hive ecosystem, this layout should feel familiar. Flow organizes files in this way to make them directly usable by tools that understand Hive partitioning, like Spark and Hive itself. Collections can also be integrated as Hive-compatible external tables in tools likeSnowflakeandBigQueryfor ad-hoc analysis. SQL queries against these tables can even utilize predicate push-down, taking query predicates over category, utc_date, and utc_hourand pushing them down into the selection of files that must be read to answer the query — often offering much faster and more efficient query execution because far less data must be read. "},{"title":"Concepts","type":0,"sectionRef":"#","url":"concepts/","content":"","keywords":""},{"title":"Working with Flow​","type":1,"pageTitle":"Concepts","url":"concepts/#working-with-flow","content":"There are two main environments in which you can work with Flow: the web application or your preferred local environment using the flowctl command-line tool. "},{"title":"Web application​","type":1,"pageTitle":"Concepts","url":"concepts/#web-application","content":"The Flow web application is where you'll most likely create, monitor, and update your Data Flows. You can find it at dashboard.estuary.dev. The app is backed by secure, cloud-hosted infrastructure that Estuary manages. Take a tour of the web app. Create your first Data Flow with the web app. "},{"title":"flowctl​","type":1,"pageTitle":"Concepts","url":"concepts/#flowctl","content":"flowctl is a command-line interface for working with Flow's public API. Using flowctl, developers can inspect, edit, test, and publish Data Flows — just as with the web application. For example, you can create a Data Flow with the web app, and then use flowctl to fetch it into files that you manage within a Git repo. Learn more about flowctl "},{"title":"Essential concepts​","type":1,"pageTitle":"Concepts","url":"concepts/#essential-concepts","content":"In this section, you'll find the most important Flow terms and concepts. If you're new to Flow, start here. A complete end-to-end Data Flow between two systems has three components: Capture: Flow ingests data from an outside source.Collections: Flow maintains the captured data in cloud storage.Materialization: Flow pushes data to an outside destination. graph LR; Capture--&gt;Collection; Collection--&gt;Materialization; It may also include: Derivations: You apply a transformation to data in a collection, resulting in a new collection. All of these entities are described in the catalog. "},{"title":"Catalog​","type":1,"pageTitle":"Concepts","url":"concepts/#catalog","content":"The catalog is the set of active entities that comprise all Data Flows: captures, materializations, derivations, collections, schemas, tests, and more. All catalog entities are defined in Flow specification files — either manually, by you, or generated by the Flow web app. You create the specifications as drafts, and publish them to add them to the catalog. You can mix and match catalog entities to create a variety of Data Flows. Learn more about the catalog  "},{"title":"Collections​","type":1,"pageTitle":"Concepts","url":"concepts/#collections","content":"Collections represent datasets within Flow. All captured documents are written to a collection, and all materialized documents are read from a collection. Collections are a real-time data lake. Like a traditional data lake, the documents that make up a collection are stored as plain JSON in your cloud storage bucket. Unlike a traditional data lake, updates to the collection are reflected downstream in the data flow within milliseconds. Documents in collections are stored indefinitely in your cloud storage bucket (or may be managed with your regular bucket lifecycle policies). This means that the full historical content of a collection is available to support future data operations and perform backfills without going back to the source. Each collection has a keyed schema against which incoming documents are validated. This ensures that data is always clean and organized. Learn more about collections  "},{"title":"Captures​","type":1,"pageTitle":"Concepts","url":"concepts/#captures","content":"A capture is a Flow task that ingests data from an external source into one or more Flow collections. Documents continuously move from the source into Flow; as new documents become available at the source, Flow validates their schema and adds them to their corresponding collection. Captures interface with source systems using connectors. Learn more about captures  "},{"title":"Materializations​","type":1,"pageTitle":"Concepts","url":"concepts/#materializations","content":"A materialization is a Flow task that pushes data from one or more collections to an external destination. Documents continuously moves from each Flow collection to the destination. Materializations are the conceptual inverse of captures. As new documents become available within bound collections, the materialization keeps the destination up to date within milliseconds, or as fast as that system allows. Materializations interface with destinations using connectors. Learn more about materializations  "},{"title":"Endpoints​","type":1,"pageTitle":"Concepts","url":"concepts/#endpoints","content":"Endpoints are the source systems from which Flow captures data and the destination systems to which Flow materializes data. All kinds of data systems can be endpoints, including databases, key/value stores, streaming pub/sub systems, SaaS products, and cloud storage locations. Flow connects to this wide variety of endpoints using connectors.  "},{"title":"Connectors​","type":1,"pageTitle":"Concepts","url":"concepts/#connectors","content":"Connectors are plugin components that allow Flow to interface with endpoint data systems. They power captures and materializations. Flow uses an open-source connector model. Many connectors are made by Estuary, and others are made by third parties. Because connectors are open-source and kept separate from Flow itself, new integrations can be added and updated quickly. This is important, as the landscape of data systems and platforms is constantly evolving. All currently supported connectors are ready to use in the Flow web application. They're also available as Docker images, each encapsulating the details of working with a particular source or destination system. Learn more about connectors  "},{"title":"Intermediate concepts​","type":1,"pageTitle":"Concepts","url":"concepts/#intermediate-concepts","content":"In this section, you'll find important concepts that are optional for basic usage. Read this to unlock more powerful workflows. "},{"title":"Derivations​","type":1,"pageTitle":"Concepts","url":"concepts/#derivations","content":"A derivation is a collection that results from the transformation of one or more other collections, which is continuously updated in sync with its source collection(s). You can use derivations to map, reshape, and filter documents. They can also be used to tackle complex stateful streaming workflows, including joins and aggregations, without windowing and scaling limitations. Learn more about derivations  "},{"title":"Schemas​","type":1,"pageTitle":"Concepts","url":"concepts/#schemas","content":"All collections in Flow have an associatedJSON schemaagainst which documents are validated every time they're written or read. Schemas are critical to how Flow ensures the integrity of your data. Flow validates your documents to ensure that bad data doesn't make it into your collections — or worse, into downstream data products! JSON schema is a flexible standard for representing structure, invariants, and other constraints over your documents. Schemas can be very permissive, highly exacting, or somewhere in between. Flow pauses catalog tasks when documents don't match the collection schema, alerting you to the mismatch and allowing you to fix it before it creates a bigger problem. Learn more about schemas  "},{"title":"Reductions​","type":1,"pageTitle":"Concepts","url":"concepts/#reductions","content":"Every Flow collection schema includes a key. The key is used to identify collection documents and determine how they are grouped. When a collection is materialized into a database table, for example, its key becomes the SQL primary key of the materialized table. Flow also uses the key to reduce documents in collections, making storage and materializations more efficient. If multiple documents of a given key are added to a collection, by default, the most recent document supersedes all previous documents of that key. You can exert more control over your data by changing the default reduction strategy. By doing so, you can deeply merge documents, maintain running counts, and achieve other complex aggregation behaviors. Learn more about reductions  "},{"title":"Tests​","type":1,"pageTitle":"Concepts","url":"concepts/#tests","content":"Tests become an important part of your Data Flows when you add derivations and customized reduction behavior. You use tests to verify the end-to-end behavior of your collections and derivations. A test is a sequence of ingestion or verification steps. Ingestion steps ingest one or more document fixtures into a collection, and verification steps assert that the contents of another derived collection match a test expectation. Learn more about tests  "},{"title":"Tasks​","type":1,"pageTitle":"Concepts","url":"concepts/#tasks","content":"Captures, derivations, and materializations are collectively referred to as catalog tasks. They are the &quot;active&quot; components of a Data Flow, each running continuously and reacting to documents as they become available. Collections, by way of comparison, are inert. They reflect data at rest, and are acted upon by catalog tasks: A capture adds documents to a collection pulled from a source endpoint.A derivation updates a collection by applying transformations to other collections.A materialization reacts to changes of a collection to update a destination endpoint.  "},{"title":"Resources and bindings​","type":1,"pageTitle":"Concepts","url":"concepts/#resources-and-bindings","content":"A resource is an addressable collection of data within a source or destination system. The exact meaning of a resource is up to the endpoint and its connector. For example: Resources of a database endpoint might be its individual tables.Resources of a Kafka cluster might be its topics.Resources of a SaaS connector might be its various API feeds. When you create capture or materialization, it connects a collection to a resource through a binding. A given capture or materialization may have multiple bindings, which connect multiple collections to different resources.  "},{"title":"Storage mappings​","type":1,"pageTitle":"Concepts","url":"concepts/#storage-mappings","content":"Flow collections use cloud storage buckets for the durable storage of data. Storage mappings define how Flow maps your various collections into your storage buckets and prefixes. Learn more about storage mappings "},{"title":"Advanced concepts​","type":1,"pageTitle":"Concepts","url":"concepts/#advanced-concepts","content":"This section discusses advanced Flow concepts. The information here unlocks a more technical understanding of how Flow works, and may be helpful in advanced use cases. "},{"title":"Journals​","type":1,"pageTitle":"Concepts","url":"concepts/#journals","content":"Journals provide the low-level storage for Flow collections. Each logical and physical partition of a collection is backed by a journal. Task shards also use journals to provide for their durability and fault tolerance. Each shard has an associated recovery log, which is a journal into which internal checkpoint states are written. Learn more about journals "},{"title":"Task shards​","type":1,"pageTitle":"Concepts","url":"concepts/#task-shards","content":"Task shards are the unit of execution for a catalog task. A single task can have many shards, which allow the task to scale across many machines to achieve more throughput and parallelism. Shards are created and managed by the Flow runtime. Each shard represents a slice of the overall work of the catalog task, including its processing status and associated internal checkpoints. Catalog tasks are created with a single shard, which can be repeatedly subdivided at any time — with no downtime — to increase the processing capacity of the task. Learn more about shards "},{"title":"Projections​","type":1,"pageTitle":"Concepts","url":"concepts/#projections","content":"Flow leverages your JSON schemas to produce other types of schemas as needed, such as TypeScript types and SQL CREATE TABLE statements. In many cases these projections provide comprehensive end-to-end type safety of Data Flows and their TypeScript transformations, all statically verified at build time. Learn more about projections "},{"title":"Logs and statistics","type":0,"sectionRef":"#","url":"concepts/advanced/logs-stats/","content":"","keywords":""},{"title":"Logs​","type":1,"pageTitle":"Logs and statistics","url":"concepts/advanced/logs-stats/#logs","content":"Each organization that uses Flow has a logs collection under the global ops prefix. For the organization Acme Co, it would have the name ops/acmeCo/logs. These can be thought of as standard application logs: they store information about events that occur at runtime. They’re distinct from recovery logs, which track the state of various task shards. Regardless of how many Data Flows your organization has, all logs are stored in the same collection, which is read-only and logically partitioned on tasks. Logs are collected from events that occur within the Flow runtime, as well as the capture and materialization connectors your Data Flow is using. "},{"title":"Log level​","type":1,"pageTitle":"Logs and statistics","url":"concepts/advanced/logs-stats/#log-level","content":"You can set the log level for each catalog task to control the level of detail at which logs are collected for that task. The available levels, listed from least to most detailed, are: error: Non-recoverable errors from the Flow runtime or connector that are critical to know aboutwarn: Errors that can be re-tried, but likely require investigationinfo: Task lifecycle events, or information you might want to collect on an ongoing basisdebug: Details that will help debug an issue with a tasktrace: Maximum level of detail that may yield gigabytes of logs The default log level is info. You can change a task’s log level by adding the shards keyword to its definition in the catalog spec: materializations: acmeCo/debugMaterialization: shards: logLevel: debug endpoint: {}  "},{"title":"Statistics​","type":1,"pageTitle":"Logs and statistics","url":"concepts/advanced/logs-stats/#statistics","content":"Each organization that uses Flow has a stats collection under the global ops prefix. For the organization Acme Co, it would have the name ops/acmeCo/stats. Regardless of how many Data Flows your organization has, all stats are stored in the same collection, which is read-only and logically partitioned on tasks. A new document is published to the stats collection for each task transaction. Each document includes information about the time and quantity of data inputs and outputs. Statistics vary by task type (capture, materialization, or derivation). Use stats to: Evaluate the data throughput of a task; for example, a derivation.Compare a data throughput of a task between platforms; for example, compare reported data capture by Flow to detected change rate in a source system.Access the same information used by Estuary for billing.Optimize your tasks for increased efficiency. See a detailed table of the properties included in stats documents. "},{"title":"Working with logs and statistics​","type":1,"pageTitle":"Logs and statistics","url":"concepts/advanced/logs-stats/#working-with-logs-and-statistics","content":"Learn more about working with logs and statistics "},{"title":"Task shards","type":0,"sectionRef":"#","url":"concepts/advanced/shards/","content":"","keywords":""},{"title":"Shard splits​","type":1,"pageTitle":"Task shards","url":"concepts/advanced/shards/#shard-splits","content":"When a task is first created, it is initialized with a single shard. Later and as required, shards may be split into two shards. This is done by the service operator on your behalf, depending on the size of your task. Shard splitting doesn't require downtime; your task will continue to run as normal on the old shard until the split occurs and then shift seamlessly to the new, split shards. This process can be repeated as needed until your required throughput is achieved. If you have questions about how shards are split for your tasks, contact your Estuary account representative. "},{"title":"Transactions​","type":1,"pageTitle":"Task shards","url":"concepts/advanced/shards/#transactions","content":"Shards process messages in dynamic transactions. Whenever a message is ready to be processed by the task (when new documents appear at the source endpoint or collection), a new transaction is initiated. The transaction will continue so long as further messages are available for processing. When no more messages are immediately available, the transaction closes. A new transaction is started whenever the next message is available. In general, shorter transaction durations decrease latency, while longer transaction durations increase efficiency. Flow automatically balances these two extremes to optimize each task, but it may be useful in some cases to control transaction duration. For example, materializations to large analytical warehouses may benefit from longer transactions, which can reduce cost by performing more data reduction before landing data in the warehouse. Some endpoint systems, like BigQuery, limit the number of table operations you can perform. Longer transaction durations ensure that you don't exceed these limits. You can set the minimum and maximum transaction duration in a task's shards configuration. "},{"title":"Recovery logs​","type":1,"pageTitle":"Task shards","url":"concepts/advanced/shards/#recovery-logs","content":"All task shards have associated state, which is managed in the shard's store. Capture tasks must track incremental checkpoints of their endpoint connectors.Derivation tasks manage a potentially very large index of registers, as well as read checkpoints of sourced collection journals.Materialization tasks track incremental checkpoints of their endpoint connectors, as well as read checkpoints of sourced collection journals. Shard stores userecovery logsto replicate updates and implement transaction semantics. Recovery logs are regular journals, but hold binary data and are not intended for direct use. However, they can hold your user data. Recovery logs of derivations hold your derivation register values. Recovery logs are stored in your cloud storage bucket, and must have a configured storage mapping. "},{"title":"Projections","type":0,"sectionRef":"#","url":"concepts/advanced/projections/","content":"","keywords":""},{"title":"Logical partitions​","type":1,"pageTitle":"Projections","url":"concepts/advanced/projections/#logical-partitions","content":"Projections can also be used to logically partition a collection, specified as a longer-form variant of a projection definition: collections: acmeCo/user-sessions: schema: session.schema.yaml key: [/user/id, /timestamp] projections: country: location: /country partition: true device: location: /agent/type partition: true network: location: /agent/network partition: true  Logical partitions isolate the storage of documents by their differing values for partitioned fields. Flow extracts partitioned fields from each document, and every unique combination of partitioned fields is a separate logical partition. Every logical partition has one or more physical partitionsinto which their documents are written, which in turn controls how files are arranged within cloud storage. For example, a document of &quot;acmeCo/user-sessions&quot; like: {&quot;country&quot;: &quot;CA&quot;, &quot;agent&quot;: {&quot;type&quot;: &quot;iPhone&quot;, &quot;network&quot;: &quot;LTE&quot;}, ...}  Might produce files in cloud storage like: s3://bucket/example/sessions/country=CA/device=iPhone/network=LTE/pivot=00/utc_date=2020-11-04/utc_hour=16/&lt;name&gt;.gz  info country, device, and network together identify a logical partition, while pivot identifies a physical partition.utc_date and utc_hour is the time at which the journal fragment was created. Learn more about physical partitions. "},{"title":"Partition selectors​","type":1,"pageTitle":"Projections","url":"concepts/advanced/projections/#partition-selectors","content":"When reading from a collection, Flow catalog entities like derivations, materializations, and tests can provide a partition selector, which identifies the subset of partitions that should be read from a source collection: # Partition selectors are included as part of a larger entity, # such as a derivation or materialization. partitions: # `include` selects partitioned fields and corresponding values that # must be matched in order for a partition to be processed. # All of the included fields must be matched. # Default: All partitions are included. type: object include: # Include partitions from North America. country: [US, CA] # AND where the device is a mobile phone. device: [iPhone, Android] # `exclude` selects partitioned fields and corresponding values which, # if matched, exclude the partition from being processed. # A match of any of the excluded fields will exclude the partition. # Default: No partitions are excluded. type: object exclude: # Skip sessions which were over a 3G network. network: [&quot;3G&quot;]  Partition selectors are efficient as they allow Flow to altogether avoid reading documents that aren’t needed. "},{"title":"Catalog","type":0,"sectionRef":"#","url":"concepts/catalogs/","content":"","keywords":""},{"title":"Data Flows​","type":1,"pageTitle":"Catalog","url":"concepts/catalogs/#data-flows","content":"You can mix and match catalog entities to create a variety of Data Flows. The simplest Data Flow has just three entities. graph LR; Capture--&gt;Collection; Collection--&gt;Materialization; It may also be more complex, combining multiple entities of each type. graph LR; capture/two--&gt;collection/D; capture/one--&gt;collection/C; capture/one--&gt;collection/A; collection/A--&gt;derivation/B; collection/D--&gt;derivation/E; collection/C--&gt;derivation/E; derivation/B--&gt;derivation/E; collection/D--&gt;materialization/one; derivation/E--&gt;materialization/two; "},{"title":"Flow specification files​","type":1,"pageTitle":"Catalog","url":"concepts/catalogs/#flow-specification-files","content":"Catalog entities are defined and described in Flow specification files.These YAML files contain the configuration details that each entity requires. You work on specification files as drafts before you publish them to a catalog. There are two ways to create and work with specification files. "},{"title":"In the Flow web app​","type":1,"pageTitle":"Catalog","url":"concepts/catalogs/#in-the-flow-web-app","content":"You don't need to write or edit the specification files directly — the web app is designed to generate them for you. You do have the option to review and edit the generated specification as you create captures and materializations using the Catalog Editor. "},{"title":"With flowctl​","type":1,"pageTitle":"Catalog","url":"concepts/catalogs/#with-flowctl","content":"If you prefer a developer workflow, you can also work with specification files directly in your local environment using flowctl. You then publish them back to the catalog. A given Data Flow may be described by one specification file, or by many, so long as a top-level file imports all the others. The files use the extension *.flow.yaml or are simply named flow.yaml by convention. Using this extension activates Flow's VS Code integration and auto-complete. Flow integrates with VS Code for development environment support, like auto-complete, tooltips, and inline documentation. Depending on your Data Flow, you may also have TypeScript modules, JSON schemas, or test fixtures. "},{"title":"Namespace​","type":1,"pageTitle":"Catalog","url":"concepts/catalogs/#namespace","content":"All catalog entities (captures, materializations, and collections) are identified by a namesuch as acmeCo/teams/manufacturing/anvils. Names have directory-like prefixes and every name within Flow is globally unique. If you've ever used database schemas to organize your tables and authorize access, you can think of name prefixes as being akin to database schemas with arbitrary nesting. All catalog entities exist together in a single namespace. As a Flow customer, you're provisioned one or more high-level prefixes for your organization. Further division of the namespace into prefixes is up to you. Prefixes of the namespace, like acmeCo/teams/manufacturing/, are the foundation for Flow's authorization model. "},{"title":"Captures","type":0,"sectionRef":"#","url":"concepts/captures/","content":"","keywords":""},{"title":"Pull captures​","type":1,"pageTitle":"Captures","url":"concepts/captures/#pull-captures","content":"Pull captures pull data from an endpoint using a connector. "},{"title":"Estuary sources​","type":1,"pageTitle":"Captures","url":"concepts/captures/#estuary-sources","content":"Estuary builds and maintains many real-time connectors for various technology systems, such as database change data capture (CDC) connectors. See the source connector reference documentation. "},{"title":"Airbyte sources​","type":1,"pageTitle":"Captures","url":"concepts/captures/#airbyte-sources","content":"Flow also natively supports Airbyte source connectors. These connectors tend to focus on SaaS APIs, and do not offer real-time streaming integrations. Flow runs the connector at regular intervals to capture updated documents. Airbyte source connectors are independently reviewed and sometime updated for compatibility with Flow. Estuary's source connectors documentation includes actively supported Airbyte connectors. A full list of Airbyte's connectors is available at Airbyte docker hub. If you see a connector you'd like to prioritize for access in the Flow web app, contact us. "},{"title":"Discovery​","type":1,"pageTitle":"Captures","url":"concepts/captures/#discovery","content":"To help you configure new pull captures, Flow offers the guided discovery workflow in the Flow web application. To begin discovery, you tell Flow the connector you'd like to use and basic information about the endpoint. Flow automatically generates a capture configuration for you. It identifies one or moreresources — tables, data streams, or the equivalent — and generates bindings so that each will be mapped to a data collection in Flow. You may then modify the generated configuration as needed before publishing the capture. "},{"title":"Specification​","type":1,"pageTitle":"Captures","url":"concepts/captures/#specification","content":"Pull captures are defined in Flow specification files per the following format: # A set of captures to include in the catalog. # Optional, type: object captures: # The name of the capture. acmeCo/example/source-s3: # Endpoint defines how to connect to the source of the capture. # Required, type: object endpoint: # This endpoint uses a connector provided as a Docker image. connector: # Docker image that implements the capture connector. image: ghcr.io/estuary/source-s3:dev # File that provides the connector's required configuration. # Configuration may also be presented inline. config: path/to/connector-config.yaml # Bindings define how collections are populated from the data source. # A capture may bind multiple resources to different collections. # Required, type: array bindings: - # The target collection to capture into. # This may be defined in a separate, imported specification file. # Required, type: string target: acmeCo/example/collection # The resource is additional configuration required by the endpoint # connector to identify and capture a specific endpoint resource. # The structure and meaning of this configuration is defined by # the specific connector. # Required, type: object resource: stream: a-bucket/and-prefix # syncMode should be set to incremental for all Estuary connectors syncMode: incremental - target: acmeCo/example/another-collection resource: stream: a-bucket/another-prefix syncMode: incremental  "},{"title":"Push captures​","type":1,"pageTitle":"Captures","url":"concepts/captures/#push-captures","content":"Push captures expose an endpoint to which documents may be pushed using a supported ingestion protocol. Beta Push captures are under development. Estuary intends to offer Webhook, Websocket, and Kafka-compatible APIs for capturing into collections. Join the Estuary Slack for more information on this and other ongoing development work. "},{"title":"Specification​","type":1,"pageTitle":"Captures","url":"concepts/captures/#specification-1","content":"Push capture configurations use the following general format: captures: # The name of the capture. acmeCo/example/webhook-ingest: endpoint: # This endpoint is an ingestion. ingest: {} bindings: - # The target collection to capture into. target: acmeCo/example/webhooks # The resource configures the specific behavior of the ingestion endpoint. resource: name: webhooks  "},{"title":"Collections","type":0,"sectionRef":"#","url":"concepts/collections/","content":"","keywords":""},{"title":"Documents​","type":1,"pageTitle":"Collections","url":"concepts/collections/#documents","content":"Flow processes and stores data in terms of documents: JSON files that consist of multiple key-value pair objects. Collections are comprised of documents; Flow tasks (captures, materializations, and derivations) process data in terms of documents. A Flow document corresponds to different units of data in different types of endpoint systems. For example, it might map to a table row, a pub/sub message, or an API response. The structure of a given collection’s documents is determined by that collection’s schema and the way in which tasks handle documents is determined by the collection key. The size of a document depends on the complexity of the source data. Flow allows documents up to 16 MB in size, but it's rare for documents to approach this limit. An example document for a collection with two fields, name and count is shown below. { &quot;_meta&quot;: { &quot;uuid&quot;: &quot;xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx&quot; }, &quot;count&quot;: 5954, &quot;message&quot;: &quot;Hello #5954&quot; }  The _meta object is present in all Flow documents, and contains metadata added by Flow. Minimally, every document _meta always has a uuid, which is a globally unique id for each document. Some capture connectors may add additional _meta properties to tie each document to a specific record within the source system. Documents that were captured from cloud storage connectors, for example, will contain /_meta/file and /_meta/offset properties that tell you where the document came from within your cloud storage bucket. "},{"title":"Viewing collection documents​","type":1,"pageTitle":"Collections","url":"concepts/collections/#viewing-collection-documents","content":"In many cases, it's not necessary to view your collection data — you're able to materialize it directly to a destination in the correct shape using a connector. However, it can be helpful to view collection documents to confirm the source data was captured as expected, or verify a schema change. In the web application​ Sign into the Flow web application and click the Collections tab. The collections to which you have access are listed. Click the Details drop down to show a sample of collection documents as well as the collection specification. The collection documents are displayed by key. Click the desired key to preview it in its native JSON format. Using the flowctl CLI​ In your authenticated flowctl session, issue the command flowctl collections read --collection &lt;full/collection-name&gt; --uncommitted. For example, flowctl collections read --collection acmeCo/inventory/anvils --uncommitted. Options are available to read a subset of data from collections. For example, --since allows you to specify an approximate start time from which to read data, and--include-partition allows you to read only data from a specified logical partition. Use flowctl collections read --help to see documentation for all options. Beta While in beta, this command currently has the following limitations. They will be removed in a later release: The --uncommitted flag is required. This means that all collection documents are read, regardless of whether they were successfully committed or not. In the future, reads of committed documents will be the default. Only reads of a single partition are supported. If you need to read from a partitioned collection, use --include-partition or --exclude-partition to narrow down to a single partition. The --output flag is not usable for this command. Only JSON data can be read from collections. "},{"title":"Specification​","type":1,"pageTitle":"Collections","url":"concepts/collections/#specification","content":"Collections are defined in Flow specification files per the following format: # A set of collections to include in the catalog. # Optional, type: object collections: # The unique name of the collection. acmeCo/products/anvils: # The schema of the collection, against which collection documents # are validated. This may be an inline definition or a relative URI # reference. # Required, type: string (relative URI form) or object (inline form) schema: anvils.schema.yaml # The key of the collection, specified as JSON pointers of one or more # locations within collection documents. If multiple fields are given, # they act as a composite key, equivalent to a SQL table PRIMARY KEY # with multiple table columns. # Required, type: array key: [/product/id] # Projections and logical partitions for this collection. # Optional, type: object projections: # Derivation that builds this collection from others through transformations. # See the &quot;Derivations&quot; concept page to learn more. # Optional, type: object derivation:  "},{"title":"Schemas​","type":1,"pageTitle":"Collections","url":"concepts/collections/#schemas","content":"Every Flow collection must declare a schema, and will never accept documents that do not validate against the schema. This helps ensure the quality of your data products and the reliability of your derivations and materializations. Schema specifications are flexible: yours could be exactingly strict, extremely permissive, or somewhere in between. For many source types, Flow is able to generate a basic schema during discovery. Schemas may either be declared inline, or provided as a reference to a file. References can also include JSON pointers as a URL fragment to name a specific schema of a larger schema document: InlineFile referenceReference with pointer collections: acmeCo/collection: schema: type: object required: [id] properties: id: string key: [/id]  Learn more about schemas "},{"title":"Keys​","type":1,"pageTitle":"Collections","url":"concepts/collections/#keys","content":"Every Flow collection must declare a key which is used to group its documents. Keys are specified as an array of JSON pointers to document locations. For example: flow.yamlschema.yaml collections: acmeCo/users: schema: schema.yaml key: [/userId]  Suppose the following JSON documents are captured into acmeCo/users: {&quot;userId&quot;: 1, &quot;name&quot;: &quot;Will&quot;} {&quot;userId&quot;: 1, &quot;name&quot;: &quot;William&quot;} {&quot;userId&quot;: 1, &quot;name&quot;: &quot;Will&quot;}  As its key is [/userId], a materialization of the collection into a database table will reduce to a single row: userId | name 1 | Will  If its key were instead [/name], there would be two rows in the table: userId | name 1 | Will 1 | William  "},{"title":"Schema restrictions​","type":1,"pageTitle":"Collections","url":"concepts/collections/#schema-restrictions","content":"Keyed document locations may be of a limited set of allowed types: booleanintegerstring Excluded types are: arraynullobjectFractional number Keyed fields also must always exist in collection documents. Flow performs static inference of the collection schema to verify the existence and types of all keyed document locations, and will report an error if the location could not exist, or could exist with the wrong type. Flow itself doesn't mind if a keyed location has multiple types, so long as they're each of the allowed types: an integer or string for example. Some materialization connectors, however, may impose further type restrictions as required by the endpoint. For example, SQL databases do not support multiple types for a primary key. "},{"title":"Composite Keys​","type":1,"pageTitle":"Collections","url":"concepts/collections/#composite-keys","content":"A collection may have multiple locations which collectively form a composite key. This can include locations within nested objects and arrays: flow.yamlschema.yaml collections: acmeCo/compound-key: schema: schema.yaml key: [/foo/a, /foo/b, /foo/c/0, /foo/c/1]  "},{"title":"Key behaviors​","type":1,"pageTitle":"Collections","url":"concepts/collections/#key-behaviors","content":"A collection key instructs Flow how documents of a collection are to be reduced, such as while being materialized to an endpoint. Flow also performs opportunistic local reductions over windows of documents to improve its performance and reduce the volumes of data at each processing stage. An important subtlety is that the underlying storage of a collection will potentially retain many documents of a given key. In the acmeCo/users example, each of the &quot;Will&quot; or &quot;William&quot; variants is likely represented in the collection's storage — so long as they didn't arrive so closely together that they were locally combined by Flow. If desired, a derivation could re-key the collection on [/userId, /name] to materialize the various /names seen for a /userId. This property makes keys less lossy than they might otherwise appear, and it is generally good practice to chose a key that reflects how you wish to query a collection, rather than an exhaustive key that's certain to be unique for every document. "},{"title":"Empty keys​","type":1,"pageTitle":"Collections","url":"concepts/collections/#empty-keys","content":"When a specification is automatically generated, there may not be an unambiguously correct key for all collections. This could occur, for example, when a SQL database doesn't have a primary key defined for some table. In cases like this, the generated specification will contain an empty collection key. However, every collection must have a non-empty key, so you'll need to manually edit the generated specification and specify keys for those collections before publishing to the catalog. "},{"title":"Projections​","type":1,"pageTitle":"Collections","url":"concepts/collections/#projections","content":"Projections are named locations within a collection document that may be used for logical partitioning or directly exposed to databases into which collections are materialized. Many projections are automatically inferred from the collection schema. The projections stanza can be used to provide additional projections, and to declare logical partitions: collections: acmeCo/products/anvils: schema: anvils.schema.yaml key: [/product/id] # Projections and logical partitions for this collection. # Keys name the unique projection field, and values are its JSON Pointer # location within the document and configure logical partitioning. # Optional, type: object projections: # Short form: define a field &quot;product_id&quot; with document pointer /product/id. product_id: &quot;/product/id&quot; # Long form: define a field &quot;metal&quot; with document pointer /metal_type # which is a logical partition of the collection. metal: location: &quot;/metal_type&quot; partition: true  Learn more about projections. "},{"title":"Storage​","type":1,"pageTitle":"Collections","url":"concepts/collections/#storage","content":"Collections are real-time data lakes. Historical documents of the collection are stored as an organized layout of regular JSON files in your cloud storage bucket. Reads of that history are served by directly reading files from your bucket. Your storage mappingsdetermine how Flow collections are mapped into your cloud storage buckets. Unlike a traditional data lake, however, it's very efficient to read collection documents as they are written. Derivations and materializations that source from a collection are notified of its new documents within milliseconds of their being published. Learn more about journals, which provide storage for collections "},{"title":"Connectors","type":0,"sectionRef":"#","url":"concepts/connectors/","content":"","keywords":""},{"title":"Using connectors​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#using-connectors","content":"Most — if not all — of your Data Flows will use at least one connector. You configure connectors within capture or materialization specifications. When you publish one of these entities, you're also deploying all the connectors it uses. You can interact with connectors using either the Flow web application or the flowctl CLI. "},{"title":"Flow web application​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#flow-web-application","content":"The Flow web application is designed to assist you with connector configuration and deployment. It's a completely no-code experience, but it's compatible with Flow's command line tools, discussed below. When you add a capture or materialization in the Flow web app, choose the desired data system from the Connector drop-down menu. The required fields for the connector appear below the drop-down. When you fill in the fields and click Discover Endpoint, Flow automatically &quot;discovers&quot; the data streams or tables — known as resources — associated with the endpoint system. From there, you can refine the configuration, save, and publish the resulting Flow specification. "},{"title":"GitOps and flowctl​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#gitops-and-flowctl","content":"Connectors are packaged as Open Container (Docker) images, and can be tagged, and pulled usingDocker Hub,GitHub Container registry, or any other public image registry provider. To interface with a connector, the Flow runtime needs to know: The specific image to use, through an image name such as ghcr.io/estuary/source-postgres:dev. Notice that the image name also conveys the specific image registry and version tag to use. Endpoint configuration such as a database address and account, with meaning that is specific to the connector. Resource configuration such as a specific database table to capture, which is also specific to the connector. To integrate a connector into your dataflow, you must define all three components within your Flow specification. The web application is intended to help you generate the Flow specification. From there, you can use flowctl to refine it in your local environment. It's also possible to manually write your Flow specification files, but this isn't the recommended workflow. materializations: acmeCo/postgres-views: endpoint: connector: # 1: Provide the image that implements your endpoint connector. # The `dev` tag uses the most recent version (the web app chooses this tag automatically) image: ghcr.io/estuary/materialize-postgres:dev # 2: Provide endpoint configuration that the connector requires. config: address: localhost:5432 password: password database: postgres user: postgres bindings: - source: acmeCo/products/anvils # 3: Provide resource configuration for the binding between the Flow # collection and the endpoint resource. This connector interfaces # with a SQL database and its resources are database tables. Here, # we provide a table to create and materialize which is bound to the # `acmeCo/products/anvils` source collection. resource: table: anvil_products # Multiple resources can be configured through a single connector. # Bind additional collections to tables as part of this connector instance: - source: acmeCo/products/TNT resource: table: tnt_products - source: acmeCo/customers resource: table: customers  Configuration​ Because connectors interface with external systems, each requires a slightly different endpoint configuration. Here you specify information such as a database hostname or account credentials — whatever that specific connector needs to function. If you're working directly with Flow specification files, you have the option of including the configuration inline or storing it in separate files: InlineReferenced file my.flow.yaml materializations: acmeCo/postgres-views: endpoint: connector: image: ghcr.io/estuary/materialize-postgres:dev config: address: localhost:5432 password: password database: postgres user: postgres bindings: []  Storing configuration in separate files serves two important purposes: Re-use of configuration across multiple captures or materializationsThe ability to protect sensitive credentials "},{"title":"Protecting secrets​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#protecting-secrets","content":"Most endpoint systems require credentials of some kind, such as a username or password. Sensitive credentials should be protected while not in use. The only time a credential needs to be directly accessed is when Flow initiates the connector. Flow integrates with Mozilla’s sops tool, which can encrypt and protect credentials. It stores a sops-protected configuration in its encrypted form, and decrypts it only when invoking a connector on the your behalf. sops, short for “Secrets Operations,” is a tool that encrypts the values of a JSON or YAML document against a key management system (KMS) such as Google Cloud Platform KMS, Azure Key Vault, or Hashicorp Vault. Encryption or decryption of a credential with sops is an active process: it requires that the user (or the Flow runtime identity) have a current authorization to the required KMS, and creates a request trace which can be logged and audited. It's also possible to revoke access to the KMS, which immediately and permanently removes access to the protected credential. When you use the Flow web application, Flow automatically adds sops protection to sensitive fields on your behalf. You can also implement sops manually if you are writing a Flow specification locally. The examples below provide a useful reference. Example: Protect a configuration​ Suppose you're given a connector configuration: config.yaml host: my.hostname password: &quot;this is sensitive!&quot; user: my-user  You can protect it using a Google KMS key that you own: # Login to Google Cloud and initialize application default credentials used by `sops`. $ gcloud auth application-default login # Use `sops` to re-write the configuration document in place, protecting its values. $ sops --encrypt --in-place --gcp-kms projects/your-project-id/locations/us-central1/keyRings/your-ring/cryptoKeys/your-key-name config.yaml  sops re-writes the file, wrapping each value in an encrypted envelope and adding a sops metadata section: config.yaml host: ENC[AES256_GCM,data:K/clly65pThTg2U=,iv:1bNmY8wjtjHFBcXLR1KFcsNMGVXRl5LGTdREUZIgcEU=,tag:5GKcguVPihXXDIM7HHuNnA==,type:str] password: ENC[AES256_GCM,data:IDDY+fl0/gAcsH+6tjRdww+G,iv:Ye8st7zJ9wsMRMs6BoAyWlaJeNc9qeNjkkjo6BPp/tE=,tag:EPS9Unkdg4eAFICGujlTfQ==,type:str] user: ENC[AES256_GCM,data:w+F7MMwQhw==,iv:amHhNCJWAJnJaGujZgjhzVzUZAeSchEpUpBau7RVeCg=,tag:62HguhnnSDqJdKdwYnj7mQ==,type:str] sops: # Some items omitted for brevity: gcp_kms: - resource_id: projects/your-project-id/locations/us-central1/keyRings/your-ring/cryptoKeys/your-key-name created_at: &quot;2022-01-05T15:49:45Z&quot; enc: CiQAW8BC2GDYWrJTp3ikVGkTI2XaZc6F4p/d/PCBlczCz8BZiUISSQCnySJKIptagFkIl01uiBQp056c lastmodified: &quot;2022-01-05T15:49:45Z&quot; version: 3.7.1  You then use this config.yaml within your Flow specification. The Flow runtime knows that this document is protected by sopswill continue to store it in its protected form, and will attempt a decryption only when invoking a connector on your behalf. If you need to make further changes to your configuration, edit it using sops config.yaml. It's not required to provide the KMS key to use again, as sops finds it within its metadata section. important When deploying catalogs onto the managed Flow runtime, you must grant access to decrypt your GCP KMS key to the Flow runtime service agent, which is: flow-258@helpful-kingdom-273219.iam.gserviceaccount.com  Example: Protect portions of a configuration​ Endpoint configurations are typically a mix of sensitive and non-sensitive values. It can be cumbersome when sops protects an entire configuration document as you lose visibility into non-sensitive values, which you might prefer to store as cleartext for ease of use. You can use the encrypted-suffix feature of sops to selectively protect credentials: config.yaml host: my.hostname password_sops: &quot;this is sensitive!&quot; user: my-user  Notice that password in this configuration has an added _sops suffix. Next, encrypt only values which have that suffix: $ sops --encrypt --in-place --encrypted-suffix &quot;_sops&quot; --gcp-kms projects/your-project-id/locations/us-central1/keyRings/your-ring/cryptoKeys/your-key-name config.yaml  sops re-writes the file, wrapping only values having a &quot;_sops&quot; suffix and adding its sops metadata section: config.yaml host: my.hostname password_sops: ENC[AES256_GCM,data:dlfidMrHfDxN//nWQTPCsjoG,iv:DHQ5dXhyOOSKI6ZIzcUM67R6DD/2MSE4LENRgOt6GPY=,tag:FNs2pTlzYlagvz7vP/YcIQ==,type:str] user: my-user sops: # Some items omitted for brevity: encrypted_suffix: _sops gcp_kms: - resource_id: projects/your-project-id/locations/us-central1/keyRings/your-ring/cryptoKeys/your-key-name created_at: &quot;2022-01-05T16:06:36Z&quot; enc: CiQAW8BC2Au779CGdMFUjWPhNleCTAj9rL949sBvPQ6eyAC3EdESSQCnySJKD3eWX8XrtrgHqx327 lastmodified: &quot;2022-01-05T16:06:37Z&quot; version: 3.7.1  You then use this config.yaml within your Flow specification. Flow looks for and understands the encrypted_suffix, and will remove this suffix from configuration keys before passing them to the connector. "},{"title":"Connecting to endpoints on secure networks​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#connecting-to-endpoints-on-secure-networks","content":"In some cases, your source or destination endpoint may be within a secure network, and you may not be able to allow direct access to its port due to your organization's security policy. tip If permitted by your organization, a quicker solution is to whitelist the Estuary IP address, 34.121.207.128. For help completing this task on different cloud hosting platforms, see the documentation for the connector you're using. SHH tunneling, or port forwarding, provides a means for Flow to access the port indirectly through an SSH server. SSH tunneling is available in Estuary connectors for endpoints that use a network address for connection. To set up and configure the SSH server, see the guide. Then, add the appropriate properties when you define the capture or materialization in the Flow web app, or add the networkTunnel stanza directly to the YAML, as shown below. Sample​ source-postgres-ssh-tunnel.flow.yaml captures: acmeCo/postgres-capture-ssh: endpoint: connector: image: ghcr.io/estuary/source-postgres:dev config: address: 127.0.0.1:5432 database: flow user: flow_user password: secret networkTunnel: sshForwarding: # Location of the remote SSH server that supports tunneling. # Formatted as ssh://user@hostname[:port]. sshEndpoint: ssh://sshUser@198.21.98.1:22 # Private key to connect to the SSH server, formatted as multiline plaintext. # Use the YAML literal block style with the indentation indicator. # See https://yaml-multiline.info/ for details. privateKey: |2 -----BEGIN RSA PRIVATE KEY----- MIICXAIBAAKBgQCJO7G6R+kv2MMS8Suw21sk2twHg8Vog0fjimEWJEwyAfFM/Toi EJ6r5RTaSvN++/+MPWUll7sUdOOBZr6ErLKLHEt7uXxusAzOjMxFKZpEARMcjwHY v/tN1A2OYU0qay1DOwknEE0i+/Bvf8lMS7VDjHmwRaBtRed/+iAQHf128QIDAQAB AoGAGoOUBP+byAjDN8esv1DCPU6jsDf/Tf//RbEYrOR6bDb/3fYW4zn+zgtGih5t CR268+dwwWCdXohu5DNrn8qV/Awk7hWp18mlcNyO0skT84zvippe+juQMK4hDQNi ywp8mDvKQwpOuzw6wNEitcGDuACx5U/1JEGGmuIRGx2ST5kCQQDsstfWDcYqbdhr 5KemOPpu80OtBYzlgpN0iVP/6XW1e5FCRp2ofQKZYXVwu5txKIakjYRruUiiZTza QeXRPbp3AkEAlGx6wMe1l9UtAAlkgCFYbuxM+eRD4Gg5qLYFpKNsoINXTnlfDry5 +1NkuyiQDjzOSPiLZ4Abpf+a+myjOuNL1wJBAOwkdM6aCVT1J9BkW5mrCLY+PgtV GT80KTY/d6091fBMKhxL5SheJ4SsRYVFtguL2eA7S5xJSpyxkadRzR0Wj3sCQAvA bxO2fE1SRqbbF4cBnOPjd9DNXwZ0miQejWHUwrQO0inXeExNaxhYKQCcnJNUAy1J 6JfAT/AbxeSQF3iBKK8CQAt5r/LLEM1/8ekGOvBh8MAQpWBW771QzHUN84SiUd/q xR9mfItngPwYJ9d/pTO7u9ZUPHEoat8Ave4waB08DsI= -----END RSA PRIVATE KEY----- bindings: []  "},{"title":"Why an open connector architecture?​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#why-an-open-connector-architecture","content":"Historically, data platforms have directly implemented integrations to external systems with which they interact. Today, there are simply so many systems and APIs that companies use, that it’s not feasible for a company to provide all possible integrations. Users are forced to wait indefinitely while the platform works through their prioritized integration list. An open connector architecture removes Estuary — or any company — as a bottleneck in the development of integrations. Estuary contributes open-source connectors to the ecosystem, and in turn is able to leverage connectors implemented by others. Users are empowered to write their own connectors for esoteric systems not already covered by the ecosystem. Furthermore, implementing a Docker-based community specification brings other important qualities to Estuary connectors: Cross-platform interoperability between Flow, Airbyte, and any other platform that supports the protocolThe abilities to write connectors in any language and run them on any machineBuilt-in solutions for version management (through image tags) and distributionThe ability to integrate connectors from different sources at will, without the centralized control of a single company, thanks to container image registries info In order to be reflected in the Flow web app and used on the managed Flow platform, connectors must be reviewed and added by the Estuary team. Have a connector you'd like to add?Contact us. "},{"title":"Available connectors​","type":1,"pageTitle":"Connectors","url":"concepts/connectors/#available-connectors","content":"Learn about available connectors in the reference section "},{"title":"flowctl","type":0,"sectionRef":"#","url":"concepts/flowctl/","content":"","keywords":""},{"title":"Installation and setup​","type":1,"pageTitle":"flowctl","url":"concepts/flowctl/#installation-and-setup","content":"flowctl binaries for MacOS and Linux are available. Copy and paste the appropriate script below into your terminal. This will download flowctl, make it executable, and add it to your PATH. For Linux: sudo curl -o /usr/local/bin/flowctl -L 'https://github.com/estuary/flow/releases/latest/download/flowctl-x86_64-linux' &amp;&amp; sudo chmod +x /usr/local/bin/flowctl For Mac: sudo curl -o /usr/local/bin/flowctl -L 'https://github.com/estuary/flow/releases/latest/download/flowctl-multiarch-macos' &amp;&amp; sudo chmod +x /usr/local/bin/flowctl Alternatively, Mac users can install with Homebrew: brew tap estuary/flowctl brew install flowctl You can also find the source files on GitHub here. flowctl isn't currently available for Windows. For Windows users, we recommend running the Linux version inside WSL, or using a remote development environment. To connect to your Flow account and start a session, use an authentication token from the web app. "},{"title":"User guides​","type":1,"pageTitle":"flowctl","url":"concepts/flowctl/#user-guides","content":"View guides for common flowctl workflows. "},{"title":"flowctl subcommands​","type":1,"pageTitle":"flowctl","url":"concepts/flowctl/#flowctl-subcommands","content":"flowctl includes several top-level subcommands representing different functional areas. Each of these include multiple nested subcommands. Important top-level flowctl subcommands are described below. auth allows you to authenticate your development session in your local development environment. It's also how you provision Flow roles and users. Learn more about authentication. catalog allows you to work with your organization's current active catalog entities. You can investigate the current Data Flows, pull specifications for local editing, and test and publish specifications that you wrote or edited locally. collections allows you to work with your Flow collections. You can read the data from the collection and output it to stdout, or list the journals or journal fragments that comprise the collection. Learn more about reading collections with flowctl. draft provides an alternative method for many of the actions you'd normally perform with catalog, but common workflows have more steps.draft also allows you to delete Flow entities. You can access full documentation of all flowctl subcommands from the command line by passing the --help or -h flag, for example: flowctl --help lists top-level flowctl subcommands. flowctl catalog --help lists subcommands of catalog. "},{"title":"Editing Data Flows with flowctl​","type":1,"pageTitle":"flowctl","url":"concepts/flowctl/#editing-data-flows-with-flowctl","content":"flowctl allows you to work locally on the specification files that define your Data Flows. You'll often need to move these specifications back and forth between your local environment and the catalogof published entities. The basic steps of this workflow are listed below, along with a diagram of the subcommands you'd use to accomplish them. Keep in mind that there's no single, correct way to work with flowctl, but we recommend this method to get started. List all the active specifications in the catalog, which you can then pull into your local environment. You can filter the output by prefix or entity type. For example, flowctl catalog list --prefix acmeCo/sales/ --collections only lists collections under theacmeCo/sales/ prefix. Pull a group of active specifications directly, resulting in local source files. You can refine results by prefix or entity type as described above (1). Note that if there are already files in your working directory, flowctl must reconcile them with the newly pulled specification.Learn more about your options. Make edits locally. Test local specifications (2). Publish local specifications to the catalog (3). graph LR; d[Local environment]; c[Active catalog]; d-- 2: flowctl catalog test --&gt;d; d-- 3: flowctl catalog publish --&gt;c; c-- 1: flowctl catalog pull-specs --&gt;d; View the step-by-step guide. "},{"title":"Reconciling specifications in local drafts​","type":1,"pageTitle":"flowctl","url":"concepts/flowctl/#reconciling-specifications-in-local-drafts","content":"When you pull specifications to your working directory directly using flowctl catalog pull-specs, there may be conflicts between the existing files in that directory and the specifications you pull. By default, flowctl catalog pull-specs will abort if it detects an existing file with the same name as a specification it is attempting to pull. You can change this behavior with the --existing flag: --existing=overwrite pulls the new versions of conflicting files in place of the old versions. --existing=keep keeps the old versions of conflicting files. --existing=merge-specs performs a simple merge of new and old versions of conflicting files. For example, if an existing flow.yaml file references collections a and b, and the new version of flow.yaml references collections a and c, the merged version will reference collections a, b, and c. "},{"title":"Development directories​","type":1,"pageTitle":"flowctl","url":"concepts/flowctl/#development-directories","content":"Flow specifications and other files are written to your working directory when you run flowctl draft develop or flowctl catalog pull-specs. They typically include: flow.yaml: The main specification file that imports all other Flow specification files created in a single operation. As part of local development, you may add new specifications that you create as imports. flow_generated/: Directory of generated files, including TypeScript classes and interfaces. See TypeScript code generation. &lt;prefix-name&gt;/: Directory of specifications that you pulled. Its name corresponds to your catalog prefix. Its contents will vary, but it may contain various YAML files and subdirectories. package.json and package-lock.json: Files used by npm to manage dependencies and your Data Flow's associated JavaScript project. You may customize package.json, but its dependencies stanza will be overwritten by thenpmDependenciesof your Flow specification source files, if any exist. When you run commands like flowctl catalog publish or flowctl draft author, you can use the --source-dir flag to push specifications from a directory other than your current working directory, for example, flowctl draft author --source-dir ../AcmeCoNew/marketing. "},{"title":"TypeScript code generation​","type":1,"pageTitle":"flowctl","url":"concepts/flowctl/#typescript-code-generation","content":"TypeScript files are used in the Flow catalog both as part of the automatic build process, and to define lambdas functions for derivations, which requires your input. As part of the Data Flow build process, Flow translates yourschemasinto equivalent TypeScript types on your behalf. These definitions live within flow_generated/ in your Data Flow's build directory , and are frequently over-written by invocations of flowctl. Files in this subdirectory are human-readable and stable. You may want to commit them as part of a GitOps-managed project, but this isn't required. Whenever you define a derivation that uses a lambda, you must define the lambda in an accompanying TypeScript module, and reference that module in the derivation's definition. To facilitate this, you can generate a stub of the module using flowctl typescript generateand simply write the function bodies.Learn more about this workflow. If a TypeScript module exists, flowctl will never overwrite it, even if you update or expand your specifications such that the required interfaces have changed. "},{"title":"Imports","type":0,"sectionRef":"#","url":"concepts/import/","content":"","keywords":""},{"title":"Specification​","type":1,"pageTitle":"Imports","url":"concepts/import/#specification","content":"The import section is structured as a list of partial or absolute URIs, which Flow always evaluates relative to the base directory of the current source file. For example, these are possible imports within a collection: # Suppose we're in file &quot;/path/dir/flow.yaml&quot; import: - sub/directory/flow.yaml # Resolves to &quot;file:///path/dir/sub/directory/flow.yaml&quot;. - ../sibling/directory/flow.yaml # Resolves to &quot;file:///path/sibling/directory/flow.yaml&quot;. - https://example/path/flow.yaml # Uses the absolute url.  The import rule is flexible; a collection doesn’t have to do anything special to be imported by another, and flowctl can even directly build remote sources: # Test an example from a GitHub repository. $ flowctl draft test --source https://raw.githubusercontent.com/estuary/flow-template/main/word-counts.flow.yaml  "},{"title":"Fetch behavior​","type":1,"pageTitle":"Imports","url":"concepts/import/#fetch-behavior","content":"Flow resolves, fetches, and validates all imports in your local environment during the catalog build process, and then includes their fetched contents within the published catalog on the Estuary servers. The resulting catalog entities are thus self-contained snapshots of all resourcesas they were at the time of publication. This means it's both safe and recommended to directly reference an authoritative source of a resource, such as a third-party JSON schema, as well as resources within your private network. It will be fetched and verified locally at build time, and thereafter that fetched version will be used for execution, regardless of whether the authority URL itself later changes or errors. "},{"title":"Import types​","type":1,"pageTitle":"Imports","url":"concepts/import/#import-types","content":"Almost always, the import stanza is used to import other Flow specification files. This is the default when given a string path: import: - path/to/source/catalog.flow.yaml  A long-form variant also accepts a content type of the imported resource: import: - url: path/to/source/catalog.flow.yaml contentType: CATALOG  Other permitted content types include JSON_SCHEMA, but these are not typically used and are needed only for advanced use cases. "},{"title":"JSON Schema $ref​","type":1,"pageTitle":"Imports","url":"concepts/import/#json-schema-ref","content":"Certain catalog entities, like collections, commonly reference JSON schemas. It's not necessary to explicitly add these to the import section; they are automatically resolved and treated as an import. You can think of this as an analog to the JSON Schema $ref keyword, which is used to reference a schema that may be contained in another file. The one exception is schemas that use the $id keyword at their root to define an alternative canonical URL. In this case, the schema must be referenced through its canonical URL, and then explicitly added to the import section with JSON_SCHEMA content type. "},{"title":"Importing derivation resources​","type":1,"pageTitle":"Imports","url":"concepts/import/#importing-derivation-resources","content":"In many cases, derivations in your catalog will need to import resources. Usually, these are TypeScript modules that define the lambda functions of a transformation, and, in certain cases, the NPM dependencies of that TypeScript module. These imports are specified in the derivation specification, not in the import section of the specification file. For more information, see Derivation specification and creating TypeScript modules. "},{"title":"Import paths​","type":1,"pageTitle":"Imports","url":"concepts/import/#import-paths","content":"If a catalog source file foo.flow.yaml references a collection in bar.flow.yaml, for example as a target of a capture, there must be an import path where either foo.flow.yamlimports bar.flow.yaml or vice versa. When you omit the import section, Flow chooses an import path for you. When you explicitly include the import section, you have more control over the import path. Import paths can be direct: graph LR; foo.flow.yaml--&gt;bar.flow.yaml; Or they can be indirect: graph LR; bar.flow.yaml--&gt;other.flow.yaml; other.flow.yaml--&gt;foo.flow.yaml; The sources must still have an import path even if referenced from a common parent. The following would not work: graph LR; parent.flow.yaml--&gt;foo.flow.yaml; parent.flow.yaml--&gt;bar.flow.yaml; These rules make your catalog sources more self-contained and less brittle to refactoring and reorganization. Consider what might otherwise happen if foo.flow.yamlwere imported in another project without bar.flow.yaml. "},{"title":"Materializations","type":0,"sectionRef":"#","url":"concepts/materialization/","content":"","keywords":""},{"title":"Discovery​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#discovery","content":"Materializations use real-time connectors to connect to many endpoint types. When you use a materialization connector in the Flow web app, flow helps you configure it through the discovery workflow. To begin discovery, you tell Flow the connector you'd like to use, basic information about the endpoint, and the collection(s) you'd like to materialize there. Flow maps the collection(s) to one or more resources — tables, data streams, or the equivalent — through one or more bindings. You may then modify the generated configuration as needed before publishing the materialization. "},{"title":"Specification​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#specification","content":"Materializations are defined in Flow specification files per the following format: # A set of materializations to include in the catalog. # Optional, type: object materializations: # The name of the materialization. acmeCo/example/database-views: # Endpoint defines how to connect to the destination of the materialization. # Required, type: object endpoint: # This endpoint uses a connector provided as a Docker image. connector: # Docker image that implements the materialization connector. image: ghcr.io/estuary/materialize-postgres:dev # File that provides the connector's required configuration. # Configuration may also be presented inline. config: path/to/connector-config.yaml # Bindings define how one or more collections map to materialized endpoint resources. # A single materialization may include many collections and endpoint resources, # each defined as a separate binding. # Required, type: object bindings: - # The source collection to materialize. # This may be defined in a separate, imported specification file. # Required, type: string source: acmeCo/example/collection # The resource is additional configuration required by the endpoint # connector to identify and materialize a specific endpoint resource. # The structure and meaning of this configuration is defined by # the specific connector. # Required, type: object resource: # The materialize-postgres connector expects a `table` key # which names a table to materialize into. table: example_table  "},{"title":"How continuous materialization works​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#how-continuous-materialization-works","content":"Flow materializations are continuous materialized views. They maintain a representation of the collection within the endpoint system that is updated in near real-time. It's indexed on thecollection key. As the materialization runs, it ensures that all collection documents and their accumulated reductions are reflected in this managed endpoint resource. When you first publish a materialization, Flow back-fills the endpoint resource with the historical documents of the collection. Once caught up, Flow applies new collection documents using incremental and low-latency updates. As collection documents arrive, Flow: Reads previously materialized documents from the endpoint for the relevant keysReduces new documents into these read documentsWrites updated documents back into the endpoint resource, indexed by their keys For example, consider a collection and its materialization:  collections: acmeCo/colors: key: [/color] schema: type: object required: [color, total] reduce: {strategy: merge} properties: color: {enum: [red, blue, purple]} total: type: integer reduce: {strategy: sum} materializations: acmeCo/example/database-views: endpoint: ... bindings: - source: acmeCo/colors resource: { table: colors }  Suppose documents are periodically added to the collection: {&quot;color&quot;: &quot;red&quot;, &quot;total&quot;: 1} {&quot;color&quot;: &quot;blue&quot;, &quot;total&quot;: 2} {&quot;color&quot;: &quot;blue&quot;, &quot;total&quot;: 3}  Its materialization into a database table will have a single row for each unique color. As documents arrive in the collection, the row total is updated within the materialized table so that it reflects the overall count:  Flow does not keep separate internal copies of collection or reduction states, as some other systems do. The endpoint resource is the one and only place where state &quot;lives&quot; within a materialization. This makes materializations very efficient and scalable to operate. They are able to maintain very large tables stored in highly scaled storage systems like OLAP data warehouses. "},{"title":"Projected fields​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#projected-fields","content":"Many endpoint systems are document-oriented and can directly work with collections of JSON documents. Others are table-oriented and require an up-front declaration of columns and types to be most useful, such as a SQL CREATE TABLE definition. Flow uses collection projections to relate locations within a hierarchical JSON document to equivalent named fields. A materialization can in turn select a subset of available projected fields where, for example, each field becomes a column in a SQL table created by the connector. It would be tedious to explicitly list projections for every materialization, though you certainly can if desired. Instead, Flow and the materialization connector negotiate a recommended field selection on your behalf, which can be fine-tuned. For example, a SQL database connector will typically require that fields comprising the primary key be included, and will recommend that scalar values be included, but will by default exclude document locations that don't have native SQL representations, such as locations which can have multiple JSON types or are arrays or maps. materializations: acmeCo/example/database-views: endpoint: ... bindings: - source: acmeCo/example/collection resource: { table: example_table } # Select (or exclude) projections of the collection for materialization as fields. # If not provided, the recommend fields of the endpoint connector are used. # Optional, type: object fields: # Whether to include fields that are recommended by the endpoint connector. # If false, then fields can still be added using `include`. # Required, type: boolean recommended: true # Fields to exclude. This is useful for deselecting a subset of recommended fields. # Default: [], type: array exclude: [myField, otherField] # Fields to include. This can supplement recommended fields, or can # designate explicit fields to use if recommended fields are disabled. # # Values of this map are used to customize connector behavior on a per-field basis. # They are passed directly to the connector and are not interpreted by Flow. # Consult your connector's documentation for details of what customizations are available. # This is an advanced feature and is not commonly used. # # default: {}, type: object include: {goodField: {}, greatField: {}}  "},{"title":"Partition selectors​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#partition-selectors","content":"Partition selectors let you materialize only a subset of a collection that haslogical partitions. For example, you might have a large collection that is logically partitioned on each of your customers: collections: acmeCo/anvil/orders: key: [/id] schema: orders.schema.yaml projections: customer: location: /order/customer partition: true  A large customer asks if you can provide an up-to-date accounting of their orders. This can be accomplished with a partition selector: materializations: acmeCo/example/database-views: endpoint: ... bindings: - source: acmeCo/anvil/orders resource: { table: coyote_orders } # Process partitions where &quot;Coyote&quot; is the customer. partitions: include: customer: [Coyote]  Learn more about partition selectors. "},{"title":"Destination-specific performance​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#destination-specific-performance","content":"Flow processes updates in transactions, as quickly as the destination endpoint can handle them. This might be milliseconds in the case of a fast key/value store, or many minutes in the case of an OLAP warehouse. If the endpoint is also transactional, Flow integrates its internal transactions with those of the endpoint for integrated end-to-end “exactly once” semantics. The materialization is sensitive to back pressure from the endpoint. As a database gets busy, Flow adaptively batches and combines documents to consolidate updates: In a given transaction, Flow reduces all incoming documents on the collection key. Multiple documents combine and result in a single endpoint read and write during the transaction.As a target database becomes busier or slower, transactions become larger. Flow does more reduction work within each transaction, and each endpoint read or write accounts for an increasing volume of collection documents. This allows you to safely materialize a collection with a high rate of changes into a small database, so long as the cardinality of the materialization is of reasonable size. "},{"title":"Delta updates​","type":1,"pageTitle":"Materializations","url":"concepts/materialization/#delta-updates","content":"As described above, Flow's standard materialization mechanism involves querying the target system for data state before reducing new documents directly into it. For these standard updates to work, the endpoint must be a stateful system, like a relational database. However, other systems — like Webhooks and Pub/Sub — may also be endpoints. None of these typically provide a state representation that Flow can query. They are write-only in nature, so Flow cannot use their endpoint state to help it fully reduce collection documents on their keys. Even some stateful systems are incompatible with Flow's standard updates due to their unique design and architecture. For all of these endpoints, Flow offers a delta-updates mode. When using delta updates, Flow does not attempt to maintain full reductions of each unique collection key. Instead, Flow locally reduces documents within each transaction (this is often called a &quot;combine&quot;), and then materializes onedelta document per key to the endpoint. In other words, when delta updates are used, Flow sends information about data changes by key, and further reduction is left up to the endpoint system. Some systems may reduce documents similar to Flow; others use a different mechanism; still others may not perform reductions at all. A given endpoint may support standard updates, delta updates, or both. This depends on the materialization connector. Expect that a connector will use standard updates only unless otherwise noted in its documentation. "},{"title":"Schemas","type":0,"sectionRef":"#","url":"concepts/schemas/","content":"","keywords":""},{"title":"JSON Schema​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#json-schema","content":"JSON Schemais an expressive open standard for defining the schema and structure of documents. Flow uses it for all schemas defined in Flow specifications. JSON Schema goes well beyond basic type information and can modeltagged unions, recursion, and other complex, real-world composite types. Schemas can also define rich data validations like minimum and maximum values, regular expressions, dates, timestamps, email addresses, and other formats. Together, these features let schemas represent structure as well asexpectations and constraints that are evaluated and must hold true for every collection document before it’s added to the collection. They’re a powerful tool for ensuring end-to-end data quality: for catching data errors and mistakes early, before they can impact your production data products. "},{"title":"Generation​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#generation","content":"When capturing data from an external system, Flow can usually generate suitable JSON schemas on your behalf. Learn more about using connectors "},{"title":"Translations​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#translations","content":"You must only provide Flow a model of a given dataset one time, as a JSON schema. Having done that, Flow leverages static inference over your schemas to perform many build-time validations of your catalog entities, helping you catch potential problems early. Schema inference is also used to provide translations into other schema flavors: Most projections of a collection are automatically inferred from its schema. Materializations use your projections to create appropriate representations in your endpoint system. A SQL connector will create table definitions with appropriate columns, types, and constraints.Flow generates TypeScript definitions from schemas to provide compile-time type checks of user lambda functions. These checks are immensely helpful for surfacing mismatched expectations around, for example, whether a field could ever be null or is misspelt — which, if not caught, might otherwise fail at runtime. "},{"title":"Annotations​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#annotations","content":"The JSON Schema standard introduces the concept ofannotations, which are keywords that attach metadata to a location within a validated JSON document. For example, title and description can be used to annotate a schema with its meaning: properties: myField: title: My Field description: A description of myField  Flow extends JSON Schema with additional annotation keywords, which provide Flow with further instruction for how documents should be processed. In particular, the reduce and default keywords help you define merge behaviors and avoid null values at your destination systems, respectively. What’s especially powerful about annotations is that they respond toconditionals within the schema. Consider a schema validating a positive or negative number: type: number oneOf: - exclusiveMinimum: 0 description: A positive number. - exclusiveMaximum: 0 description: A negative number. - const: 0 description: Zero.  Here, the activated description of this schema location depends on whether the integer is positive, negative, or zero. "},{"title":"Writing schemas​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#writing-schemas","content":"Your schema can be quite permissive or as strict as you wish. There are a few things to know, however. The top-level type must be object. Flow adds a bit of metadata to each of your documents under the _meta property, which can only be done with a top-level object. Any fields that are part of the collection's key must provably exist in any document that validates against the schema. Put another way, every document within a collection must include all of the fields of the collection's key, and the schema must guarantee that. For example, the following collection schema would be invalid because the id field, which is used as its key, is not required, so it might not actually exist in all documents: collections: acmeCo/whoops: schema: type: object required: [value] properties: id: {type: integer} value: {type: string} key: [/id]  To fix the above schema, change required to [id, value]. Learn more of how schemas can be expressed within collections. "},{"title":"Organization​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#organization","content":"JSON schema has a $ref keyword which is used to reference a schema stored elsewhere. Flow resolves $ref as a relative URL of the current file, and also supportsJSON fragment pointersfor referencing a specific schema within a larger schema document, such as ../my/widget.schema.yaml#/path/to/schema. It's recommended to use references in order to organize your schemas for reuse. $ref can also be used in combination with other schema keywords to further refine a base schema. Here's an example that uses references to organize and further tighten the constraints of a reused base schema: flow.yamlschemas.yaml collections: acmeCo/coordinates: key: [/id] schema: schemas.yaml#/definitions/coordinate acmeCo/integer-coordinates: key: [/id] schema: schemas.yaml#/definitions/integer-coordinate acmeCo/positive-coordinates: key: [/id] schema: # Compose a restriction that `x` &amp; `y` must be positive. $ref: schemas.yaml#/definitions/coordinate properties: x: {exclusiveMinimum: 0} y: {exclusiveMinimum: 0}  tip You can write your JSON schemas as either YAML or JSON across any number of files, all referenced from Flow catalog files or other schemas. Schema references are always resolved as URLs relative to the current file, but you can also use absolute URLs to a third-party schema likeschemastore.org. "},{"title":"Write and read schemas​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#write-and-read-schemas","content":"In some cases, you may want to impose different constraints to data that is being added (written) to the collection and data that is exiting (read from) the collection. For example, you may need to start capturing data now from a source system; say, a pub-sub system with short-lived historical data support or an HTTP endpoint, but don't know or don't control the endpoint's schema. You can capture the data with a permissive write schema, and impose a stricter read schema on the data as you need to perform a derivation or materialization. You can safely experiment with the read schema at your convenience, knowing the data has already been captured. To achieve this, edit the collection, re-naming the standard schema to writeSchema and adding a readSchema. Make sure that the field used as the collection key is defined in both schemas. You can either perform this manually, or use Flow's Schema Inference tool to infer a read schema. Schema Inference is available in the web app when you edit a capture or materialization and create a materialization. Before separating your write and read schemas, have the following in mind: The write schema comes from the capture connector that produced the collection and shouldn't be modified. Always apply your schema changes to the read schema. Separate read and write schemas are typically useful for collections that come from a source system with a flat or loosely defined data structure, such as cloud storage or pub-sub systems. Collections sourced from databases and most SaaS systems come with an explicitly defined data structure and shouldn't need a different read schema. If you're using standard projections, you must only define them in the read schema. However, if your projections are logical partitions, you must define them in both schemas. Here's a simple example in which you don't know how purchase prices are formatted when capturing them, but find out later that number is the appropriate data type: collections: purchases: writeSchema: type: object title: Store price as strings description: Not sure if prices are formatted as numbers or strings. properties: id: { type: integer} price: {type: [string, number]} readSchema: type: object title: Prices as numbers properties: id: { type: integer} price: {type: number} key: [/id]  "},{"title":"Reductions​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#reductions","content":"Flow collections have keys, and multiple documents may be added to collections that share a common key. When this happens, Flow will opportunistically merge all such documents into a single representative document for that key through a process known as reduction. Flow's default is simply to retain the most recent document of a given key, which is often the behavior that you're after. Schema reduce annotations allow for far more powerful behaviors. The Flow runtime performs reductions frequently and continuously to reduce the overall movement and cost of data transfer and storage. A torrent of input collection documents can often become a trickle of reduced updates that must be stored or materialized into your endpoints. info Flow never delays processing in order to batch or combine more documents, as some systems do (commonly known as micro-batches, or time-based polling). Every document is processed as quickly as possible, from end to end. Instead Flow uses optimistic transaction pipelining to do as much useful work as possible, while it awaits the commit of a previous transaction. This natural back-pressure affords plenty of opportunity for data reductions while minimizing latency. "},{"title":"reduce annotations​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#reduce-annotations","content":"Reduction behaviors are defined by reduceJSON schema annotationswithin your document schemas. These annotations provide Flow with the specific reduction strategies to use at your various document locations. If you're familiar with the map and reduce primitives present in Python, Javascript, and many other languages, this should feel familiar. When multiple documents map into a collection with a common key, Flow reduces them on your behalf by using your reduce annotations. Here's an example that sums an integer: type: integer reduce: { strategy: sum } # 1, 2, -1 =&gt; 2  Or deeply merges a map: type: object reduce: { strategy: merge } # {&quot;a&quot;: &quot;b&quot;}, {&quot;c&quot;: &quot;d&quot;} =&gt; {&quot;a&quot;: &quot;b&quot;, &quot;c&quot;: &quot;d&quot;}  Learn more in thereductions strategiesreference documentation. Reductions and collection keys​ Reduction annotations change the common patterns for how you think about collection keys. Suppose you are building a reporting fact table over events of your business. Today you would commonly consider a unique event ID to be its natural key. You would load all events into your warehouse and perform query-time aggregation. When that becomes too slow, you periodically refresh materialized views for fast-but-stale queries. With Flow, you instead use a collection key of your fact table dimensions, and use reduce annotations to define your metric aggregations. A materialization of the collection then maintains a database table which is keyed on your dimensions, so that queries are both fast and up to date. Composition with conditionals​ Like any other JSON Schema annotation,reduce annotations respond to schema conditionals. Here we compose append and lastWriteWins strategies to reduce an appended array which can also be cleared: type: array oneOf: # If the array is non-empty, reduce by appending its items. - minItems: 1 reduce: { strategy: append } # Otherwise, if the array is empty, reset the reduced array to be empty. - maxItems: 0 reduce: { strategy: lastWriteWins } # [1, 2], [3, 4, 5] =&gt; [1, 2, 3, 4, 5] # [1, 2], [], [3, 4, 5] =&gt; [3, 4, 5] # [1, 2], [3, 4, 5], [] =&gt; []  You can combine schema conditionals with annotations to buildrich behaviors. "},{"title":"default annotations​","type":1,"pageTitle":"Schemas","url":"concepts/schemas/#default-annotations","content":"You can use default annotations to prevent null values from being materialized to your endpoint system. When this annotation is absent for a non-required field, missing values in that field are materialized as null. When the annotation is present, missing values are materialized with the field's default value: collections: acmeCo/coyotes: schema: type: object required: [id] properties: id: {type: integer} anvils_dropped: {type: integer} reduce: {strategy: sum } default: 0 key: [/id]  default annotations are only used for materializations; they're ignored by captures and derivations. If your collection has both a write and read schema, make sure you add this annotation to the read schema. "},{"title":"Storage mappings","type":0,"sectionRef":"#","url":"concepts/storage-mappings/","content":"","keywords":""},{"title":"Recovery logs​","type":1,"pageTitle":"Storage mappings","url":"concepts/storage-mappings/#recovery-logs","content":"In addition to collection data, Flow uses your storage mapping to temporarily store recovery logs. Flow tasks — captures, derivations, and materializations — use recovery logs to durably store their processing context as a backup. Recovery logs are an opaque binary log, but may contain user data. The recovery logs of a task are always prefixed by recovery/, so a task named acmeCo/produce-TNT would have a recovery log called recovery/acmeCo/roduce-TNT Flow prunes data from recovery logs once it is no longer required. warning Deleting data from recovery logs while it is still in use can cause Flow processing tasks to fail permanently. "},{"title":"Tests","type":0,"sectionRef":"#","url":"concepts/tests/","content":"","keywords":""},{"title":"Ingest​","type":1,"pageTitle":"Tests","url":"concepts/tests/#ingest","content":"ingest steps add documents to a named collection. All documents must validate against the collection'sschema, or a catalog build error will be reported. All documents from a single ingest step are added in one transaction. This means that multiple documents with a common key will be combined priorto their being appended to the collection. Suppose acmeCo/people had key [/id]: tests: acmeCo/tests/greetings: - ingest: description: Zeldas are combined to one added document. collection: acmeCo/people documents: - { userId: 1, name: &quot;Zelda One&quot; } - { userId: 1, name: &quot;Zelda Two&quot; } - verify: description: Only one Zelda is greeted. collection: acmeCo/greetings documents: - { userId: 1, greeting: &quot;Hello Zelda Two&quot; }  "},{"title":"Verify​","type":1,"pageTitle":"Tests","url":"concepts/tests/#verify","content":"verify steps assert that the current contents of a collection match the provided document fixtures. Verified documents are fully reduced, with one document for each unique key, ordered under the key's natural order. You can verify the contents of both derivations and captured collections. Documents given in verify steps do not need to be comprehensive. It is not an error if the actual document has additional locations not present in the document to verify, so long as all matched document locations are equal. Verified documents also do not need to validate against the collection's schema. They do, however, need to include all fields that are part of the collection's key. tests: acmeCo/tests/greetings: - ingest: collection: acmeCo/people documents: - { userId: 1, name: &quot;Zelda&quot; } - { userId: 2, name: &quot;Link&quot; } - ingest: collection: acmeCo/people documents: - { userId: 1, name: &quot;Zelda Again&quot; } - { userId: 3, name: &quot;Pikachu&quot; } - verify: collection: acmeCo/greetings documents: # greetings are keyed on /userId, and the second greeting is kept. - { userId: 1, greeting: &quot;Hello Zelda Again&quot; } # `greeting` is &quot;Hello Link&quot;, but is not asserted here. - { userId: 2 } - { userId: 3, greeting: &quot;Hello Pikachu&quot; }  "},{"title":"Partition selectors​","type":1,"pageTitle":"Tests","url":"concepts/tests/#partition-selectors","content":"Verify steps may include a partition selector to verify only documents of a specific partition: tests: acmeCo/tests/greetings: - verify: collection: acmeCo/greetings description: Verify only documents which greet Nintendo characters. documents: - { userId: 1, greeting: &quot;Hello Zelda&quot; } - { userId: 3, greeting: &quot;Hello Pikachu&quot; } partitions: include: platform: [Nintendo]  Learn more about partition selectors. "},{"title":"Tips​","type":1,"pageTitle":"Tests","url":"concepts/tests/#tips","content":"The following tips can aid in testing large or complex derivations. "},{"title":"Testing reductions​","type":1,"pageTitle":"Tests","url":"concepts/tests/#testing-reductions","content":"Reduction annotations are expressive and powerful, and their use should thus be tested thoroughly. An easy way to test reduction annotations on captured collections is to write a two-step test that ingests multiple documents with the same key and then verifies the result. For example, the following test might be used to verify the behavior of a simple sum reduction: tests: acmeCo/tests/sum-reductions: - ingest: description: Ingest documents to be summed. collection: acmeCo/collection documents: - {id: 1, value: 5} - {id: 1, value: 4} - {id: 1, value: -3} - verify: description: Verify value was correctly summed. collection: acmeCo/collection documents: - {id: 1, value: 6}  "},{"title":"Reusing common fixtures​","type":1,"pageTitle":"Tests","url":"concepts/tests/#reusing-common-fixtures","content":"When you write a lot of tests, it can be tedious to repeat documents that are used multiple times. YAML supports anchors and references, which you can implement to re-use common documents throughout your tests. One nice pattern is to define anchors for common ingest steps in the first test, which can be re-used by subsequent tests. For example: tests: acmeCo/tests/one: - ingest: &amp;mySetup collection: acmeCo/collection documents: - {id: 1, ...} - {id: 2, ...} ... - verify: ... acmeCo/tests/two: - ingest: *mySetup - verify: ...  This allows all the subsequent tests to re-use the documents from the first ingest step without having to duplicate them. "},{"title":"Web application","type":0,"sectionRef":"#","url":"concepts/web-app/","content":"","keywords":""},{"title":"When to use the web app​","type":1,"pageTitle":"Web application","url":"concepts/web-app/#when-to-use-the-web-app","content":"The web app and flowctl are designed to work together as a complete platform. You can use either, or both, to work on your Data Flows, depending on your preference. With the Flow web app, you can perform most common workflows, including: Creating end-to-end Data Flows: capturing data from source systems and materializing it to destinations.Creating, viewing, and editing individual captures and materializations.Viewing data collections.Viewing users and permissions.Authenticating with the flowctl CLI. Some advanced workflows, like granting or revoking permissions and transforming data with derivations, aren't available in the web app. Even if you prefer the command line or plan to perform a task that's only available through flowctl, we recommend you begin your work in the web app; it provides a quicker and easier path to create captures and materializations. You can then switch to flowctl to continue working. "},{"title":"Signing in​","type":1,"pageTitle":"Web application","url":"concepts/web-app/#signing-in","content":"You use either a Google or GitHub account to sign into Flow.  If you've never used Flow before, you'll be prompted to register before being issued a trial account. If you want to use Flow for production workflows or collaborate with team members, you'll need an organizational account.Contact Estuary to create a new organizational account or join an existing organization. "},{"title":"Navigating the web app​","type":1,"pageTitle":"Web application","url":"concepts/web-app/#navigating-the-web-app","content":"When you log into the web app, you land on the Welcome page. There are four additional pages visible as tabs in the side navigation: Captures, Collections, Materializations, and Admin. The order of the tabs mirrors the order of a basic Data Flow: graph LR; Capture--&gt;Collection; Collection--&gt;Materialization; While you may choose to use the tabs in this sequence, it's not necessary. All Flow entities exist individually, outside of the context of complete Data Flow. You can use the different pages in the web app to monitor and manage your items in a number of other ways, as described below. "},{"title":"Captures page​","type":1,"pageTitle":"Web application","url":"concepts/web-app/#captures-page","content":"The Captures page shows you a table of existing Flow captures to which you have access. The New Capture button is also visible. You use the table to monitor your captures.  1: Select all or deselect all. 2: Enable, Disable, and Delete buttons. These actions will be applied to the selected table rows. Choose Disable to temporarily pause the flow of data, Enable to resume, and Delete to permanently remove the capture(s). 3: Materialize button. When you click this button, you're directed to the Create Materializations page. All the collections of the selected capture(s) will be added to the materialization. 4: Filter captures. Type a catalog prefix, unique capture name, or connector name to return captures that match your query. Capture names follow the pattern prefix/unique-identifier/connector-name, with prefix supporting multiple layers of nesting. You can search for any part of this full capture name. You can also use the * wildcard. For example, if you have a capture called acmeCo/logistics/anvil-locations/source-postgres, you can find it by filtering for acmeCo*source-postgres. 5: Status indicator. Shows the status of the primary task shard that backs this capture. Primary (Green): Data is actively flowing through the capture.Pending (Yellow): The capture is attempting to re-connect. Often, you'll see this after you re-enable the capture as Flow backfills historical data.Failed (Red): The capture has failed with an unrecoverable error.Disabled (Hollow circle): The capture is disabled.Unknown (Black when app is in light mode; white when app is in dark mode): The web app is unable to determine shard status. Usually, this is due to a temporary connection error. 6: Capture name. The full name is shown, including all prefixes. 7: Capture type. The icon shows the type of source system data is captured from. 8: Capture statistics. The Data Written column shows the total amount of data, in bytes and in documents, that the capture has written to its associated collections within a configurable time interval. Click the time interval in the header to select from Today, Yesterday, This Week, Last Week, This Month, or Last Month. 9: Associated collections. The Writes to column shows all the collections to which the capture writes data. For captures with a large number of collections, hover over this column and scroll to view the full list. 10: Publish time. Hover over this value to see the exact time the capture was first published. 11: Options. Choose to View Details or Edit Specification. "},{"title":"Detail view​","type":1,"pageTitle":"Web application","url":"concepts/web-app/#detail-view","content":"When you click View Details for a capture, the Status and Specification viewers are revealed. The Status section shows the full identifier of the shard(s) that back your capture. If there's an error, you'll see an alert identifying the failing shard(s). Use the drop-down to open an expanded view of the failed shard's logs. In the Specification section, you can view the specification of the capture itself, as well as each collection to which it writes. Select a specification from the Files list to view the JSON. tip To change the size of each side of the Specification section, click and drag the center divider. "},{"title":"Editing captures and collections​","type":1,"pageTitle":"Web application","url":"concepts/web-app/#editing-captures-and-collections","content":"When you click Edit specification for a capture, you're taken to the Edit Capture page. This page is similar to the Create Capture page as it was filled out just before the capture was published. For detailed steps to edit a capture, see the guide. "},{"title":"Creating a capture​","type":1,"pageTitle":"Web application","url":"concepts/web-app/#creating-a-capture","content":"When you click Create Capture, you're taken to the Create Capture page. In the first view, all available capture connectors are displayed. Select the tile of the system from which you want to capture data to show the full capture form. The form details are specific to the connector you chose. For detailed steps to create a capture, see the guide. After you successfully publish a capture, you're given the option to materialize the collections you just captured. You can proceed to the materialization, or opt to exit to a different page of the web app. "},{"title":"Collections page​","type":1,"pageTitle":"Web application","url":"concepts/web-app/#collections-page","content":"The Collections page shows a read-only table of collections to which you have access. The table has many of the same features as the Captures table, with several important distinctions that are called out in the image below. You can use the table to view each collection's specification and see a sample of its data. This can help you verify that collection data was captured as expected and that you'll be able to materialize it how you want, and troubleshoot it necessary.  1: Status indicator. If the collection does not contain a derivation, the indicator should always show green, and hover text will say &quot;Collection.&quot; In the event that the server cannot be reached, the indicator will show &quot;Unknown&quot; status (black in light mode and white in dark mode). If the collection contains a derivation, the status of the derivation's primary task shard will be indicated: Primary (Green): Data is actively flowing through the derivation.Pending (Yellow): The derivation is attempting to re-connect.Failed (Red): The derivation has failed with an unrecoverable error.Disabled (Hollow circle): The derivation is disabled.Unknown (Black when app is in light mode; white when app is in dark mode): The web app is unable to determine shard status. Usually, this is due to a temporary connection error. 2: Collection statistics. The Data Written column shows the total amount of data, in bytes and in documents, that has been written to each collection from its associated capture or derivation within a configurable time interval. Click the time interval in the header to select from Today, Yesterday, This Week, Last Week, This Month, or Last Month. 3: To reveal the Specification and Data Preview sections, expand Details next to a collection name. 4: The Specification section shows the collection specification as JSON in a read-only editor. (If you need to modify a collection, edit the capture that it came from.) 5: The Data Preview section shows a sample of collection documents: the individual JSON files that comprise the collection. Documents are organized by their collection key value. Click a key from the list to view its document. "},{"title":"Materializations page​","type":1,"pageTitle":"Web application","url":"concepts/web-app/#materializations-page","content":"The Materializations page shows you a table of existing Flow materializations to which you have access. The New Materialization button is also visible. You use the table to monitor your materializations. It's nearly identical to the table on the Captures page, with a few exceptions.  1: Select all or deselect all. 2: Enable, Disable, and Delete buttons. These actions will be applied to the selected table rows. Choose Disable to temporarily pause the flow of data, Enable to resume, and Delete to permanently remove the materialization(s). 3: Filter materializations by name. Type a catalog prefix, unique materialization name, or connector name to return materializations that match your query. Materialization names follow the pattern prefix/unique-identifier, with prefix supporting multiple layers of nesting. You can search for any part of this full materialization name. You can also use the * wildcard. For example, if you have a materialization called acmeCo/logistics/anvil-locations, you can find it by filtering for acmeCo*locations. Unlike capture names, materialization names don't contain the connector name, but you can still filter them by connector. 4: Status indicator. Shows the status of the primary task shard that backs this materialization. Primary (Green): Data is actively flowing through the materialization.Pending (Yellow): The materialization is attempting to re-connect. Often, you'll see this after you re-enable the materialization as Flow backfills historical data.Failed (Red): The materialization has failed with an unrecoverable error.Disabled (Hollow circle): The materialization is disabled.Unknown (Black when app is in light mode; white when app is in dark mode): The web app is unable to determine shard status. Usually, this is due to a temporary connection error. 5: Materialization name. The full name is shown, including all prefixes. 6: Materialization type. The icon shows the type of destination system data is materialized to. 7: Materialization statistics. The Data Read column shows the total amount of data, in bytes and in documents, that the materialization has read from its associated collections within a configurable time interval. Click the time interval in the header to select from Today, Yesterday, This Week, Last Week, This Month, or Last Month. 8: Associated collections. The Reads from column shows all the collections from which the materialization reads data. For materializations with a large number of collections, hover over this column and scroll to view the full list. 9: Publish time. Hover over this value to see the exact time the materialization was first published. 10: Options. Choose to View Details or Edit Specification. "},{"title":"Detail view​","type":1,"pageTitle":"Web application","url":"concepts/web-app/#detail-view-1","content":"When you click View Details for a materialization, the Status and Specification viewers are revealed. The Status section shows the full identifier of the shard(s) that backs your materialization. If there's an error, you'll see an alert identifying the failing shard(s). Use the drop-down to open an expanded view of the failed shard's logs. In the Specification window, you can view the specification of the materialization itself, as well as each collection from which it reads. Select a specification from the Files list to view the JSON. "},{"title":"Editing materializations​","type":1,"pageTitle":"Web application","url":"concepts/web-app/#editing-materializations","content":"When you click Edit specification for a materialization, you're taken to the Edit Materialization page. This page is similar to the Create Materialization page as it was filled out just before the materialization was published. For detailed steps to edit a materialization, see the guide. "},{"title":"Creating a materialization​","type":1,"pageTitle":"Web application","url":"concepts/web-app/#creating-a-materialization","content":"There are three ways to begin creating a materialization: Clicking New Materialization on the Materializations page.Selecting one or more captures from the Captures page and clicking Materialize.Clicking Materialize Collections immediately after publishing a capture. When you initiate the workflow in any of these ways, all available materialization connectors are displayed. Select a connector to reveal the full form with configuration options specific to your desired destination. Fill out the Endpoint Config form and use the Collection Selector to map Flow collections to resources in the destination system. tip You can filter to quickly find the collections you want to include in your materialization. Hover your cursor within the Collection Selector table header, next to the Remove All button, to reveal an expandable menu icon (three dots). Click the menu icon, and then choose Filter. Note that if you entered the workflow from the Captures page or after publishing a capture, collections will be pre-populated for you. For detailed steps to create a materialization, see the guide. "},{"title":"Admin page​","type":1,"pageTitle":"Web application","url":"concepts/web-app/#admin-page","content":"On the Admin page, you can view users' access grants, your organization's cloud storage locations, and a complete list of connectors. You can also get an access token to authenticate with flowctl and update your cookie preferences. Users​ The Users tab shows you all provisioned access grants on objects to which you also have access. Both users and catalog prefixes can receive access grants. These are split up into two tables called Users and Prefixes. Each access grant has its own row, so a given user or prefix may have multiple rows. For example, if you had read access to foo/ and write access to bar/, you'd have a separate table row in the Users table for each of these capabilities. If users Alice, Bob, and Carol each had write access on foo/, you'd see three more table rows representing these access grants. Taking this a step further, the prefix foo/ could have read access to buz/. You'd see this in the Prefixes table, and it'd signify that everyone who has access to foo/ also inherits read access to buz/. Use the search boxes to filter by username, prefix, or object. Learn more about capabilities and access. Storage Mappings​ The Storage Mappings tab includes a table of the cloud storage locations that back your Flow collections. You're able to view the table if you're an admin. Each top-level Flow prefix is backed by one or more cloud storage bucket that you own. You typically have just one prefix: your organization name, which you provided when configuring your Flow organizational account. If you're a trial user, your prefix is trial/, and this tab isn't applicable to you; your data is stored temporarily in Estuary's cloud storage bucket for your trial period. Learn more about storage mappings. Connectors​ The Connectors tab offers a complete view of all connectors that are currently available through the web application, including both capture and materialization connectors. If a connector you need is missing, you can request it. CLI-API​ The CLI-API tab provides the access token required to authenticate with flowctl. Cookie Preferences​ You use the Cookie Preferences tab to view and modify cookie settings. "},{"title":"Registration and setup","type":0,"sectionRef":"#","url":"getting-started/installation/","content":"","keywords":""},{"title":"Get started with the Flow web application​","type":1,"pageTitle":"Registration and setup","url":"getting-started/installation/#get-started-with-the-flow-web-application","content":"You can get started with a Flow trial by visiting the web application here. As a trial user, you can create one end-to-end Data Flow. Trial user data is deleted from Flow after 30 days. To skip the limitations of the trial and begin with an organizational account instead, contact the Estuary team. "},{"title":"Get started with the Flow CLI​","type":1,"pageTitle":"Registration and setup","url":"getting-started/installation/#get-started-with-the-flow-cli","content":"After your account has been activated through the web app, you can begin to work with your data flows from the command line. This is not required, but it enables more advanced workflows or might simply be your preference. Flow has a single binary, flowctl. flowctl is available for: Linux x86-64. All distributions are supported.MacOS 11 (Big Sur) or later. Both Intel and M1 chips are supported. To install, copy and paste the appropriate script below into your terminal. This will download flowctl, make it executable, and add it to your PATH. For Linux: sudo curl -o /usr/local/bin/flowctl -L 'https://github.com/estuary/flow/releases/latest/download/flowctl-x86_64-linux' &amp;&amp; sudo chmod +x /usr/local/bin/flowctl For Mac: sudo curl -o /usr/local/bin/flowctl -L 'https://github.com/estuary/flow/releases/latest/download/flowctl-multiarch-macos' &amp;&amp; sudo chmod +x /usr/local/bin/flowctl Alternatively, Mac users can install with Homebrew: brew tap estuary/flowctl brew install flowctl  flowctl isn't currently available for Windows. For Windows users, we recommend running the Linux version inside WSL, or using a remote development environment. The flowctl source files are also on GitHub here. Once you've installed flowctl and are ready to begin working, authenticate your session using an access token. Ensure that you have an Estuary account and have signed into the Flow web app before. In the terminal of your local development environment, run: flowctl auth login In a browser window, the web app opens to the CLI-API tab. Copy the access token. Return to the terminal, paste the access token, and press Enter. The token will expire after a predetermined duration. Repeat this process to re-authenticate. Learn more about using flowctl. "},{"title":"Configuring your cloud storage bucket for use with Flow​","type":1,"pageTitle":"Registration and setup","url":"getting-started/installation/#configuring-your-cloud-storage-bucket-for-use-with-flow","content":"During your trial period, Flow uses Estuary's cloud storage to temporarily store your data. When you upgrade from a trial to an organizational account, you're provisioned a unique prefix in the Flow namespace, and transition to using your own cloud storage bucket to store your Flow data. This is called a storage mapping. Flow supports Google Cloud Storage and Amazon S3 buckets. Before your account manager configures your bucket as your storage mapping, you must grant access to Estuary. Google Cloud Storage buckets​ Follow the steps to add a principal to a bucket level policy. For the principal, enter flow-258@helpful-kingdom-273219.iam.gserviceaccount.comSelect the roles/storage.admin role. Amazon S3 buckets​ Contact your Estuary account manager for instructions. "},{"title":"Self-hosting Flow​","type":1,"pageTitle":"Registration and setup","url":"getting-started/installation/#self-hosting-flow","content":"The Flow runtime is available under the Business Source License. It's possible to self-host Flow using a cloud provider of your choice. Beta Setup for self-hosting is not covered in this documentation, and full support is not guaranteed at this time. We recommend using the hosted version of Flow for the best experience. If you'd still like to self-host, refer to the GitHub repository or the Estuary Slack. "},{"title":"What's next?​","type":1,"pageTitle":"Registration and setup","url":"getting-started/installation/#whats-next","content":"Start using Flow with these recommended resources. Create your first data flow: Follow this guide to create your first data flow in the Flow web app, while learning essential flow concepts. High level concepts: Start here to learn more about important Flow terms. "},{"title":"Flow tutorials","type":0,"sectionRef":"#","url":"getting-started/tutorials/","content":"Flow tutorials Flow tutorials are complete learning experiences that help you get to know Flow using sample data. You'll find these helpful if: You're testing out Flow with a trial account. You're a new user looking for practice before you implement production Data Flows. You'd rather learn the Flow concepts in a hands-on setting. Beta Flow is in private beta. Sign up for a free discovery callor email support@estuary.dev for your free trial account. If you're looking for more streamlined guidance for your own use case, check out the user guides.","keywords":""},{"title":"Derivations","type":0,"sectionRef":"#","url":"concepts/derivations/","content":"","keywords":""},{"title":"Creating derivations​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#creating-derivations","content":"You can create a derivation in your local development environment using flowctl. Use flowctl draft to begin work with a draft, and manually add a derivation to the Flow specification file. If necessary, generate a typescript file and define lambda functions there. "},{"title":"Specification​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#specification","content":"A derivation is specified as a regular collection with an additional derivation stanza: collections: # The unique name of the derivation. acmeCo/my/derivation: schema: my-schema.yaml key: [/key] # Presence of a `derivation` stanza makes this collection a derivation. # Type: object derivation: # Register definition of the derivation. # If not provided, registers have an unconstrained schema # and initialize to the `null` value. # Optional, type: object register: # JSON Schema of register documents. As with collection schemas, # this is either an inline definition or a relative URL reference. # Required, type: string (relative URL form) or object (inline form) schema: type: integer # Initial value taken by a register which has never been updated before. # Optional, default: null initial: 0 # TypeScript module that implements any lambda functions invoked by this derivation. # Optional, type: object typescript: # TypeScript module implementing this derivation. # Module is either a relative URL of a TypeScript module file (recommended), # or an inline representation of a TypeScript module. # The file specified will be created when you run `flowctl typescript generate` module: acmeModule.ts # NPM package dependencies of the module # Version strings can take any form understood by NPM. # See https://docs.npmjs.com/files/package.json#dependencies npmDependencies: {} # Transformations of the derivation, # specified as a map of named transformations. transform: # Unique name of the transformation, containing only Unicode # Letters and Numbers (no spaces or punctuation). myTransformName: # Source collection read by this transformation. # Required, type: object source: # Name of the collection to be read. # Required. name: acmeCo/my/source/collection # Partition selector of the source collection. # Optional. Default is to read all partitions. partitions: {} # Delay applied to sourced documents before being processed # by this transformation. # Default: No delay, pattern: ^\\\\d+(s|m|h)$ readDelay: &quot;48h&quot; # Shuffle determines the key by which source documents are # shuffled (mapped) to a register. # Optional, type: object. # If not provided, documents are shuffled on the source collection key. shuffle: # Key is a composite key which is extracted from documents # of the source. key: [/shuffle/key/one, /shuffle/key/two] # Update lambda of the transformation. # Optional, type: object update: {lambda: typescript} # Publish lambda of the transformation. # Optional, type: object publish: {lambda: typescript} # Priority applied to processing documents of this transformation # relative to other transformations of the derivation. # Default: 0, integer &gt;= 0 priority: 0  "},{"title":"Background​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#background","content":"The following sections will refer to the following common example to illustrate concepts. Suppose you have an application through which users send one another some amount of currency, like in-game tokens or dollars or digital kittens: transfers.flow.yamltransfers.schema.yaml collections: # Collection of 💲 transfers between accounts: # {id: 123, sender: alice, recipient: bob, amount: 32.50} acmeBank/transfers: schema: transfers.schema.yaml key: [/id]  There are many views over this data that you might require, such as summaries of sender or receiver activity, or current account balances within your application. "},{"title":"Transformations​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#transformations","content":"A transformation binds a source collection to a derivation. As documents of the source collection arrive, the transformation processes the document to publish new documents,update aregister, or both. Read source documents are shuffled on a shuffle key to co-locate the processing of documents that have equal shuffle keys. The transformation then processes documents by invoking lambdas: user-defined functions that accept documents as arguments and return documents in response. A derivation may have many transformations, and each transformation has a long-lived and stable name. Each transformation independently reads documents from its source collection and tracks its own read progress. More than one transformation can read from the same source collection, and transformations may also source from their own derivation, enabling cyclic data-flows and graph algorithms. Transformations may be added to or removed from a derivation at any time. This makes it possible to, for example, add a new collection into an existing multi-way join, or gracefully migrate to a new source collection without incurring downtime. However, renaming a running transformation is not possible. If attempted, the old transformation is dropped and a new transformation under the new name is created, which begins reading its source collection all over again. graph LR; d[Derivation]; t[Transformation]; r[Registers]; p[Publish λ]; u[Update λ]; c[Sourced Collection]; d-- has many --&gt;t; t-- reads from --&gt;c; t-- invokes --&gt;u; t-- invokes --&gt;p; u-- updates --&gt;r; r-- reads --&gt;p; d-- indexes --&gt;r; "},{"title":"Sources​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#sources","content":"The source of a transformation is a collection. As documents are published into the source collection, they are continuously read and processed by the transformation. A partition selector may be provided to process only a subset of the source collection's logical partitions. Selectors are efficient: only partitions that match the selector are read, and Flow can cheaply skip over partitions that don't. Derivations re-validate their source documents against the source collection's schema as they are read. This is because collection schemas may evolve over time, and could have inadvertently become incompatible with historical documents of the source collection. Upon a schema error, the derivation will pause and give you an opportunity to correct the problem. You may also provide an alternative source schema. Source schemas aide in processing third-party sources of data that you don't control, which can have unexpected schema changes without notice. You may want to capture this data with a minimal and very permissive schema. Then, a derivation can apply a significantly stricter source schema, which verifies your current expectations of what the data should be. If those expectations turn out to be wrong, little harm is done: your derivation is paused but the capture continues to run. You must simply update your transformations to account for the upstream changes and then continue without any data loss. "},{"title":"Shuffles​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#shuffles","content":"As each source document is read, it's shuffled — or equivalently, mapped — on an extracted key. If you're familiar with data shuffles in tools like MapReduce, Apache Spark, or Flink, the concept is very similar. Flow catalog tasks scale to run across many machines at the same time, where each machine processes a subset of source documents. Shuffles let Flow know how to group documents so that they're co-located, which can increase processing efficiency and reduce data volumes. They are also used to map source documents to registers. graph LR; subgraph s1 [Source Partitions] p1&gt;acmeBank/transfers/part-1]; p2&gt;acmeBank/transfers/part-2]; end subgraph s2 [Derivation Task Shards] t1([task/shard-1]); t2([task/shard-2]); end p1-- sender: alice --&gt;t1; p1-- sender: bob --&gt;t2; p2-- sender: alice --&gt;t1; p2-- sender: bob --&gt;t2; If you don't provide a shuffle key, Flow will shuffle on the source collection key, which is typically what you want. If a derivation has more than one transformation, the shuffle keys of all transformations must align with one another in terms of the extracted key types (string or integer) as well as the number of components in a composite key. For example, one transformation couldn't shuffle transfers on [/id]while another shuffles on [/sender], because sender is a string andid an integer. Similarly mixing a shuffle of [/sender] alongside [/sender, /recipient]is prohibited because the keys have different numbers of components. However, one transformation can shuffle on [/sender]while another shuffles on [/recipient], as in the examples below. "},{"title":"Registers​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#registers","content":"Registers are the internal memory of a derivation. They are a building block that enable derivations to tackle advanced stateful streaming computations like multi-way joins, windowing, and transaction processing. As we've already seen, not all derivations require registers, but they are essential for a variety of important use cases. Each register is a document with a user-definedschema. Registers are keyed, and every derivation maintains an index of keys and their corresponding register documents. Every source document is mapped to a specific register document through its extracted shuffle key. For example, when shuffling acmeBank/transfers on [/sender] or [/recipient], each account (&quot;alice&quot;, &quot;bob&quot;, or &quot;carol&quot;) is allocated its own register. You might use that register to track a current account balance given the received inflows and sent outflows of each account. If you instead shuffle on [/sender, /recipient], each pair of accounts (&quot;alice -&gt; bob&quot;, &quot;alice -&gt; carol&quot;, &quot;bob -&gt; carol&quot;) is allocated a register. Transformations of a derivation may have different shuffle keys, but the number of key components and their JSON types must agree. Two transformations could map on [/sender] and [/recipient], but not [/sender] and [/recipient, /sender]. Registers are best suited for relatively small, fast-changing documents that are shared within and across the transformations of a derivation. The number of registers indexed within a derivation may be very large, and if a register has never before been used, it starts with a user-defined initial value. From there, registers may be modified through an update lambda. info Under the hood, registers are backed by replicated, embedded RocksDB instances, which co-locate with the lambda execution contexts that Flow manages. As contexts are assigned and re-assigned, their register databases travel with them. If any single RocksDB instance becomes too large, Flow is able to perform an online split, which subdivides its contents into two new databases — and paired execution contexts — which are re-assigned to other machines. "},{"title":"Lambdas​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#lambdas","content":"Lambdas are user-defined functions that are invoked by derivations. They accept documents as arguments and return transformed documents in response. Lambdas can be used to update registers, publish documents into a derived collection, or compute a non-trivial shuffle key of a document. Beta The ability for lambdas to compute a document's shuffle key is coming soon. Flow supports TypeScript lambdas, which you define in an accompanying TypeScript module and reference in a derivation's typescript stanzas. See the derivation specification and Creating TypeScript modules for more details on how to get started. TypeScript lambdas are &quot;serverless&quot;; Flow manages the execution and scaling of your Lambda on your behalf. Alternatively, Flow also supports remote lambdas, which invoke an HTTP endpoint you provide, such as an AWS Lambda or Google Cloud Run function. In terms of the MapReduce functional programming paradigm, Flow lambdas are mappers, which map documents into new user-defined shapes. Reductions are implemented by Flow using the reduction annotations of your collection schemas. "},{"title":"Publish lambdas​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#publish-lambdas","content":"A publish lambda publishes documents into the derived collection. To illustrate first with an example, suppose you must know the last transfer from each sender that was over $100: last-large-send.flow.yamllast-large-send.tslast-large-send-test.flow.yaml import: - transfers.flow.yaml collections: examples/acmeBank/last-large-send: schema: transfers.schema.yaml key: [/sender] derivation: typescript: module: last-large-send.ts transform: fromTransfers: source: name: examples/acmeBank/transfers publish: lambda: typescript  This transformation defines a TypeScript publish lambda, which is implemented in an accompanying TypeScript module. The lambda is invoked as each source transfer document arrives. It is given the source document, and also includes the a _register and _previous register, which are not used here. The lambda outputs zero or more documents, each of which must conform to the derivation's schema. As this derivation's collection is keyed on /sender, the last published document (the last large transfer) of each sender is retained. If it were instead keyed on /id, then all transfers with large amounts would be retained. In SQL terms, the collection key acts as a GROUP BY.  Derivation collection schemas may havereduction annotations, and publish lambdas can be combined with reductions in interesting ways. You may be familiar with map and reduce functions built into languages likePython,JavaScript; and many others, or have used tools like MapReduce or Spark. In functional terms, lambdas you write within Flow are &quot;mappers,&quot; and reductions are always done by the Flow runtime using your schema annotations. Suppose you need to know the runningaccount balancesof your users given all of their transfers thus far. Tackle this by reducing the final account balance for each user from all of the credit and debit amounts of their transfers: balances.flow.yamlbalances.tsbalances-test.flow.yaml import: - transfers.flow.yaml collections: examples/acmeBank/balances: schema: type: object required: [user] reduce: { strategy: merge } properties: user: { type: string } balance: type: number reduce: { strategy: sum } key: [/user] derivation: typescript: module: balances.ts transform: fromTransfers: source: name: examples/acmeBank/transfers publish: lambda: typescript  "},{"title":"Update lambdas​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#update-lambdas","content":"An update lambda transforms a source document into an update of the source document's register. To again illustrate through an example, suppose your compliance department wants you to flag the first transfer a sender sends to a new recipient. You achieve this by shuffling on pairs of[/sender, /recipient] and using a register to track whether this account pair has been seen before: first-send.flow.yamlfirst-send.tsfirst-send-test.flow.yaml import: - transfers.flow.yaml collections: examples/acmeBank/first-send: schema: transfers.schema.yaml key: [/id] derivation: # We'll store a `true/false` boolean in our register documents, # which is initially `false` and becomes `true` after the first transfer. register: schema: { type: boolean } initial: false typescript: module: first-send.ts transform: fromTransfers: source: name: examples/acmeBank/transfers # Shuffle so that each account pair # is allocated its own register. shuffle: key: [/sender, /recipient] update: lambda: typescript publish: lambda: typescript  This transformation uses both a publish and an update lambda, implemented in an accompanying TypeScript module. The update lambda is invoked first for each source document, and it returns zero or more documents, which each must conform to the derivation's register schema (in this case, a simple boolean). The publish lambda is invoked next, and is given the sourcedocument as well as the before (previous) and after (_register) values of the updated register. In this case, we don't need the after value: our update lambda implementation implies that it's always true. The before value, however, tells us whether this was the very first update of this register, and by implication was the first transfer for this pair of accounts. sequenceDiagram autonumber Derivation-&gt;&gt;Update λ: update({sender: alice, recipient: bob})? Update λ--&gt;&gt;Derivation: return &quot;true&quot; Derivation-&gt;&gt;Registers: lookup(key = [alice, bob])? Registers--&gt;&gt;Derivation: not found, initialize as &quot;false&quot; Derivation--&gt;&gt;Derivation: Register: &quot;false&quot; =&gt; &quot;true&quot; Derivation-)Registers: store(key = [alice, bob], value = &quot;true&quot;) Derivation-&gt;&gt;Publish λ: publish({sender: alice, recipient: bob}, register = &quot;true&quot;, previous = &quot;false&quot;)? Publish λ--&gt;&gt;Derivation: return {sender: alice, recipient: bob} FAQ Why not have one lambda that can return a register update and derived documents? Performance.Update and publish are designed to be parallelized and pipelined over many source documents simultaneously, while still giving the appearance and correctness of lambdas are invoked in strict serial order. Notice that (1) above doesn't depend on actually knowing the register value, which doesn't happen until (4). Many calls like (1) can also happen in parallel, so long as their applications to the register value (5) happen in the correct order. In comparison, a single-lambda design would require Flow to await each invocation before it can begin the next.  Register schemas may also havereduction annotations, and documents returned by update lambdas are reduced into the current register value. The compliance department reached out again, and this time they need you to identify transfers where the sender's account had insufficient funds. You manage this by tracking the running credits and debits of each account in a register. Then, you enrich each transfer with the account's current balance and whether the account was overdrawn: flagged-transfers.flow.yamlflagged-transfers.tsflagged-transfers-test.flow.yaml import: - transfers.flow.yaml collections: examples/acmeBank/flagged-transfers: schema: # Extend transfer schema with `balance` and `overdrawn` fields. $ref: transfers.schema.yaml required: [balance, overdrawn] properties: balance: { type: number } overdrawn: { type: boolean } key: [/id] projections: # Logically partition on transfers which are flagged as overdrawn. overdrawn: location: /overdrawn partition: true derivation: # Registers track the current balance of each account. register: schema: type: number reduce: { strategy: sum } initial: 0 typescript: module: flagged-transfers.ts transform: fromTransferSender: source: { name: examples/acmeBank/transfers } shuffle: { key: [/sender] } # Debit the sender's register balance. update: { lambda: typescript } # Publish transfer enriched with current sender balance. publish: { lambda: typescript } fromTransferRecipient: source: { name: examples/acmeBank/transfers } shuffle: { key: [/recipient] } # Credit the recipient's register balance. update: { lambda: typescript }  Source transfers are read twice. The first read shuffles on /recipientto track account credits, and the second shuffles on /senderto track account debits and to publish enriched transfer events. Update lambdas return the amount of credit or debit, and these amounts are summed into a derivation register keyed on the account. sequenceDiagram autonumber Derivation-&gt;&gt;Registers: lookup(key = alice)? Registers--&gt;&gt;Derivation: not found, initialize as 0 Derivation-&gt;&gt;Update λ: update({recipient: alice, amount: 50, ...})? Update λ--&gt;&gt;Derivation: return +50 Derivation-&gt;&gt;Update λ: update({sender: alice, amount: 75, ...})? Update λ--&gt;&gt;Derivation: return -75 Derivation--&gt;&gt;Derivation: Register: 0 + 50 =&gt; 50 Derivation--&gt;&gt;Derivation: Register: 50 - 75 =&gt; -25 Derivation-&gt;&gt;Publish λ: publish({sender: alice, amount: 75, ...}, register = -25, previous = 50)? Publish λ--&gt;&gt;Derivation: return {sender: alice, amount: 75, balance: -25, overdrawn: true} "},{"title":"Creating TypeScript modules​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#creating-typescript-modules","content":"To create a new TypeScript module for the lambdas of your derivation, you can use flowctl typescript generate to generate it. In the derivation specification, choose the name for the new module and run flowctl typescript generate. Flow creates a module with the name you specified, stubs of the required interfaces, and TypeScript types that match your schemas. Update the module with your lambda function bodies, and proceed to test and deploy your catalog. Using the example below, flowctl typescript generate --source=acmeBank.flow.yaml will generate the stubbed-out acmeBank.ts. acmeBank.flow.yamlacmeBank.ts (generated stub) collections: acmeBank/balances: schema: balances.schema.yaml key: [/account] derivation: typescript: module: acmeBank.ts transform: fromTransfers: source: { name: acmeBank/transfers } publish: { lambda: typescript }  Learn more about TypeScript generation "},{"title":"NPM dependencies​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#npm-dependencies","content":"Your TypeScript modules may depend on otherNPM packages, which can be be imported through the npmDependenciesstanza of the derivation spec. For example, moment is a common library for working with times: derivation.flow.yamlfirst-send.ts derivation: typescript: module: first-send.ts npmDependencies: moment: &quot;^2.24&quot; transform: { ... }  Use any version string understood by package.json, which can include local packages, GitHub repository commits, and more. See package.json documentation. During the catalog build process, Flow gathers NPM dependencies across all Flow specification files and patches them into the catalog's managed package.json. Flow organizes its generated TypeScript project structure for a seamless editing experience out of the box with VS Code and other common editors. "},{"title":"Remote lambdas​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#remote-lambdas","content":"A remote Lambda is one that you implement and host yourself as a web-accessible endpoint, typically via a service like AWS Lambda or Google Cloud Run. Flow will invoke your remote Lambda as needed, POST-ing JSON documents to process and expecting JSON documents in the response. "},{"title":"Processing order​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#processing-order","content":"Derivations may have multiple transformations that simultaneously read from different source collections, or even multiple transformations that read from the same source collection. Roughly speaking, the derivation will globally process transformations and their source documents in the time-based order in which the source documents were originally written to their source collections. This means that a derivation started a month ago and a new copy of the derivation started today, will process documents in the same order and arrive at the same result. Derivations are repeatable. More precisely, processing order is stable for each individual shuffle key, though different shuffle keys may process in different orders if more than one task shard is used. Processing order can be attenuated through a read delayor differing transformation priority. "},{"title":"Read delay​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#read-delay","content":"A transformation can define a read delay, which will hold back the processing of its source documents until the time delay condition is met. For example, a read delay of 15 minutes would mean that a source document cannot be processed until it was published at least 15 minutes ago. If the derivation is working through a historical backlog of source documents, than a delayed transformation will respect its ordering delay relative to the publishing times of other historical documents also being read. Event-driven workflows are a great fit for reacting to events as they occur, but aren’t terribly good at taking action when something hasn’t happened: A user adds a product to their cart, but then doesn’t complete a purchase.A temperature sensor stops producing its expected, periodic measurements. A common pattern for tackling these workflows in Flow is to read a source collection without a delay and update a register. Then, read a collection with a read delay and determine whether the desired action has happened or not. For example, source from a collection of sensor readings and index the last timestamp of each sensor in a register. Then, source the same collection again with a read delay: if the register timestamp isn't more recent than the delayed source reading, the sensor failed to produce a measurement. Flow read delays are very efficient and scale better than managing very large numbers of fine-grain timers. Learn more from the Citi Bike idle bikes example "},{"title":"Read priority​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#read-priority","content":"Sometimes it's necessary for all documents of a source collection to be processed by a transformation before any documents of some other source collection are processed, regardless of their relative publishing time. For example, a collection may have corrections that should be applied before the historical data of another collection is re-processed. Transformation priorities allow you to express the relative processing priority of a derivation's various transformations. When priorities are not equal, all available source documents of a higher-priority transformation are processed before any source documents of a lower-priority transformation. "},{"title":"Where to accumulate?​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#where-to-accumulate","content":"When you build a derived collection, you must choose where accumulation will happen: whether Flow will reduce into documents held within your materialization endpoint, or within the derivation's registers. These two approaches can produce equivalent results, but they do so in very different ways. "},{"title":"Accumulate in your database​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#accumulate-in-your-database","content":"To accumulate in your materialization endpoint, such as a database, you define a derivation with a reducible schema and use only publish lambdas and no registers. The Flow runtime uses your reduction annotations to combine published documents, which are written to the collection. It then fully reduces collection documents into the values stored in the database. This keeps the materialized table up to date. A key insight is that the database is the only stateful system in this scenario, and Flow uses reductions in two steps: To combine many published documents into intermediate delta documents, which are the documents written to collection storage.To reduce delta states into the final database-stored document. For example, consider a collection that’s summing a value: Time\tDB\tLambdas\tDerived DocumentT0\t0\tpublish(2, 1, 2)\t5 T1\t5\tpublish(-2, 1)\t-1 T2\t4\tpublish(3, -2, 1)\t2 T3\t6\tpublish()\t This works especially well when materializing into a transactional database. Flow couples its processing transactions with corresponding database transactions, ensuring end-to-end “exactly once” semantics. When materializing into a non-transactional store, Flow is only able to provide weaker “at least once” semantics; it’s possible that a document may be combined into a database value more than once. Whether that’s a concern depends a bit on the task at hand. Some reductions can be applied repeatedly without changing the result (they're &quot;idempotent&quot;), while in other use cases approximations are acceptable. For the summing example above, &quot;at-least-once&quot; semantics could give an incorrect result. "},{"title":"Accumulate in registers​","type":1,"pageTitle":"Derivations","url":"concepts/derivations/#accumulate-in-registers","content":"To accumulate in registers, you use a derivation that defines a reducible register schema that's updated through update lambdas. The Flow runtime allocates, manages, and scales durable storage for registers; you don’t have to. Then you use publish lambdas to publish a snapshot of your register value into your collection. Returning to our summing example: Time\tRegister\tLambdas\tDerived DocumentT0\t0\tupdate(2, 1, 2), publish(register)\t5 T1\t5\tupdate(-2, 1), publish(register)\t4 T2\t4\tupdate(3, -2, 1), publish(register)\t6 T3\t6\tupdate()\t Register derivations are a great solution for materializations into non-transactional stores because the documents they produce can be applied multiple times without breaking correctness. They’re also well-suited for materializations into endpoints that aren't stateful, such as pub/sub systems or Webhooks, because they can produce fully reduced values as stand-alone updates. Learn more in the derivation pattern examples of Flow's repository "},{"title":"Create a real-time materialized view in PostgreSQL","type":0,"sectionRef":"#","url":"getting-started/tutorials/continuous-materialized-view/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Create a real-time materialized view in PostgreSQL","url":"getting-started/tutorials/continuous-materialized-view/#prerequisites","content":"An Estuary Flow trial account (or a full account). If you don't have one, visit the Flow web app to register for your free trial. The flowctl CLI installed (for the optional section). A Postgres database for testing set up to allow connections from Flow. Amazon RDS, Amazon Aurora, Google Cloud SQL, Azure Database for PostgreSQL, and self-hosted databases are supported. "},{"title":"Introduction​","type":1,"pageTitle":"Create a real-time materialized view in PostgreSQL","url":"getting-started/tutorials/continuous-materialized-view/#introduction","content":"Materialized views in Postgres give you a powerful way narrow down a huge dataset into a compact one that you can easily monitor. But if your data is updating in real-time, traditional materialized views introduce latency. They're batch workflows — the query is run at a set interval. To get around this, you'll need to perform a real-time transformation elsewhere. Flow derivations are a great way to do this. For this example, you'll use Estuary's public data collection of recent changes to Wikipedia, captured from the Wikimedia Foundation's event stream. The raw dataset is quite large. It captures every change to the platform — about 30 per second — and includes various properties. Written to a Postgres table, it quickly grows to an size that's very expensive to query. First, you'll scope the raw data down to a small fact table with a derivation. You'll then materialize both the raw and transformed datasets to your Postgres instance and compare performance. "},{"title":"Add a derivation to transform data.​","type":1,"pageTitle":"Create a real-time materialized view in PostgreSQL","url":"getting-started/tutorials/continuous-materialized-view/#add-a-derivation-to-transform-data","content":"Derivations are currently available in the flowctl CLI. If you'd prefer to only work in the web app today, you can skip to the next section. Estuary provides a pre-computed copy of the derivation that you can use to compare performance. On the CLI-API tab in the Flow web app, copy the access token. Authorize Flow in your local development environment: flowctl auth token --token &lt;your-token-here&gt; Next, pull the raw wikipedia collection. flowctl catalog pull-specs --name estuary/public/wikipedia/recentchange Source files are written to your current working directory. Open Estuary &gt; Public &gt; Wikipedia and examine the contents of flow.yaml and recentchange.schema.yaml. The collection is keyed on its metadata, so every new change event is seen as unique. Its schema has many fields. This would yield a large, unwieldy table in Postgres. Learn more about Flow collections and schemas. Next, you'll add the derivation. Technically, a derivation is a new collection that contains a transformation within it. First, you'll define the collection. Then, you'll flesh out the transformation. Create a new file called fact-table.flow.yaml and add a new collection called &lt;your-prefix&gt;/wikipedia/user-fact-table. tip Your prefix is likely your organization name. You can find it in the web app's admin tab. You must have write or admin access to create a collection in the prefix. Copy the sample below: --- collections: yourprefix/wikipedia/user-fact-table: schema: properties: count: reduce: strategy: sum type: - integer last_updated: format: date type: string userid: type: - integer reduce: strategy: merge required: - last_updated - userid type: object key: - /userid - /last_updated derivation: register: schema: allOf: - true initial: ~ transform: new_fact_table: source: name: estuary/public/wikipedia/recentchange publish: lambda: typescript typescript: module: user-fact-table.ts  The new collection's schema contains reduction annotations. These merge the data based on the user ID and the date they were last updated. Generate a TypeScript file for the derivation's transformation function. flowctl typescript generate --source flow.yaml Open user-fact-table.ts. It contains a stubbed-out transformation. You'll populate it with a function that counts the number of changes associated with each user on a given date and converts the timestamp in the source data to a familiar date format. Copy and paste from the below sample (beginning at line 4): import { IDerivation, Document, Register, NewFactTableSource } from 'flow/yourprefix/wikipedia/user-fact-table'; // Implementation for derivation estuary/public/wikipedia/flow.yaml#/collections/yourprefix~1wikipedia~1user-fact-table/derivation. export class Derivation implements IDerivation { newFactTablePublish( source: NewFactTableSource, _register: Register, _previous: Register, ): Document[] { let user_id = 0; if (typeof source.log_params == &quot;object&quot; &amp;&amp; !Array.isArray(source.log_params) &amp;&amp; source.log_params.userid != undefined) { user_id = source.log_params.userid; } const [yyyy, mm, dd] = source.meta.dt.split('-'); const dd2 = dd.substring(0, 2); let date = yyyy + '-' + mm + '-' + dd2; return [ { userid: user_id, count: 1, last_updated: date, }, ] } } Publish the derivation: flowctl catalog publish --source path/to/your/fact-table.flow.yaml  Your transformation will continue in real time based on the raw dataset, which is also updating in real time. "},{"title":"Create the continuous materialized view​","type":1,"pageTitle":"Create a real-time materialized view in PostgreSQL","url":"getting-started/tutorials/continuous-materialized-view/#create-the-continuous-materialized-view","content":"Now, you'll materialize your new fact table to Postgres. You'll also materialize the source dataset to compare performance. Go to the Materializations page in the Flow web app. Click New Materialization. For Connector, choose PostgreSQL. Add a unique name for the materialization, for example, yourprefix/yourname-materialized-views-demo. Fill out the Basic Config with: A username and password for the Postgres instance. Your database host and port. The database name (if in doubt, use the default, postgres). See the connector documentation if you need help finding these properties. In the Collection Selector, search for and add the collection estuary/public/wikipedia/recentchange and name the corresponding Postgres Table wikipedia_raw. Also search for and add the collection you just derived, (for example, yourprefix/wikipedia/user-fact-table). If you skipped the derivation, use the provided version, estuary/public/wikipedia/user-fact-table. Name the corresponding Postgres table wikipedia_data_by_user. Click Next to test the connection. Click Save and Publish. "},{"title":"Explore the results​","type":1,"pageTitle":"Create a real-time materialized view in PostgreSQL","url":"getting-started/tutorials/continuous-materialized-view/#explore-the-results","content":"In your Postgres client of choice, note the size of each table and how they quickly change. Try running some basic queries against both and compare performance. See the blog post for ideas. Once you're satisfied, and to prevent continual resource use, disable or delete your materialization from theMaterializations page. "},{"title":"Resources​","type":1,"pageTitle":"Create a real-time materialized view in PostgreSQL","url":"getting-started/tutorials/continuous-materialized-view/#resources","content":"Detailed guide to create derivations. "},{"title":"Flow user guides","type":0,"sectionRef":"#","url":"guides/","content":"Flow user guides In this section, you'll find step-by-step guides that walk you through common Flow tasks. These guides are designed to help you work with Data Flows in production — we assume you have your own data and are familiar with your source and destination systems. You might be here to get your data moving with Flow as quickly as possible, reshape your collection with a derivation, or create a secure connection to your database. If you'd prefer a tailored learning experience with sample data, check out the Flow tutorials.","keywords":""},{"title":"Create your first dataflow with Amazon S3 and Snowflake","type":0,"sectionRef":"#","url":"getting-started/tutorials/dataflow-s3-snowflake/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Create your first dataflow with Amazon S3 and Snowflake","url":"getting-started/tutorials/dataflow-s3-snowflake/#prerequisites","content":"You'll need: An Estuary Flow trial account (or a full account). If you don't have one, visit the Flow web app to register for your free trial. A Snowflake free trial account (or a full account). Snowflake trials are valid for 30 days. "},{"title":"Introduction​","type":1,"pageTitle":"Create your first dataflow with Amazon S3 and Snowflake","url":"getting-started/tutorials/dataflow-s3-snowflake/#introduction","content":"The data​ New York City hosts the United States' largest bike share program, Citi Bike. Citi Bike shares ride data in CSV format with the public, including the starting and ending times and locations for every ride. They upload new data monthly to their Amazon S3 bucket as a zipped CSV file. In this scenario, let's imagine you're interested in urban bike safety, or perhaps you plan to open a bike store and entice Citi Bike renters to buy their own bikes. You'd like to access the Citi Bike data in your Snowflake data warehouse. From there, you plan to use your data analytics platform of choice to explore the data, and perhaps integrate it with business intelligence apps. You can use Estuary Flow to build a real-time Data Flow that will capture all the new data from Citi Bike as soon as it appears, convert it to Snowflake's format, and land the data in your warehouse. Estuary Flow​ In Estuary Flow, you create Data Flows to connect data source and destination systems. The simplest Data Flow comprises three types of entities: A data capture, which ingests data from the source. In this case, you'll capture from Amazon S3. One or more collections, which Flow uses to store that data inside a cloud-backed data lake A materialization, to push the data to an external destination. In this case, you'll materialize to a Snowflake data warehouse. graph LR; Capture--&gt;Collection; Collection--&gt;Materialization; For the capture and materialization to work, they need to integrate with outside systems: in this case, S3 and Snowflake, but many other systems can be used. To accomplish this, Flow uses connectors. Connectors are plug-in components that interface between Flow and an outside system. Today, you'll use Flow's S3 capture connector and Snowflake materialization connector. You'll start by creating your capture. "},{"title":"Capture Citi Bike data from S3​","type":1,"pageTitle":"Create your first dataflow with Amazon S3 and Snowflake","url":"getting-started/tutorials/dataflow-s3-snowflake/#capture-citi-bike-data-from-s3","content":"Go to the Flow web app at dashboard.estuary.dev and sign in. Click the Captures tab and choose New Capture All of the available capture connectors — representing the possible data sources — appear as tiles. Choose the Amazon S3 tile. A form appears with the properties required for an S3 capture. Every connector requires different properties to configure. First, you'll name your capture. Click inside the Name box. Names of entities in Flow must be unique. They're organized by prefixes, similar to paths in a file system. If you're using a Flow trial account, you'll see the trial/ prefix available to you in the drop-down menu. If you're not a trial user, you'll see one or more prefixes pertaining to your organization. These prefixes represent the namespaces of Flow to which you have access. Click trial/ (or another prefix) from the dropdown and append a unique name after it. For example, trial/yourname/citibiketutorial. Next, fill out the required properties for S3. AWS Access Key ID and AWS Secret Access Key: The bucket is public, so you can leave these fields blank. AWS Region: us-east-1 Bucket: tripdata Prefix: The storage bucket isn't organized by prefixes, so leave this blank. Match Keys: 2022 The Citi Bike storage bucket has been around for a while. Some of the older datasets have incorrect file extensions or contain data in different formats. By selecting a subset of files from the year 2022, you'll make things easier to manage for the purposes of this tutorial. (In a real-world use case, you'd likely reconcile the different schemas of the various data formats using a derivation.Derivations are a more advanced Flow skill.) Click Next. Flow uses the configuration you provided to initiate a connection with S3. It generates a specification with details of the capture, and another that contains details and a schema for the collection that will store the captured data in Flow. Once this process completes, you can move on to the next step. If there's an error, go back and check your configuration. Click Save and Publish. Flow deploys, or publishes, your capture, including your change to the schema. You'll see a notification when the this is complete. A subset of data from the Citi Bike tripdata bucket has been captured to a Flow collection. Now, you can materialize that data to Snowflake. Click Materialize Collections. "},{"title":"Prepare Snowflake to use with Flow​","type":1,"pageTitle":"Create your first dataflow with Amazon S3 and Snowflake","url":"getting-started/tutorials/dataflow-s3-snowflake/#prepare-snowflake-to-use-with-flow","content":"Before you can materialize from Flow to Snowflake, you need to complete some setup steps. Leave the Flow web app open. In a new window or tab, go to your Snowflake console. If you're a new trial user, you should have received instructions by email. For additional help in this section, see the Snowflake documentation. Create a new Snowflake worksheet if you don't have one open. This provides an interface where you can run queries. Paste the follow script into the console, changing the value for estuary_password from secret to a strong password): set database_name = 'ESTUARY_DB'; set warehouse_name = 'ESTUARY_WH'; set estuary_role = 'ESTUARY_ROLE'; set estuary_user = 'ESTUARY_USER'; set estuary_password = 'secret'; set estuary_schema = 'ESTUARY_SCHEMA'; -- create role and schema for Estuary create role if not exists identifier($estuary_role); grant role identifier($estuary_role) to role SYSADMIN; -- Create snowflake DB create database if not exists identifier($database_name); use database identifier($database_name); create schema if not exists identifier($estuary_schema); -- create a user for Estuary create user if not exists identifier($estuary_user) password = $estuary_password default_role = $estuary_role default_warehouse = $warehouse_name; grant role identifier($estuary_role) to user identifier($estuary_user); grant all on schema identifier($estuary_schema) to identifier($estuary_role); -- create a warehouse for estuary create warehouse if not exists identifier($warehouse_name) warehouse_size = xsmall warehouse_type = standard auto_suspend = 60 auto_resume = true initially_suspended = true; -- grant Estuary role access to warehouse grant USAGE on warehouse identifier($warehouse_name) to role identifier($estuary_role); -- grant Estuary access to database grant CREATE SCHEMA, MONITOR, USAGE on database identifier($database_name) to role identifier($estuary_role); -- change role to ACCOUNTADMIN for STORAGE INTEGRATION support to Estuary (only needed for Snowflake on GCP) use role ACCOUNTADMIN; grant CREATE INTEGRATION on account to role identifier($estuary_role); use role sysadmin; COMMIT;  Run all the queries. If you're using the the Snowflake Classic Web Interface, check All queries and click Run. If you're using the Snowsight interface, highlight the whole script and click the blue run button. Snowflake is ready to use with Flow. Return to the Flow web application. "},{"title":"Materialize your Flow collection to Snowflake​","type":1,"pageTitle":"Create your first dataflow with Amazon S3 and Snowflake","url":"getting-started/tutorials/dataflow-s3-snowflake/#materialize-your-flow-collection-to-snowflake","content":"You were directed to the Materializations page. All of the available materialization connectors — representing the possible data destinations — are shown as tiles. Locate and select the Snowflake tile. A new form appears with the properties required to materialize to Snowflake. Click inside the Name box. Click trial/ (or another prefix) from the dropdown and append a unique name after it. For example, trial/yourname/citibiketutorial. Next, fill out the required properties for Snowflake (most of these come from the script you just ran). Host URL: This is the URL you use to log into Snowflake. If you recently signed up for a trial, it should be in your email. Omit the protocol from the beginning. For example, ACCOUNTID.region.cloudprovider.snowflakecomputing.com or orgname-accountname.snowflakecomputing.com. Learn more about account identifiers and host URLs. Account: Your account identifier. This is part of the Host URL. Using the previous examples, it would be ACCOUNTID or accountname. User: ESTUARY_USER Password: secret (Substitute the password you set in the script.) Database: ESTUARY_DB Schema: ESTUARY_SCHEMA Warehouse: ESTUARY_WH Role: ESTUARY_ROLE Scroll down to view the Collection Selector and change the default name in the Table field to CitiBikeData or another name of your choosing. Click Next. Flow uses the configuration you provided to initiate a connection with Snowflake and generate a specification with details of the materialization. Once this process completes, you can move on to the next step. If there's an error, go back and check your configuration. Click Save and Publish. Flow publishes the materialization. Return to the Snowflake console and expand ESTUARY_DB and ESTUARY_SCHEMA. You'll find the materialized table there. "},{"title":"Conclusion​","type":1,"pageTitle":"Create your first dataflow with Amazon S3 and Snowflake","url":"getting-started/tutorials/dataflow-s3-snowflake/#conclusion","content":"You've created a complete Data Flow that ingests the Citi Bike CSV files from an Amazon S3 bucket and materializes them into your Snowflake database. When Citi Bike uploads new data, it'll be reflected in Snowflake in near-real-time, so long as you don't disable your capture or materialization. Data warehouses like Snowflake are designed to power data analytics. From here, you can begin any number of analytical workflows. Want to learn more?​ For more information on the connectors you used today, see the pages on S3 and Snowflake. You can create a Data Flow using any combination of supported connectors with a similar process to the one you followed in this tutorial. For a more generalized procedure, see the guide to create a Data Flow. "},{"title":"Create a basic Data Flow","type":0,"sectionRef":"#","url":"guides/create-dataflow/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Create a basic Data Flow","url":"guides/create-dataflow/#prerequisites","content":"This guide is intended for new Flow users and briefly introduces Flow's key concepts. Though it's not required, you may find it helpful to read the high level concepts documentation for more detail before you begin. "},{"title":"Introduction​","type":1,"pageTitle":"Create a basic Data Flow","url":"guides/create-dataflow/#introduction","content":"In Estuary Flow, you create Data Flows to connect data source and destination systems. The simplest Data Flow comprises three types of entities: A data capture, which ingests data from an external sourceOne or more collections, which store that data in a cloud-backed data lakeA materialization, to push the data to an external destination Almost always, the capture and materialization each rely on a connector. A connector is a plug-in component that interfaces between Flow and whatever data system you need to connect to. Here, we'll walk through how to leverage various connectors, configure them, and deploy your Data Flow. "},{"title":"Create a capture​","type":1,"pageTitle":"Create a basic Data Flow","url":"guides/create-dataflow/#create-a-capture","content":"You'll first create a capture to connect to your data source system. This process will create one or more collections in Flow, which you can then materialize to another system. Go to the Flow web application at dashboard.estuary.dev and sign in using the credentials provided by your Estuary account manager. Click the Captures tab and choose New capture. Choose the appropriate Connector for your desired data source. A form appears with the properties required for that connector. More details are on each connector are provided in the connectors reference. Type a name for your capture. Your capture name must begin with a prefix to which you have access. Click inside the Name field to generate a drop-down menu of available prefixes, and select your prefix. Append a unique capture name after the / to create the full name, for example acmeCo/myFirstCapture. Fill out the required properties and click Next. Flow uses the provided information to initiate a connection to the source system. It identifies one or more data resources — these may be tables, data streams, or something else, depending on the connector. These are each mapped to a collection. The Collection Selector appears, showing this list of available collections. You can decide which ones you want to capture. Look over the list of available collections. All are selected by default. You can remove collections you don't want to capture, change collection names, and for some connectors, modify other properties. tip Use a filter to narrow down a large list of available collections. Hover your cursor within the Collection Selector table header, beneath the Remove All button, to reveal an expandable menu icon (three dots). Click the menu icon, and then choose Filter. Note that the Remove All button will always remove all collections — even those that are hidden by a filter. Use this button with caution. If you're unsure which collections you want to keep or remove, you can look at their schemas. Scroll down to the Specification Editor Here, you can view the generated capture definition and the schema for each collection. Flow displays these specifications as JSON in a read-only editor. For many source systems, you'll notice that the collection schemas are quite permissive. You'll have the option to apply more restrictive schemas later, when you materialize the collections. (Those who prefer a command-line interface can manage and edit the schema in their preferred development environment. Click the CLI button near the Collection Specification viewer to get started.). If you made any changes in the Collection Editor, click Next again. Once you're satisfied with the configuration, click Save and publish. You'll see a notification when the capture publishes successfully. Click Materialize collections to continue. "},{"title":"Create a materialization​","type":1,"pageTitle":"Create a basic Data Flow","url":"guides/create-dataflow/#create-a-materialization","content":"Now that you've captured data into one or more collections, you can materialize it to a destination. Select the Connector tile for your desired data destination. The page populates with the properties required for that connector. More details are on each connector are provided in the connectors reference. Choose a unique name for your materialization like you did when naming your capture; for example, acmeCo/myFirstMaterialization. Fill out the required properties in the Endpoint Configuration. Click Next. Flow initiates a connection with the destination system. The Endpoint Config has collapsed and the Collection Selector is now prominent. It shows each collection you captured previously. All of them will be mapped to a resource in the destination. Again, these may be tables, data streams, or something else. When you publish the Data Flow, Flow will create these new resources in the destination. Now's your chance to make changes to the collections before you materialize them. Optionally remove some collections or add additional collections. To easily find collections, you can use a filter. Hover your cursor within to the Collection Selector table header, next to the Remove All button, to reveal an expandable menu icon (three dots). Click the menu icon, and then choose Filter. To remove a collection, click the x in its table row. You can also click the Remove All button, but keep in mind that this button always removes allcollections from the materialization, regardless of whether they're hidden by a filter. Optionally apply a stricter schema to each collection to use for the materialization. Depending on the data source, you may have captured data with a fairly permissive schema. You can tighten up the schema so it'll materialize to your destination in the correct shape. (This shouldn't be necessary for database and SaaS data sources.) In the Collection Selector, choose a collection and click its Specification tab. Click Schema Inference The Schema Inference window appears. Flow scans the data in your collection and infers a new schema, called the readSchema, to use for the materialization. Review the new schema and click Apply Inferred Schema. Click Save and publish. You'll see a notification when the full Data Flow publishes successfully. "},{"title":"What's next?​","type":1,"pageTitle":"Create a basic Data Flow","url":"guides/create-dataflow/#whats-next","content":"Now that you've deployed your first Data Flow, you can explore more possibilities. Read the high level concepts to better understand how Flow works and what's possible. Create more complex Data Flows by mixing and matching collections in your captures and materializations. For example: Materialize the same collection to multiple destinations. If a capture produces multiple collections, materialize each one to a different destination. Materialize collections that came from different sources to the same destination. Advanced users can modify collection schemas, apply data reductions, or transform data with a derivation(derivations are currently available using the CLI, but support in the web application is coming soon.) "},{"title":"Configure connections with SSH tunneling","type":0,"sectionRef":"#","url":"guides/connect-network/","content":"","keywords":""},{"title":"General setup​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"guides/connect-network/#general-setup","content":"Activate an SSH implementation on a server, if you don't have one already. Consult the documentation for your server's operating system and/or cloud service provider, as the steps will vary. Configure the server to your organization's standards, or reference the SSH documentation for basic configuration options. Referencing the config files and shell output, collect the following information: The SSH user, which will be used to log into the SSH server, for example, sshuser. You may choose to create a new user for this workflow.The SSH endpoint for the SSH server, formatted as ssh://user@hostname[:port]. This may look like the any of following: ssh://sshuser@ec2-198-21-98-1.compute-1.amazonaws.comssh://sshuser@198.21.98.1ssh://sshuser@198.21.98.1:22 Hint The SSH default port is 22. Depending on where your server is hosted, you may not be required to specify a port, but we recommend specifying :22 in all cases to ensure a connection can be made. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key Taken together, these configuration details would allow you to log into the SSH server from your local machine. They'll allow the connector to do the same. Configure your internal network to allow the SSH server to access your capture or materialization endpoint. Configure your network to expose the SSH server endpoint to external traffic. The method you use depends on your organization's IT policies. Currently, Estuary doesn't provide a list of static IPs for whitelisting purposes, but if you require one, contact Estuary support. "},{"title":"Setup for AWS​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"guides/connect-network/#setup-for-aws","content":"To allow SSH tunneling to a database instance hosted on AWS, you'll need to create a virtual computing environment, or instance, in Amazon EC2. Begin by finding your public SSH key on your local machine. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key Import your SSH key into AWS. Launch a new instance in EC2. During setup: Configure the security group to allow SSH connection from anywhere.When selecting a key pair, choose the key you just imported. Connect to the instance, setting the user name to ec2-user. Find and note the instance's public DNS. This will be formatted like: ec2-198-21-98-1.compute-1.amazonaws.com. "},{"title":"Setup for Google Cloud​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"guides/connect-network/#setup-for-google-cloud","content":"To allow SSH tunneling to a database instance hosted on Google Cloud, you must set up a virtual machine (VM). Begin by finding your public SSH key on your local machine. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key If your Google login differs from your local username, generate a key that includes your Google email address as a comment: ssh-keygen -m PEM -t rsa -C user@domain.com Create and start a new VM in GCP, choosing an image that supports OS Login. Add your public key to the VM. Reserve an external IP address and connect it to the VM during setup. Note the generated address. "},{"title":"Setup for Azure​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"guides/connect-network/#setup-for-azure","content":"To allow SSH tunneling to a database instance hosted on Azure, you'll need to create a virtual machine (VM) in the same virtual network as your endpoint database. Begin by finding your public SSH key on your local machine. In the .ssh subdirectory of your user home directory, look for the PEM file that contains the private SSH key. Check that it starts with -----BEGIN RSA PRIVATE KEY-----, which indicates it is an RSA-based file. If no such file exists, generate one using the command: ssh-keygen -m PEM -t rsa If a PEM file exists, but starts with -----BEGIN OPENSSH PRIVATE KEY-----, convert it with the command: ssh-keygen -p -N &quot;&quot; -m pem -f /path/to/key Create and connect to a VM in a virtual network, and add the endpoint database to the network. Create a new virtual network and subnet. Create a Linux or Windows VM within the virtual network, directing the SSH public key source to the public key you generated previously. Note the VM's public IP; you'll need this later. Create a service endpoint for your database in the same virtual network as your VM. Instructions for Azure Database For PostgreSQL can be found here; note that instructions for other database engines may be different. "},{"title":"Configuration​","type":1,"pageTitle":"Configure connections with SSH tunneling","url":"guides/connect-network/#configuration","content":"After you've completed the prerequisites, you should have the following parameters: SSH Endpoint / sshEndpoint: the remote SSH server's hostname, or public IP address, formatted as ssh://user@hostname[:port] The SSH default port is 22. Depending on where your server is hosted, you may not be required to specify a port, but we recommend specifying :22 in all cases to ensure a connection can be made. Private Key / privateKey: the contents of the SSH private key file Use these to add SSH tunneling to your capture or materialization definition, either by filling in the corresponding fields in the web app, or by working with the YAML directly. Reference the Connectors page for a YAML sample. "},{"title":"Edit Data Flows in the web app","type":0,"sectionRef":"#","url":"guides/edit-data-flows/","content":"","keywords":""},{"title":"Edit a capture​","type":1,"pageTitle":"Edit Data Flows in the web app","url":"guides/edit-data-flows/#edit-a-capture","content":"Go to the Captures page of the web app. Locate the capture you'd like to edit. Click the Options button in its table row, then click Edit specification. The Edit Capture page opens. Edit the connection to the destination system, if desired. You can either update fields in the Endpoint Config section or manually update the JSON in the Specification Editor. caution You may have to re-authenticate with the source system. Be sure to have current credentials on hand before editing the endpoint configuration. Use the Collection Selector to add or remove collections from the capture, if desired. To refresh your connection with the source and see an updated list of possible collections, click the Refresh button, but be aware that it will overwrite all existing collection selections. Use the Schema Inference tool, if desired. Depending on the data source, you may have captured data with a fairly permissive schema. Flow can help you tighten up the schema to be used for downstream tasks in your Data Flow. In the Collection Selector, choose a collection and click its Specification tab. Click Schema Inference The Schema Inference window appears. Flow scans the data in your collection and infers a new schema, called the readSchema, to use for downstream tasks like materializations and derivations. Review the new schema and click Apply Inferred Schema. When you're done making changes, click Next. Click Save and Publish. Editing a capture only affects how it will work going forward. Data that was captured before editing will reflect the original configuration. Edit a materialization To edit a materialization: Go to the Materializations page of the web app. Locate the materialization you'd like to edit. Click the Options button in its table row, then click Edit specification. The Edit Materialization page opens. Edit the connection to the destination system, if desired. You can either update fields in the Endpoint Config section or manually update the JSON in the Specification Editor. caution You may have to re-authenticate with the destination system. Be sure to have current credentials on hand before editing the endpoint configuration. Use the Collection Selector to add or remove collections from the materialization, if desired. Optionally apply a stricter schema to each collection to use for the materialization. Depending on the data source, you may have captured data with a fairly permissive schema. Flow can help you tighten up the schema so it'll materialize to your destination in the correct shape. In the Collection Selector, choose a collection and click its Specification tab. Click Schema Inference The Schema Inference window appears. Flow scans the data in your collection and infers a new schema, called the readSchema, to use for the materialization. Review the new schema and click Apply Inferred Schema. When you're done making changes, click Next. Click Save and Publish. Editing a materialization only affects how it will work going forward. Data that was materialized before editing will reflect the original configuration. "},{"title":"Advanced editing​","type":1,"pageTitle":"Edit Data Flows in the web app","url":"guides/edit-data-flows/#advanced-editing","content":"For more fine-grain control over editing, you can use flowctl and work directly on specification files in your local environment.View the tutorial. "},{"title":"flowctl guides","type":0,"sectionRef":"#","url":"guides/flowctl/","content":"flowctl guides The guides in this section cover common workflows using the Estuary Flow CLI, flowctl. Learn how to edit published Flow entities, create derivations, and more. To get to know flowctl, see the concepts page.","keywords":""},{"title":"Edit a draft created in the web app","type":0,"sectionRef":"#","url":"guides/flowctl/edit-draft-from-webapp/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Edit a draft created in the web app","url":"guides/flowctl/edit-draft-from-webapp/#prerequisites","content":"To complete this workflow, you need: An Estuary account flowctl installed locally "},{"title":"Identify the draft and pull it locally​","type":1,"pageTitle":"Edit a draft created in the web app","url":"guides/flowctl/edit-draft-from-webapp/#identify-the-draft-and-pull-it-locally","content":"Drafts aren't currently visible in the Flow web app, but you can get a list with flowctl. Authorize flowctl. Go to the CLI-API tab of the web app and copy your access token. Run flowctl auth token --token &lt;paste-token-here&gt; Run flowctl draft list flowctl outputs a table of all the drafts to which you have access, from oldest to newest. Use the name and timestamp to find the draft you're looking for. Each draft has an ID, and most have a name in the Details column. Note the # of Specs column. For drafts created in the web app, materialization drafts will always contain one specification. A number higher than 1 indicates a capture with its associated collections. Copy the draft ID. Select the draft: flowctl draft select --id &lt;paste-id-here&gt;. Pull the draft source files to your working directory: flowctl draft develop. Browse the source files. The source files and their directory structure will look slightly different depending on the draft. Regardless, there will always be a top-level file called flow.yaml that imports all other YAML files, which you'll find in a subdirectory named for your catalog prefix. These, in turn, contain the specifications you'll want to edit. "},{"title":"Edit the draft and publish​","type":1,"pageTitle":"Edit a draft created in the web app","url":"guides/flowctl/edit-draft-from-webapp/#edit-the-draft-and-publish","content":"Next, you'll make changes to the specification(s), test, and publish the draft. Open the YAML files that contain the specification you want to edit. Make changes. For guidance on how to construct Flow specifications, see the documentation for the entity type: CapturesCollectionsMaterializations When you're done, sync the local work to the global draft: flowctl draft author --source flow.yaml. Specifying the top-level flow.yaml file as the source ensures that all entities in the draft are imported. Publish the draft: flowctl draft publish Once this operation completes successfully, check to verify if the entity or entities are live. You can: Go to the appropriate tab in the Flow web app. Run flowctl catalog list, filtering by --name, --prefix, or entity type, for example --capture. If you're not satisfied with the published entities, you can continue to edit them. See the other guides for help: Edit in the web app.Edit with flowctl. "},{"title":"Create a derivation with flowctl","type":0,"sectionRef":"#","url":"guides/flowctl/create-derivation/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Create a derivation with flowctl","url":"guides/flowctl/create-derivation/#prerequisites","content":"A Flow account and access to the web app. If you don't have an account yet, go to the web app to register for a free trial. An existing Flow collection. Typically, you create this through a capture in the Flow web application. If you need help, see the guide to create a Data Flow. flowctl installed locally. For help, see the installation instructions. "},{"title":"Pull your specification files locally​","type":1,"pageTitle":"Create a derivation with flowctl","url":"guides/flowctl/create-derivation/#pull-your-specification-files-locally","content":"To begin working in your local environment, you must authenticate Flow from the command line. Then, you'll need to add your source collection's specification files to a draft and bring the draft into your local environment for editing. Authorize flowctl. Go to the CLI-API tab of the web app and copy your access token. Run flowctl auth token --token &lt;paste-token-here&gt; Begin by creating a fresh draft. This is where you'll add the specification files you need from the catalog. flowctl draft create The output table shows the draft ID and creation time. It doesn't have any catalog entities in it yet. You'll add the source collection's specification to the draft. Identify the collection (or collections) in the catalog that contains the data you want to derive, and add it to your draft. flowctl catalog draft --name namespace/data-flow-name/my-collection tip If you're unsure of the name of the collection, check Collections page in the web application. You can also use flowctl draft list locally to begin exploring the catalog items available to you. The name of your collection may not follow the structure of the examples provided; simply copy the entire name as you see it, including all prefixes. The output confirms that the entity name you specified has been added to your draft, and is of the type collection. Your draft is set up, but still exists only on the Estuary servers. Pull the draft locally to edit the specification files. flowctl draft develop The specification files are written to your working directory. "},{"title":"Add a derivation​","type":1,"pageTitle":"Create a derivation with flowctl","url":"guides/flowctl/create-derivation/#add-a-derivation","content":"Locate the specification YAML file for the collection you want to transform. In your working directory, you'll see a top-level file called flow.yaml. Within a subdirectory that shares the name of your Data Flow, you'll find a second flow.yaml — this contains the collection specification. tip Use tree to visualize your current working directory. This is a helpful tool to understand the files that underlie your local draft. For example: . ├── namespace │ └── data-flow-name │ └── flow.yaml ├── flow.yaml ├── flow_generated │ ├── flow │ │ ├── main.ts │ │ ├── routes.ts │ │ └── server.ts │ ├── tsconfig-files.json │ └── types │ └── namespace │ └── data-flow-name │ └── my-collection.d.ts ├── package.json └── tsconfig.json Open the specification file in your preferred editor. It will look similar to the following. (This example uses the default collection from the Hello World test capture, available in the web app): collections: #The Hello World capture outputs a collection called `greetings`. namespace/data-flow-name/greetings: schema: properties: count: type: integer message: type: string required: - count - message type: object key: - /count You'll add the derivation to the same file. tip You may also create the derivation specification in a separate file; the results will be the same. However, if using separate files, you must make sure that the file with the derivation imports the source collection's specification, and that the top-level flow.yaml in your working directory imports all specification files.Learn more about imports. Add a new collection below the first one. The collection must have a schema that reflects your desired transformation output. They can be whatever you want, as long as they follow Flow's standard formatting. For help, see the schemas and collection key documentation.Add the derivation stanza. The TypeScript module you name will be generated next, and you'll define the transformation's function there. collections: namespace/data-flow-name/greetings: {...} #The name for your new collection can be whatever you want, #so long as you have permissions in the namespace. #Typically, you'll want to simply copy the source prefix #and add a unique collection name. namespace/data-flow-name/dozen-greetings: #In this example, our objective is to round the number of greetings to the nearest dozen. #We keep the `count` and `message` properties from the source, #and add a new field called `dozens`. schema: properties: count: type: integer message: type: string dozens: type: integer required: - dozens - count - message type: object #Since we're interested in estimating by the dozen, we make `dozens` our collection key. key: - /dozens derivation: transform: #The transform name can be anything you'd like. greetings-by-dozen: #Paste the full name of the source collection. source: {name: namespace/data-flow-name/greetings} #This simple transform only requires a **publish lambda* function. #More complex transforms also use **update lambdas**. #See the Derivations documentation to learn more about lambdas. publish: {lambda: typescript} #The name you provide for the module will be generated next. typescript: {module: divide-by-twelve.ts} Save the file. "},{"title":"Transform with a TypeScript module​","type":1,"pageTitle":"Create a derivation with flowctl","url":"guides/flowctl/create-derivation/#transform-with-a-typescript-module","content":"Generate the TypeScript module from the newly updated specification file. flowctl typescript generate --source ./path-to/your-file/flow.yaml The TypeScript file you named has been created and stubbed out. You only need to add the function body. Open the new TypeScript module. It will look similar to the following: import { IDerivation, Document, Register, GreetingsByDozenSource } from 'flow/namespace/data-flow-name/dozen-greetings'; // Implementation for derivation flow.yaml#/collections/namespace~1data-flow-name~1dozen-greetings/derivation. export class Derivation implements IDerivation { greetingsByDozenPublish( _source: GreetingsByDozenSource, _register: Register, _previous: Register, ): Document[] { throw new Error(&quot;Not implemented&quot;); } } Remove the underscore in front of source and fill out the function body as required for your required transformation. For more advanced transforms, you may need to activate register and previous by removing their underscores.Learn more about derivations and see examples. This simple example rounds the count field to the nearest dozen. import { IDerivation, Document, Register, GreetingsByDozenSource } from 'flow/namespace/data-flow-name/dozen-greetings'; // Implementation for derivation namespace/data-flow-name/flow.yaml#/collections/namespace~1data-flow-name~1dozen-greetings/derivation. export class Derivation implements IDerivation { greetingsByDozenPublish( source: GreetingsByDozenSource, _register: Register, _previous: Register, ): Document[] { let count = source.count; let dozen = count / 12; let dozenround = Math.floor(dozen) let out = { dozens: dozenround, ...source } return [out] } } Save the file. Optional: add a test to the flow.yaml file containing your collections. This helps you verify that your data is transformed correctly. collections: {...} tests: namespace/data-flow-name/divide-test: - ingest: collection: namespace/data-flow-name/greetings documents: - { count: 13, message: &quot;Hello #13&quot; } - verify: collection: namespace/data-flow-name/dozen-greetings documents: - { dozens: 1, count: 13, message: &quot;Hello #13&quot;} Learn about tests. "},{"title":"Publish the derivation​","type":1,"pageTitle":"Create a derivation with flowctl","url":"guides/flowctl/create-derivation/#publish-the-derivation","content":"Author your draft. This adds the changes you made locally to the draft on the Estuary servers: flowctl draft author --source flow.yaml Note that the file source is the top level flow.yaml in your working directory, not the file you worked on. This file imports all others in the local draft, so your changes will be included. Run generic tests, as well as your custom tests, if you created any. flowctl draft test Publish the draft to the catalog. flowctl draft publish  The derivation you created is now live and ready for further use. You can access it from the web application and materialize it to a destination, just as you would any other Flow collection. "},{"title":"Edit a Flow specification locally","type":0,"sectionRef":"#","url":"guides/flowctl/edit-specification-locally/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Edit a Flow specification locally","url":"guides/flowctl/edit-specification-locally/#prerequisites","content":"To complete this workflow, you need: An Estuary account flowctl installed locally One or more published Flow entities. (To edit unpublished drafts, use this guide.) "},{"title":"Pull specifications locally​","type":1,"pageTitle":"Edit a Flow specification locally","url":"guides/flowctl/edit-specification-locally/#pull-specifications-locally","content":"Every entity (including active tasks, like captures and materializations, and static collections) has a globally unique name in the Flow catalog. For example, a given Data Flow may comprise: A capture, myOrg/marketing/leads, which writes to...Two collections, myOrg/marketing/emailList and myOrg/marketing/socialMedia, which are materialized as part of...A materialization, myOrg/marketing/contacts. Using these names, you'll identify and pull the relevant specifications for editing. Authorize flowctl. Go to the CLI-API tab of the web app and copy your access token. Run flowctl auth token --token &lt;paste-token-here&gt; Determine which entities you need to pull from the catalog. You can: Check the web app's Captures, Collections, and Materializations pages. All published entities to which you have access are listed and may be searched. Run flowctl catalog list. This command returns a complete list of entities to which you have access. You can refine by specifying a --prefix and filter by entity type: --captures, --collections, --materializations, or --tests. From the above example, flowctl catalog list --prefix myOrg/marketing --captures --materializations would returnmyOrg/marketing/leads and myOrg/marketing/contacts. Pull the specifications you need by running flowctl catalog pull-specs: Pull one or more specifications by name, for example: flowctl catalog pull-specs --name myOrg/marketing/emailList Pull a group of specifications by prefix or type filter, for example: flowctl catalog pull-specs --prefix myOrg/marketing --collections The source files are written to your current working directory. Browse the source files. flowctl pulls specifications into subdirectories organized by entity name, and specifications sharing a catalog prefix are written to the same YAML file. Regardless of what you pull, there is always a top-level file called flow.yaml that imports all other nested YAML files. These, in turn, contains the entities' specifications. "},{"title":"Edit source files and re-publish specifications​","type":1,"pageTitle":"Edit a Flow specification locally","url":"guides/flowctl/edit-specification-locally/#edit-source-files-and-re-publish-specifications","content":"Next, you'll complete your edits, test that they were performed correctly, and re-publish everything. Open the YAML files that contain the specification you want to edit. Make changes. For guidance on how to construct Flow specifications, see the documentation for the task type: CapturesCollectionsMaterializationsDerivationsTests When you're done, you can test your changes:flowctl catalog test --source flow.yaml You'll almost always use the top-level flow.yaml file as the source here because it imports all other Flow specifications in your working directory. Once the test has passed, you can publish your specifications. Re-publish all the specifications you pulled: flowctl catalog publish --source flow.yaml Again you'll almost always want to use the top-level flow.yaml file. If you want to publish only certain specifications, you can provide a path to a different file. Return to the web app or use flowctl catalog list to check the status of the entities you just published. Their publication time will be updated to reflect the work you just did. If you're not satisfied with the results of your edits, repeat the process iteratively until you are. "},{"title":"Troubleshoot a task with flowctl","type":0,"sectionRef":"#","url":"guides/flowctl/troubleshoot-task/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Troubleshoot a task with flowctl","url":"guides/flowctl/troubleshoot-task/#prerequisites","content":"To complete this workflow, you need: An Estuary account flowctl installed locally "},{"title":"Print task logs​","type":1,"pageTitle":"Troubleshoot a task with flowctl","url":"guides/flowctl/troubleshoot-task/#print-task-logs","content":"Authorize flowctl. Go to the CLI-API tab of the web app and copy your access token. Run flowctl auth token --token &lt;paste-token-here&gt; Identify the name of the failing task in the web app; for example myOrg/marketing/leads. Use the tables on the Captures or Materializations pages of the web app to do so. Run flowctl logs --task &lt;task-name&gt;. You have several options to get more specific. For example: flowctl logs --task myOrg/marketing/leads --follow — If the task hasn't failed, continuously print logs as they're generated. flowctl logs --task myOrg/marketing/leads --since 1h — Print logs from approximately the last hour. The actual output window is approximate and may somewhat exceed this time boundary. You may use any time, for example 10m and 1d. "},{"title":"Change log level​","type":1,"pageTitle":"Troubleshoot a task with flowctl","url":"guides/flowctl/troubleshoot-task/#change-log-level","content":"If your logs aren't providing enough detail, you can change the log level. Flow offers several log levels. From least to most detailed, these are: errorwarninfo (default)debugtrace Follow the guide to edit a specification with flowctl. Working in your local specification file, add the shards stanza to the capture or materialization specification: myOrg/marketing/leads: shards: logLevel: debug endpoint: {} Finish the workflow as described, re-publishing the task. Learn more about working with logs "},{"title":"System-specific Data Flows","type":0,"sectionRef":"#","url":"guides/system-specific-dataflows/","content":"System-specific Data Flows The guides in this section cover popular Estuary Flow use cases. Each guide walks you through the process of capturing data from a specific source system and materializing it to a specific destination. These are supplemental to the main guide to create a Data Flow. If you don't see your exact Data Flow here, use the main guide and the connector referenceto mix and match your required source and destination systems.","keywords":""},{"title":"Google Firestore to Snowflake","type":0,"sectionRef":"#","url":"guides/system-specific-dataflows/firestore-to-dwh/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Google Firestore to Snowflake","url":"guides/system-specific-dataflows/firestore-to-dwh/#prerequisites","content":"You'll need: (Recommended) understanding of the basic Flow concepts. Access to the Flow web application through an Estuary account. If you don't have one, visit the web app to register for your free trial. A Firestore database that contains the data you'd like to move to Snowflake. You create this as part of a Google Firebase project. A Google service account with: Read access to your Firestore database, via roles/datastore.viewer. You can assign this role when you create the service account, or add it to an existing service account. A generated JSON service account key for the account. A Snowflake account with: A target database, schema, and virtual warehouse; and a user with a role assigned that grants the appropriate access levels to these resources.You can use a script to quickly create all of these items. Have these details on hand for setup with Flow. The account identifier and host URL noted. The URL is formatted using the account identifier. For example, you might have the account identifier orgname-accountname.snowflakecomputing.com. "},{"title":"Introduction​","type":1,"pageTitle":"Google Firestore to Snowflake","url":"guides/system-specific-dataflows/firestore-to-dwh/#introduction","content":"In Estuary Flow, you create Data Flows to transfer data from source systems to destination systems in real time. In this use case, your source is an Google Firestore NoSQL database and your destination is a Snowflake data warehouse. After following this guide, you'll have a Data Flow that comprises: A capture, which ingests data from FirestoreSeveral collection, cloud-backed copies of Firestore collections in the Flow systemA materialization, which pushes the collections to Snowflake The capture and materialization rely on plug-in components called connectors. We'll walk through how to configure the Firestore and Snowflake connectors to integrate these systems with Flow. "},{"title":"Capture from Firestore​","type":1,"pageTitle":"Google Firestore to Snowflake","url":"guides/system-specific-dataflows/firestore-to-dwh/#capture-from-firestore","content":"You'll first create a capture to connect to your Firestore database, which will yield one Flow collection for each Firestore collection in your database. Go to the Flow web application at dashboard.estuary.dev and sign in using the credentials provided by your Estuary account manager. Click the Captures tab and choose New Capture. Click the Google Firestore tile. A form appears with the properties required for a Firestore capture. Type a name for your capture. Your capture name must begin with a prefix to which you have access. Click inside the Name field to generate a drop-down menu of available prefixes, and select your prefix. Append a unique capture name after the / to create the full name, for example, acmeCo/myFirestoreCapture. Fill out the required properties for Firestore. Database: Flow can autodetect the database name, but you may optionally specify it here. This is helpful if the service account used has access to multiple Firebase projects. Your database name usually follows the format projects/$PROJECTID/databases/(default). Credentials: The JSON service account key created per the prerequisites. Click Next. Flow uses the provided configuration to initiate a connection with Firestore. It maps each available Firestore collection to a possible Flow collection. It also generates a capture specification and minimal schemas for each collection. You can use the Collection Selector to remove or modify collections. You'll have the chance to tighten up each collection's JSON schema later, when you materialize to Snowflake. tip If you make any changes in the collection editor, click Next again. tip If you'd rather work on the specification files in their native YAML format, you can use the flowctl CLI. flowctl provides a developer-focused path to build full Data Flows in your preferred development environment. Once you're satisfied with the collections to be captured, click Save and publish. You'll see a notification when the capture publishes successfully. The data currently in your Firestore database has been captured, and future updates to it will be captured continuously. Click Materialize Collections to continue. "},{"title":"Materialize to Snowflake​","type":1,"pageTitle":"Google Firestore to Snowflake","url":"guides/system-specific-dataflows/firestore-to-dwh/#materialize-to-snowflake","content":"Next, you'll add a Snowflake materialization to connect the captured data to its destination: your data warehouse. On the Create Materialization page, search for and select the Snowflake tile. A form appears with the properties required for a Snowflake materialization. Choose a unique name for your materialization like you did when naming your capture; for example, acmeCo/mySnowflakeMaterialization. Fill out the required properties for Snowflake (you should have most of these handy from the prerequisites). Host URLAccountUserPasswordDatabaseSchemaWarehouse: optionalRole: optional Click Next. Flow uses the provided configuration to initiate a connection to Snowflake. You'll be notified if there's an error. In that case, fix the configuration form or Snowflake setup as needed and click Next to try again. Once the connection is successful, the Endpoint Config collapses and the Collection Selector becomes prominent. It shows each collection you captured previously. Each of them will be mapped to a Snowflake table. In the Collection Selector, fill in the Table field for each collection. The collections you just created have already been selected, but you must provide names for the tables to which they'll be materialized in Snowflake. For each table, choose whether to enable delta updates. For each collection, apply a stricter schema to be used for the materialization. Firestore has a flat data structure. To materialize data effectively from Firestore to Snowflake, you should apply a schema can translate to a table structure. Flow's Schema Inference tool can help. In the Collection Selector, choose a collection and click its Specification tab. Click Schema Inference The Schema Inference window appears. Flow scans the data in your collection and infers a new schema, called the readSchema, to use for the materialization. Review the new schema and click Apply Inferred Schema. Click Save and Publish. You'll see a notification when the full Data Flow publishes successfully. "},{"title":"What's next?​","type":1,"pageTitle":"Google Firestore to Snowflake","url":"guides/system-specific-dataflows/firestore-to-dwh/#whats-next","content":"Your Data Flow has been deployed, and will run continuously until it's stopped. Updates in your Firestore database will be reflected in your Snowflake table as they occur. You can advance your Data Flow by adding a derivation. Derivations are real-time data transformations. See the guide to create a derivation. "},{"title":"Amazon S3 to Snowflake","type":0,"sectionRef":"#","url":"guides/system-specific-dataflows/s3-to-snowflake/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon S3 to Snowflake","url":"guides/system-specific-dataflows/s3-to-snowflake/#prerequisites","content":"You'll need: (Recommended) understanding of the basic Flow concepts. Access to the Flow web application through an Estuary account. If you don't have one, visit the web app to register for your free trial. An S3 bucket that contains the data you'd like to move to Snowflake. For public buckets, verify that the access policy allows anonymous reads. For buckets accessed by a user account, you'll need the AWS access key and secret access key for the user. See the AWS blog for help finding these credentials. A Snowflake account with: A target database, schema, and virtual warehouse; and a user with a role assigned that grants the appropriate access levels to these resources.You can use a script to quickly create all of these items. Have these details on hand for setup with Flow. The account identifier and host URL noted. The URL is formatted using the account identifier. For example, you might have the account identifier orgname-accountname.snowflakecomputing.com. "},{"title":"Introduction​","type":1,"pageTitle":"Amazon S3 to Snowflake","url":"guides/system-specific-dataflows/s3-to-snowflake/#introduction","content":"In Estuary Flow, you create Data Flows to transfer data from source systems to destination systems in real time. In this use case, your source is an Amazon S3 bucket and your destination is a Snowflake data warehouse. After following this guide, you'll have a Data Flow that comprises: A capture, which ingests data from S3A collection, a cloud-backed copy of that data in the Flow systemA materialization, which pushes the data to Snowflake The capture and materialization rely on plug-in components called connectors. We'll walk through how to configure the S3 and Snowflake connectors to integrate these systems with Flow. "},{"title":"Capture from S3​","type":1,"pageTitle":"Amazon S3 to Snowflake","url":"guides/system-specific-dataflows/s3-to-snowflake/#capture-from-s3","content":"You'll first create a capture to connect to your S3 bucket, which will yield one or more Flow collections. Go to the Flow web application at dashboard.estuary.dev and sign in using the credentials provided by your Estuary account manager. Click the Captures tab and choose New Capture. Click the Amazon S3 tile. A form appears with the properties required for an S3 capture. Type a name for your capture. Your capture name must begin with a prefix to which you have access. Click inside the Name field to generate a drop-down menu of available prefixes, and select your prefix. Append a unique capture name after the / to create the full name, for example, acmeCo/myS3Capture. Fill out the required properties for S3. AWS Access Key ID and AWS Secret Access Key: Required for private buckets. AWS Region and Bucket: These are listed in your S3 console. Prefix: You might organize your S3 bucket using prefixes, which emulate a directory structure. To capture only from a specific prefix, add it here. Match Keys: Filters to apply to the objects in the S3 bucket. If provided, only data whose absolute path matches the filter will be captured. For example, *\\.json will only capture JSON file. See the S3 connector documentation for information on advanced fields and parser settings. (You're unlikely to need these for most use cases.) Click Next. Flow uses the provided configuration to initiate a connection to S3. It generates a capture specification and details of the collection that it will create, once published. You can change the collection name in the Collection Selector. You'll have the chance to tighten up each collection's JSON schema later, when you materialize to Snowflake. tip If you'd rather work on the specification files in their native YAML format, you can use the flowctl CLI. flowctl provides a developer-focused path to build full Data Flows in your preferred development environment. flowctl also offers access to advanced features — with S3, for instance, you can map multiple prefixes to different collections within a single capture. Click Save and publish. You'll see a notification when the capture publishes successfully. The data currently in your S3 bucket has been captured, and future updates to it will be captured continuously. Click Materialize Collections to continue. "},{"title":"Materialize to Snowflake​","type":1,"pageTitle":"Amazon S3 to Snowflake","url":"guides/system-specific-dataflows/s3-to-snowflake/#materialize-to-snowflake","content":"Next, you'll add a Snowflake materialization to connect the captured data to its destination: your data warehouse. On the Create Materialization page, search for and select the Snowflake tile. A form appears with the properties required for a Snowflake materialization. Choose a unique name for your materialization like you did when naming your capture; for example, acmeCo/mySnowflakeMaterialization. Fill out the required properties for Snowflake (you should have most of these handy from the prerequisites). Host URLAccountUserPasswordDatabaseSchemaWarehouse: optionalRole: optional Click Next. Flow uses the provided configuration to initiate a connection to Snowflake. You'll be notified if there's an error. In that case, fix the configuration form or Snowflake setup as needed and click Next to try again. Once the connection is successful, the Endpoint Config collapses and the Collection Selector becomes prominent. It shows the collection you captured previously, which will be mapped to a Snowflake table. In the Collection Selector, fill in the Table field. The collection you just created is already selected, but you must provide a name for the table to which it'll be materialized in Snowflake. Choose whether to enable delta updates. Apply a stricter schema to the collection for the materialization. S3 has a flat data structure. To materialize this data effectively to Snowflake, you should apply a schema that can translate to a table structure. Flow's Schema Inference tool can help. In the Collection Selector, click the collection's Specification tab. Click Schema Inference The Schema Inference window appears. Flow scans the data in your collection and infers a new schema, called the readSchema, to use for the materialization. Review the new schema and click Apply Inferred Schema. Click Save and Publish. You'll see a notification when the full Data Flow publishes successfully. "},{"title":"What's next?​","type":1,"pageTitle":"Amazon S3 to Snowflake","url":"guides/system-specific-dataflows/s3-to-snowflake/#whats-next","content":"Your Data Flow has been deployed, and will run continuously until it's stopped. Updates in your S3 bucket will be reflected in your Snowflake table as they occur. You can advance your Data Flow by adding a derivation. Derivations are real-time data transformations. See the guide to create a derivation. "},{"title":"Who should use Flow?","type":0,"sectionRef":"#","url":"overview/who-should-use-flow/","content":"","keywords":""},{"title":"Benefits​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#benefits","content":"These characteristics set Flow apart from other data integration workflows and address the pain points listed above. "},{"title":"Fully integrated pipelines​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#fully-integrated-pipelines","content":"With Flow, you can build, test, and evolve pipelines that continuously capture, transform, and materialize data across all of your systems. With one tool, you can power workflows that have historically required you to first piece together services, then integrate and operate them in-house to meet your needs. To achieve comparable capabilities to Flow you would need: A low-latency streaming system, such as AWS KinesisData lake build-out, such as Kinesis Firehose to S3Custom ETL application development, such as Spark, Flink, or AWS λSupplemental data stores for intermediate transformation statesETL job management and execution, such as a self-hosting or Google Cloud DataflowCustom reconciliation of historical vs streaming datasets, including onerous backfills of new streaming applications from historical data Flow dramatically simplifies this inherent complexity. It saves you time and costs, catches mistakes before they hit production, and keeps your data fresh across all the places you use it. With both a UI-forward web application and a powerful CLI , more types of professionals can contribute to what would otherwise require a highly specialized set of technical skills. "},{"title":"Efficient architecture​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#efficient-architecture","content":"Flow mixes a variety of architectural techniques to deliver great throughput, avoid latency, and minimize operating costs. These include: Leveraging reductions to reduce the amount of data that must be ingested, stored, and processed, often dramaticallyExecuting transformations predominantly in-memoryOptimistic pipelining and vectorization of internal remote procedure calls (RPCs) and operationsA cloud-native design that optimizes for public cloud pricing models Flow also makes it easy to materialize focused data views directly into your warehouse, so you don't need to repeatedly query the much larger source datasets. This can dramatically lower warehouse costs. "},{"title":"Powerful transformations​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#powerful-transformations","content":"With Flow, you can build pipelines that join a current event with an event that happened days, weeks, even years in the past. Flow can model arbitrary stream-to-stream joins without the windowing constraints imposed by other systems, which limit how far back in time you can join. Flow transforms data in durable micro-transactions, meaning that an outcome, once committed, won't be silently re-ordered or changed due to a crash or machine failure. This makes Flow uniquely suited for operational workflows, like assigning a dynamic amount of available inventory to a stream of requests — decisions that, once made, should not be forgotten. You can also evolve transformations as business requirements change, enriching them with new datasets or behaviors without needing to re-compute from scratch. "},{"title":"Data integrity​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#data-integrity","content":"Flow is architected to ensure that your data is accurate and that changes don't break pipelines. It supports strong schematization, durable transactions with exactly-once semantics, and easy end-to-end testing. Required JSON schemas ensure that only clean, consistent data is ingested into Flow or written to external systems. If a document violates its schema, Flow pauses the pipeline, giving you a chance to fix the error.Schemas can encode constraints, like that a latitude value must be between +90 and -90 degrees, or that a field must be a valid email address.Flow can project JSON schema into other flavors, like TypeScript types or SQL tables. Strong type checking catches bugs before they're applied to production.Flow's declarative tests verify the integrated, end-to-end behavior of data flows. "},{"title":"Dynamic scaling​","type":1,"pageTitle":"Who should use Flow?","url":"overview/who-should-use-flow/#dynamic-scaling","content":"The Flow runtime scales from a single process up to a large Kubernetes cluster for high-volume production deployments. Processing tasks are quickly reassigned upon any machine failure for high availability. Each process can also be scaled independently, at any time, and without downtime. This is unique to Flow. Comparable systems require that an arbitrary data partitioning be decided upfront, a crucial performance knob that's awkward and expensive to change. Instead, Flow can repeatedly split a running task into two new tasks, each half the size, without stopping it or impacting its downstream uses. "},{"title":"Comparisons","type":0,"sectionRef":"#","url":"overview/comparisons/","content":"","keywords":""},{"title":"Apache Beam and Google Cloud Dataflow​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#apache-beam-and-google-cloud-dataflow","content":"Flow’s most apt comparison is to Apache Beam. You may use a variety of runners (processing engines) for your Beam deployment. One of the most popular, Google Cloud Dataflow, is a more robust redistribution under an additional SDK. Regardless of how you use Beam, there’s a lot of conceptual overlap with Flow. This makes Beam and Flow alternatives rather than complementary technologies, but there are key differences. Like Beam, Flow’s primary primitive is a collection. You build a processing graph (called a pipeline in Beam and a Data Flow in Flow) by relating multiple collections together through procedural transformations, or lambdas. As with Beam, Flow’s runtime performs automatic data shuffles and is designed to allow fully automatic scaling. Also like Beam, collections have associated schemas. Unlike Beam, Flow doesn’t distinguish between batch and streaming contexts. Flow unifies these paradigms under a single collection concept, allowing you to seamlessly work with both data types. Also, while Beam allows you the option to define combine operators, Flow’s runtime always applies combine operators. These are built using the declared semantics of the document’s schema, which makes it much more efficient and cost-effective to work with streaming data. Finally, Flow allows stateful stream-to-stream joins without the windowing semantics imposed by Beam. Notably, Flow’s modeling of state – via its per-key register concept – is substantially more powerful than Beam's per-key-and-window model. For example, registers can trivially model the cumulative lifetime value of a customer. "},{"title":"Kafka​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#kafka","content":"Flow inhabits a different space than Kafka does by itself. Kafka is an infrastructure that supports streaming applications running elsewhere. Flow is an opinionated framework for working with real-time data. You might think of Flow as an analog to an opinionated bundling of several important features from the broader Kafka ecosystem. Flow is built on Gazette, a highly-scalable streaming broker similar to log-oriented pub/sub systems. Thus, Kafka is more directly comparable to Gazette. Flow also uses Gazette’s consumer framework, which has similarities to Kafka consumers. Both manage scale-out execution contexts for consumer tasks, offer durable local task stores, and provide exactly-once semantics. Journals in Gazette and Flow are roughly analogous to Kafka partitions. Each journal is a single append-only log. Gazette has no native notion of a topic, but instead supports label-based selection of subsets of journals, which tends to be more flexible. Gazette journals store data in contiguous chunks called fragments, which typically live in cloud storage. Each journal can have its own separate storage configuration, which Flow leverages to allow users to bring their own cloud storage buckets. Another unique feature of Gazette is its ability to serve reads of historical data by providing clients with pre-signed cloud storage URLs, which enables it to serve many readers very efficiently. Generally, Flow users don't need to know or care much about Gazette and its architecture, since Flow provides a higher-level interface over groups of journals, called collections. Flow collections are somewhat similar to Kafka streams, but with some important differences. Collections always store JSON and must have an associated JSON schema. Collections also support automatic logical and physical partitioning. Each collection is backed by one or more journals, depending on the partitioning. Flow tasks are most similar to Kafka stream processors, but are more opinionated. Tasks fall into one of three categories: captures, derivations, and materializations. Tasks may also have more than one process, which Flow calls shards, to allow for parallel processing. Tasks and shards are fully managed by Flow. This includes transactional state management and zero-downtime splitting of shards, which enables turnkey scaling. "},{"title":"Spark​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#spark","content":"Spark can be described as a batch engine with stream processing add-ons, where Flow is fundamentally a streaming system that is able to easily integrate with batch systems. You can think of a Flow collection as a set of RDDs with common associated metadata. In Spark, you can save an RDD to a variety of external systems, like cloud storage or a database. Likewise, you can load from a variety of external systems to create an RDD. Finally, you can transform one RDD into another. You use Flow collections in a similar manner. They represent a logical dataset, which you can materialize to push the data into some external system like cloud storage or a database. You can also create a collection that is derived by applying stateful transformations to one or more source collections. Unlike Spark RDDs, Flow collections are backed by one or more unbounded append-only logs. Therefore, you don't create a new collection each time data arrives; you simply append to the existing one. Collections can be partitioned and can support extremely large volumes of data. Spark's processing primitives, applications, jobs, and tasks, don't translate perfectly to Flow, but we can make some useful analogies. This is partly because Spark is not very opinionated about what an application does. Your Spark application could read data from cloud storage, then transform it, then write the results out to a database. The closest analog to a Spark application in Flow is the Data Flow. A Data Flow is a composition of Flow tasks, which are quite different from tasks in Spark. In Flow, a task is a logical unit of work that does one of capture (ingest), derive (transform), or materialize (write results to an external system). What Spark calls a task is actually closer to a Flow shard. In Flow, a task is a logical unit of work, and shards represent the potentially numerous processes that actually carry out that work. Shards are the unit of parallelism in Flow, and you can easily split them for turnkey scaling. Composing Flow tasks is also a little different than composing Spark jobs. Flow tasks always produce and/or consume data in collections, instead of piping data directly from one shard to another. This is because every task in Flow is transactional and, to the greatest degree possible, fault-tolerant. This design also affords painless backfills of historical data when you want to add new transformations or materializations. "},{"title":"Hadoop, HDFS, and Hive​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#hadoop-hdfs-and-hive","content":"There are many different ways to use Hadoop, HDFS, and the ecosystem of related projects, several of which are useful comparisons to Flow. To gain an understanding of Flow's processing model for derivations, see this blog post about MapReduce in Flow. HDFS is sometimes used as a system of record for analytics data, typically paired with an orchestration system for analytics jobs. If you do this, you likely export datasets from your source systems into HDFS. Then, you use some other tool to coordinate running various MapReduce jobs, often indirectly through systems like Hive. For this use case, the best way of describing Flow is that it completely changes the paradigm. In Flow, you always append data to existing collections, rather than creating a new one each time a job is run. In fact, Flow has no notion of a job like there is in Hadoop. Flow tasks run continuously and everything stays up to date in real time, so there's never a need for outside orchestration or coordination. Put simply, Flow collections are log-like, and files in HDFS typically store table-like data. This blog post explores those differences in greater depth. To make this more concrete, imagine a hypothetical example of a workflow in the Hadoop world where you export data from a source system, perform some transformations, and then run some Hive queries. In Flow, you instead define a capture of data from the source, which runs continuously and keeps a collection up to date with the latest data from the source. Then you transform the data with Flow derivations, which again apply the transformations incrementally and in real time. While you could actually use tools like Hive to directly query data from Flow collections — the layout of collection data in cloud storage is intentionally compatible with this — you could also materialize a view of your transformation results to any database, which is also kept up to date in real time. "},{"title":"Fivetran, Airbyte, and other ELT solutions​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#fivetran-airbyte-and-other-elt-solutions","content":"Tools like Fivetran and Airbyte are purpose-built to move data from one place to another. These ELT tools typically model sources and destinations, and run regularly scheduled jobs to export from the source directly to the destination. Flow models things differently. Instead of modeling the world in terms of independent scheduled jobs that copy data from source to destination, Data Flows model a directed graph ofcaptures (reads from sources),derivations (transforms), andmaterializations (writes to destinations). Collectively, these are called tasks. Tasks in Flow are only indirectly linked. Captures read data from a source and output to collections. Flow collections store all the data in cloud storage, with configurable retention for historical data. You can then materialize each collection to any number of destination systems. Each one will be kept up to date in real time, and new materializations can automatically backfill all your historical data. Collections in Flow always have an associated JSON schema, and they use that to ensure the validity of all collection data. Tasks are also transactional and generally guarantee end-to-end exactly-once processing*. Like Airbyte, Flow uses connectors for interacting with external systems in captures and materializations. For captures, Flow integrates the Airbyte specification, so all Airbyte source connectors can be used with Flow. For materializations, Flow uses its own protocol which is not compatible with the Airbyte spec. In either case, the usage of connectors is pretty similar. In terms of technical capabilities, Flow can do everything that these tools can and more. Both Fivetran and Airbyte both currently have graphical interfaces that make them much easier for non-technical users to configure. Flow, too, is focused on empowering non-technical users through its web application. At the same time, it Flow offers declarative YAML for configuration, which works excellently in a GitOps workflow. * Some materialization endpoints can only make at-least-once guarantees. "},{"title":"dbt​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#dbt","content":"dbt is a tool that enables data analysts and engineers to transform data in their warehouses more effectively. In addition to – and perhaps more important than – its transform capability, dbt brought an entirely new workflow for working with data: one that prioritizes version control, testing, local development, documentation, composition, and re-use. Like dbt, Flow uses a declarative model and tooling, but the similarities end there. dbt is a tool for defining transformations, which are executed within your analytics warehouse. Flow is a tool for delivering data to that warehouse, as well as continuous operational transforms that are applied everywhere else. These two tools can make lots of sense to use together. First, Flow brings timely, accurate data to the warehouse. Within the warehouse, analysts can use tools like dbt to explore the data. The Flow pipeline is then ideally suited to productionize important insights as materialized views or by pushing to another destination. Put another way, Flow is a complete ELT platform, but you might choose to perform and manage more complex transformations in a separate, dedicated tool like dbt. While Flow and dbt don’t interact directly, both offer easy integration through your data warehouse. "},{"title":"Materialize, Rockset, ksqlDB, and other real-time databases​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#materialize-rockset-ksqldb-and-other-real-time-databases","content":"Modern real-time databases like Materialize, Rockset, and ksqlDB consume streams of data, oftentimes from Kafka brokers, and can keep SQL views up to date in real time. These real-time databases have a lot of conceptual overlap with Flow. The biggest difference is that Flow can materialize this same type of incrementally updated view into any database, regardless of whether that database has real-time capabilities or not. However, this doesn't mean that Flow should replace these systems in your stack. In fact, it can be optimal to use Flow to feed data into them. Flow adds real-time data capture and materialization options that many real-time databases don't support. Once data has arrived in the database, you have access to real-time SQL analysis and other analytical tools not native to Flow. For further explanation, read the section below on OLAP databases. "},{"title":"Snowflake, BigQuery, and other OLAP databases​","type":1,"pageTitle":"Comparisons","url":"overview/comparisons/#snowflake-bigquery-and-other-olap-databases","content":"Flow differs from OLAP databases mainly in that it's not a database. Flow has no query interface, and no plans to add one. Instead, Flow allows you to use the query interfaces of any database by materializing views into it. Flow is similar to OLAP databases in that it can be the source of truth for all analytics data (though it's also capable enough to handle operational workloads). Instead of schemas and tables, Flow defines collections. These collections are conceptually similar to database tables in the sense that they are containers for data with an associated (primary) key. Under the hood, Flow collections are each backed by append-only logs, where each document in the log represents a delta update for a given key. Collections can be easily materialized into a variety of external systems, such as Snowflake or BigQuery. This creates a table in your OLAP database that is continuously kept up to date with the collection. With Flow, there's no need to schedule exports to these systems, and thus no need to orchestrate the timing of those exports. You can also materialize a given collection into multiple destination systems, so you can always use whichever system is best for the type of queries you want to run. Like Snowflake, Flow uses inexpensive cloud storage for all collection data. It even lets you bring your own storage bucket, so you're always in control. Unlike data warehouses, Flow is able to directly capture data from source systems, and continuously and incrementally keep everything up to date. A common pattern is to use Flow to capture data from multiple different sources and materialize it into a data warehouse. Flow can also help you avoid expenses associated with queries you frequently pull from a data warehouse by keeping an up-to-date view of them where you want it. Because of Flow’s exactly-once processing guarantees, these materialized views are always correct, consistent, and fault-tolerant. "},{"title":"Authorizing users and authenticating with Flow","type":0,"sectionRef":"#","url":"reference/authentication/","content":"","keywords":""},{"title":"Subjects, objects, and inherited capabilities​","type":1,"pageTitle":"Authorizing users and authenticating with Flow","url":"reference/authentication/#subjects-objects-and-inherited-capabilities","content":"The entity to which you grant a capability is called the subject, and the entity over which access is granted is called the object. The subject can be either a user or a prefix, and the object is always a prefix. This allows subjects to inherit nested capabilities, so long as they are granted admin. For example, user X of Acme Co has admin access to the acmeCo/ prefix, and user Y has write access. A third party has granted acmeCo/ read access to shared data at outside-org/acmeCo-share/. User X automatically inherits read access to outside-org/acmeCo-share/, but user Y does not. "},{"title":"Default authorization settings​","type":1,"pageTitle":"Authorizing users and authenticating with Flow","url":"reference/authentication/#default-authorization-settings","content":"When you first sign up to use Flow, your organization is provisioned a prefix, and your username is granted admin access to the prefix. Your prefix is granted write access to itself and read access to its logs, which are stored under a unique sub-prefix of the global ops/ prefix. Using the same example, say user X signs up on behalf of their company, AcmeCo. User X is automatically granted admin access to the acmeCo/ prefix.acmeCo/, in turn, has write access to acmeCo/ and read access to ops/acmeCo/. As more users and prefixes are added, admins can provision capabilities using the CLI. "},{"title":"Authenticating Flow in the web app​","type":1,"pageTitle":"Authorizing users and authenticating with Flow","url":"reference/authentication/#authenticating-flow-in-the-web-app","content":"You must sign in to begin a new session using the Flow web application. For the duration of the session, you'll be able to perform actions depending on the capabilities granted to the user profile. You can view the capabilities currently provisioned in your organization on the Admin tab. "},{"title":"Authenticating Flow using the CLI​","type":1,"pageTitle":"Authorizing users and authenticating with Flow","url":"reference/authentication/#authenticating-flow-using-the-cli","content":"You can use the flowctl CLI to work with your organization's catalogs and drafts in your local development environment. To authenticate a local development session using the CLI, do the following: Ensure that you have an Estuary account and have signed into the Flow web app before. In the terminal of your local development environment, run: flowctl auth login In a browser window, the web app opens to the CLI-API tab. Copy the access token. Return to the terminal, paste the access token, and press Enter. The token will expire after a predetermined duration. Repeat this process to re-authenticate. "},{"title":"Provisioning capabilities​","type":1,"pageTitle":"Authorizing users and authenticating with Flow","url":"reference/authentication/#provisioning-capabilities","content":"As an admin, you can provision capabilities using the CLI with the subcommands of flowctl auth roles. For example: flowctl auth roles list returns a list of all currently provisioned capabilities flowctl auth roles grant --object-role=acmeCo/ --capability=admin --subject-user-id=userZ grants user Z admin access to acmeCo flowctl auth roles revoke --object-role=outside-org/acmeCo-share/ --capability=read --subject-role=acmeCo/ would be used by an admin of outside-orgto revoke acmeCo/'s read access to outside-org/acmeCo-share/. You can find detailed help for all subcommands using the --help or -h flag. "},{"title":"Connectors","type":0,"sectionRef":"#","url":"reference/Connectors/","content":"Connectors A current list and configuration details for Estuary's connectors can be found on the following pages: Capture connectorsMaterialization connectors You can learn more about how connectors work and how to use them in their conceptual documentation.","keywords":""},{"title":"Capture connectors","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/","content":"","keywords":""},{"title":"Available capture connectors​","type":1,"pageTitle":"Capture connectors","url":"reference/Connectors/capture-connectors/#available-capture-connectors","content":""},{"title":"Estuary connectors​","type":1,"pageTitle":"Capture connectors","url":"reference/Connectors/capture-connectors/#estuary-connectors","content":"These connectors are created by Estuary. We prioritize high-scale technology systems for development. All Estuary connectors capture data in real time, as it appears in the source system AlloyDB ConfigurationPackage - ghcr.io/estuary/source-alloydb:dev Alpaca ConfigurationPackage - ghcr.io/estuary/source-alpaca:dev Amazon Kinesis ConfigurationPackage — ghcr.io/estuary/source-kinesis:dev Amazon S3 ConfigurationPackage — ghcr.io/estuary/source-s3:dev Apache Kafka ConfigurationPackage — ghcr.io/estuary/source-kafka:dev Google Cloud Storage ConfigurationPackage — ghcr.io/estuary/source-gcs:dev Google Firestore ConfigurationPackage - ghcr.io/estuary/source-firestore:dev HTTP file ConfigurationPackage - ghcr.io/estuary/source-http-file:dev MariaDB ConfigurationPackage - ghcr.io/estuary/source-mariadb:dev MySQL ConfigurationPackage - ghcr.io/estuary/source-mysql:dev PostgreSQL ConfigurationPackage — ghcr.io/estuary/source-postgres:dev "},{"title":"Third party connectors​","type":1,"pageTitle":"Capture connectors","url":"reference/Connectors/capture-connectors/#third-party-connectors","content":"Estuary supports open-source connectors from third parties. These connectors operate in a batch fashion, capturing data in increments. When you run these connectors in Flow, you'll get as close to real time as possible within the limitations set by the connector itself. Typically, we enable SaaS connectors from third parties to allow more diverse data flows. All the third-party connectors available currently were created by Airbyte. The versions made available in Flow have been adapted for compatibility. Amazon Ads ConfigurationPackage - ghrc.io/estuary/source-amazon-ads.dev Amplitude ConfigurationPackage - ghcr.io/estuary/source-amplitude:dev Bing Ads ConfigurationPackage - ghcr.io/estuary/source-bing-ads:dev Exchange Rates API ConfigurationPackage - ghcr.io/estuary/source-exchange-rates:dev Facebook Marketing ConfigurationPackage - ghcr.io/estuary/source-facebook-marketing:dev Freshdesk ConfigurationPackage - ghcr.io/estuary/source-freshdesk:dev GitHub ConfigurationPackage - ghcr.io/estuary/source-github:dev Google Ads ConfigurationPackage - ghcr.io/estuary/source-google-ads:dev Google Analytics ConfigurationPackage - ghcr.io/estuary/source-google-analytics-v4:dev Google Search Console ConfigurationPackage - ghcr.io/estuary/source-google-search-console:dev Google Sheets ConfigurationPackage - ghcr.io/estuary/source-google-sheets:dev Hubspot ConfigurationPackage - ghcr.io/estuary/source-hubspot:dev Intercom ConfigurationPackage - ghcr.io/estuary/source-intercom:dev LinkedIn Ads ConfigurationPackage - ghcr.io/estuary/source-linkedin-ads:dev Mailchimp ConfigurationPackage - ghcr.io/estuary/source-mailchimp:dev Salesforce ConfigurationPackage - ghcr.io/estuary/source-salesforce:dev Notion ConfigurationPackage - ghcr.io/estuary/source-notion:dev Stripe ConfigurationPackage - ghcr.io/estuary/source-stripe:dev SurveyMonkey ConfigurationPackage - ghcr.io/estuary/source-surveymonkey:dev Zendesk Support ConfigurationPackage - ghcr.io/estuary/source-zendesk-support:dev "},{"title":"Configuring task shards","type":0,"sectionRef":"#","url":"reference/Configuring-task-shards/","content":"","keywords":""},{"title":"Properties​","type":1,"pageTitle":"Configuring task shards","url":"reference/Configuring-task-shards/#properties","content":"Property\tTitle\tDescription\tType/disable\tDisable\tDisable processing of the task's shards.\tBoolean /logLevel\tLog level\tLog levels may currently be \\&quot;error\\&quot;, \\&quot;warn\\&quot;, \\&quot;info\\&quot;, \\&quot;debug\\&quot;, or \\&quot;trace\\&quot;. If not set, the effective log level is \\&quot;info\\&quot;.\tString /maxTxnDuration\tMaximum transaction duration\tThis duration upper-bounds the amount of time during which a transaction may process documents before it must initiate a commit. Note that it may take some additional time for the commit to complete after it is initiated. The shard may run for less time if there aren't additional ready documents for it to process. If not set, the maximum duration defaults to one second.\tString /minTxnDuration\tMinimum transaction duration\tThis duration lower-bounds the amount of time during which a transaction must process documents before it must flush and commit. It may run for more time if additional documents are available. The default value is zero seconds.\tString For more information about these controls and when you might need to use them, see: TransactionsLog level "},{"title":"Sample​","type":1,"pageTitle":"Configuring task shards","url":"reference/Configuring-task-shards/#sample","content":"materializations: acmeCo/snowflake-materialization: endpoint: connector: config: account: acmeCo database: acmeCo_db password: secret cloud_provider: aws region: us-east-1 schema: acmeCo_flow_schema user: snowflake_user warehouse: acmeCo_warehouse image: ghcr.io/estuary/materialize-snowflake:dev bindings: - resource: table: anvils source: acmeCo/anvils shards: logLevel: debug minTxnDuration: 30s maxTxnDuration: 4m  "},{"title":"Alpaca","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/alpaca/","content":"","keywords":""},{"title":"Real-time and historical trade data​","type":1,"pageTitle":"Alpaca","url":"reference/Connectors/capture-connectors/alpaca/#real-time-and-historical-trade-data","content":"The Alpaca Market Data API comprises multiple APIs for stock trades, including the Trades REST API for historical trade data and websocket streaming via the Data API for real-time trade data. This connector uses both APIs to capture historical and real-time data in parallel. It uses the Trades API to perform a historical backfill starting from the start date you specify and stopping when it reaches the present. At the same time, the connector uses websocket streaming to initiate a real-time stream of trade data starting at the present moment and continuing indefinitely until you stop the capture process. As a result, you'll get data from a historical time period you specify, as well as the lowest-latency possible updates of new trade data, but there will be some overlap in the two data streams. See limitations to learn more about reconciling historical and real-time data. "},{"title":"Supported data resources​","type":1,"pageTitle":"Alpaca","url":"reference/Connectors/capture-connectors/alpaca/#supported-data-resources","content":"Alpaca supports over 8000 stocks and EFTs. You simply supply a list of symbols to Flow when you configure the connector. To check whether Alpaca supports a symbol, you can use the Alpaca Broker API. You can use this connector to capture data from up to 20 stock symbols into Flow collections in a single capture (to add more than 20, set up multiple captures). For a given capture, data from all symbols is captured to a single collection. "},{"title":"Prerequisites​","type":1,"pageTitle":"Alpaca","url":"reference/Connectors/capture-connectors/alpaca/#prerequisites","content":"To use this connector, you'll need: An Alpaca account. To access complete stock data in real-time, you'll need the Unlimited plan. To access a smaller sample of trade data with a 15-minute delay, you can use a Free plan, making sure to set Feed to iex and choose the Free Plan option when configuring the connector. Your Alpaca API Key ID and Secret Key. "},{"title":"Configuration​","type":1,"pageTitle":"Alpaca","url":"reference/Connectors/capture-connectors/alpaca/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Alpaca source connector. "},{"title":"Properties​","type":1,"pageTitle":"Alpaca","url":"reference/Connectors/capture-connectors/alpaca/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/advanced\tAdvanced Options\tOptions for advanced users. You should not typically need to modify these.\tobject /advanced/disable_backfill\tDisable Historical Data Backfill\tDisables historical data backfill via the historical data API. Data will only be collected via streaming.\tboolean /advanced/disable_real_time\tDisable Real-Time Streaming\tDisables real-time streaming via the websocket API. Data will only be collected via the backfill mechanism.\tboolean /advanced/is_free_plan\tFree Plan\tSet this if you are using a free plan. Delays data by 15 minutes.\tboolean /advanced/max_backfill_interval\tMaximum Backfill Interval\tThe largest time interval that will be requested for backfills. Using smaller intervals may be useful when tracking many symbols. Must be a valid Go duration string.\tstring /advanced/min_backfill_interval\tMinimum Backfill Interval\tThe smallest time interval that will be requested for backfills after the initial backfill is complete. Must be a valid Go duration string.\tstring /advanced/stop_date\tStop Date\tStop backfilling historical data at this date.\tstring /api_key_id\tAlpaca API Key ID\tYour Alpaca API key ID.\tstring\tRequired /api_secret_key\tAlpaca API Secret Key\tYour Alpaca API Secret key.\tstring\tRequired /feed\tFeed\tThe feed to pull market data from. Choose from iex or sip; set iex if using a free plan.\tstring\tRequired /start_date\tStart Date\tGet trades starting at this date. Has no effect if changed after the capture has started.\tstring\tRequired /symbols\tSymbols\tComma separated list of symbols to monitor.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/name\tName\tUnique name for this binding. Cannot be changed once set.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Alpaca","url":"reference/Connectors/capture-connectors/alpaca/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: &quot;ghcr.io/estuary/source-alpaca:dev&quot; config: api_key_id: &lt;SECRET&gt; api_secret_key: &lt;SECRET&gt; feed: iex start_date: 2022-11-01T00:00:00Z symbols: AAPL,MSFT,AMZN,TSLA,GOOGL,GOOG,NVDA,BRK.B,META,UNH advanced: is_free_plan: true bindings: - resource: name: trades target: ${PREFIX}/${CAPTURE_NAME}/trades  "},{"title":"Limitations​","type":1,"pageTitle":"Alpaca","url":"reference/Connectors/capture-connectors/alpaca/#limitations","content":"Capturing data for more than 20 symbols in a single capture could result in API errors.​ If you need to capture data for more than 20 symbols, we recommend splitting them between two captures. Support for a larger number of symbols in a single capture is planned for a future release. Separate historical and real-time data streams will result in some duplicate trade documents.​ As discussed above, the connector captures historical and real-time data in two different streams. As the historical data stream catches up to the present, it will overlap with the beginning of the real-time data stream, resulting in some duplicated documents. These will have identical properties from Alpaca, but different metadata from Flow. There are several ways to resolve this: If you plan to materialize to an endpoint for which standard (non-delta) updates are supported, Flow will resolve the duplicates during the materialization process. Unless otherwise specified in their documentation page, materialization connectors run in standard updates mode. If a connector supports both modes, it will default to standard updates. If you plan to materialize to an endpoint for which delta updates is the only option, ensure that the endpoint system supports the equivalent of lastWriteWins reductions. "},{"title":"AlloyDB","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/alloydb/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"AlloyDB","url":"reference/Connectors/capture-connectors/alloydb/#prerequisites","content":"You'll need a AlloyDB database setup with the following: Logical decoding enabledUser role with REPLICATION attributeA replication slot. This represents a “cursor” into the PostgreSQL write-ahead log from which change events can be read. Optional; if none exist, one will be created by the connector.If you wish to run multiple captures from the same database, each must have its own slot. You can create these slots yourself, or by specifying a name other than the default in the advanced configuration. A publication. This represents the set of tables for which change events will be reported. In more restricted setups, this must be created manually, but can be created automatically if the connector has suitable permissions. A watermarks table. The watermarks table is a small “scratch space” to which the connector occasionally writes a small amount of data to ensure accuracy when backfilling preexisting table contents. In more restricted setups, this must be created manually, but can be created automatically if the connector has suitable permissions. You'll also need a virtual machine to connect securely to the instance via SSH tunnelling (AlloyDB doesn't support IP whitelisting). "},{"title":"Setup​","type":1,"pageTitle":"AlloyDB","url":"reference/Connectors/capture-connectors/alloydb/#setup","content":"To meet the prerequisites, complete these steps. Set the alloydb.logical_decoding flag to on to enable logical replication on your AlloyDB instance. In your psql client, connect to your instance and issue the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table and publication. CREATE USER flow_capture WITH REPLICATION IN ROLE alloydbsuperuser LOGIN PASSWORD 'secret'; GRANT SELECT ON ALL TABLES IN SCHEMA public TO flow_capture; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO flow_capture; CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication FOR ALL TABLES; Follow the instructions to create a virtual machine for SSH tunnelingin the same Google Cloud project as your instance. "},{"title":"Backfills and performance considerations​","type":1,"pageTitle":"AlloyDB","url":"reference/Connectors/capture-connectors/alloydb/#backfills-and-performance-considerations","content":"When the a AlloyDB capture is initiated, by default, the connector first backfills, or captures the targeted tables in their current state. It then transitions to capturing change events on an ongoing basis. This is desirable in most cases, as in ensures that a complete view of your tables is captured into Flow. However, you may find it appropriate to skip the backfill, especially for extremely large tables. In this case, you may turn of backfilling on a per-table basis. See properties for details. "},{"title":"Configuration​","type":1,"pageTitle":"AlloyDB","url":"reference/Connectors/capture-connectors/alloydb/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the PostgreSQL source connector. "},{"title":"Properties​","type":1,"pageTitle":"AlloyDB","url":"reference/Connectors/capture-connectors/alloydb/#properties","content":"Endpoint​ The SSH config section is required for this connector. You'll fill in the database address with a localhost IP address, and specify your VM's IP address as the SSH address. See the table below and the sample config. Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tThe host or host:port at which the database can be reached. Set to 127.0.0.1:5432 to enable SSH tunneling.\tstring\tRequired /database\tDatabase\tLogical database name to capture from.\tstring\tRequired, &quot;postgres&quot; /user\tUser\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tPassword\tPassword for the specified database user.\tstring\tRequired /advanced\tAdvanced Options\tOptions for advanced users. You should not typically need to modify these.\tobject /advanced/backfill_chunk_size\tBackfill Chunk Size\tThe number of rows which should be fetched from the database in a single backfill query.\tinteger\t4096 /advanced/publicationName\tPublication Name\tThe name of the PostgreSQL publication to replicate from.\tstring\t&quot;flow_publication&quot; /advanced/skip_backfills\tSkip Backfills\tA comma-separated list of fully-qualified table names which should not be backfilled.\tstring /advanced/slotName\tSlot Name\tThe name of the PostgreSQL replication slot to replicate from.\tstring\t&quot;flow_slot&quot; /advanced/watermarksTable\tWatermarks Table\tThe name of the table used for watermark writes during backfills. Must be fully-qualified in '&lt;schema&gt;.&lt;table&gt;' form.\tstring\t&quot;public.flow_watermarks&quot; networkTunnel\tNetwork Tunnel\tConnect to your system through an SSH server that acts as a bastion host for your network.\tObject networkTunnel/sshForwarding\tSSH Forwarding Object networkTunnel/sshForwarding/sshEndpoint\tSSH Endpoint\tEndpoint of the remote SSH server (in this case, your Google Cloud VM) that supports tunneling (in the form of ssh://user@address).\tString networkTunnel/sshForwarding/privateKey\tSSH Private Key\tPrivate key to connect to the remote SSH server.\tString\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe namespace/instance of the table.\tstring\tRequired /stream\tStream\tTable name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"AlloyDB","url":"reference/Connectors/capture-connectors/alloydb/#sample","content":"A minimal capture definition will look like the following: captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: &quot;ghcr.io/estuary/source-alloydb:dev&quot; config: address: &quot;127.0.0.1:5432&quot; database: &quot;postgres&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; networkTunnel: sshForwarding: sshEndpoint: ssh://sshUser@vm-ip-address privateKey: |2 -----BEGIN RSA PRIVATE KEY----- MIICXAIBAAKBgQCJO7G6R+kv2MMS8Suw21sk2twHg8Vog0fjimEWJEwyAfFM/Toi EJ6r5RTaSvN++/+MPWUll7sUdOOBZr6ErLKLHEt7uXxusAzOjMxFKZpEARMcjwHY v/tN1A2OYU0qay1DOwknEE0i+/Bvf8lMS7VDjHmwRaBtRed/+iAQHf128QIDAQAB AoGAGoOUBP+byAjDN8esv1DCPU6jsDf/Tf//RbEYrOR6bDb/3fYW4zn+zgtGih5t CR268+dwwWCdXohu5DNrn8qV/Awk7hWp18mlcNyO0skT84zvippe+juQMK4hDQNi ywp8mDvKQwpOuzw6wNEitcGDuACx5U/1JEGGmuIRGx2ST5kCQQDsstfWDcYqbdhr 5KemOPpu80OtBYzlgpN0iVP/6XW1e5FCRp2ofQKZYXVwu5txKIakjYRruUiiZTza QeXRPbp3AkEAlGx6wMe1l9UtAAlkgCFYbuxM+eRD4Gg5qLYFpKNsoINXTnlfDry5 +1NkuyiQDjzOSPiLZ4Abpf+a+myjOuNL1wJBAOwkdM6aCVT1J9BkW5mrCLY+PgtV GT80KTY/d6091fBMKhxL5SheJ4SsRYVFtguL2eA7S5xJSpyxkadRzR0Wj3sCQAvA bxO2fE1SRqbbF4cBnOPjd9DNXwZ0miQejWHUwrQO0inXeExNaxhYKQCcnJNUAy1J 6JfAT/AbxeSQF3iBKK8CQAt5r/LLEM1/8ekGOvBh8MAQpWBW771QzHUN84SiUd/q xR9mfItngPwYJ9d/pTO7u9ZUPHEoat8Ave4waB08DsI= -----END RSA PRIVATE KEY----- bindings: - resource: stream: ${TABLE_NAME} namespace: ${TABLE_NAMESPACE} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your capture definition will likely be more complex, with additional bindings for each table in the source database. Learn more about capture definitions. "},{"title":"Amazon Ads","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/amazon-ads/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Amazon Ads","url":"reference/Connectors/capture-connectors/amazon-ads/#supported-data-resources","content":"The following data resources are supported: ProfilesSponsored brands ad groupsSponsored brands campaignsSponsored brands keywordsSponsored brands report streamSponsored brands video report streamSponsored display ad groupsSponsored display ad campaignsSponsored display product ads Sponsored display report streamSponsored display targetingsSponsored product ad groupsSponsored product adsSponsored product campaignsSponsored product keywordsSponsored product negative keywordsSponsored product targetingsSponsored product report stream By default, each resource is mapped to a Flow collection through a separate binding. "},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon Ads","url":"reference/Connectors/capture-connectors/amazon-ads/#prerequisites","content":"This connector uses OAuth2 to authenticate with Amazon. You can do this in the Flow web app, or configure manually if you're using the flowctl CLI. "},{"title":"Using OAuth2 to authenticate with Amazon in the Flow web app​","type":1,"pageTitle":"Amazon Ads","url":"reference/Connectors/capture-connectors/amazon-ads/#using-oauth2-to-authenticate-with-amazon-in-the-flow-web-app","content":"You'll need an Amazon user account with access to the Amazon Ads account from which you wish to capture data. You'll use these credentials to sign in. "},{"title":"Authenticating manually using the CLI​","type":1,"pageTitle":"Amazon Ads","url":"reference/Connectors/capture-connectors/amazon-ads/#authenticating-manually-using-the-cli","content":"When you configure this connector manually, you provide the same credentials that OAuth2 would automatically fetch if you used the web app. These are: Client IDClient secretRefresh token To obtain these credentials: Complete the Amazon Ads API onboarding process. Retrieve your client ID and client secret. Retrieve a refresh token. "},{"title":"Selecting data region and profiles​","type":1,"pageTitle":"Amazon Ads","url":"reference/Connectors/capture-connectors/amazon-ads/#selecting-data-region-and-profiles","content":"When you configure the endpoint for this connector, you must choose an Amazon region from which to capture data. Optionally, you may also select profiles from which to capture data. The region must be one of: NA (North America)EU (European Union)FE (Far East) These represent the three URL endpoints provided by Amazon through which you can access the marketing API. Each region encompasses multiple Amazon marketplaces, which are broken down by country. See the Amazon docs for details. If you run your Amazon ads in multiple marketplaces, you may have separate profiles for each. If this is the case, you can specify the profiles from which you wish to capture data by supplying their profile IDs. Be sure to specify only profiles that correspond to marketplaces within the region you chose. "},{"title":"Configuration​","type":1,"pageTitle":"Amazon Ads","url":"reference/Connectors/capture-connectors/amazon-ads/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Amazon Ads source connector. "},{"title":"Properties​","type":1,"pageTitle":"Amazon Ads","url":"reference/Connectors/capture-connectors/amazon-ads/#properties","content":"Endpoint​ The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so many of these properties aren't required. Property\tTitle\tDescription\tType\tRequired/Default/credentials object\tRequired /credentials/auth_type\tAuth Type\tSet to oauth2.0 for manual integration (in this method, you're re-creating the same credentials of the OAuth user interface, but doing so manually)\tstring /credentials/client_id\tClient ID\tThe client ID of your Amazon Ads developer application.\tstring\tRequired /credentials/client_secret\tClient Secret\tThe client secret of your Amazon Ads developer application.\tstring\tRequired /credentials/refresh_token\tRefresh Token\tAmazon Ads refresh token.\tstring\tRequired /profiles\tProfile IDs (Optional)\tProfile IDs you want to fetch data for.\tarray /region\tRegion *\tRegion to pull data from (EU/NA/FE).\tstring\t&quot;NA&quot; /report_generation_max_retries\tReport Generation Maximum Retries *\tMaximum retries the connector will attempt for fetching report data.\tinteger\t5 /report_wait_timeout\tReport Wait Timeout *\tTimeout duration in minutes for reports.\tinteger\t60 /start_date\tStart Date (Optional)\tThe start date for collecting reports, in YYYY-MM-DD format. This should not be more than 60 days in the past.\tstring\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tAmazon Ads resource from which a collection is captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Amazon Ads","url":"reference/Connectors/capture-connectors/amazon-ads/#sample","content":"This sample specification reflects the manual authentication method. captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-amazon-ads:dev config: credentials: auth_type: oauth2.0 client_id: amzn1.application-oa2-client.XXXXXXXXX client_secret: &lt;secret&gt; refresh_token: Atzr|XXXXXXXXXXXX region: NA report_generation_max_retries: 5 report_wait_timeout: 60 start_date: 2022-03-01 bindings: - resource: stream: profiles syncMode: full_refresh target: ${PREFIX}/profiles {}  "},{"title":"Amazon Kinesis","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/amazon-kinesis/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon Kinesis","url":"reference/Connectors/capture-connectors/amazon-kinesis/#prerequisites","content":"To use this connector, you'll need: One or more Amazon Kinesis streams. For a given capture, all streams must: Contain JSON data onlyBe in the same AWS region An IAM user with the following permissions: ListShards on all resourcesGetRecords on all streams usedGetShardIterator on all streams usedDescribeStream on all streams usedDescribeStreamSummary on all streams used These permissions should be specified with the kinesis: prefix in an IAM policy document. For more details and examples, see Controlling Access to Amazon Kinesis Data in the Amazon docs. The AWS access key and secret access key for the user. See the AWS blog for help finding these credentials. "},{"title":"Configuration​","type":1,"pageTitle":"Amazon Kinesis","url":"reference/Connectors/capture-connectors/amazon-kinesis/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Amazon Kinesis source connector. "},{"title":"Properties​","type":1,"pageTitle":"Amazon Kinesis","url":"reference/Connectors/capture-connectors/amazon-kinesis/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/awsAccessKeyId\tAWS access key ID\tPart of the AWS credentials that will be used to connect to Kinesis.\tstring\tRequired, &quot;example-aws-access-key-id&quot; /awsSecretAccessKey\tAWS secret access key\tPart of the AWS credentials that will be used to connect to Kinesis.\tstring\tRequired, &quot;example-aws-secret-access-key&quot; /endpoint\tAWS endpoint\tThe AWS endpoint URI to connect to, useful if you're capturing from a kinesis-compatible API that isn't provided by AWS.\tstring /region\tAWS region\tThe name of the AWS region where the Kinesis stream is located.\tstring\tRequired, &quot;us-east-1&quot; Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tStream name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Amazon Kinesis","url":"reference/Connectors/capture-connectors/amazon-kinesis/#sample","content":"A minimal capture definition will look like the following: captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-kinesis:dev config: awsAccessKeyId: &quot;example-aws-access-key-id&quot; awsSecretAccessKey: &quot;example-aws-secret-access-key&quot; region: &quot;us-east-1&quot; bindings: - resource: stream: ${STREAM_NAME} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your capture definition will likely be more complex, with additional bindings for each Kinesis stream. Learn more about capture definitions.. "},{"title":"Amplitude","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/amplitude/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Amplitude","url":"reference/Connectors/capture-connectors/amplitude/#supported-data-resources","content":"The following data resources are supported through the Amplitude APIs: Active User CountsAnnotationsAverage Session LengthCohortsEvents By default, each resource is mapped to a Flow collection through a separate binding. "},{"title":"Prerequisites​","type":1,"pageTitle":"Amplitude","url":"reference/Connectors/capture-connectors/amplitude/#prerequisites","content":"An Amplitude project with an API Key and Secret Key "},{"title":"Configuration​","type":1,"pageTitle":"Amplitude","url":"reference/Connectors/capture-connectors/amplitude/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Amplitude source connector. "},{"title":"Properties​","type":1,"pageTitle":"Amplitude","url":"reference/Connectors/capture-connectors/amplitude/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/api_key\tAPI Key\tAmplitude API Key.\tstring\tRequired /secret_key\tSecret Key\tAmplitude Secret Key.\tstring\tRequired /start_date\tReplication Start Date\tUTC date and time in the format 2021-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Amplitude project from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Amplitude","url":"reference/Connectors/capture-connectors/amplitude/#sample","content":" captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-amplitude:dev config: api_key: &lt;secret&gt; secret_key: &lt;secret&gt; start_date: 2022-06-18T00:00:00Z bindings: - resource: stream: cohorts syncMode: full_refresh target: ${PREFIX}/cohorts - resource: stream: annotations syncMode: full_refresh target: ${PREFIX}/annotations - resource: stream: events syncMode: incremental target: ${PREFIX}/events - resource: stream: active_users syncMode: incremental target: ${PREFIX}/activeusers - resource: stream: average_session_length syncMode: incremental target: ${PREFIX}/averagesessionlength  "},{"title":"Amazon S3","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/amazon-s3/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Amazon S3","url":"reference/Connectors/capture-connectors/amazon-s3/#prerequisites","content":"You can use this connector to capture data from an entire S3 bucket or for a prefix within a bucket. This bucket or prefix must be either be: Publicly accessible and allowing anonymous reads. Accessible via a root or IAM user. In either case, you'll need an access policy. Policies in AWS are JSON objects that define permissions. You attach them to resources, which include both IAM users and S3 buckets. See the steps below to set up access. "},{"title":"Setup: Public buckets​","type":1,"pageTitle":"Amazon S3","url":"reference/Connectors/capture-connectors/amazon-s3/#setup-public-buckets","content":"For a public buckets, the bucket access policy must allow anonymous reads on the whole bucket or a specific prefix. Create a bucket policy using the templates below. Anonymous reads policy - Full bucketAnonymous reads policy - Specific prefix { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Sid&quot;: &quot;BucketAnonymousRead&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: &quot;*&quot;, &quot;Action&quot;: [ &quot;s3:ListBucket&quot; ], &quot;Resource&quot;: [ &quot;arn:aws:s3:::EXAMPLE_BUCKET&quot; ] }, { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: &quot;*&quot;, &quot;Action&quot;: [ &quot;s3:GetObject&quot; ], &quot;Resource&quot;: [ &quot;arn:aws:s3:::EXAMPLE_BUCKET/*&quot; ] } ] }  Add the policy to your bucket. Paste over the existing policy and resolve any errors or warnings before saving. Confirm that the Block public access setting on the bucket is disabled. "},{"title":"Setup: Accessing with a user account​","type":1,"pageTitle":"Amazon S3","url":"reference/Connectors/capture-connectors/amazon-s3/#setup-accessing-with-a-user-account","content":"For buckets accessed by a user account, you'll need the AWS access key and secret access key for the user. You'll also need to apply an access policy to the user to grant access to the specific bucket or prefix. Create an IAM user if you don't yet have one to use with Flow. Note the user's access key and secret access key. See the AWS blog for help finding these credentials. Create an IAM policy using the templates below. IAM user access policy - Full bucketIAM user access policy - Specific prefix { &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Sid&quot;: &quot;UserAccessFullBucket&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;s3:ListBucket&quot; ], &quot;Resource&quot;: [ &quot;arn:aws:s3:::EXAMPLE_BUCKET&quot; ] }, { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;s3:GetObject&quot; ], &quot;Resource&quot;: [ &quot;arn:aws:s3:::EXAMPLE_BUCKET/*&quot; ] } ] }  Add the policy to AWS. Attach the policy to the IAM user. "},{"title":"Configuration​","type":1,"pageTitle":"Amazon S3","url":"reference/Connectors/capture-connectors/amazon-s3/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the S3 source connector. tip You might organize your S3 bucket using prefixes to emulate a directory structure. This connector can use prefixes in two ways: first, to perform the discovery phase of setup, and later, when the capture is running. You can specify a prefix in the endpoint configuration to limit the overall scope of data discovery.If using the flowctl CLI to write your specification locally, you can specify prefixes on a per-binding basis. This allows you to map each prefix to a distinct Flow collection, and informs how the capture will behave in production. To capture the entire bucket, omit prefix in the endpoint configuration and set stream to the name of the bucket. (This is the only available method in the web app.) "},{"title":"Properties​","type":1,"pageTitle":"Amazon S3","url":"reference/Connectors/capture-connectors/amazon-s3/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/advanced Options for advanced users. You should not typically need to modify these.\tobject /advanced/ascendingKeys\tAscending Keys\tImprove sync speeds by listing files from the end of the last sync, rather than listing the entire bucket prefix. This requires that you write objects in ascending lexicographic order, such as an RFC-3339 timestamp, so that key ordering matches modification time ordering. If data is not ordered correctly, using ascending keys could cause errors.\tboolean\tfalse /advanced/endpoint\tAWS Endpoint\tThe AWS endpoint URI to connect to. Use if you're capturing from a S3-compatible API that isn't provided by AWS\tstring /awsAccessKeyId\tAWS Access Key ID\tPart of the AWS credentials that will be used to connect to S3. Required unless the bucket is public and allows anonymous listings and reads.\tstring /awsSecretAccessKey\tAWS Secret Access Key\tPart of the AWS credentials that will be used to connect to S3. Required unless the bucket is public and allows anonymous listings and reads.\tstring /bucket\tBucket\tName of the S3 bucket\tstring\tRequired /matchKeys\tMatch Keys\tFilter applied to all object keys under the prefix. If provided, only objects whose absolute path matches this regex will be read. For example, you can use &quot;.*\\.json&quot; to only capture json files.\tstring /parser\tParser Configuration\tConfigures how files are parsed (optional, see below)\tobject /parser/compression\tCompression\tDetermines how to decompress the contents. The default, 'Auto', will try to determine the compression automatically.\tnull, string\tnull /parser/format\tFormat\tDetermines how to parse the contents. The default, 'Auto', will try to determine the format automatically based on the file extension or MIME type, if available.\tobject\t{&quot;type&quot;:&quot;auto&quot;} /parser/format/type\tType string /prefix\tPrefix\tPrefix within the bucket to capture from.\tstring /region\tAWS Region\tThe name of the AWS region where the S3 bucket is located. &quot;us-east-1&quot; is a popular default you can try, if you're unsure what to put here.\tstring\tRequired, &quot;us-east-1&quot; Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tPrefix\tPath to dataset in the bucket, formatted as bucket-name/prefix-name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Amazon S3","url":"reference/Connectors/capture-connectors/amazon-s3/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-s3:dev config: bucket: &quot;my-bucket&quot; parser: compression: zip format: type: csv config: delimiter: &quot;,&quot; encoding: UTF-8 errorThreshold: 5 headers: [ID, username, first_name, last_name] lineEnding: &quot;\\\\r&quot; quote: &quot;\\&quot;&quot; region: &quot;us-east-1&quot; bindings: - resource: stream: my-bucket/${PREFIX} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your capture definition may be more complex, with additional bindings for different S3 prefixes within the same bucket. Learn more about capture definitions. "},{"title":"Advanced: Parsing cloud storage data​","type":1,"pageTitle":"Amazon S3","url":"reference/Connectors/capture-connectors/amazon-s3/#advanced-parsing-cloud-storage-data","content":"Cloud storage platforms like S3 can support a wider variety of file types than other data source systems. For each of these file types, Flow must parse and translate data into collections with defined fields and JSON schemas. By default, the parser will automatically detect the type and shape of the data in your bucket, so you won't need to change the parser configuration for most captures. However, the automatic detection may be incorrect in some cases. To fix or prevent this, you can provide explicit information in the parser configuration, which is part of the endpoint configuration for this connector. The parser configuration includes: Compression: Specify how the bucket contents are compressed. If no compression type is specified, the connector will try to determine the compression type automatically. Options are: zipgzipzstdnone Format: Specify the data format, which determines how it will be parsed. Options are: Auto: If no format is specified, the connector will try to determine it automatically. Avro CSV JSON Protobuf W3C Extended Log info At this time, Flow only supports S3 captures with data of a single file type. Support for multiple file types, which can be configured on a per-binding basis, will be added in the future. For now, use a prefix in the endpoint configuration to limit the scope of each capture to data of a single file type. CSV configuration​ CSV files include several additional properties that are important to the parser. In most cases, Flow is able to automatically determine the correct values, but you may need to specify for unusual datasets. These properties are: Delimiter. Options are: Comma (&quot;,&quot;)Pipe (&quot;|&quot;)Space (&quot;0x20&quot;)Semicolon (&quot;;&quot;)Tab (&quot;0x09&quot;)Vertical tab (&quot;0x0B&quot;)Unit separator (&quot;0x1F&quot;)SOH (&quot;0x01&quot;)Auto Encoding type, specified by its WHATWG label. Optionally, an Error threshold, as an acceptable percentage of errors. If set to a number greater than zero, malformed rows that fall within the threshold will be excluded from the capture. Escape characters. Options are: Backslash (&quot;\\\\&quot;)Disable escapes (&quot;&quot;)Auto Optionally, a list of column Headers, if not already included in the first row of the CSV file. If any headers are provided, it is assumed that the provided list of headers is complete and authoritative. The first row of your CSV file will be assumed to be data (not headers), and you must provide a header value for every column in the file. Line ending values CRLF (&quot;\\\\r\\\\n&quot;) (Windows)CR (&quot;\\\\r&quot;)LF (&quot;\\\\n&quot;)Record Separator (&quot;0x1E&quot;)Auto Quote character Double Quote (&quot;\\&quot;&quot;)Single Quote (&quot;)Disable Quoting (&quot;&quot;)Auto The sample specification above includes these fields. "},{"title":"Apache Kafka","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/apache-kafka/","content":"","keywords":""},{"title":"Supported data types​","type":1,"pageTitle":"Apache Kafka","url":"reference/Connectors/capture-connectors/apache-kafka/#supported-data-types","content":"This connector supports Kafka messages that contain JSON data. Flow collections store data as JSON. Before deploying this connector, you should modify schema(s)of the Flow collection(s) you're creating to reflect the structure of your JSON Kafka messages. At this time, the connector does not support other data types in Kafka messages. Beta Support for Avro Kafka messages will be added soon. For more information, contact the Estuary team. "},{"title":"Prerequisites​","type":1,"pageTitle":"Apache Kafka","url":"reference/Connectors/capture-connectors/apache-kafka/#prerequisites","content":"A Kafka cluster with: bootstrap.servers configured so that clients may connect via the desired host and portAn authentication mechanism of choice set up (highly recommended for production environments)Connection security enabled with TLS (highly recommended for production environments) "},{"title":"Authentication and connection security​","type":1,"pageTitle":"Apache Kafka","url":"reference/Connectors/capture-connectors/apache-kafka/#authentication-and-connection-security","content":"Neither authentication nor connection security are enabled by default in your Kafka cluster, but both are important considerations. Similarly, Flow's Kafka connectors do not strictly require authentication or connection security mechanisms. You may choose to omit them for local development and testing; however, both are strongly encouraged for production environments. A wide variety of authentication methods is available in Kafka clusters. Flow supports SASL/SCRAM-SHA-256, SASL/SCRAM-SHA-512, and SASL/PLAIN. Behavior using other authentication methods is not guaranteed. When authentication details are not provided, the client connection will attempt to use PLAINTEXT (insecure) protocol. If you don't already have authentication enabled on your cluster, Estuary recommends either of listed SASL/SCRAM methods. With SCRAM, you set up a username and password, making it analogous to the traditional authentication mechanisms you use in other applications. For connection security, Estuary recommends that you enable TLS encryption for your SASL mechanism of choice, as well as all other components of your cluster. Note that because TLS replaced now-deprecated SSL encryption, Kafka still uses the acronym &quot;SSL&quot; to refer to TLS encryption. See Confluent's documentation for details. Beta TLS encryption is currently the only supported connection security mechanism for this connector. Other connection security methods may be enabled in the future. "},{"title":"Configuration​","type":1,"pageTitle":"Apache Kafka","url":"reference/Connectors/capture-connectors/apache-kafka/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Apache Kafka source connector. "},{"title":"Properties​","type":1,"pageTitle":"Apache Kafka","url":"reference/Connectors/capture-connectors/apache-kafka/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/bootstrap_servers\tBootstrap servers\tThe initial servers in the Kafka cluster to connect to. The Kafka client will be informed of the rest of the cluster nodes by connecting to one of these nodes.\tarray\tRequired /tls\tTLS\tTLS connection settings.\tstring\t&quot;system_certificates&quot; /authentication\tAuthentication\tConnection details used to authenticate a client connection to Kafka via SASL.\tnull, object /authentication/mechanism\tSASL Mechanism\tSASL mechanism describing how to exchange and authenticate client servers.\tstring /authentication/password\tPassword\tPassword, if applicable for the authentication mechanism chosen.\tstring /authentication/username\tUsername\tUsername, if applicable for the authentication mechanism chosen.\tstring\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tKafka topic name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Apache Kafka","url":"reference/Connectors/capture-connectors/apache-kafka/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-kafka:dev config: bootstrap_servers: [localhost:9093] tls: system_certificates authentication: mechanism: SCRAM-SHA-512 username: bruce.wayne password: definitely-not-batman bindings: - resource: stream: ${TOPIC_NAME} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your capture definition will likely be more complex, with additional bindings for each Kafka topic. Learn more about capture definitions.. "},{"title":"Exchange Rates API","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/exchange-rates/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Exchange Rates API","url":"reference/Connectors/capture-connectors/exchange-rates/#prerequisites","content":"An API key generated through an Exchange Rate API account. After you sign up, your API key can be found on your account page. You may use the free account, but note that you'll be limited to the default base currency, EUR. "},{"title":"Configuration​","type":1,"pageTitle":"Exchange Rates API","url":"reference/Connectors/capture-connectors/exchange-rates/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Exchange Rates source connector. "},{"title":"Properties​","type":1,"pageTitle":"Exchange Rates API","url":"reference/Connectors/capture-connectors/exchange-rates/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/access_key\tAccess key\tYour API access key. The key is case sensitive.\tstring\tRequired /base\tBase currency\tISO reference currency. See the documentation. Free plan doesn't support Source Currency Switching, default base currency is EUR\tstring\tEUR /ignore_weekends\tIgnore weekends\tIgnore weekends? (Exchanges don't run on weekends)\tboolean\ttrue /start_date\tStart date\tThe date in the format YYYY-MM-DD. Data will begin from this date.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tData stream from which Flow captures data. Always set to exchange_rates.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Exchange Rates API","url":"reference/Connectors/capture-connectors/exchange-rates/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-exchange-rates:dev config: base: EUR access_key: &lt;secret&gt; start_date: 2022-01-01 ignore_weekends: true bindings: - resource: stream: exchange_rates syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  This capture definition should only have one binding, as exchange_rates is the only available data stream. Learn more about capture definitions. "},{"title":"Bing Ads","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/bing-ads/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Bing Ads","url":"reference/Connectors/capture-connectors/bing-ads/#supported-data-resources","content":"The following data resources are supported: AccountsAccount performance reports: hourly, daily, weekly, and monthly (four resources)Ad groupsAd group performance reports: hourly, daily, weekly, and monthly (four resources)AdsAd performance reports: hourly, daily, weekly, and monthly (four resources).Budget summary reportCampaignsCampaign performance reports: hourly, daily, weekly, and monthly (four resources).Keyword performance reports: hourly, daily, weekly, and monthly (four resources). By default, each resource is mapped to a Flow collection through a separate binding. "},{"title":"Prerequisites​","type":1,"pageTitle":"Bing Ads","url":"reference/Connectors/capture-connectors/bing-ads/#prerequisites","content":"This connector uses OAuth2 to authenticate with Microsoft. You can do this in the Flow web app, or configure manually if you're using the flowctl CLI. "},{"title":"Using OAuth2 to authenticate with Microsoft in the Flow web app​","type":1,"pageTitle":"Bing Ads","url":"reference/Connectors/capture-connectors/bing-ads/#using-oauth2-to-authenticate-with-microsoft-in-the-flow-web-app","content":"You'll need: User credentials with access to the Bing Ads account. A developer token associated with the user. "},{"title":"Authenticating manually using the CLI​","type":1,"pageTitle":"Bing Ads","url":"reference/Connectors/capture-connectors/bing-ads/#authenticating-manually-using-the-cli","content":"You'll need: A registered Bing Ads application with the following credentials retrieved: Client ID Client Secret Refresh Token To set get these items, complete the following steps: Register your Bing Ads Application in the Azure Portal. During setup, note the client_id and client_secret. Get a user access token. Redeem the user authorization code for OAuth tokens, and note the refresh_token. "},{"title":"Configuration​","type":1,"pageTitle":"Bing Ads","url":"reference/Connectors/capture-connectors/bing-ads/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Bing Ads source connector. "},{"title":"Properties​","type":1,"pageTitle":"Bing Ads","url":"reference/Connectors/capture-connectors/bing-ads/#properties","content":"Endpoint​ The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so many of these properties aren't required. Property\tTitle\tDescription\tType\tRequired/Default/credentials\tCredentials object\tRequired /credentials/auth_method\tAuthentication method\tSet to oauth2.0\tString\toauth2.0 /credentials/client_id\tClient ID\tThe Client ID of your Microsoft Advertising developer application.\tString\tRequired /credentials/client_secret\tClient Secret\tThe Client Secret of your Microsoft Advertising developer application.\tString\tRequired /credentials/refresh_token\tRefresh Token\tRefresh Token to renew the expired Access Token.\tString\tRequired /developer_token\tDeveloper Token\tDeveloper token associated with user.\tString\tRequired /reports_start_date\tCredentials\tThe start date from which to begin replicating report data. Any data generated before this date will not be replicated in reports. This is a UTC date in YYYY-MM-DD format.\tString\tRequired, 2020-01-01 /tenant_id\tCredentials\tThe Tenant ID of your Microsoft Advertising developer application. Set this to common unless you know you need a different value.\tString\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tBing Ads resource from which a collection is captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Bing Ads","url":"reference/Connectors/capture-connectors/bing-ads/#sample","content":"This sample specification reflects the manual authentication method. captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-bing-ads:dev config: credentials: auth_type: oauth2.0 client_id: 6731de76-14a6-49ae-97bc-6eba6914391e client_secret: &lt;secret&gt; refresh_token: &lt;token&gt; developer_token: &lt;token&gt; reports_start_date: 2020-01-01 tenant_id: common bindings: - resource: stream: accounts syncMode: full_refresh target: ${PREFIX}/accounts {}  "},{"title":"Freshdesk","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/freshdesk/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Freshdesk","url":"reference/Connectors/capture-connectors/freshdesk/#supported-data-resources","content":"The following data resources are supported: AgentsBusiness hoursCanned response foldersCanned responsesCompaniesContactsConversationsDiscussion categoriesDiscussion commentsDiscussion forumsDiscussion topicsEmail configsEmail mailboxesGroupsProductsRolesSatisfaction ratingsScenario automationsSettingsSkillsSLA policiesSolution articlesSolution categoriesSolution foldersSurveysTicket fieldsTicketsTime entries By default, each resource is mapped to a Flow collection through a separate binding. "},{"title":"Prerequisites​","type":1,"pageTitle":"Freshdesk","url":"reference/Connectors/capture-connectors/freshdesk/#prerequisites","content":"To use this connector, you'll need: Your Freshdesk account URLYour Freshdesk API key "},{"title":"Configuration​","type":1,"pageTitle":"Freshdesk","url":"reference/Connectors/capture-connectors/freshdesk/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Freshdesk source connector. "},{"title":"Properties​","type":1,"pageTitle":"Freshdesk","url":"reference/Connectors/capture-connectors/freshdesk/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/api_key\tAPI Key\tFreshdesk API Key.\tstring\tRequired /domain\tDomain\tFreshdesk domain\tstring\tRequired /requests_per_minute\tRequests per minute\tThe number of requests per minute that this source is allowed to use. There is a rate limit of 50 requests per minute per app per account.\tinteger /start_date\tStart Date\tUTC date and time. Any data created after this date will be replicated. If this parameter is not set, all data will be replicated.\tstring\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource from the Freshdesk API from which a collection is captured.\tstring\tRequired /syncMode\tSync mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Freshdesk","url":"reference/Connectors/capture-connectors/freshdesk/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-freshdesk:dev config: api_key: xxxxxxxxxxxxxxxx domain: acmesupport.freshdesk.com bindings: - resource: stream: agents syncMode: incremental target: ${PREFIX}/agents {...}  "},{"title":"Facebook Marketing","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/facebook-marketing/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Facebook Marketing","url":"reference/Connectors/capture-connectors/facebook-marketing/#supported-data-resources","content":"The following data resources are supported: AdsAd activitiesAd creativesAd insightsBusiness ad accountsCampaignsImagesVideos By default, each resource associated with your Facebook Business account is mapped to a Flow collection through a separate binding. "},{"title":"Prerequisites​","type":1,"pageTitle":"Facebook Marketing","url":"reference/Connectors/capture-connectors/facebook-marketing/#prerequisites","content":"There are two ways to authenticate with Facebook when capturing data into Flow: signing in with OAuth2, and manually supplying an access token. Their prerequisites differ. OAuth is recommended for simplicity in the Flow web app; the manual method is the only supported method using the command line. "},{"title":"Signing in with OAuth2​","type":1,"pageTitle":"Facebook Marketing","url":"reference/Connectors/capture-connectors/facebook-marketing/#signing-in-with-oauth2","content":"To use OAuth2 in the Flow web app, you'll need A Facebook Business account and its Ad Account ID. "},{"title":"Configuring manually with an access token​","type":1,"pageTitle":"Facebook Marketing","url":"reference/Connectors/capture-connectors/facebook-marketing/#configuring-manually-with-an-access-token","content":"To configure manually with an access token, you'll need: A Facebook Business account, and its Ad Account ID.A Facebook app with: The Marketing API enabled.A Marketing API access token generated.Access upgrade from Standard Access (the default) to Advanced Access. This allows a sufficient rate limit to support the connector. Follow the steps below to meet these requirements. Setup​ Find your Facebook Ad Account ID. In Meta for Developers, create a new app of the type Business. On your new app's dashboard, click the button to set up the Marketing API. On the Marketing API Tools tab, generate a Marketing API access token with all available permissions (ads_management, ads_read, read_insights, and business_management). Request Advanced Access for your app. Specifically request the Advanced Access to the following: The feature Ads Management Standard Access The permission ads_read The permission ads_management Once your request is approved, you'll have a high enough rate limit to proceed with running the connector. "},{"title":"Configuration​","type":1,"pageTitle":"Facebook Marketing","url":"reference/Connectors/capture-connectors/facebook-marketing/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Facebook Marketing source connector. "},{"title":"Properties​","type":1,"pageTitle":"Facebook Marketing","url":"reference/Connectors/capture-connectors/facebook-marketing/#properties","content":"Endpoint​ By default, this connector captures all data associated with your Business Ad Account. You can refine the data you capture from Facebook Marketing using the optional Custom Insights configuration. You're able to specify certain fields to capture and apply data breakdowns.Breakdowns are a feature of the Facebook Marketing Insights API that allows you to group API output by common metrics.Action breakdownsare a subset of breakdowns that must be specified separately. Property\tTitle\tDescription\tType\tRequired/Default/access_token\tAccess Token\tThe value of the access token generated.\tstring\tRequired /account_id\tAccount ID\tThe Facebook Ad account ID to use when pulling data from the Facebook Marketing API.\tstring\tRequired for manual authentication only /custom_insights\tCustom Insights\tA list which contains insights entries. Each entry must have a name and can contains fields, breakdowns or action_breakdowns\tarray /custom_insights/-/action_breakdowns\tAction Breakdowns\tA list of chosen action_breakdowns to apply\tarray\t[] /custom_insights/-/action_breakdowns/-\tValidActionBreakdowns\tGeneric enumeration. Derive from this class to define new enumerations.\tstring /custom_insights/-/breakdowns\tBreakdowns\tA list of chosen breakdowns to apply\tarray\t[] /custom_insights/-/breakdowns/-\tValidBreakdowns\tGeneric enumeration. Derive from this class to define new enumerations.\tstring /custom_insights/-/end_date\tEnd Date\tThe date until which you'd like to replicate data for this stream, in the format YYYY-MM-DDT00:00:00Z. All data generated between the start date and this date will be replicated. Not setting this option will result in always syncing the latest data.\tstring /custom_insights/-/fields\tFields\tA list of chosen fields to capture\tarray\t[] /custom_insights/-/fields/-\tValidEnums\tGeneric enumeration. Derive from this class to define new enumerations.\tstring /custom_insights/-/name\tName\tThe name of the insight\tstring /custom_insights/-/start_date\tStart Date\tThe date from which you'd like to replicate data for this stream, in the format YYYY-MM-DDT00:00:00Z.\tstring /custom_insights/-/time_increment\tTime Increment\tTime window in days by which to aggregate statistics. The sync will be chunked into N day intervals, where N is the number of days you specified. For example, if you set this value to 7, then all statistics will be reported as 7-day aggregates by starting from the start_date. If the start and end dates are October 1st and October 30th, then the connector will output 5 records: 01 - 06, 07 - 13, 14 - 20, 21 - 27, and 28 - 30 (3 days only).\tinteger\t1 /end_date\tEnd Date\tThe date until which you'd like to capture data, in the format YYYY-MM-DDT00:00:00Z. All data generated between start_date and this date will be replicated. Not setting this option will result in always syncing the latest data.\tstring /fetch_thumbnail_images\tFetch Thumbnail Images\tIn each Ad Creative, fetch the thumbnail_url and store the result in thumbnail_data_url\tboolean\tfalse /include_deleted\tInclude Deleted\tInclude data from deleted Campaigns, Ads, and AdSets\tboolean\tfalse /insights_lookback_window\tInsights Lookback Window\tThe attribution window\tinteger\t28 /max_batch_size\tMaximum size of Batched Requests\tMaximum batch size used when sending batch requests to Facebook API. Most users do not need to set this field unless they specifically need to tune the connector to address specific issues or use cases.\tinteger\t50 /page_size\tPage Size of Requests\tPage size used when sending requests to Facebook API to specify number of records per page when response has pagination. Most users do not need to set this field unless they specifically need to tune the connector to address specific issues or use cases.\tinteger\t25 /start_date\tStart Date\tThe date from which you'd like to begin capturing data, in the format YYYY-MM-DDT00:00:00Z. All data generated after this date will be replicated.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource of your Facebook Marketing account from which collections are captured.\tstring\tRequired /syncMode\tSync mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Facebook Marketing","url":"reference/Connectors/capture-connectors/facebook-marketing/#sample","content":"This sample specification reflects the manual authentication method. captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-facebook-marketing:dev config: access_token: &lt;secret&gt; account_id: 000000000000000 start_date: 2022-03-01T00:00:00Z custom_insights: - name: my-custom-insight fields: [ad_id, account_currency] breakdowns: [device_platform] action_breakdowns: [action_type] start_date: 2022-03-01T00:00:00Z bindings: - resource: stream: ad_account syncMode: incremental target: ${PREFIX}/ad_account - resource: stream: ad_sets syncMode: incremental target: ${PREFIX}/ad_sets - resource: stream: ads_insights syncMode: incremental target: ${PREFIX}/ads_insights - resource: stream: ads_insights_age_and_gender syncMode: incremental target: ${PREFIX}/ads_insights_age_and_gender - resource: stream: ads_insights_country syncMode: incremental target: ${PREFIX}/ads_insights_country - resource: stream: ads_insights_region syncMode: incremental target: ${PREFIX}/ads_insights_region - resource: stream: ads_insights_dma syncMode: incremental target: ${PREFIX}/ads_insights_dma - resource: stream: ads_insights_platform_and_device syncMode: incremental target: ${PREFIX}/ads_insights_platform_and_device - resource: stream: ads_insights_action_type syncMode: incremental target: ${PREFIX}/ads_insights_action_type - resource: stream: campaigns syncMode: incremental target: ${PREFIX}/campaigns - resource: stream: activities syncMode: incremental target: ${PREFIX}/activities - resource: stream: ads syncMode: incremental target: ${PREFIX}/ads - resource: stream: ad_creatives syncMode: full_refresh target: ${PREFIX}/ad_creatives  Learn more about capture definitions. "},{"title":"GitHub","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/github/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"GitHub","url":"reference/Connectors/capture-connectors/github/#supported-data-resources","content":"When you configure the connector, you specify a list of GitHub organizations and/or repositories from which to capture data. From your selection, the following data resources are captured: Full refresh (batch) resources\tIncremental (real-time supported) resourcesAssignees\tComments Branches\tCommit comment reactions Collaborators\tCommit comments Issue labels\tCommits Pull request commits\tDeployments Tags\tEvents Team members\tIssue comment reactions Team memberships\tIssue events Teams\tIssue milestones Users\tIssue reactions Issues Project cards Project columns Projects Pull request comment reactions Pull request stats Pull requests Releases Repositories Review comments Reviews Stargazers Workflow runs Workflows Each resource is mapped to a Flow collection through a separate binding. info The /start_date field is not applicable to the following resources: AssigneesBranchesCollaboratorsIssue labelsOrganizationsPull request commitsPull request statsRepositoriesTagsTeamsUsers "},{"title":"Prerequisites​","type":1,"pageTitle":"GitHub","url":"reference/Connectors/capture-connectors/github/#prerequisites","content":"There are two ways to authenticate with GitHub when capturing data into Flow: using OAuth2, and manually, by generating a personal access token. Their prerequisites differ. OAuth is recommended for simplicity in the Flow web app; the access token method is the only supported method using the command line. "},{"title":"Using OAuth2 to authenticate with GitHub in the Flow web app​","type":1,"pageTitle":"GitHub","url":"reference/Connectors/capture-connectors/github/#using-oauth2-to-authenticate-with-github-in-the-flow-web-app","content":"A GitHub user account with access to the repositories of interest, and which is a member of organizations of interest. "},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"GitHub","url":"reference/Connectors/capture-connectors/github/#configuring-the-connector-specification-manually","content":"A GitHub user account with access to the repositories of interest, and which is a member of organizations of interest. A GitHub personal access token. You may use multiple tokens to balance the load on your API quota. "},{"title":"Configuration​","type":1,"pageTitle":"GitHub","url":"reference/Connectors/capture-connectors/github/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the GitHub source connector. "},{"title":"Properties​","type":1,"pageTitle":"GitHub","url":"reference/Connectors/capture-connectors/github/#properties","content":"Endpoint​ The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so some of these properties aren't required. Property\tTitle\tDescription\tType\tRequired/Default/branch\tBranch (Optional)\tSpace-delimited list of GitHub repository branches to pull commits for, e.g. `estuary/flow/your-branch`. If no branches are specified for a repository, the default branch will be pulled.\tstring /credentials\tAuthentication\tChoose how to authenticate to GitHub\tobject\tRequired /credentials/option_title\tAuthentication method\tSet to PAT Credentials for manual authentication\tstring /credentials/personal_access_token\tAccess token\tPersonal access token, used for manual authentication. You may include multiple access tokens as a comma separated list. /page_size_for_large_streams\tPage size for large streams (Optional)\tThe Github connector captures from several resources with a large amount of data. The page size of such resources depends on the size of your repository. We recommended that you specify values between 10 and 30.\tinteger\t10 /repository\tGitHub Repositories\tSpace-delimited list of GitHub organizations/repositories, e.g. `estuary/flow` for a single repository, `estuary/*` to get all repositories from an organization and `estuary/flow estuary/another-repo` for multiple repositories.\tstring\tRequired /start_date\tStart date\tThe date from which you'd like to replicate data from GitHub in the format YYYY-MM-DDT00:00:00Z. For the resources that support this configuration, only data generated on or after the start date will be replicated. This field doesn't apply to all resources.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tGitHub resource from which collection is captured.\tstring\tRequired /syncMode\tSync mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"GitHub","url":"reference/Connectors/capture-connectors/github/#sample","content":"This sample specification reflects the manual authentication method. captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-github:dev config: credentials: option_title: PAT Credentials personal_access_token: {secret} page_size_for_large_streams: 10 repository: estuary/flow start_date: 2022-01-01T00:00:00Z bindings: - resource: stream: assignees syncMode: full_refresh target: ${PREFIX}/assignees {...}  "},{"title":"Google Cloud Storage","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/gcs/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Google Cloud Storage","url":"reference/Connectors/capture-connectors/gcs/#prerequisites","content":"To use this connector, either your GCS bucket must be public, or you must have access via a Google service account. For public buckets, verify that objects in the bucket are publicly readable.For buckets accessed by a Google Service Account: Ensure that the user has been assigned a role with read access.Create a JSON service account key. Google's Application Default Credentials will use this file for authentication. "},{"title":"Configuration​","type":1,"pageTitle":"Google Cloud Storage","url":"reference/Connectors/capture-connectors/gcs/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the GCS source connector. tip You might use prefixes to organize your GCS bucket in a way that emulates a directory structure. This connector can use prefixes in two ways: first, to perform the discovery phase of setup, and later, when the capture is running. You can specify a prefix in the endpoint configuration to limit the overall scope of data discovery.You're required to specify prefixes on a per-binding basis. This allows you to map each prefix to a distinct Flow collection, and informs how the capture will behave in production. To capture the entire bucket, omit prefix in the endpoint configuration and set stream to the name of the bucket. "},{"title":"Properties​","type":1,"pageTitle":"Google Cloud Storage","url":"reference/Connectors/capture-connectors/gcs/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/advanced Options for advanced users. You should not typically need to modify these.\tobject /advanced/ascendingKeys\tAscending Keys\tImprove sync speeds by listing files from the end of the last sync, rather than listing the entire bucket prefix. This requires that you write objects in ascending lexicographic order, such as an RFC-3339 timestamp, so that key ordering matches modification time ordering. If data is not ordered correctly, using ascending keys could cause errors.\tboolean\tfalse /bucket\tBucket\tName of the Google Cloud Storage bucket\tstring\tRequired /googleCredentials\tGoogle Service Account\tService account JSON key to use as Application Default Credentials\tstring /matchKeys\tMatch Keys\tFilter applied to all object keys under the prefix. If provided, only objects whose key (relative to the prefix) matches this regex will be read. For example, you can use &quot;.*\\.json&quot; to only capture json files.\tstring /parser\tParser Configuration\tConfigures how files are parsed\tobject /parser/compression\tCompression\tDetermines how to decompress the contents. The default, 'Auto', will try to determine the compression automatically.\tnull, string\tnull /parser/format\tFormat\tDetermines how to parse the contents. The default, 'Auto', will try to determine the format automatically based on the file extension or MIME type, if available.\tobject\t{&quot;type&quot;:&quot;auto&quot;} /parser/format/type\tType string /prefix\tPrefix\tPrefix within the bucket to capture from\tstring\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tPrefix\tPath to dataset in the bucket, formatted as bucket-name/prefix-name\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Google Cloud Storage","url":"reference/Connectors/capture-connectors/gcs/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-gcs:dev config: bucket: my-bucket googleCredentials: &quot;type&quot;: &quot;service_account&quot;, &quot;project_id&quot;: &quot;project-id&quot;, &quot;private_key_id&quot;: &quot;key-id&quot;, &quot;private_key&quot;: &quot;-----BEGIN PRIVATE KEY-----\\nprivate-key\\n-----END PRIVATE KEY-----\\n&quot;, &quot;client_email&quot;: &quot;service-account-email&quot;, &quot;client_id&quot;: &quot;client-id&quot;, &quot;auth_uri&quot;: &quot;https://accounts.google.com/o/oauth2/auth&quot;, &quot;token_uri&quot;: &quot;https://accounts.google.com/o/oauth2/token&quot;, &quot;auth_provider_x509_cert_url&quot;: &quot;https://www.googleapis.com/oauth2/v1/certs&quot;, &quot;client_x509_cert_url&quot;: &quot;https://www.googleapis.com/robot/v1/metadata/x509/service-account-email&quot; parser: compression: zip format: type: csv config: delimiter: &quot;,&quot; encoding: UTF-8 errorThreshold: 5 headers: [ID, username, first_name, last_name] lineEnding: &quot;\\\\r&quot; quote: &quot;\\&quot;&quot; bindings: - resource: stream: my-bucket/${PREFIX} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your capture definition may be more complex, with additional bindings for different GCS prefixes within the same bucket. Learn more about capture definitions. "},{"title":"Advanced: Parsing cloud storage data​","type":1,"pageTitle":"Google Cloud Storage","url":"reference/Connectors/capture-connectors/gcs/#advanced-parsing-cloud-storage-data","content":"Cloud storage platforms like GCS can support a wider variety of file types than other data source systems. For each of these file types, Flow must parse and translate data into collections with defined fields and JSON schemas. By default, the parser will automatically detect the type and shape of the data in your bucket, so you won't need to change the parser configuration for most captures. However, the automatic detection may be incorrect in some cases. To fix or prevent this, you can provide explicit information in the parser configuration, which is part of the endpoint configuration for this connector. The parser configuration includes: Compression: Specify how the bucket contents are compressed. If no compression type is specified, the connector will try to determine the compression type automatically. Options are: zipgzipzstdnone Format: Specify the data format, which determines how it will be parsed. Options are: Auto: If no format is specified, the connector will try to determine it automatically. Avro CSV JSON Protobuf W3C Extended Log info At this time, Flow only supports GCS captures with data of a single file type. Support for multiple file types, which can be configured on a per-binding basis, will be added in the future. For now, use a prefix in the endpoint configuration to limit the scope of each capture to data of a single file type. CSV configuration​ CSV files include several additional properties that are important to the parser. In most cases, Flow is able to automatically determine the correct values, but you may need to specify for unusual datasets. These properties are: Delimiter. Options are: Comma (&quot;,&quot;)Pipe (&quot;|&quot;)Space (&quot;0x20&quot;)Semicolon (&quot;;&quot;)Tab (&quot;0x09&quot;)Vertical tab (&quot;0x0B&quot;)Unit separator (&quot;0x1F&quot;)SOH (&quot;0x01&quot;)Auto Encoding type, specified by its WHATWG label. Optionally, an Error threshold, as an acceptable percentage of errors. If set to a number greater than zero, malformed rows that fall within the threshold will be excluded from the capture. Escape characters. Options are: Backslash (&quot;\\\\&quot;)Disable escapes (&quot;&quot;)Auto Optionally, a list of column Headers, if not already included in the first row of the CSV file. If any headers are provided, it is assumed that the provided list of headers is complete and authoritative. The first row of your CSV file will be assumed to be data (not headers), and you must provide a header value for every column in the file. Line ending values CRLF (&quot;\\\\r\\\\n&quot;) (Windows)CR (&quot;\\\\r&quot;)LF (&quot;\\\\n&quot;)Record Separator (&quot;0x1E&quot;)Auto Quote character Double Quote (&quot;\\&quot;&quot;)Single Quote (&quot;)Disable Quoting (&quot;&quot;)Auto The sample specification above includes these fields. "},{"title":"Advanced: Configure Google service account impersonation​","type":1,"pageTitle":"Google Cloud Storage","url":"reference/Connectors/capture-connectors/gcs/#advanced-configure-google-service-account-impersonation","content":"As part of your Google IAM management, you may have configured one service account to impersonate another service account. You may find this useful when you want to easily control access to multiple service accounts with only one set of keys. If necessary, you can configure this authorization model for a GCS capture in Flow using the GitOps workflow. To do so, you'll enable sops encryption and impersonate the target account with JSON credentials. Before you begin, make sure you're familiar with how to encrypt credentials in Flow using sops. Use the following sample as a guide to add the credentials JSON to the capture's endpoint configuration. The sample uses the encrypted suffix feature of sops to encrypt only the sensitive credentials, but you may choose to encrypt the entire configuration. config: bucket: &lt;bucket-name&gt; googleCredentials_sops: # URL containing the account to impersonate and the associated project service_account_impersonation_url: https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/&lt;target-account&gt;@&lt;project&gt;.iam.gserviceaccount.com:generateAccessToken # Credentials for the account that has been configured to impersonate the target. source_credentials: # In addition to the listed fields, copy and paste the rest of your JSON key file as your normally would # for the `googleCredentials` field client_email: &lt;origin-account&gt;@&lt;anotherproject&gt;.iam.gserviceaccount.com token_uri: https://oauth2.googleapis.com/token type: service_account type: impersonated_service_account  "},{"title":"Google Analytics","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/google-analytics/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#supported-data-resources","content":"The following data resources are captured to Flow collections by default: Website overviewTraffic sourcesPagesLocationsMonthly active usersFour weekly active usersTwo weekly active usersWeekly active usersDaily active usersDevices Each resource is mapped to a Flow collection through a separate binding. You can also configure custom reports. "},{"title":"Prerequisites​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#prerequisites","content":"There are two ways to authenticate with Google when capturing data from a Google Analytics view: using OAuth2, and manually, by generating a service account key. Their prerequisites differ. OAuth is recommended for simplicity in the Flow web app; the service account key method is the only supported method using the command line. "},{"title":"Using OAuth2 to authenticate with Google in the Flow web app​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#using-oauth2-to-authenticate-with-google-in-the-flow-web-app","content":"The View ID for your Google Analytics account. You can find this using Google's Account Explorer tool. Your Google account username and password. "},{"title":"Authenticating manually with a service account key​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#authenticating-manually-with-a-service-account-key","content":"The View ID for your Google Analytics account. You can find this using Google's Account Explorer tool. Google Analytics and Google Analytics Reporting APIs enabled on your Google account. A Google service account with: A JSON key generated.Access to the source Google Analytics view. Follow the steps below to meet these prerequisites: Enable the Google Analytics and Google Analytics Reporting APIs for the Google project with which your Analytics view is associated. (Unless you actively develop with Google Cloud, you'll likely just have one option). Create a service account and generate a JSON keyDuring setup, grant the account the Viewer role on your project. You'll copy the contents of the downloaded key file into the Service Account Credentials parameter when you configure the connector. Add the service account to the Google Analytics view. Grant the account Viewer permissions (formerly known as Read &amp; Analyze permissions). "},{"title":"Configuration​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Google Analytics source connector. "},{"title":"Properties​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#properties","content":"Endpoint​ The following properties reflect the Service Account Key authentication method. If you're working in the Flow web app, you'll use OAuth2, so some of these properties aren't required. Property\tTitle\tDescription\tType\tRequired/Default/credentials\tCredentials\tCredentials for the service\tobject /credentials/auth_type\tAuthentication Type\tAuthentication method. Set to Service for manual configuration, or use OAuth in the web app.\tstring\tRequired credentials/credentials_json\tService Account Credentials\tContents of the JSON key file generated during setup.\tstring\tRequired /custom_reports\tCustom Reports (Optional)\tA JSON array describing the custom reports you want to sync from GA.\tstring /start_date\tStart Date\tThe date in the format YYYY-MM-DD. Any data before this date will not be replicated.\tstring\tRequired /view_id\tView ID\tThe ID for the Google Analytics View you want to fetch data from. This can be found from the Google Analytics Account Explorer: https://ga-dev-tools.appspot.com/account-explorer/\tstring\tRequired /window_in_days\tWindow in days (Optional)\tThe amount of days each stream slice would consist of beginning from start_date. Bigger the value - faster the fetch. (Min=1, as for a Day; Max=364, as for a Year).\tinteger\t1 Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tData resource from the Google Analytics view.\tstring\tRequired /syncMode\tSync Mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Custom reports​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#custom-reports","content":"You can include data beyond the default data resources with Custom Reports. These replicate the functionality of Custom Reports in the Google Analytics Web console. To do so, fill out the Custom Reports property witha JSON array as a string with the following schema: [{&quot;name&quot;: string, &quot;dimensions&quot;: [string], &quot;metrics&quot;: [string]}]  You may specify default Google Analytics dimensions and metrics from the table below, or custom dimensions and metrics you've previously defined. Each custom report may contain up to 7 unique dimensions and 10 unique metrics. You must include the ga:date dimension for proper data flow. Supported GA dimensions\tSupported GA metricsga:browser\tga:14dayUsers ga:city\tga:1dayUsers ga:continent\tga:28dayUsers ga:country\tga:30dayUsers ga:date\tga:7dayUsers ga:deviceCategory\tga:avgSessionDuration ga:hostname\tga:avgTimeOnPage ga:medium\tga:bounceRate ga:metro\tga:entranceRate ga:operatingSystem\tga:entrances ga:pagePath\tga:exits ga:region\tga:newUsers ga:socialNetwork\tga:pageviews ga:source\tga:pageviewsPerSession ga:subContinent\tga:sessions ga:sessionsPerUser ga:uniquePageviews ga:users "},{"title":"Sample​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-google-analytics-v4:dev config: view_id: 000000000 start_date: 2022-03-01 credentials: auth_type: service credentials_json: &lt;secret&gt; window_in_days: 1 bindings: - resource: stream: daily_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: devices syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: four_weekly_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: locations syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: monthly_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: pages syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: traffic_sources syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: two_weekly_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: website_overview syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: weekly_active_users syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Learn more about capture definitions. "},{"title":"Performance considerations​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#performance-considerations","content":""},{"title":"Data sampling​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#data-sampling","content":"The Google Analytics Reporting API enforces compute thresholds for ad-hoc queries and reports. If a threshold is exceeded, the API will apply sampling to limit the number of sessions analyzed for the specified time range. These thresholds can be found here. If your account is on the Analytics 360 tier, you're less likely to run into these limitations. For Analytics Standard accounts, you can avoid sampling by keeping the window_in_days parameter set to its default value, 1. This makes it less likely that you will exceed the threshold. When sampling occurs, a warning is written to the capture log. "},{"title":"Processing latency​","type":1,"pageTitle":"Google Analytics","url":"reference/Connectors/capture-connectors/google-analytics/#processing-latency","content":"Data in Google Analytics reports may continue to update up to 48 hours after it appears. To ensure data correctness, each time it reads from Google Analytics, this connector automatically applies a lookback window of 2 days prior to its last read. This allows it to double-check and correct for any changes in reports resulting from latent data updates. This mechanism relies on the isDataGolden flag in the Google Analytics Reporting API. "},{"title":"Google Ads","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/google-ads/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Google Ads","url":"reference/Connectors/capture-connectors/google-ads/#supported-data-resources","content":"The following data resources are supported. Resources ending in _report represent legacy resources from the Google Adwords API. ad_group_adsad_group_ad_labelad_groupsad_group_labelcampaignscampaign_labelsclick_viewcustomergeographic_viewkeyword_viewuser_location_viewaccount_performance_reportad_performance_reportdisplay_keyword_performance_reportdisplay_topics_performance_reportshopping_performance_report By default, each resource is mapped to a Flow collection through a separate binding. You may also generate custom resources using GAQL queries. "},{"title":"Prerequisites​","type":1,"pageTitle":"Google Ads","url":"reference/Connectors/capture-connectors/google-ads/#prerequisites","content":"There are two ways to authenticate with Google when capturing data into Flow: using OAuth2, and manually, using tokens and secret credentials. Their prerequisites differ. OAuth is recommended for simplicity in the Flow web app; the manual method is the only supported method using the command line. "},{"title":"Using OAuth2 to authenticate with Google in the Flow web app​","type":1,"pageTitle":"Google Ads","url":"reference/Connectors/capture-connectors/google-ads/#using-oauth2-to-authenticate-with-google-in-the-flow-web-app","content":"One or more Google Ads accounts. Note each account's customer ID A Google Account that has access to the Google Ads account(s). This account may be a manager account. If so, ensure that it is linked to each Google Ads account and make note of its customer ID. "},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"Google Ads","url":"reference/Connectors/capture-connectors/google-ads/#configuring-the-connector-specification-manually","content":"One or more Google Ads accounts. Note each account's customer ID A Google Ads manager account that has been linked to each Google Ads account A Google Ads developer token. Your Google Ads manager account must be configured prior to applying for a developer token. caution Developer token applications are independently reviewed by Google and may take one or more days to be approved. Be sure to carefully review Google's requirements before submitting an application. A refresh token, which fetches a new developer tokens for you as the previous token expires. A generated Client ID and Client Secret, used for authentication. "},{"title":"Configuration​","type":1,"pageTitle":"Google Ads","url":"reference/Connectors/capture-connectors/google-ads/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the Flow specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Google Ads source connector. "},{"title":"Properties​","type":1,"pageTitle":"Google Ads","url":"reference/Connectors/capture-connectors/google-ads/#properties","content":"Endpoint​ The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so many of these properties aren't required. Property\tTitle\tDescription\tType\tRequired/Default/conversion_window_days\tConversion Window (Optional)\tA conversion window is the period of time after an ad interaction (such as an ad click or video view) during which a conversion, such as a purchase, is recorded in Google Ads. For more information, see Google's docs.\tinteger\t14 /credentials\tGoogle Credentials object\tRequired /credentials/client_id\tClient ID\tThe Client ID of your Google Ads developer application.\tstring\tRequired /credentials/client_secret\tClient Secret\tThe Client Secret of your Google Ads developer application.\tstring\tRequired /credentials/developer_token\tDeveloper Token\tDeveloper token granted by Google to use their APIs.\tstring\tRequired /credentials/refresh_token\tRefresh Token\tThe token for obtaining a new access token.\tstring\tRequired /custom_queries\tCustom GAQL Queries (Optional) array /custom_queries/-/query\tCustom Query\tA custom defined GAQL query for building the report. Should not contain segments.date expression. See Google's query builder for more information.\tstring /custom_queries/-/table_name\tDestination Table Name\tThe table name in your destination database for chosen query.\tstring /customer_id\tCustomer ID(s)\tComma separated list of (client) customer IDs. Each customer ID must be specified as a 10-digit number without dashes. More instruction on how to find this value in our docs. Metrics streams like AdGroupAdReport cannot be requested for a manager account.\tstring\tRequired /end_date\tEnd Date (Optional)\tUTC date in the format 2017-01-25. Any data after this date will not be replicated.\tstring /login_customer_id\tLogin Customer ID for Managed Accounts (Optional)\tIf your access to the customer account is through a manager account, this field is required and must be set to the customer ID of the manager account (10-digit number without dashes).\tstring /start_date\tStart Date\tUTC date in the format 2017-01-25. Any data before this date will not be replicated.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tGoogle Ad resource from which a collection is captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Google Ads","url":"reference/Connectors/capture-connectors/google-ads/#sample","content":"This sample specification reflects the manual authentication method. captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-google-ads:dev config: conversion_window_days: 7 credentials: client_id: {secret_client_ID} client_secret: {secret_secret} developer_token: {access_token} refresh_token: {refresh_token} customer_id: 0123456789, 1234567890 login_customer_id: 0987654321 end_date: 2022-01-01 start_date: 2020-01-01 custom_queries: - query: SELECT campaign.id, campaign.name, campaign.status FROM campaign ORDER BY campaign.id table_name: campaigns_custom bindings: - resource: stream: campaign syncMode: incremental target: ${PREFIX}/campaign {...}  "},{"title":"Custom queries​","type":1,"pageTitle":"Google Ads","url":"reference/Connectors/capture-connectors/google-ads/#custom-queries","content":"You can create custom resources using Google Analytics Query Language (GAQL) queries. Each generated resource will be mapped to a Flow collection. For help generating a valid query, see Google's query builder documentation. If a query fails to validate against a given Google Ads account, it will be skipped. "},{"title":"Google Firestore","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/google-firestore/","content":"","keywords":""},{"title":"Data model​","type":1,"pageTitle":"Google Firestore","url":"reference/Connectors/capture-connectors/google-firestore/#data-model","content":"Firestore is a NoSQL database. Its data model consists of documents (lightweight records that contain mappings of fields and values) organized in collections. Collections are organized hierarchically. A given document in a collection can, in turn, be associated with a subcollection. For example, you might have a collection called users, which contains two documents, alice and bob. Each document has a subcollection called messages (for example, users/alice/messages), which contain more documents (for example, users/alice/messages/1). users ├── alice │ └── messages │ ├── 1 │ └── 2 └── bob └── messages └── 1  The connector works by identifying documents associated with a particular sequence of Firestore collection names, regardless of documents that split the hierarchy. These document groupings are mapped to Flow collections using a path in the pattern collection/*/subcollection. In this example, we'd end up with users and users/*/messages Flow collections, with the latter contain messages from both users. The /_meta/path property for each document contains its full, original path, so we'd still know which messages were Alice's and which were Bob's. "},{"title":"Prerequisites​","type":1,"pageTitle":"Google Firestore","url":"reference/Connectors/capture-connectors/google-firestore/#prerequisites","content":"You'll need: A Google service account with: Read access to your Firestore database, via roles/datastore.viewer. You can assign this role when you create the service account, or add it to an existing service account. A generated JSON service account key for the account. "},{"title":"Configuration​","type":1,"pageTitle":"Google Firestore","url":"reference/Connectors/capture-connectors/google-firestore/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the Flow specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Firestore source connector. "},{"title":"Properties​","type":1,"pageTitle":"Google Firestore","url":"reference/Connectors/capture-connectors/google-firestore/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/googleCredentials\tCredentials\tGoogle Cloud Service Account JSON credentials.\tstring\tRequired /database\tDatabase\tOptional name of the database to capture from. Leave blank to autodetect. Typically &quot;projects/$PROJECTID/databases/(default)&quot;.\tstring\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/backfillMode\tBackfill Mode\tConfigures the handling of data already in the collection. See below for details or just stick with 'async'\tstring\tRequired /path\tPath to Collection\tSupports parent/*/nested to capture all nested collections of parent's children\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Google Firestore","url":"reference/Connectors/capture-connectors/google-firestore/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-firestore:dev config: googleCredentials: &quot;type&quot;: &quot;service_account&quot;, &quot;project_id&quot;: &quot;project-id&quot;, &quot;private_key_id&quot;: &quot;key-id&quot;, &quot;private_key&quot;: &quot;-----BEGIN PRIVATE KEY-----\\nprivate-key\\n-----END PRIVATE KEY-----\\n&quot;, &quot;client_email&quot;: &quot;service-account-email&quot;, &quot;client_id&quot;: &quot;client-id&quot;, &quot;auth_uri&quot;: &quot;https://accounts.google.com/o/oauth2/auth&quot;, &quot;token_uri&quot;: &quot;https://accounts.google.com/o/oauth2/token&quot;, &quot;auth_provider_x509_cert_url&quot;: &quot;https://www.googleapis.com/oauth2/v1/certs&quot;, &quot;client_x509_cert_url&quot;: &quot;https://www.googleapis.com/robot/v1/metadata/x509/service-account-email&quot; bindings: - resource: #The below `path` will capture all Firestore documents that match the pattern #`orgs/&lt;orgID&gt;/runs/&lt;runID&gt;/runResults/&lt;runResultID&gt;/queryResults`. #See the Data Model section above for details. path: orgs/*/runs/*/runResults/*/queryResults backfillMode: async target: ${PREFIX}/orgs_runs_runResults_queryResults - resource: path: orgs/*/runs/*/runResults backfillMode: async target: ${PREFIX}/orgs_runs_runResults - resource: path: orgs/*/runs backfillMode: async target: ${PREFIX}/orgs_runs - resource: path: orgs backfillMode: async target: ${PREFIX}/orgs  "},{"title":"Backfill mode​","type":1,"pageTitle":"Google Firestore","url":"reference/Connectors/capture-connectors/google-firestore/#backfill-mode","content":"In each captured collection's binding configuration, you can choose whether and how to backfill historical data. There are three options: none: Skip preexisting data in the Firestore collection. Capture only new documents and changes to existing documents that occur after the capture is published. async: Use two threads to capture data. The first captures new documents, as with none. The second progressively ingests historical data in chunks. This mode is most reliable for Firestore collections of all sizes but provides slightly weaker guarantees against data duplication. The connector uses a reduction to reconcile changes to the same document found on the parallel threads. The version with the most recent timestamp the document metadata will be preserved ({&quot;strategy&quot;: &quot;maximize&quot;, &quot;key&quot;: &quot;/_meta/mtime&quot;}). For most collections, this produces an accurate copy of your Firestore collections in Flow. sync: Request that Firestore stream all changes to the collection since its creation, in order. This mode provides the strongest guarantee against duplicated data, but can cause errors for large datasets. Firestore may terminate the process if the backfill of historical data has not completed within about ten minutes, forcing the capture to restart from the beginning. If this happens once it is likely to recur continuously. If left unattended for an extended time this can result in a massive number of read operations and a correspondingly large bill from Firestore. This mode should only be used when somebody can keep an eye on the backfill and shut it down if it has not completed within half an hour at most, and on relatively small collections. 100,000 documents or fewer should generally be safe, although this can vary depending on the average document size in the collection. If you're unsure which backfill mode to use, choose async. "},{"title":"Google Search Console","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/google-search-console/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Google Search Console","url":"reference/Connectors/capture-connectors/google-search-console/#supported-data-resources","content":"The following data resources are supported: Search analytics: all fields This resource contains all data in for your search analytics, and can be large. The following five collections come from queries applied to this dataset. Search analytics by countrySearch analytics by dateSearch analytics by deviceSearch analytics by pageSearch analytics by querySitemapsSites By default, each resource is mapped to a Flow collection through a separate binding. "},{"title":"Custom reports​","type":1,"pageTitle":"Google Search Console","url":"reference/Connectors/capture-connectors/google-search-console/#custom-reports","content":"In addition to the resources listed above, you can add custom reports created with the Google Analytics Search Console integration. You add these to the endpoint configuration in the format {&quot;name&quot;: &quot;&lt;report-name&gt;&quot;, &quot;dimensions&quot;: [&quot;&lt;dimension-name&gt;&quot;, ...]}. Each report is mapped to an additional Flow collection. caution Custom reports involve an integration with Google Universal Analytics, which Google will deprecate in July 2023. "},{"title":"Prerequisites​","type":1,"pageTitle":"Google Search Console","url":"reference/Connectors/capture-connectors/google-search-console/#prerequisites","content":"There are two ways to authenticate with Google when capturing data from Google Search Console: using OAuth2, and manually, by generating a service account key. Their prerequisites differ. OAuth2 is recommended for simplicity in the Flow web app; the service account key method is the only supported method using the command line. "},{"title":"Using OAuth2 to authenticate with Google in the Flow web app​","type":1,"pageTitle":"Google Search Console","url":"reference/Connectors/capture-connectors/google-search-console/#using-oauth2-to-authenticate-with-google-in-the-flow-web-app","content":"You'll need: Google credentials with Owner access on the Google Search Console property. This can be a user account or a service account. You'll use these credentials to log in to Google in the Flow web app. "},{"title":"Authenticating manually with a service account key​","type":1,"pageTitle":"Google Search Console","url":"reference/Connectors/capture-connectors/google-search-console/#authenticating-manually-with-a-service-account-key","content":"You'll need: A Google service account with: A JSON key generated.Access to the Google Search Console view through the API. Follow the steps below to meet these prerequisites: Create a service account and generate a JSON keyYou'll copy the contents of the downloaded key file into the Service Account Credentials parameter when you configure the connector. Set up domain-wide delegation for the service account. During this process, grant the https://www.googleapis.com/auth/webmasters.readonly OAuth scope. "},{"title":"Configuration​","type":1,"pageTitle":"Google Search Console","url":"reference/Connectors/capture-connectors/google-search-console/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Google Search Console source connector. "},{"title":"Properties​","type":1,"pageTitle":"Google Search Console","url":"reference/Connectors/capture-connectors/google-search-console/#properties","content":"Endpoint​ The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so many of these properties aren't required. Property\tTitle\tDescription\tType\tRequired/Default/credentials\tAuthentication object\tRequired /credentials/auth_type\tAuthentication Type\tSet to Service for manual authentication\tstring\tRequired /credentials/service_account_info\tService Account JSON Key\tThe JSON key of the service account to use for authorization.\tRequired /credentials/email\tAdmin Email\tThe email of your Google Workspace administrator. This is likely the account used during setup. /custom_reports\tCustom Reports (Optional)\tA JSON array describing the custom reports you want to sync from Google Search Console.\tstring /end_date\tEnd Date\tUTC date in the format 2017-01-25. Any data after this date will not be replicated. Must be greater or equal to the start date field.\tstring /site_urls\tWebsite URL\tThe URLs of the website properties attached to your GSC account.\tarray\tRequired /start_date\tStart Date\tUTC date in the format 2017-01-25. Any data before this date will not be replicated.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tGoogle Search Consol resource from which a collection is captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Google Search Console","url":"reference/Connectors/capture-connectors/google-search-console/#sample","content":"This sample specification reflects the manual authentication method. captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-google-search-console:dev config: credentials: auth_type: Service service_account_info: &lt;secret&gt; email: admin@yourdomain.com site_urls: https://yourdomain.com start_date: 2022-03-01 bindings: - resource: stream: sites syncMode: full_refresh target: ${PREFIX}/sites {}  "},{"title":"Google Sheets","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/google-sheets/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/capture-connectors/google-sheets/#prerequisites","content":"There are two ways to authenticate with Google when capturing data from a Sheet: using OAuth2, and manually,by generating a service account key. Their prerequisites differ. OAuth is recommended for simplicity in the Flow web app; the service account key method is the only supported method using the command line. "},{"title":"Using OAuth2 to authenticate with Google in the Flow web app​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/capture-connectors/google-sheets/#using-oauth2-to-authenticate-with-google-in-the-flow-web-app","content":"A link to a Google spreadsheet. Simply copy the link from your browser. Your Google account username and password. "},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/capture-connectors/google-sheets/#configuring-the-connector-specification-manually","content":"A link to a Google spreadsheet. Simply copy the link from your browser. Google Sheets and Google Drive APIs enabled on your Google account. A Google service account with: A JSON key generated.Access to the source spreadsheet. Follow the steps below to meet these prerequisites: Enable the Google Sheets and Google Drive APIs for the Google project with which your spreadsheet is associated. (Unless you actively develop with Google Cloud, you'll likely just have one option). Create a service account and generate a JSON key. During setup, grant the account the Viewer role on your project. You'll copy the contents of the downloaded key file into the Service Account Credentials parameter when you configure the connector. Share your Google spreadsheet with the service account. You may either share the sheet so that anyone with the link can view it, or share explicitly with the service account's email address. "},{"title":"Configuration​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/capture-connectors/google-sheets/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Google Sheets source connector. "},{"title":"Properties​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/capture-connectors/google-sheets/#properties","content":"Endpoint​ The following properties reflect the Service Account Key authentication method. Property\tTitle\tDescription\tType\tRequired/Default/credentials\tCredentials\tGoogle API Credentials for connecting to Google Sheets and Google Drive APIs\tobject\tRequired /credentials/auth_type\tAuthentication Type\tAuthentication method. Set to Service.\tstring\tRequired credentials/service_account_info\tService Account Credentials\tContents of the JSON key file generated during setup.\tstring\tRequired /spreadsheet_id\tSpreadsheet Link\tThe link to your spreadsheet.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tSheet\tEach sheet in your Google Sheets document.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to full_refresh.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/capture-connectors/google-sheets/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-google-sheets:dev config: credentials: auth_type: Service service_account_info: &lt;secret&gt; spreadsheet_id: https://docs.google.com/spreadsheets/... bindings: - resource: stream: Sheet1 syncMode: full_refresh target: ${PREFIX}/${COLLECTION_NAME}  Learn more about capture definitions. "},{"title":"Hubspot","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/hubspot/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Hubspot","url":"reference/Connectors/capture-connectors/hubspot/#supported-data-resources","content":"By default, each resource associated with your Hubspot account is mapped to a Flow collection through a separate binding. The following data resources are supported for all subscription levels: CampaignsCompaniesContact ListsContactsContacts List MembershipsDeal PipelinesDealsEmail EventsEngagementsEngagements CallsEngagements EmailsEngagements MeetingsEngagements NotesEngagements TasksFormsForm SubmissionsLine ItemsOwnersProductsProperty HistoryQuotesSubscription ChangesTicketsTicket Pipelines The following data resources are supported for pro accounts (set Subscription type to pro in the configuration): Feedback SubmissionsMarketing EmailsWorkflows "},{"title":"Prerequisites​","type":1,"pageTitle":"Hubspot","url":"reference/Connectors/capture-connectors/hubspot/#prerequisites","content":"There are two ways to authenticate with Hubspot when capturing data: using OAuth2, and manually, with a private app access token. Their prerequisites differ. OAuth is recommended for simplicity in the Flow web app; the access token method is the only supported method using the command line. "},{"title":"Using OAuth2 to authenticate with Hubspot in the Flow web app​","type":1,"pageTitle":"Hubspot","url":"reference/Connectors/capture-connectors/hubspot/#using-oauth2-to-authenticate-with-hubspot-in-the-flow-web-app","content":"A Hubspot account "},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"Hubspot","url":"reference/Connectors/capture-connectors/hubspot/#configuring-the-connector-specification-manually","content":"A Hubspot account The access token for an appropriately configured private app on the Hubspot account. Setup​ To create a private app in Hubspot and generate its access token, do the following. Ensure that your Hubspot user account has super admin privileges. In Hubspot, create a new private app. Name the app &quot;Estuary Flow,&quot; or choose another name that is memorable to you. Grant the new app Read access for all available scopes. Copy the access token for use in the connector configuration. "},{"title":"Configuration​","type":1,"pageTitle":"Hubspot","url":"reference/Connectors/capture-connectors/hubspot/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Hubspot source connector. "},{"title":"Properties​","type":1,"pageTitle":"Hubspot","url":"reference/Connectors/capture-connectors/hubspot/#properties","content":"Endpoint​ The following properties reflect the access token authentication method. Property\tTitle\tDescription\tType\tRequired/Default/credentials\tPrivate Application\tAuthenticate with a private app access token\tobject\tRequired /credentials/access_token\tAccess Token\tHubSpot Access token.\tstring\tRequired /credentials/credentials_title\tCredentials\tName of the credentials set\tstring\tRequired, &quot;Private App Credentials&quot; /start_date\tStart Date\tUTC date and time in the format 2017-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired /subscription_type\tYour HubSpot account subscription type\tSome streams are only available to certain subscription packages, we use this information to select which streams to pull data from.\tstring\t&quot;starter&quot; Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tData resource\tName of the data resource.\tstring\tRequired /syncMode\tSync Mode\tConnection method\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Hubspot","url":"reference/Connectors/capture-connectors/hubspot/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-hubspot:dev config: credentials: credentials_title: Private App Credentials access_token: &lt;secret&gt; bindings: - resource: stream: companies syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your configuration will have many more bindings representing all supported resourcesin your Hubspot account. Learn more about capture definitions. "},{"title":"Intercom","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/intercom/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Intercom","url":"reference/Connectors/capture-connectors/intercom/#supported-data-resources","content":"The following data resources are supported through the Intercom API: AdminsCompaniesCompany attributesCompany segmentsContactsContact attributesConversationsConversation partsSegmentsTagsTeams By default, each resource is mapped to a Flow collection through a separate binding. "},{"title":"Prerequisites​","type":1,"pageTitle":"Intercom","url":"reference/Connectors/capture-connectors/intercom/#prerequisites","content":"There are two ways to authenticate with Intercom: In the Flow web app, you sign in directly. You'll need the username and password associated with a user with full permissions on your Intercom workspace. Using the flowctl CLI, you configure authentication manually. You'll need the access token for you Intercom account. "},{"title":"Configuration​","type":1,"pageTitle":"Intercom","url":"reference/Connectors/capture-connectors/intercom/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Intercom source connector. "},{"title":"Properties​","type":1,"pageTitle":"Intercom","url":"reference/Connectors/capture-connectors/intercom/#properties","content":"Endpoint​ The properties in the table below reflect manual authentication using the CLI. In the Flow web app, you'll sign in directly and won't need the access token. Property\tTitle\tDescription\tType\tRequired/Default/access_token\tAccess token\tAccess token for making authenticated requests.\tstring\tRequired /start_date\tStart date\tUTC date and time in the format 2017-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource from Intercom from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Intercom","url":"reference/Connectors/capture-connectors/intercom/#sample","content":"The sample below reflects manual authentication in the CLI. captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-intercom:dev config: access_token: &lt;secret&gt; start_date: 2022-06-18T00:00:00Z bindings: - resource: stream: admins syncMode: full_refresh target: ${PREFIX}/admins - resource: stream: companies syncMode: incremental target: ${PREFIX}/companies - resource: stream: company_segments syncMode: incremental target: ${PREFIX}/companysegments - resource: stream: conversations syncMode: incremental target: ${PREFIX}/conversations - resource: stream: conversation_parts syncMode: incremental target: ${PREFIX}/conversationparts - resource: stream: contacts syncMode: incremental target: ${PREFIX}/contacts - resource: stream: company_attributes syncMode: full_refresh target: ${PREFIX}/companyattributes - resource: stream: contact_attributes syncMode: full_refresh target: ${PREFIX}/contactattributes - resource: stream: segments syncMode: incremental target: ${PREFIX}/segments - resource: stream: tags syncMode: full_refresh target: ${PREFIX}/tags - resource: stream: teams syncMode: full_refresh target: ${PREFIX}/teams  "},{"title":"HTTP file","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/http-file/","content":"","keywords":""},{"title":"Supported data types​","type":1,"pageTitle":"HTTP file","url":"reference/Connectors/capture-connectors/http-file/#supported-data-types","content":"This connector automatically captures the data hosted at the specified URL into a single Flow collection. The following file types are supported: AvroCSVJSONProtobufW3C Extended Log The following compression methods are supported: ZIPGZIPZSTD By default, Flow automatically detects the file type and compression method. If necessary, you can specify the correct file type, compression, and other properties (CSV only) using the optional parser configuration. "},{"title":"Prerequisites​","type":1,"pageTitle":"HTTP file","url":"reference/Connectors/capture-connectors/http-file/#prerequisites","content":"To use this connector, you'll need the URL to an HTTP endpoint that hosts data of one of the supported types. The HTTP endpoint must support HEAD HTTP requests, and the response to this request must include a Last-Modified header. tip You can send a test HEAD request using Curl with the -I parameter, for example:curl -I https://my-site.com/my_hosted_dataset.json.zipUse this online tool to easily do so in your browser. Some HTTP endpoints require credentials for access. If this is the case, have your username and password ready. "},{"title":"Configuration​","type":1,"pageTitle":"HTTP file","url":"reference/Connectors/capture-connectors/http-file/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the HTTP file source connector. "},{"title":"Properties​","type":1,"pageTitle":"HTTP file","url":"reference/Connectors/capture-connectors/http-file/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/credentials\tCredentials\tUser credentials, if required to access the data at the HTTP URL.\tobject /credentials/password\tPassword\tPassword, if required to access the HTTP endpoint.\tstring /credentials/user\tUser\tUsername, if required to access the HTTP endpoint.\tstring /headers\tHeaders object /headers/items\tAdditional HTTP Headers\tAdditional HTTP headers when requesting the file. These are uncommon.\tarray /headers/items/-/key\tHeader Key string /headers/items/-/value\tHeader Value string /parser\tParser Configuration\tConfigures how files are parsed\tobject /parser/compression\tCompression\tDetermines how to decompress the contents. The default, 'Auto', will try to determine the compression automatically.\tnull, string\tnull /parser/format\tFormat\tDetermines how to parse the contents. The default, 'Auto', will try to determine the format automatically based on the file extension or MIME type, if available.\tobject\t{&quot;type&quot;:&quot;auto&quot;} /parser/format/type\tType string /url\tHTTP File URL\tA valid HTTP url for downloading the source file.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tName of the dataset\tstring\tRequired /syncMode\tSync mode\tConnection method. Set to incremental for real-time updates.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"HTTP file","url":"reference/Connectors/capture-connectors/http-file/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-http-file:dev config: url: https://my-site.com/my_hosted_dataset.json.zip parser: compression: zip format: type: csv config: delimiter: &quot;,&quot; encoding: UTF-8 errorThreshold: 5 headers: [ID, username, first_name, last_name] lineEnding: &quot;\\\\r&quot; quote: &quot;\\&quot;&quot; bindings: - resource: stream: my_hosted_dataset.json.zip syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  "},{"title":"Advanced: Parsing HTTP-hosted data​","type":1,"pageTitle":"HTTP file","url":"reference/Connectors/capture-connectors/http-file/#advanced-parsing-http-hosted-data","content":"HTTP endpoints can support a variety of file types. For each file type, Flow must parse and translate data into collections with defined fields and JSON schemas. By default, the parser will automatically detect the type and shape of the data at the HTTP endpoint, so you won't need to change the parser configuration for most captures. However, the automatic detection may be incorrect in some cases. To fix or prevent this, you can provide explicit information in the parser configuration, which is part of the endpoint configuration for this connector. The parser configuration includes: Compression: Specify how the data is compressed. If no compression type is specified, the connector will try to determine the compression type automatically. Options are: zipgzipzstdnone Format: Specify the data format, which determines how it will be parsed. If no file type is specified, the connector will try to determine the file type automatically Options are: AvroCSVJSONProtobufW3C Extended Log CSV configuration​ CSV files include several additional properties that are important to the parser. In most cases, Flow is able to automatically determine the correct values, but you may need to specify for unusual datasets. These properties are: Delimiter. Options are: Comma (&quot;,&quot;)Pipe (&quot;|&quot;)Space (&quot;0x20&quot;)Semicolon (&quot;;&quot;)Tab (&quot;0x09&quot;)Vertical tab (&quot;0x0B&quot;)Unit separator (&quot;0x1F&quot;)SOH (&quot;0x01&quot;)Auto Encoding type, specified by its WHATWG label. Optionally, an Error threshold, as an acceptable percentage of errors. If set to a number greater than zero, malformed rows that fall within the threshold will be excluded from the capture. Escape characters. Options are: Backslash (&quot;\\\\&quot;)Disable escapes (&quot;&quot;)Auto Optionally, a list of column Headers, if not already included in the first row of the CSV file. If any headers are provided, it is assumed that the provided list of headers is complete and authoritative. The first row of your CSV file will be assumed to be data (not headers), and you must provide a header value for every column in the file. Line ending values CRLF (&quot;\\\\r\\\\n&quot;) (Windows)CR (&quot;\\\\r&quot;)LF (&quot;\\\\n&quot;)Record Separator (&quot;0x1E&quot;)Auto Quote character Double Quote (&quot;\\&quot;&quot;)Single Quote (&quot;)Disable Quoting (&quot;&quot;)Auto The sample specification above includes these fields. "},{"title":"Advanced: Using HTTP headers​","type":1,"pageTitle":"HTTP file","url":"reference/Connectors/capture-connectors/http-file/#advanced-using-http-headers","content":"For data accessed through certain APIs, you may need to send headers as part of your HTTP request. This is uncommon, and is supported by the optional Headers configuration. This configuration section is encrypted with sops, so you can safely include secretes such as API keys. See the source data's API documentation for headers that may be required for your capture. "},{"title":"LinkedIn Ads","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/linkedin-ads/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"LinkedIn Ads","url":"reference/Connectors/capture-connectors/linkedin-ads/#supported-data-resources","content":"The following data resources are supported: AccountsAccount usersCampaign groupsCampaignsCreativesAdDirectSponsoredContents (Video ads)Ad analytics by campaignAd analytics by creative By default, each resource is mapped to a Flow collection through a separate binding. "},{"title":"Prerequisites​","type":1,"pageTitle":"LinkedIn Ads","url":"reference/Connectors/capture-connectors/linkedin-ads/#prerequisites","content":"There are two ways to authenticate with LinkedIn when capturing data into Flow: using OAuth2, and manually, by creating a developer application. Their prerequisites differ. OAuth is recommended for simplicity in the Flow web app; the developer application method is the only supported method using the command line. "},{"title":"Using OAuth2 to authenticate with LinkedIn in the Flow web app​","type":1,"pageTitle":"LinkedIn Ads","url":"reference/Connectors/capture-connectors/linkedin-ads/#using-oauth2-to-authenticate-with-linkedin-in-the-flow-web-app","content":"One or more LinkedIn Ad Accounts with active campaigns. A LinkedIn user with access to the Ad Accounts from which you want to capture data. "},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"LinkedIn Ads","url":"reference/Connectors/capture-connectors/linkedin-ads/#configuring-the-connector-specification-manually","content":"To configure without using OAuth, you'll need to create an application using the LinkedIn Marketing API, and generate its access token. Setup​ Create a marketing application on LinkedIn Developers.Apply to the LinkedIn Developer Program.Generate your access token. caution LinkedIn access tokens expire in 60 days. You must manually update your capture configuration to continue to capture data from LinkedIn. "},{"title":"Configuration​","type":1,"pageTitle":"LinkedIn Ads","url":"reference/Connectors/capture-connectors/linkedin-ads/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the capture specification. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the LinkedIn Ads source connector. "},{"title":"Properties​","type":1,"pageTitle":"LinkedIn Ads","url":"reference/Connectors/capture-connectors/linkedin-ads/#properties","content":"Endpoint​ The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so some of these properties aren't required. Property\tTitle\tDescription\tType\tRequired/Default/account_ids\tAccount IDs (Optional)\tA space-separated list of the account IDs from which to capture data. Leave empty if you want to capture data from all linked accounts.\tarray\t[] /credentials\tAuthentication object /credentials/auth_method\tAuthentication method\tSet to access_token to authenticate manually.\tstring /credentials/access_token\tAccess token\tAccess token generated from your LinkedIn Developers app.\tstring /start_date\tStart date\tUTC date in the format 2020-09-17. Any data before this date will not be replicated.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tLinkedIn Ads stream from which a collection is captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"LinkedIn Ads","url":"reference/Connectors/capture-connectors/linkedin-ads/#sample","content":"This sample specification reflects the manual authentication method. captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-linkedin-ads:dev config: account_ids: - 000000000 - 111111111 credentials: auth_method: access_token access_token: {secret} start_date: 2022-01-01 bindings: - resource: stream: campaigns syncMode: incremental target: ${PREFIX}/campaign {...}  "},{"title":"Mailchimp","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/mailchimp/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Mailchimp","url":"reference/Connectors/capture-connectors/mailchimp/#prerequisites","content":"There are two ways to authenticate with MailChimp when capturing data: using OAuth2, and manually, with an API key. Their prerequisites differ. OAuth is recommended for simplicity in the Flow web app; the API key method is the only supported method using the command line. "},{"title":"Using OAuth2 to authenticate with Mailchimp in the Flow web app​","type":1,"pageTitle":"Mailchimp","url":"reference/Connectors/capture-connectors/mailchimp/#using-oauth2-to-authenticate-with-mailchimp-in-the-flow-web-app","content":"A Mailchimp account "},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"Mailchimp","url":"reference/Connectors/capture-connectors/mailchimp/#configuring-the-connector-specification-manually","content":"A Mailchimp account A Mailchimp API key "},{"title":"Configuration​","type":1,"pageTitle":"Mailchimp","url":"reference/Connectors/capture-connectors/mailchimp/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Mailchimp source connector. "},{"title":"Properties​","type":1,"pageTitle":"Mailchimp","url":"reference/Connectors/capture-connectors/mailchimp/#properties","content":"Endpoint​ The following properties reflect the API Key authentication method. Property\tTitle\tDescription\tType\tRequired/Default/credentials\tAuthentication\tAuthentication Type and Details\tobject\tRequired /credentials/auth_type\tAuthentication Type\tAuthentication type. Set to apikey.\tstring\tRequired /credentials/apikey\tAPI Key\tYour Mailchimp API key\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tResource\tMailchimp lists, campaigns, or email_activity\tstring\tRequired /syncMode\tSync Mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Mailchimp","url":"reference/Connectors/capture-connectors/mailchimp/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-mailchimp:dev config: credentials: auth_type: apikey apikey: &lt;secret&gt; bindings: - resource: stream: lists syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: campaigns syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME} - resource: stream: email_activity syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Learn more about capture definitions. "},{"title":"MariaDB","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/mariadb/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"MariaDB","url":"reference/Connectors/capture-connectors/mariadb/#prerequisites","content":"To use this connector, you'll need a MariaDB database setup with the following. binlog_formatsystem variable set to ROW.Binary log expiration period set to at least 30 days (2592000 seconds) if at all possible. This value may be set lower if necessary, but we strongly discourage going below 7 days as this may increase the likelihood of unrecoverable failures. MariaDB's default value is 0 (no expiration). A watermarks table. The watermarks table is a small &quot;scratch space&quot; to which the connector occasionally writes a small amount of data (a UUID, specifically) to ensure accuracy when backfilling preexisting table contents. The default name is &quot;flow.watermarks&quot;, but this can be overridden in config.json. A database user with appropriate permissions: REPLICATION CLIENT and REPLICATION SLAVE privileges.Permission to insert, update, and delete on the watermarks table.Permission to read the tables being captured.Permission to read from information_schema tables, if automatic discovery is used. If the table(s) to be captured include columns of type DATETIME, the time_zone system variable must be set to an IANA zone name or numerical offset. "},{"title":"Setup​","type":1,"pageTitle":"MariaDB","url":"reference/Connectors/capture-connectors/mariadb/#setup","content":"To meet these requirements, do the following: Create the watermarks table. This table can have any name and be in any database, so long as the capture's config.json file is modified accordingly. CREATE DATABASE IF NOT EXISTS flow; CREATE TABLE IF NOT EXISTS flow.watermarks (slot INTEGER PRIMARY KEY, watermark TEXT);  Create the flow_capture user with replication permission, the ability to read all tables, and the ability to read and write the watermarks table. The SELECT permission can be restricted to just the tables that need to be captured, but automatic discovery requires information_schema access as well. CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture'; GRANT INSERT, UPDATE, DELETE ON flow.watermarks TO 'flow_capture';  Configure the binary log to retain data for 30 days, if previously set lower. SET PERSIST binlog_expire_logs_seconds = 2592000;  Configure the database's time zone. See below for more information. SET PERSIST time_zone = '-05:00'  "},{"title":"Setting the MariaDB time zone​","type":1,"pageTitle":"MariaDB","url":"reference/Connectors/capture-connectors/mariadb/#setting-the-mariadb-time-zone","content":"MariaDB's time_zone server system variable is set to SYSTEM by default. Flow is not able to detect your time zone when it's set this way, so you must explicitly set the variable for your database. If you intend to capture tables including columns of the type DATETIME, and time_zone is set to SYSTEM, Flow won't be able to detect the time zone and convert the column to RFC3339 format. To avoid this, you must explicitly set the time zone for your database. You can: Specify a numerical offset from UTC. Specify a named timezone in IANA timezone format. For example, if you're located in New Jersey, USA, you could set time_zone to -05:00 or -04:00, depending on the time of year. Because this region observes daylight savings time, you'd be responsible for changing the offset. Alternatively, you could set time_zone to America/New_York, and time changes would occur automatically. If using IANA time zones, your database must include time zone tables. Learn more in the MariaDB docs. "},{"title":"Backfills and performance considerations​","type":1,"pageTitle":"MariaDB","url":"reference/Connectors/capture-connectors/mariadb/#backfills-and-performance-considerations","content":"When the a MariaDB capture is initiated, by default, the connector first backfills, or captures the targeted tables in their current state. It then transitions to capturing change events on an ongoing basis. This is desirable in most cases, as in ensures that a complete view of your tables is captured into Flow. However, you may find it appropriate to skip the backfill, especially for extremely large tables. In this case, you may turn of backfilling on a per-table basis. See properties for details. "},{"title":"Configuration​","type":1,"pageTitle":"MariaDB","url":"reference/Connectors/capture-connectors/mariadb/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the MariaDB source connector. "},{"title":"Properties​","type":1,"pageTitle":"MariaDB","url":"reference/Connectors/capture-connectors/mariadb/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/address\tServer Address\tThe host or host:port at which the database can be reached.\tstring\tRequired /user\tLogin User\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tLogin Password\tPassword for the specified database user.\tstring\tRequired /advanced/watermarks_table\tWatermarks Table Name\tThe name of the table used for watermark writes. Must be fully-qualified in '&lt;schema&gt;.&lt;table&gt;' form.\tstring\t&quot;flow.watermarks&quot; /advanced/dbname\tDatabase Name\tThe name of database to connect to. In general this shouldn't matter. The connector can discover and capture from all databases it's authorized to access.\tstring\t&quot;mysql&quot; /advanced/node_id\tNode ID\tNode ID for the capture. Each node in a replication cluster must have a unique 32-bit ID. The specific value doesn't matter so long as it is unique. If unset or zero the connector will pick a value.\tinteger /advanced/skip_backfills\tSkip Backfills\tA comma-separated list of fully-qualified table names which should not be backfilled.\tstring /advanced/backfill_chunk_size\tBackfill Chunk Size\tThe number of rows which should be fetched from the database in a single backfill query.\tinteger\t131072 /advanced/skip_binlog_retention_check\tSkip Binlog Retention Sanity Check\tBypasses the 'dangerously short binlog retention' sanity check at startup. Only do this if you understand the danger and have a specific need.\tboolean\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe database in which the table resides.\tstring\tRequired /stream\tStream\tName of the table to be captured from the database.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired info When you configure this connector in the web application, the automatic discovery process sets up a binding for most tables it finds in your database, but there are exceptions. Tables in the MariaDB system databases information_schema, mysql, and performance_schema will not be discovered. You can add bindings for such tables manually. "},{"title":"Sample​","type":1,"pageTitle":"MariaDB","url":"reference/Connectors/capture-connectors/mariadb/#sample","content":"A minimal capture definition will look like the following: captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-mariadb:dev config: address: &quot;127.0.0.1:3306&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; bindings: - resource: namespace: ${TABLE_NAMESPACE} stream: ${TABLE_NAME} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your capture definition will likely be more complex, with additional bindings for each table in the source database. Learn more about capture definitions. "},{"title":"MariaDB on managed cloud platforms​","type":1,"pageTitle":"MariaDB","url":"reference/Connectors/capture-connectors/mariadb/#mariadb-on-managed-cloud-platforms","content":"In addition to standard MariaDB, this connector supports cloud-based MariaDB instances on certain platforms. "},{"title":"Amazon RDS​","type":1,"pageTitle":"MariaDB","url":"reference/Connectors/capture-connectors/mariadb/#amazon-rds","content":"You can use this connector for MariaDB instances on Amazon RDS using the following setup instructions. Estuary recommends creating a read replicain RDS for use with Flow; however, it's not required. You're able to apply the connector directly to the primary instance if you'd like. Setup​ Allow connections to the database from the Estuary Flow IP address. Modify the database, setting Public accessibility to Yes. Edit the VPC security group associated with your database, or create a new VPC security group and associate it with the database. Refer to the steps in the Amazon documentation. Create a new inbound rule and a new outbound rule that allow all traffic from the IP address 34.121.207.128. info Alternatively, you can allow secure connections via SSH tunneling. To do so: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. Create a RDS parameter group to enable replication in MariaDB. Create a parameter group. Create a unique name and description and set the following properties: Family: mariadb10.6Type: DB Parameter group Modify the new parameter group and update the following parameters: binlog_format: ROWbinlog_row_metadata: FULLread_only: 0 If using the primary instance (not recommended), associate the parameter groupwith the database and set Backup Retention Period to 7 days. Reboot the database to allow the changes to take effect. Create a read replica with the new parameter group applied (recommended). Create a read replicaof your MariaDB database. Modify the replicaand set the following: DB parameter group: choose the parameter group you created previouslyBackup retention period: 7 daysPublic access: Publicly accessible Reboot the replica to allow the changes to take effect. Switch to your MariaDB client. Run the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table: CREATE DATABASE IF NOT EXISTS flow; CREATE TABLE IF NOT EXISTS flow.watermarks (slot INTEGER PRIMARY KEY, watermark TEXT); CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture'; GRANT INSERT, UPDATE, DELETE ON flow.watermarks TO 'flow_capture';  Run the following command to set the binary log retention to 7 days, the maximum value which RDS MariaDB permits: CALL mysql.rds_set_configuration('binlog retention hours', 168);  In the RDS console, note the instance's Endpoint and Port. You'll need these for the address property when you configure the connector. "},{"title":"Azure Database for MariaDB​","type":1,"pageTitle":"MariaDB","url":"reference/Connectors/capture-connectors/mariadb/#azure-database-for-mariadb","content":"You can use this connector for MariaDB instances on Azure Database for MariaDB using the following setup instructions. Setup​ Allow connections to the database from the Estuary Flow IP address. Create a new firewall rulethat grants access to the IP address 34.121.207.128. info Alternatively, you can allow secure connections via SSH tunneling. To do so: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. Set the binlog_expire_logs_seconds server perameterto 2592000. Using your preferred MariaDB client, create the watermarks table. tip Your username must be specified in the format username@servername. CREATE DATABASE IF NOT EXISTS flow; CREATE TABLE IF NOT EXISTS flow.watermarks (slot INTEGER PRIMARY KEY, watermark TEXT);  Create the flow_capture user with replication permission, the ability to read all tables, and the ability to read and write the watermarks table. The SELECT permission can be restricted to just the tables that need to be captured, but automatic discovery requires information_schema access as well. CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture'; GRANT INSERT, UPDATE, DELETE ON flow.watermarks TO 'flow_capture';  Note the instance's host under Server name, and the port under Connection Strings (usually 3306). Together, you'll use the host:port as the address property when you configure the connector. "},{"title":"Troubleshooting Capture Errors​","type":1,"pageTitle":"MariaDB","url":"reference/Connectors/capture-connectors/mariadb/#troubleshooting-capture-errors","content":"The source-mariadb connector is designed to halt immediately if something wrong or unexpected happens, instead of continuing on and potentially outputting incorrect data. What follows is a non-exhaustive list of some potential failure modes, and what action should be taken to fix these situations: "},{"title":"Unsupported Operations​","type":1,"pageTitle":"MariaDB","url":"reference/Connectors/capture-connectors/mariadb/#unsupported-operations","content":"If your capture is failing with an &quot;unsupported operation {ALTER,DROP,TRUNCATE,etc} TABLE&quot; error, this indicates that such an operation has taken place impacting a table which is currently being captured. In the case of DROP TABLE and other destructive operations this is not supported, and can only be resolved by removing the offending table(s) from the capture bindings list, after which you may recreate the capture if desired (causing the latest state of the table to be recaptured in its entirety). In the case of ALTER TABLE, we intend to support a limited subset of table alterations in the future; however, this error indicates that whatever alteration took place is not currently supported. Practically speaking the immediate resolution is the same as for a DROP or TRUNCATE TABLE, but if you frequently perform schema migrations it may be worth reaching out to see if we can add support for whatever table alteration you just did. "},{"title":"Data Manipulation Queries​","type":1,"pageTitle":"MariaDB","url":"reference/Connectors/capture-connectors/mariadb/#data-manipulation-queries","content":"If your capture is failing with an &quot;unsupported DML query&quot; error, this means that an INSERT, UPDATE, DELETE or other data manipulation query is present in the binlog. This should generally not happen if binlog_format = 'ROW' as described in the Prerequisites section. Resolving this error requires fixing the binlog_format system variable, and then either tearing down and recreating the entire capture so that it restarts at a later point in the binlog, or in the case of an INSERT/DELETE query it may suffice to remove the capture binding for the offending table and then re-add it. "},{"title":"Unhandled Queries​","type":1,"pageTitle":"MariaDB","url":"reference/Connectors/capture-connectors/mariadb/#unhandled-queries","content":"If your capture is failing with an &quot;unhandled query&quot; error, some SQL query is present in the binlog which the connector does not (currently) understand. In general, this error suggests that the connector should be modified to at least recognize this type of query, and most likely categorize it as either an unsupported DML Query, an unsupported Table Operation, or something that can safely be ignored. Until such a fix is made the capture cannot proceed, and you will need to tear down and recreate the entire capture so that it restarts from a later point in the binlog. "},{"title":"Metadata Errors​","type":1,"pageTitle":"MariaDB","url":"reference/Connectors/capture-connectors/mariadb/#metadata-errors","content":"If your capture is failing with a &quot;metadata error&quot; then something has gone badly wrong with the capture's tracking of table metadata, such as column names or datatypes. This should never happen, and most likely means that the binlog itself is corrupt in some way. If this occurs, it can be resolved by removing the offending table(s) from the capture bindings list and then recreating the capture (generally into a new collection, as this process will cause the table to be re-captured in its entirety). "},{"title":"Insufficient Binlog Retention​","type":1,"pageTitle":"MariaDB","url":"reference/Connectors/capture-connectors/mariadb/#insufficient-binlog-retention","content":"If your capture fails with a &quot;binlog retention period is too short&quot; error, it is informing you that the MariaDB binlog retention period is set to a dangerously low value, and your capture would risk unrecoverable failure if it were paused or the server became unreachable for a nontrivial amount of time, such that the database expired a binlog segment that the capture was still reading from. (If this were to happen, then change events would be permanently lost and that particular capture would never be able to make progress without potentially producing incorrect data. Thus the capture would need to be torn down and recreated so that each table could be re-captured in its entirety, starting with a complete backfill of current contents.) The &quot;binlog retention period is too short&quot; error should normally be fixed by setting binlog_expire_logs_seconds = 2592000 as described in the Prerequisites section (and when running on a managed cloud platform additional steps may be required, refer to the managed cloud setup instructions above). However, advanced users who understand the risks can use the skip_binlog_retention_check configuration option to disable this safety. "},{"title":"MySQL","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/MySQL/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#prerequisites","content":"To use this connector, you'll need a MySQL database setup with the following. binlog_formatsystem variable set to ROW (the default value).Binary log expiration period set to MySQL's default value of 30 days (2592000 seconds) if at all possible. This value may be set lower if necessary, but we strongly discourage going below 7 days as this may increase the likelihood of unrecoverable failures. A watermarks table. The watermarks table is a small &quot;scratch space&quot; to which the connector occasionally writes a small amount of data (a UUID, specifically) to ensure accuracy when backfilling preexisting table contents. The default name is &quot;flow.watermarks&quot;, but this can be overridden in config.json. A database user with appropriate permissions: REPLICATION CLIENT and REPLICATION SLAVE privileges.Permission to insert, update, and delete on the watermarks table.Permission to read the tables being captured.Permission to read from information_schema tables, if automatic discovery is used. If the table(s) to be captured include columns of type DATETIME, the time_zone system variable must be set to an IANA zone name or numerical offset. "},{"title":"Setup​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#setup","content":"To meet these requirements, do the following: Create the watermarks table. This table can have any name and be in any database, so long as the capture's config.json file is modified accordingly. CREATE DATABASE IF NOT EXISTS flow; CREATE TABLE IF NOT EXISTS flow.watermarks (slot INTEGER PRIMARY KEY, watermark TEXT);  Create the flow_capture user with replication permission, the ability to read all tables, and the ability to read and write the watermarks table. The SELECT permission can be restricted to just the tables that need to be captured, but automatic discovery requires information_schema access as well. CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' COMMENT 'User account for Flow MySQL data capture'; GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture'; GRANT INSERT, UPDATE, DELETE ON flow.watermarks TO 'flow_capture';  Configure the binary log to retain data for the default MySQL setting of 30 days, if previously set lower. SET PERSIST binlog_expire_logs_seconds = 2592000;  Configure the database's time zone. See below for more information. SET PERSIST time_zone = '-05:00'  "},{"title":"Setting the MySQL time zone​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#setting-the-mysql-time-zone","content":"MySQL's time_zone server system variable is set to SYSTEM by default. If you intend to capture tables including columns of the type DATETIME, and time_zone is set to SYSTEM, Flow won't be able to detect the time zone and convert the column to RFC3339 format. To avoid this, you must explicitly set the time zone for your database. You can: Specify a numerical offset from UTC. For MySQL version 8.0.19 or higher, values from -13:59 to +14:00, inclusive, are permitted.Prior to MySQL 8.0.19, values from -12:59 to +13:00, inclusive, are permitted Specify a named timezone in IANA timezone format. For example, if you're located in New Jersey, USA, you could set time_zone to -05:00 or -04:00, depending on the time of year. Because this region observes daylight savings time, you'd be responsible for changing the offset. Alternatively, you could set time_zone to America/New_York, and time changes would occur automatically. If using IANA time zones, your database must include time zone tables. Learn more in the MySQL docs. "},{"title":"Backfills and performance considerations​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#backfills-and-performance-considerations","content":"When the a MySQL capture is initiated, by default, the connector first backfills, or captures the targeted tables in their current state. It then transitions to capturing change events on an ongoing basis. This is desirable in most cases, as in ensures that a complete view of your tables is captured into Flow. However, you may find it appropriate to skip the backfill, especially for extremely large tables. In this case, you may turn of backfilling on a per-table basis. See properties for details. "},{"title":"Configuration​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the MySQL source connector. "},{"title":"Properties​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/address\tServer Address\tThe host or host:port at which the database can be reached.\tstring\tRequired /user\tLogin User\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tLogin Password\tPassword for the specified database user.\tstring\tRequired /advanced/watermarks_table\tWatermarks Table Name\tThe name of the table used for watermark writes. Must be fully-qualified in '&lt;schema&gt;.&lt;table&gt;' form.\tstring\t&quot;flow.watermarks&quot; /advanced/dbname\tDatabase Name\tThe name of database to connect to. In general this shouldn't matter. The connector can discover and capture from all databases it's authorized to access.\tstring\t&quot;mysql&quot; /advanced/node_id\tNode ID\tNode ID for the capture. Each node in a replication cluster must have a unique 32-bit ID. The specific value doesn't matter so long as it is unique. If unset or zero the connector will pick a value.\tinteger /advanced/skip_backfills\tSkip Backfills\tA comma-separated list of fully-qualified table names which should not be backfilled.\tstring /advanced/backfill_chunk_size\tBackfill Chunk Size\tThe number of rows which should be fetched from the database in a single backfill query.\tinteger\t131072 /advanced/skip_binlog_retention_check\tSkip Binlog Retention Sanity Check\tBypasses the 'dangerously short binlog retention' sanity check at startup. Only do this if you understand the danger and have a specific need.\tboolean\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe database/schema in which the table resides.\tstring\tRequired /stream\tStream\tName of the table to be captured from the database.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired info When you configure this connector in the web application, the automatic discovery process sets up a binding for most tables it finds in your database, but there are exceptions. Tables in the MySQL system schemas information_schema, mysql, performance_schema, and sys will not be discovered. You can add bindings for such tables manually. "},{"title":"Sample​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#sample","content":"A minimal capture definition will look like the following: captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-mysql:dev config: address: &quot;127.0.0.1:3306&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; bindings: - resource: namespace: ${TABLE_NAMESPACE} stream: ${TABLE_NAME} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your capture definition will likely be more complex, with additional bindings for each table in the source database. Learn more about capture definitions. "},{"title":"MySQL on managed cloud platforms​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#mysql-on-managed-cloud-platforms","content":"In addition to standard MySQL, this connector supports cloud-based MySQL instances on certain platforms. "},{"title":"Amazon RDS​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#amazon-rds","content":"You can use this connector for MySQL instances on Amazon RDS using the following setup instructions. For Amazon Aurora, see below. Estuary recommends creating a read replicain RDS for use with Flow; however, it's not required. You're able to apply the connector directly to the primary instance if you'd like. Setup​ Allow connections to the database from the Estuary Flow IP address. Modify the database, setting Public accessibility to Yes. Edit the VPC security group associated with your database, or create a new VPC security group and associate it with the database. Refer to the steps in the Amazon documentation. Create a new inbound rule and a new outbound rule that allow all traffic from the IP address 34.121.207.128. info Alternatively, you can allow secure connections via SSH tunneling. To do so: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. Create a RDS parameter group to enable replication in MySQL. Create a parameter group. Create a unique name and description and set the following properties: Family: mysql8.0Type: DB Parameter group Modify the new parameter group and update the following parameters: binlog_format: ROWbinlog_row_metadata: FULLread_only: 0 If using the primary instance (not recommended), associate the parameter groupwith the database and set Backup Retention Period to 7 days. Reboot the database to allow the changes to take effect. Create a read replica with the new parameter group applied (recommended). Create a read replicaof your MySQL database. Modify the replicaand set the following: DB parameter group: choose the parameter group you created previouslyBackup retention period: 7 daysPublic access: Publicly accessible Reboot the replica to allow the changes to take effect. Switch to your MySQL client. Run the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table: CREATE DATABASE IF NOT EXISTS flow; CREATE TABLE IF NOT EXISTS flow.watermarks (slot INTEGER PRIMARY KEY, watermark TEXT); CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' COMMENT 'User account for Flow MySQL data capture'; GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture'; GRANT INSERT, UPDATE, DELETE ON flow.watermarks TO 'flow_capture';  Run the following command to set the binary log retention to 7 days, the maximum value which RDS MySQL permits: CALL mysql.rds_set_configuration('binlog retention hours', 168);  In the RDS console, note the instance's Endpoint and Port. You'll need these for the address property when you configure the connector. "},{"title":"Amazon Aurora​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#amazon-aurora","content":"You can use this connector for MySQL-compatible Amazon Aurora instances using the following setup instructions. You must apply some of the settings to the entire Aurora DB cluster, and others to a database instance within the cluster (we recommend you use a replica, or reader instance to connect with Flow). For each step, take note of which entity you're working with. Setup​ Allow connections to the database from the Estuary Flow IP address. Modify the instance, choosing Publicly accessible in the Connectivity settings. Edit the VPC security group associated with your instance, or create a new VPC security group and associate it with the instance. Refer to the steps in the Amazon documentation. Create a new inbound rule and a new outbound rule that allow all traffic from the IP address 34.121.207.128. info Alternatively, you can allow secure connections via SSH tunneling. To do so: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. Create a RDS parameter group to enable replication on your Aurora DB cluster. Create a parameter group. Create a unique name and description and set the following properties: Family: aurora-mysql8.0Type: DB ClusterParameter group Modify the new parameter group and update the following parameters: binlog_format: ROWbinlog_row_metadata: FULLread_only: 0 Associate the parameter groupwith the DB cluster. While you're modifying the cluster, also set Backup Retention Period to 7 days. Reboot the cluster to allow the changes to take effect. Switch to your MySQL client. Run the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table: CREATE DATABASE IF NOT EXISTS flow; CREATE TABLE IF NOT EXISTS flow.watermarks (slot INTEGER PRIMARY KEY, watermark TEXT); CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' COMMENT 'User account for Flow MySQL data capture'; GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture'; GRANT INSERT, UPDATE, DELETE ON flow.watermarks TO 'flow_capture';  Run the following command to set the binary log retention to 7 days, the maximum value Aurora permits: CALL mysql.rds_set_configuration('binlog retention hours', 168);  In the RDS console, note the instance's Endpoint and Port. You'll need these for the address property when you configure the connector. "},{"title":"Google Cloud SQL​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#google-cloud-sql","content":"You can use this connector for MySQL instances on Google Cloud SQL using the following setup instructions. Setup​ Allow connections to the DB instance from the Estuary Flow IP address. Enable public IP on your database and add34.121.207.128 as an authorized IP address. info Alternatively, you can allow secure connections via SSH tunneling. To do so: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. Set the instance's binlog_expire_logs_seconds flagto 2592000. Using Google Cloud Shell or your preferred client, create the watermarks table. CREATE DATABASE IF NOT EXISTS flow; CREATE TABLE IF NOT EXISTS flow.watermarks (slot INTEGER PRIMARY KEY, watermark TEXT);  Create the flow_capture user with replication permission, the ability to read all tables, and the ability to read and write the watermarks table. The SELECT permission can be restricted to just the tables that need to be captured, but automatic discovery requires information_schema access as well. CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' COMMENT 'User account for Flow MySQL data capture'; GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture'; GRANT INSERT, UPDATE, DELETE ON flow.watermarks TO 'flow_capture';  In the Cloud Console, note the instance's host under Public IP Address. Its port will always be 3306. Together, you'll use the host:port as the address property when you configure the connector. "},{"title":"Azure Database for MySQL​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#azure-database-for-mysql","content":"You can use this connector for MySQL instances on Azure Database for MySQL using the following setup instructions. Setup​ Allow connections to the database from the Estuary Flow IP address. Create a new firewall rulethat grants access to the IP address 34.121.207.128. info Alternatively, you can allow secure connections via SSH tunneling. To do so: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. Set the binlog_expire_logs_seconds server perameterto 2592000. Using MySQL workbench or your preferred client, create the watermarks table. tip Your username must be specified in the format username@servername. CREATE DATABASE IF NOT EXISTS flow; CREATE TABLE IF NOT EXISTS flow.watermarks (slot INTEGER PRIMARY KEY, watermark TEXT);  Create the flow_capture user with replication permission, the ability to read all tables, and the ability to read and write the watermarks table. The SELECT permission can be restricted to just the tables that need to be captured, but automatic discovery requires information_schema access as well. CREATE USER IF NOT EXISTS flow_capture IDENTIFIED BY 'secret' COMMENT 'User account for Flow MySQL data capture'; GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'flow_capture'; GRANT SELECT ON *.* TO 'flow_capture'; GRANT INSERT, UPDATE, DELETE ON flow.watermarks TO 'flow_capture';  Note the instance's host under Server name, and the port under Connection Strings (usually 3306). Together, you'll use the host:port as the address property when you configure the connector. "},{"title":"Troubleshooting Capture Errors​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#troubleshooting-capture-errors","content":"The source-mysql connector is designed to halt immediately if something wrong or unexpected happens, instead of continuing on and potentially outputting incorrect data. What follows is a non-exhaustive list of some potential failure modes, and what action should be taken to fix these situations: "},{"title":"Unsupported Operations​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#unsupported-operations","content":"If your capture is failing with an &quot;unsupported operation {ALTER,DROP,TRUNCATE,etc} TABLE&quot; error, this indicates that such an operation has taken place impacting a table which is currently being captured. In the case of DROP TABLE and other destructive operations this is not supported, and can only be resolved by removing the offending table(s) from the capture bindings list, after which you may recreate the capture if desired (causing the latest state of the table to be recaptured in its entirety). In the case of ALTER TABLE query we intend to support a limited subset of table alterations in the future, however this error indicates that whatever alteration took place is not currently supported. Practically speaking the immediate resolution is the same as for a DROP or TRUNCATE TABLE, but if you frequently perform schema migrations it may be worth reaching out to see if we can add support for whatever table alteration you just did. "},{"title":"Data Manipulation Queries​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#data-manipulation-queries","content":"If your capture is failing with an &quot;unsupported DML query&quot; error, this means that an INSERT, UPDATE, DELETE or other data manipulation query is present in the MySQL binlog. This should generally not happen if binlog_format = 'ROW' as described in the Prerequisites section. Resolving this error requires fixing the binlog_format system variable, and then either tearing down and recreating the entire capture so that it restarts at a later point in the binlog, or in the case of an INSERT/DELETE query it may suffice to remove the capture binding for the offending table and then re-add it. "},{"title":"Unhandled Queries​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#unhandled-queries","content":"If your capture is failing with an &quot;unhandled query&quot; error, some SQL query is present in the binlog which the connector does not (currently) understand. In general, this error suggests that the connector should be modified to at least recognize this type of query, and most likely categorize it as either an unsupported DML Query, an unsupported Table Operation, or something that can safely be ignored. Until such a fix is made the capture cannot proceed, and you will need to tear down and recreate the entire capture so that it restarts from a later point in the binlog. "},{"title":"Metadata Errors​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#metadata-errors","content":"If your capture is failing with a &quot;metadata error&quot; then something has gone badly wrong with the capture's tracking of table metadata, such as column names or datatypes. This should never happen, and most likely means that the MySQL binlog itself is corrupt in some way. If this occurs, it can be resolved by removing the offending table(s) from the capture bindings list and then recreating the capture (generally into a new collection, as this process will cause the table to be re-captured in its entirety). "},{"title":"Insufficient Binlog Retention​","type":1,"pageTitle":"MySQL","url":"reference/Connectors/capture-connectors/MySQL/#insufficient-binlog-retention","content":"If your capture fails with a &quot;binlog retention period is too short&quot; error, it is informing you that the MySQL binlog retention period is set to a dangerously low value, and your capture would risk unrecoverable failure if it were paused or the server became unreachable for a nontrivial amount of time, such that the database expired a binlog segment that the capture was still reading from. (If this were to happen, then change events would be permanently lost and that particular capture would never be able to make progress without potentially producing incorrect data. Thus the capture would need to be torn down and recreated so that each table could be re-captured in its entirety, starting with a complete backfill of current contents.) The &quot;binlog retention period is too short&quot; error should normally be fixed by setting binlog_expire_logs_seconds = 2592000 as described in the Prerequisites section (and when running on a managed cloud platform additional steps may be required, refer to the managed cloud setup instructions above). However, advanced users who understand the risks can use the skip_binlog_retention_check configuration option to disable this safety. "},{"title":"Notion","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/notion/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Notion","url":"reference/Connectors/capture-connectors/notion/#supported-data-resources","content":"The following data resources are supported: BlocksDatabasesPagesUsers By default, each resource is mapped to a Flow collection through a separate binding. "},{"title":"Prerequisites​","type":1,"pageTitle":"Notion","url":"reference/Connectors/capture-connectors/notion/#prerequisites","content":"To use this connector, you'll need a Notion account with an integration created to connect with Flow. Before you create your integration, choose how you'll authenticate with Notion. There are two ways: using OAuth to sign in directly in the web app, or manually, using an access token. OAuth is recommended in the web app; only manual configuration is supported when using the CLI. "},{"title":"Setup for OAuth authentication​","type":1,"pageTitle":"Notion","url":"reference/Connectors/capture-connectors/notion/#setup-for-oauth-authentication","content":"Go to your integrations page and create a new integration. On the new integration's Secrets page, change the integration type to Public. Fill in the required fields. Redirect URIs: http://dashboard.estuary.devWebsite homepage: http://dashboard.estuary.devPrivacy policy: https://www.estuary.dev/privacy-policy/Terms of use: https://www.estuary.dev/terms/ "},{"title":"Setup for manual authentication​","type":1,"pageTitle":"Notion","url":"reference/Connectors/capture-connectors/notion/#setup-for-manual-authentication","content":"Go to your integrations page and create a new internal integration. Notion integrations are internal by default. Copy the generated token for use in the connector configuration. "},{"title":"Configuration​","type":1,"pageTitle":"Notion","url":"reference/Connectors/capture-connectors/notion/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Notion source connector. "},{"title":"Properties​","type":1,"pageTitle":"Notion","url":"reference/Connectors/capture-connectors/notion/#properties","content":"Endpoint​ The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so many of these properties aren't required. Property\tTitle\tDescription\tType\tRequired/Default/credentials\tAuthenticate using\tPick an authentication method.\tobject\tRequired /credentials/auth_type\tAuthentication type\tSet to token for manual authentication\tstring\tRequired /credentials/token\tAccess Token\tNotion API access token\tstring /start_date\tStart Date\tUTC date and time in the format 2017-01-25T00:00:00.000Z. Any data before this date will not be replicated.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tNotion resource from which a collection is captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Notion","url":"reference/Connectors/capture-connectors/notion/#sample","content":"This sample specification reflects the manual authentication method. captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-notion:dev config: credentials: auth_type: token token: {secret} start_date: 2021-01-25T00:00:00Z bindings: - resource: stream: blocks syncMode: incremental target: ${PREFIX}/blocks {...}  "},{"title":"Salesforce","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/salesforce/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Salesforce","url":"reference/Connectors/capture-connectors/salesforce/#supported-data-resources","content":"This connector can capture the following Salesforce standard objects, if present in your account: AccountContactUserOpportunityFilledHistoryLeadHistoryOpportunityCampaignCaseContactLineItemEntitlementLeadLiveChatTranscriptMessagingSessionQuoteQuoteLineItemServiceAppointmentServiceContractTaskUserServicePresenceWorkOrderWorkOrderLineItem Custom objects aren't currently supported. Each captured object is mapped to a Flow collection through a separate binding. Because most Salesforce accounts contain large volumes of data, you may only want to capture a subset of the available objects. There are several ways to control this: Create a dedicated Salesforce user with access only to the objects you'd like to capture. Apply a filter when you configure the connector. If you don't apply a filter, the connector captures all objects available to the user. During capture creation in the web application, remove the bindings for objects you don't want to capture. "},{"title":"Prerequisites​","type":1,"pageTitle":"Salesforce","url":"reference/Connectors/capture-connectors/salesforce/#prerequisites","content":""},{"title":"Using OAuth2 to authenticate with Salesforce in the Flow web app​","type":1,"pageTitle":"Salesforce","url":"reference/Connectors/capture-connectors/salesforce/#using-oauth2-to-authenticate-with-salesforce-in-the-flow-web-app","content":"If you're using the Flow web app, you'll be prompted to authenticate with Salesforce using OAuth. You'll need the following: A Salesforce organization on the Enterprise tier, or with an equivalent API request allocation. Salesforce user credentials. We recommend creating a dedicated read-only Salesforce user. "},{"title":"Configuring the connector specification manually​","type":1,"pageTitle":"Salesforce","url":"reference/Connectors/capture-connectors/salesforce/#configuring-the-connector-specification-manually","content":"If you're working with flowctl and writing specifications in a local development environment, you'll need to manually supply OAuth credentials. You'll need: The items required to set up with OAuth2. A Salesforce developer application with a generated client ID, client secret, and refresh token. See setup steps. "},{"title":"Setup​","type":1,"pageTitle":"Salesforce","url":"reference/Connectors/capture-connectors/salesforce/#setup","content":"Create a read-only Salesforce user​ Creating a dedicated read-only Salesforce user is a simple way to specify which objects Flow will capture. This is useful if you have a large amount of data in your Salesforce organization. While signed in as an administrator, create a new profile by cloning the standard Minimum Access profile. Edit the new profile's permissions. Grant it read access to all the standard and custom objects you'd like to capture with Flow. Create a new user, applying the profile you just created. You'll use this user's email address and password to authenticate Salesforce in Flow. Create a developer application and generate authorization tokens​ To manually write a capture specification for Salesforce, you need to create and configure a developer application. Through this process, you'll obtain the client ID, client secret, and refresh token. Create a new developer application. a. When selecting Scopes for your app, select Manage user data via APIs (api), Perform requests at any time (refresh_token, offline_access), and Manage user data via Web browsers (web). Edit the app to ensure that Permitted users is set to All users may self-authorize. Locate the Consumer Key and Consumer Secret. These are equivalent to the client id and client secret, respectively. Follow the Salesforce Web Server Flow. The final POST response will include your refresh token. "},{"title":"Configuration​","type":1,"pageTitle":"Salesforce","url":"reference/Connectors/capture-connectors/salesforce/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the Flow specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Salesforce source connector. "},{"title":"Properties​","type":1,"pageTitle":"Salesforce","url":"reference/Connectors/capture-connectors/salesforce/#properties","content":"Endpoint​ The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so you won't need the /credentials values listed here. Property\tTitle\tDescription\tType\tRequired/Default/credentials object\tRequired /credentials/auth_type\tAuthorization type\tSet to Client\tstring /credentials/client_id\tClient ID\tThe Salesforce Client ID, also known as a Consumer Key, for your developer application.\tstring\tRequired /credentials/client_secret\tClient Secret\tThe Salesforce Client Secret, also known as a Consumer Secret, for your developer application.\tstring\tRequired /credentials/refresh_token\tRefresh Token\tThe refresh token generated by your developer application.\tstring\tRequired /is_sandbox\tSandbox\tWhether you're using a Salesforce Sandbox.\tboolean\tfalse /start_date\tStart Date\tStart date in the format YYYY-MM-DD. Data added on and after this date will be captured. If this field is blank, all data will be captured.\tstring /streams_criteria\tFilter Salesforce Objects (Optional)\tFilter Salesforce objects for capture.\tarray /streams_criteria/-/criteria\tSearch criteria\tPossible criteria are &quot;starts with&quot;, &quot;ends with&quot;, &quot;contains&quot;, &quot;exacts&quot;, &quot;starts not with&quot;, &quot;ends not with&quot;, &quot;not contains&quot;, and &quot;not exacts&quot;.\tstring\t&quot;contains&quot; /streams_criteria/-/value\tSearch value\tSearch term used with the selected criterion to filter objects.\tstring\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/cursorField\tCursor field\tField used as a cursor to track data replication; typically a timestamp field.\tarray, null /stream\tStream\tSalesforce object from which a collection is captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Salesforce","url":"reference/Connectors/capture-connectors/salesforce/#sample","content":"This sample specification reflects the manual authentication method. captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-salesforce:dev config: credentials: auth_type: Client client_id: {your_client_id} client_secret: {secret} refresh_token: {XXXXXXXX} is_sandbox: false start_date: 2022-01-01 streams_criteria: - criteria: &quot;starts with&quot; value: &quot;Work&quot; bindings: - resource: cursorField: [SystemModstamp] stream: WorkOrder syncMode: incremental target: ${PREFIX}/WorkOrder - resource: cursorField: [SystemModstamp] stream: WorkOrderLineItem syncMode: incremental target: ${PREFIX}/WorkOrderLineItem  "},{"title":"Stripe","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/stripe/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Stripe","url":"reference/Connectors/capture-connectors/stripe/#supported-data-resources","content":"The following data resources are supported through the Stripe API: Balance transactionsBank accountsChargesCheckout sessionsCheckout sessions line itemsCouponsCustomer balance transactionsCustomersDisputesEventsInvoice itemsInvoice line itemsInvoicesPayment intentsPayoutsPlansProductsPromotion codesRefundsSubscription itemsSubscriptionsTransfers By default, each resource is mapped to a Flow collection through a separate binding. "},{"title":"Prerequisites​","type":1,"pageTitle":"Stripe","url":"reference/Connectors/capture-connectors/stripe/#prerequisites","content":"Account ID of your Stripe account.Secret key for the Stripe API. "},{"title":"Configuration​","type":1,"pageTitle":"Stripe","url":"reference/Connectors/capture-connectors/stripe/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Stripe source connector. "},{"title":"Properties​","type":1,"pageTitle":"Stripe","url":"reference/Connectors/capture-connectors/stripe/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/account_id\tAccount ID\tYour Stripe account ID (starts with 'acct_', find yours here https://dashboard.stripe.com/settings/account\tstring\tRequired /client_secret\tSecret Key\tStripe API key (usually starts with 'sk_live_'; find yours here https://dashboard.stripe.com/apikeys\tstring\tRequired /lookback_window_days\tLookback Window in days (Optional)\tWhen set, the connector will always re-export data from the past N days, where N is the value set here. This is useful if your data is frequently updated after creation.\tinteger\t0 /start_date\tReplication start date\tUTC date and time in the format 2017-01-25T00:00:00Z. Only data generated after this date will be replicated.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource from Stripe from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired "},{"title":"Choosing your start date and lookback window​","type":1,"pageTitle":"Stripe","url":"reference/Connectors/capture-connectors/stripe/#choosing-your-start-date-and-lookback-window","content":"The connector will continually capture data beginning on the Replication start date you choose. However, some data from the Stripe API is mutable; for example, a draft invoice can be completed at a later date than it was created. To account for this, it's useful to set the Lookback Window. When this is set, at a given point in time, the connector will not only look for new data; it will also capture changes made to data within the window. For example, if you start the connector with the start date of 2022-06-06T00:00:00Z (June 6) and the lookback window of 3, the connector will begin to capture data starting from June 3. As time goes on while the capture remains active, the lookback window rolls forward along with the current timestamp. On June 10, the connector will continue to monitor data starting from June 7 and capture any changes to that data, and so on. "},{"title":"Sample​","type":1,"pageTitle":"Stripe","url":"reference/Connectors/capture-connectors/stripe/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-stripe:dev config: account_id: 00000000 client_secret: &lt;secret&gt; start_date: 2022-06-18T00:00:00Z bindings: - resource: stream: balance_transactions syncMode: incremental target: ${PREFIX}/balancetransactions - resource: stream: bank_accounts syncMode: full_refresh target: ${PREFIX}/bankaccounts - resource: stream: charges syncMode: incremental target: ${PREFIX}/charges - resource: stream: checkout_sessions syncMode: incremental target: ${PREFIX}/checkoutsessions - resource: stream: checkout_sessions_line_items syncMode: incremental target: ${PREFIX}/checkoutsessionslineitems - resource: stream: coupons syncMode: incremental target: ${PREFIX}/coupons - resource: stream: customer_balance_transactions syncMode: full_refresh target: ${PREFIX}/customerbalancetransactions - resource: stream: customers syncMode: incremental target: ${PREFIX}/customers - resource: stream: disputes syncMode: incremental target: ${PREFIX}/disputes - resource: stream: events syncMode: incremental target: ${PREFIX}/events - resource: stream: invoice_items syncMode: incremental target: ${PREFIX}/invoice_items - resource: stream: invoice_line_items syncMode: full_refresh target: ${PREFIX}/invoicelineitems - resource: stream: invoices syncMode: incremental target: ${PREFIX}/invoices - resource: stream: payment_intents syncMode: incremental target: ${PREFIX}/paymentintents - resource: stream: payouts syncMode: incremental target: ${PREFIX}/payouts - resource: stream: plans syncMode: incremental target: ${PREFIX}/plans - resource: stream: products syncMode: incremental target: ${PREFIX}/products - resource: stream: promotion_codes syncMode: incremental target: ${PREFIX}/promotioncodes - resource: stream: refunds syncMode: incremental target: ${PREFIX}/refunds - resource: stream: subscription_items syncMode: full_refresh target: ${PREFIX}/subscriptionitems - resource: stream: subscriptions syncMode: incremental target: ${PREFIX}/subscriptions - resource: stream: transfers syncMode: incremental target: ${PREFIX}/transfers  "},{"title":"PostgreSQL","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/PostgreSQL/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#prerequisites","content":"This connector supports PostgreSQL versions 10.0 and later. You'll need a PostgreSQL database setup with the following: Logical replication enabled — wal_level=logicalUser role with REPLICATION attributeA replication slot. This represents a “cursor” into the PostgreSQL write-ahead log from which change events can be read. Optional; if none exist, one will be created by the connector.If you wish to run multiple captures from the same database, each must have its own slot. You can create these slots yourself, or by specifying a name other than the default in the advanced configuration. A publication. This represents the set of tables for which change events will be reported. In more restricted setups, this must be created manually, but can be created automatically if the connector has suitable permissions. A watermarks table. The watermarks table is a small “scratch space” to which the connector occasionally writes a small amount of data to ensure accuracy when backfilling preexisting table contents. In more restricted setups, this must be created manually, but can be created automatically if the connector has suitable permissions. "},{"title":"Setup​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#setup","content":"info These setup instructions are PostgreSQL instances you manage yourself. If you use a cloud-based managed service for your database, see below. The simplest way to meet the above prerequisites is to change the WAL level and have the connector use a database superuser role. For a more restricted setup, create a new user with just the required permissions as detailed in the following steps: Connect to your instance and create a new user and password: CREATE USER flow_capture WITH PASSWORD 'secret' REPLICATION;  Assign the appropriate role. If using PostgreSQL v14 or later: GRANT pg_read_all_data TO flow_capture; If using an earlier version: ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES to flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA public, &lt;others&gt; TO flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA information_schema, pg_catalog TO flow_capture; where &lt;others&gt; lists all schemas that will be captured from. info If an even more restricted set of permissions is desired, you can also grant SELECT on just the specific table(s) which should be captured from. The ‘information_schema’ and ‘pg_catalog’ access is required for stream auto-discovery, but not for capturing already configured streams. Create the watermarks table, grant privileges, and create publication: CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication FOR ALL TABLES;  Set WAL level to logical: ALTER SYSTEM SET wal_level = logical;  Restart PostgreSQL to allow the WAL level change to take effect. "},{"title":"Backfills and performance considerations​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#backfills-and-performance-considerations","content":"When the a PostgreSQL capture is initiated, by default, the connector first backfills, or captures the targeted tables in their current state. It then transitions to capturing change events on an ongoing basis. This is desirable in most cases, as in ensures that a complete view of your tables is captured into Flow. However, you may find it appropriate to skip the backfill, especially for extremely large tables. In this case, you may turn of backfilling on a per-table basis. See properties for details. "},{"title":"Configuration​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the PostgreSQL source connector. "},{"title":"Properties​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/address\tAddress\tThe host or host:port at which the database can be reached.\tstring\tRequired /database\tDatabase\tLogical database name to capture from.\tstring\tRequired, &quot;postgres&quot; /user\tUser\tThe database user to authenticate as.\tstring\tRequired, &quot;flow_capture&quot; /password\tPassword\tPassword for the specified database user.\tstring\tRequired /advanced\tAdvanced Options\tOptions for advanced users. You should not typically need to modify these.\tobject /advanced/backfill_chunk_size\tBackfill Chunk Size\tThe number of rows which should be fetched from the database in a single backfill query.\tinteger\t4096 /advanced/publicationName\tPublication Name\tThe name of the PostgreSQL publication to replicate from.\tstring\t&quot;flow_publication&quot; /advanced/skip_backfills\tSkip Backfills\tA comma-separated list of fully-qualified table names which should not be backfilled.\tstring /advanced/slotName\tSlot Name\tThe name of the PostgreSQL replication slot to replicate from.\tstring\t&quot;flow_slot&quot; /advanced/watermarksTable\tWatermarks Table\tThe name of the table used for watermark writes during backfills. Must be fully-qualified in '&lt;schema&gt;.&lt;table&gt;' form.\tstring\t&quot;public.flow_watermarks&quot; Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/namespace\tNamespace\tThe namespace/schema of the table.\tstring\tRequired /stream\tStream\tTable name.\tstring\tRequired /syncMode\tSync mode\tConnection method. Always set to incremental.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#sample","content":"A minimal capture definition will look like the following: captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: &quot;ghcr.io/estuary/source-postgres:dev&quot; config: address: &quot;localhost:5432&quot; database: &quot;postgres&quot; user: &quot;flow_capture&quot; password: &quot;secret&quot; bindings: - resource: stream: ${TABLE_NAME} namespace: ${TABLE_NAMESPACE} syncMode: incremental target: ${PREFIX}/${COLLECTION_NAME}  Your capture definition will likely be more complex, with additional bindings for each table in the source database. Learn more about capture definitions.. "},{"title":"PostgreSQL on managed cloud platforms​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#postgresql-on-managed-cloud-platforms","content":"In addition to standard PostgreSQL, this connector supports cloud-based PostgreSQL instances on certain platforms. "},{"title":"Amazon RDS​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#amazon-rds","content":"You can use this connector for PostgreSQL instances on Amazon RDS using the following setup instructions. For Amazon Aurora, see below. Setup​ Allow connections to the database from the Estuary Flow IP address. Modify the database, setting Public accessibility to Yes. Edit the VPC security group associated with your database, or create a new VPC security group and associate it with the database. Refer to the steps in the Amazon documentation. Create a new inbound rule and a new outbound rule that allow all traffic from the IP address 34.121.207.128. info Alternatively, you can allow secure connections via SSH tunneling. To do so: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. Enable logical replication on your RDS PostgreSQL instance. Create a parameter group. Create a unique name and description and set the following properties: Family: postgres13Type: DB Parameter group Modify the new parameter group and set rds.logical_replication=1. Associate the parameter group with the database. Reboot the database to allow the new parameter group to take effect. In the PostgreSQL client, connect to your instance and run the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table and publication. CREATE USER flow_capture WITH PASSWORD 'secret'; GRANT rds_replication TO flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA public TO flow_capture; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO flow_capture; CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication FOR ALL TABLES; In the RDS console, note the instance's Endpoint and Port. You'll need these for the address property when you configure the connector. "},{"title":"Amazon Aurora​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#amazon-aurora","content":"You can use this connector for PostgreSQL-compatible Amazon Aurora instances using the following setup instructions. You must apply some of the settings to the entire Aurora DB cluster, and others to a database instance within the cluster (typically, you'll want to use a replica, or reader instance). For each step, take note of which entity you're working with. Setup​ Allow connections to the DB instance from the Estuary Flow IP address. Modify the instance, choosing Publicly accessible in the Connectivity settings. Edit the VPC security group associated with your instance, or create a new VPC security group and associate it with the instance. Refer to the steps in the Amazon documentation. Create a new inbound rule and a new outbound rule that allow all traffic from the IP address 34.121.207.128. info Alternatively, you can allow secure connections via SSH tunneling. To do so: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. Enable logical replication on your Aurora DB cluster. Create a parameter group. Create a unique name and description and set the following properties: Family: aurora-postgresql13, or substitute the version of Aurora PostgreSQL used for your cluster.Type: DB Cluster Parameter group Modify the new parameter group and set rds.logical_replication=1. Associate the parameter group with the DB cluster. Reboot the cluster to allow the new parameter group to take effect. In the PostgreSQL client, connect to your instance and run the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table and publication. CREATE USER flow_capture WITH PASSWORD 'secret'; GRANT rds_replication TO flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA public TO flow_capture; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO flow_capture; CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication FOR ALL TABLES; In the RDS console, note the instance's Endpoint and Port. You'll need these for the address property when you configure the connector. "},{"title":"Google Cloud SQL​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#google-cloud-sql","content":"You can use this connector for PostgreSQL instances on Google Cloud SQL using the following setup instructions. Setup​ Allow connections to the database from the Estuary Flow IP address. Enable public IP on your database and add34.121.207.128 as an authorized IP address. info Alternatively, you can allow secure connections via SSH tunneling. To do so: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. Set the cloudsql.logical_decoding flag to on to enable logical replication on your Cloud SQL PostgreSQL instance. In your PostgreSQL client, connect to your instance and issue the following commands to create a new user for the capture with appropriate permissions, and set up the watermarks table and publication. CREATE USER flow_capture WITH REPLICATION IN ROLE cloudsqlsuperuser LOGIN PASSWORD 'secret'; GRANT SELECT ON ALL TABLES IN SCHEMA public TO flow_capture; ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO flow_capture; CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication FOR ALL TABLES; In the Cloud Console, note the instance's host under Public IP Address. Its port will always be 5432. Together, you'll use the host:port as the address property when you configure the connector. "},{"title":"Azure Database for PostgreSQL​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#azure-database-for-postgresql","content":"You can use this connector for instances on Azure Database for PostgreSQL using the following setup instructions. Setup​ Allow connections to the database from the Estuary Flow IP address. Create a new firewall rulethat grants access to the IP address 34.121.207.128. info Alternatively, you can allow secure connections via SSH tunneling. To do so: Follow the guide to configure an SSH server for tunnelingWhen you configure your connector as described in the configuration section above, including the additional networkTunnel configuration to enable the SSH tunnel. See Connecting to endpoints on secure networksfor additional details and a sample. In your Azure PostgreSQL instance's support parameters, set replication to logical to enable logical replication. In the PostgreSQL client, connect to your instance and run the following commands to create a new user for the capture with appropriate permissions. CREATE USER flow_capture WITH PASSWORD 'secret' REPLICATION;  If using PostgreSQL v14 or later: GRANT pg_read_all_data TO flow_capture;  If using an earlier version: ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES to flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA public, &lt;others&gt; TO flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA information_schema, pg_catalog TO flow_capture; where &lt;others&gt; lists all schemas that will be captured from. info If an even more restricted set of permissions is desired, you can also grant SELECT on just the specific table(s) which should be captured from. The ‘information_schema’ and ‘pg_catalog’ access is required for stream auto-discovery, but not for capturing already configured streams. Set up the watermarks table and publication. ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES to flow_capture; GRANT SELECT ON ALL TABLES IN SCHEMA public, &lt;others&gt; TO flow_capture; GRANT SELECT ON information_schema.columns, information_schema.tables, pg_catalog.pg_attribute, pg_catalog.pg_class, pg_catalog.pg_index, pg_catalog.pg_namespace TO flow_capture; CREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT); GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture; CREATE PUBLICATION flow_publication FOR TABLE schema.table1, schema.table2;  Note the following important items for configuration: Find the instance's host under Server Name, and the port under Connection Strings (usually 5432). Together, you'll use the host:port as the address property when you configure the connector.Format user as username@databasename; for example, flow_capture@myazuredb. "},{"title":"TOASTed values​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#toasted-values","content":"PostgreSQL has a hard page size limit, usually 8 KB, for performance reasons. If your tables contain values that exceed the limit, those values can't be stored directly. PostgreSQL uses TOAST (The Oversized-Attribute Storage Technique) to store them separately. TOASTed values can sometimes present a challenge for systems that rely on the PostgreSQL write-ahead log (WAL), like this connector. If a change event occurs on a row that contains a TOASTed value, but the TOASTed value itself is unchanged, it is omitted from the WAL. As a result, the connector emits a row update with the a value omitted, which might cause unexpected results in downstream catalog tasks if adjustments are not made. The PostgreSQL connector handles TOASTed values for you when you follow the standard discovery workflowor use the Flow UI to create your capture. It uses merge reductionsto fill in the previous known TOASTed value in cases when that value is omitted from a row update. However, due to the event-driven nature of certain tasks in Flow, it's still possible to see unexpected results in your data flow, specifically: When you materialize the captured data to another system using a connector that requires delta updatesWhen you perform a derivation that uses TOASTed values "},{"title":"Troubleshooting​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/capture-connectors/PostgreSQL/#troubleshooting","content":"If you encounter an issue that you suspect is due to TOASTed values, try the following: Ensure your collection's schema is using the merge reduction strategy.Set REPLICA IDENTITY to FULL for the table. This circumvents the problem by forcing the WAL to record all values regardless of size. However, this can have performance impacts on your database and must be carefully evaluated.Contact Estuary support for assistance. "},{"title":"Survey Monkey","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/survey-monkey/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Survey Monkey","url":"reference/Connectors/capture-connectors/survey-monkey/#supported-data-resources","content":"The following data resources are supported: SurveysSurvey pagesSurvey questionsSurvey responses By default, each resource is mapped to a Flow collection through a separate binding. "},{"title":"Prerequisites​","type":1,"pageTitle":"Survey Monkey","url":"reference/Connectors/capture-connectors/survey-monkey/#prerequisites","content":"You'll need to configure a SurveyMonkey private app to integrate with Flow. "},{"title":"Setup​","type":1,"pageTitle":"Survey Monkey","url":"reference/Connectors/capture-connectors/survey-monkey/#setup","content":"Go to your your SurveyMonkey apps page and create a new private app.Set the following required scopes: View surveysView responses Deploy the app. This requires a paid SurveyMonkey plan; otherwise, the app will be deleted in 90 days. Once the app is set up, there are two ways to authenticate SurveyMonkey in Flow: using OAuth in the web app, or using an access token with the flowctl CLI. OAuth authentication in the web app​ You'll need the username and password of a SurveyMonkey user that is part of the teamfor which the private app was created. Manual authentication with flowctl​ Note the client ID, secret, and access token for the private app you created. You'll use these in the connector configuration. "},{"title":"Performance considerations​","type":1,"pageTitle":"Survey Monkey","url":"reference/Connectors/capture-connectors/survey-monkey/#performance-considerations","content":"The SurveyMonkey API imposes call limits of 500 per day and 120 per minute. This connector uses caching to avoid exceeding these limits. "},{"title":"Configuration​","type":1,"pageTitle":"Survey Monkey","url":"reference/Connectors/capture-connectors/survey-monkey/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the SurveyMonkey source connector. "},{"title":"Properties​","type":1,"pageTitle":"Survey Monkey","url":"reference/Connectors/capture-connectors/survey-monkey/#properties","content":"Endpoint​ The properties in the table below reflect the manual authentication method. If you're working in the Flow web app, you'll use OAuth2, so many of these properties aren't required. Property\tTitle\tDescription\tType\tRequired/Default/credentials\tCredentials\tCredentials for the service\tobject\tRequired /credentials/access_token\tAccess Token\tAccess Token for your SurveyMonkey private app.\tstring\tRequired /credentials/client_id\tClient ID\tClient ID associated with your SurveyMonkey private app.\tstring\tRequired /credentials/client_secret\tClient Secret\tClient secret associated with your SurveyMonkey private app.\tstring\tRequired /start_date\tStart Date\tUTC date and time in the format 2017-01-25T00:00:00Z. Any data before this date will not be replicated.\tstring\tRequired /survey_ids\tSurvey Monkey survey IDs\tIDs of the surveys from which you'd like to replicate data. If left empty, data from all boards to which you have access will be replicated.\tarray\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tSurveyMonkey resource from which a collection is captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Survey Monkey","url":"reference/Connectors/capture-connectors/survey-monkey/#sample","content":"This sample specification reflects the manual authentication method. captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-surveymonkey:dev config: credentials: access_token: {secret} client_id: XXXXXXXXXXXXXXXX client_secret: {secret} start_date: 2021-01-25T00:00:00Z bindings: - resource: stream: surveys syncMode: incremental target: ${PREFIX}/surveys {...}  "},{"title":"TikTok Marketing","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/tiktok/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"TikTok Marketing","url":"reference/Connectors/capture-connectors/tiktok/#supported-data-resources","content":"The following data resources are supported: Resource\tProduction\tSandboxAdvertisers\tX\tX Ad Groups\tX\tX Ads\tX\tX Campaigns\tX\tX Ads Reports Hourly\tX\tX Ads Reports Daily\tX\tX Ads Reports Lifetime\tX\tX Advertisers Reports Hourly\tX Advertisers Reports Daily\tX Advertisers Reports Lifetime\tX Ad Groups Reports Hourly\tX\tX Ad Groups Reports Daily\tX\tX Ad Groups Reports Lifetime\tX\tX Campaigns Reports Hourly\tX\tX Campaigns Reports Daily\tX\tX Campaigns Reports Lifetime\tX\tX Advertisers Audience Reports Hourly\tX Advertisers Audience Reports Daily\tX Advertisers Audience Reports Lifetime\tX Ad Group Audience Reports Hourly\tX\tX Ad Group Audience Reports Daily\tX\tX Ads Audience Reports Hourly\tX\tX Ads Audience Reports Daily\tX\tX Campaigns Audience Reports By Country Hourly\tX\tX Campaigns Audience Reports By Country Daily\tX\tX By default, each resource is mapped to a Flow collection through a separate binding. "},{"title":"Prerequisites​","type":1,"pageTitle":"TikTok Marketing","url":"reference/Connectors/capture-connectors/tiktok/#prerequisites","content":"Prerequisites differ depending on whether you have a production or sandboxTikTok for Business account, and on whether you'll use the Flow web app or the flowctl CLI. "},{"title":"OAuth authentication in the web app (production accounts)​","type":1,"pageTitle":"TikTok Marketing","url":"reference/Connectors/capture-connectors/tiktok/#oauth-authentication-in-the-web-app-production-accounts","content":"If you have a TikTok marketing account in production and will use the Flow web app, you'll be able to quickly log in using OAuth. You'll need: A TikTok for Business account with one or more active campaigns. Note the username and password used to sign into this account "},{"title":"Sandbox access token authentication in the web app or CLI​","type":1,"pageTitle":"TikTok Marketing","url":"reference/Connectors/capture-connectors/tiktok/#sandbox-access-token-authentication-in-the-web-app-or-cli","content":"If you're working in a Sandbox TikTok for Business account, you'll authenticate with an access token in both the web app and CLI. You'll need: A TikTok for Business account. A Sandbox account created under an existingdeveloper application. Generate an access token and note the advertiser ID for the Sandbox. "},{"title":"Configuration​","type":1,"pageTitle":"TikTok Marketing","url":"reference/Connectors/capture-connectors/tiktok/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification file. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the TikTok Marketing source connector. "},{"title":"Properties​","type":1,"pageTitle":"TikTok Marketing","url":"reference/Connectors/capture-connectors/tiktok/#properties","content":"Endpoint​ The properties in the table below reflect the manual authentication method for Sandbox accounts. If you're using a production account, you'll use OAuth2 to authenticate in the Flow web app, so many of these properties aren't required. Property\tTitle\tDescription\tType\tRequired/Default/credentials\tAuthentication Method\tAuthentication method\tobject\tRequired /credentials/auth_type\tAuthentication type\tSet to sandbox_access_token to manually authenticate a Sandbox.\tstring\tRequired /credentials/advertiser_id\tAdvertiser ID\tThe Advertiser ID generated for the developer's Sandbox application.\tstring /credentials/access_token\tAccess Token\tThe long-term authorized access token.\tstring /end_date\tEnd Date\tThe date until which you'd like to replicate data for all incremental streams, in the format YYYY-MM-DD. All data generated between start_date and this date will be replicated. Not setting this option will result in always syncing the data till the current date.\tstring /report_granularity\tReport Aggregation Granularity\tThe granularity used for aggregating performance data in reports. Choose DAY, LIFETIME, or HOUR.\tstring /start_date\tStart Date\tReplication Start Date\tThe Start Date in format: YYYY-MM-DD. Any data before this date will not be replicated. If this parameter is not set, all data will be replicated.\tstring Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tTikTok resource from which a collection is captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"TikTok Marketing","url":"reference/Connectors/capture-connectors/tiktok/#sample","content":"This sample specification reflects the access token method for Sandbox accounts. captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-tiktok-marketing:dev config: credentials: auth_type: sandbox_access_token access_token: {secret} advertiser_id: {secret} end_date: 2022-01-01 report_granularity: DAY start_date: 2020-01-01 bindings: - resource: stream: campaigns syncMode: incremental target: ${PREFIX}/campaigns {...}  "},{"title":"Report aggregation​","type":1,"pageTitle":"TikTok Marketing","url":"reference/Connectors/capture-connectors/tiktok/#report-aggregation","content":"Many of the resources this connector supports are reports. Data in these reports is aggregated into rows based on the granularity you select in the configuration. You can choose hourly, daily, or lifetime granularity. For example, if you choose daily granularity, the report will contain one row for each day. "},{"title":"Materialization connectors","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/","content":"","keywords":""},{"title":"Available materialization connectors​","type":1,"pageTitle":"Materialization connectors","url":"reference/Connectors/materialization-connectors/#available-materialization-connectors","content":"AlloyDB ConfigurationPackage - ghcr.io/estuary/materialize-alloydb:dev Apache Parquet in S3 ConfigurationPackage — ghcr.io/estuary/materialize-s3-parquet:dev Elasticsearch ConfigurationPackage — ghcr.io/estuary/materialize-elasticsearch:dev Firebolt ConfigurationPackage - ghcr.io/estuary/materialize-firebolt:dev Google BigQuery ConfigurationPackage — ghcr.io/estuary/materialize-bigquery:dev Google Cloud Pub/Sub ConfigurationPackage - ghcr.io/estuary/materialize-google-pubsub:dev PostgreSQL ConfigurationPackage — ghcr.io/estuary/materialize-postgres:dev Rockset ConfigurationPackage — ghcr.io/estuary/materialize-rockset:dev Snowflake ConfigurationPackage — ghcr.io/estuary/materialize-snowflake:dev TimescaleDB ConfigurationPackage - ghcr.io/estuary/materialize-timescaledb:dev "},{"title":"Zendesk Support","type":0,"sectionRef":"#","url":"reference/Connectors/capture-connectors/zendesk-support/","content":"","keywords":""},{"title":"Supported data resources​","type":1,"pageTitle":"Zendesk Support","url":"reference/Connectors/capture-connectors/zendesk-support/#supported-data-resources","content":"The following data resources are supported through the Zendesk API: BrandsCustom rolesGroup membershipsGroupsMacrosOrganizationsSatisfaction ratingsSchedulesSLA policiesTagsTicket auditsTicket commentsTicket fieldsTicket formsTicket metricsTicket metric eventsTicketsUsers By default, each resource is mapped to a Flow collection through a separate binding. "},{"title":"Prerequisites​","type":1,"pageTitle":"Zendesk Support","url":"reference/Connectors/capture-connectors/zendesk-support/#prerequisites","content":"Subdomain of your Zendesk URL. In the URL https://MY_SUBDOMAIN.zendesk.com/, MY_SUBDOMAIN is the subdomain.Email address associated with your Zendesk account.A Zendesk API token. See the Zendesk docs to enable tokens and generate a new token. "},{"title":"Configuration​","type":1,"pageTitle":"Zendesk Support","url":"reference/Connectors/capture-connectors/zendesk-support/#configuration","content":"You configure connectors either in the Flow web app, or by directly editing the catalog specification files. See connectors to learn more about using connectors. The values and specification sample below provide configuration details specific to the Zendesk Support source connector. "},{"title":"Properties​","type":1,"pageTitle":"Zendesk Support","url":"reference/Connectors/capture-connectors/zendesk-support/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/credentials/api_token\tAPI Token\tThe value of the API token generated.\tstring /credentials/credentials\tCredentials method\tType of credentials used. Set to api-token\tstring /credentials/email\tEmail\tThe user email for your Zendesk account.\tstring /start_date\tStart Date\tThe date from which you'd like to replicate data for Zendesk Support API, in the format YYYY-MM-DDT00:00:00Z. All data generated after this date will be replicated.\tstring\tRequired /subdomain\tSubdomain\tThis is your Zendesk subdomain that can be found in your account URL. For example, in https://{MY_SUBDOMAIN}.zendesk.com/, where MY_SUBDOMAIN is the value of your subdomain.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/stream\tStream\tResource in Zendesk from which collections are captured.\tstring\tRequired /syncMode\tSync Mode\tConnection method.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Zendesk Support","url":"reference/Connectors/capture-connectors/zendesk-support/#sample","content":"captures: ${PREFIX}/${CAPTURE_NAME}: endpoint: connector: image: ghcr.io/estuary/source-zendesk-support:dev config: credentials: api_token: &lt;secret&gt; credentials: api_token email: user@domain.com start_date: 2022-03-01T00:00:00Z subdomain: my_subdomain bindings: - resource: stream: group_memberships syncMode: incremental target: ${PREFIX}/groupmemberships - resource: stream: groups syncMode: incremental target: ${PREFIX}/groups - resource: stream: macros syncMode: incremental target: ${PREFIX}/macros - resource: stream: organizations syncMode: incremental target: ${PREFIX}/organizations - resource: stream: satisfaction_ratings syncMode: incremental target: ${PREFIX}/satisfactionratings - resource: stream: sla_policies syncMode: full_refresh target: ${PREFIX}/slapoliciies - resource: stream: tags syncMode: full_refresh target: ${PREFIX}/tags - resource: stream: ticket_audits syncMode: incremental target: ${PREFIX}/ticketaudits - resource: stream: ticket_comments syncMode: incremental target: ${PREFIX}/ticketcomments - resource: stream: ticket_fields syncMode: incremental target: ${PREFIX}/ticketfields - resource: stream: ticket_forms syncMode: incremental target: ${PREFIX}/ticketforms - resource: stream: ticket_metrics syncMode: incremental target: ${PREFIX}/ticketmetrics - resource: stream: ticket_metric_events syncMode: incremental target: ${PREFIX}/ticketmetricevents - resource: stream: tickets syncMode: incremental target: ${PREFIX}/tickets - resource: stream: users syncMode: incremental target: ${PREFIX}/users - resource: stream: brands syncMode: full_refresh target: ${PREFIX}/brands - resource: stream: custom_roles syncMode: full_refresh target: ${PREFIX}/customroles - resource: stream: schedules syncMode: full_refresh target: ${PREFIX}/schedules  "},{"title":"Google BigQuery","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/BigQuery/","content":"","keywords":""},{"title":"Performance considerations​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#performance-considerations","content":"Like other Estuary connectors, this is a real-time connector that materializes documents using continuous transactions. However, in practice, there are speed limitations. Standard BigQuery tables are limited to 1500 operations per day. This means that the connector is limited 1500 transactions per day. To avoid running up against this limit, you should set the minimum transaction time to a recommended value of 2 minutes, or a minimum value of 1 minute. You do this by configuring the materialization's task shard. This causes an apparent delay in the materialization, but is necessary to prevent error. This also makes transactions more efficient, which reduces costs in BigQuery, especially for large datasets. Instructions to set the minimum transaction time are detailed below. "},{"title":"Prerequisites​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#prerequisites","content":"To use this connector, you'll need: A new Google Cloud Storage bucket in the same region as the BigQuery destination dataset. A Google Cloud service account with a key file generated and the following roles: roles/bigquery.dataEditor on the destination dataset roles/bigquery.jobUser on the project with which the BigQuery destination dataset is associated roles/storage.objectAdminon the GCS bucket created above See Setup for detailed steps to set up your service account. The Flow collections you materialize must accommodate the following naming restrictions: Field names may not contain hyphens (-), or the materialization will fail.Field names must begin with a letter or underscore (_), or the materialization will fail.Field names may contain non-alphanumeric characters, but these are replaced with underscores in the corresponding BigQuery column name.If two field names become identical after special characters are replaced with underscores (for example, field! and field$ both become field_), the materialization will fail.Collection names may contain non-alphanumeric characters, but all such characters except hyphens are replaced with underscores in the BigQuery table name. If necessary, you can add projections to your collection specification to change field names. tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Setup​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#setup","content":"To configure your service account, complete the following steps. Log into the Google Cloud console and create a service account. During account creation: Grant the user access to the project.Grant the user roles roles/bigquery.dataEditor, roles/bigquery.jobUser, and roles/storage.objectAdmin.Click Done. Select the new service account from the list of service accounts. On the Keys tab, click Add key and create a new JSON key. The key is automatically downloaded. You'll use it to configure the connector. "},{"title":"Configuration​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a BigQuery materialization, which will direct one or more of your Flow collections to your desired tables within a BigQuery dataset. A BigQuery dataset is the top-level container within a project, and comprises multiple tables. You can think of a dataset as somewhat analogous to a schema in a relational database. For a complete introduction to resource organization in Bigquery, see the BigQuery docs. "},{"title":"Properties​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/project_id\tProject ID\tThe project ID for the Google Cloud Storage bucket and BigQuery dataset.\tString\tRequired /billing_project_id\tBilling project ID\tThe project ID to which these operations are billed in BigQuery. Typically, you want this to be the same as project_id (the default).\tString\tSame as project_id /dataset\tDataset\tName of the target BigQuery dataset.\tString\tRequired /region\tRegion\tThe GCS region.\tString\tRequired /bucket\tBucket\tName of the GCS bucket.\tString\tRequired /bucket_path\tBucket path\tBase path within the GCS bucket. Also called &quot;Folder&quot; in the GCS console.\tString /credentials_json\tService Account JSON\tThe JSON credentials of the service account to use for authorization.\tString\tRequired To learn more about project billing, see the BigQuery docs. Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tTable name.\tstring\tRequired /delta_updates\tDelta updates.\tWhether to use standard or delta updates\tboolean\tfalse "},{"title":"Shard configuration​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#shard-configuration","content":"Beta UI controls for this workflow will be added to the Flow web app soon. For now, you must edit the materialization specification manually, either in the web app or using the CLI. To avoid exceeding your BigQuery tables' daily operation limits as discussed in Performance considerations, complete the following steps when configuring your materialization: Using the Flow web application or the flowctl CLI, create a draft materialization as you normally would. If using the web app, input the required values and click Discover Endpoint.If using the flowctl, create your materialization specification manually. Add the shards configuration to the materialization specification at the same indentation level as endpoint and bindings. Set the minTxnDuration property to at least 1m (we recommend 2m). In the web app, you do this in the Catalog Editor. shards: minTxnDuration: 2m A full sample is included below. Continue to test, save, and publish the materialization as usual. "},{"title":"Sample​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#sample","content":"materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: project_id: our-bigquery-project dataset: materialized-data region: US bucket: our-gcs-bucket bucket_path: bucket-path/ credentials_json: &lt;secret&gt; image: ghcr.io/estuary/materialize-bigquery:dev bindings: - resource: table: ${table_name} source: ${PREFIX}/${source_collection} shards: minTxnDuration: 2m  "},{"title":"Delta updates​","type":1,"pageTitle":"Google BigQuery","url":"reference/Connectors/materialization-connectors/BigQuery/#delta-updates","content":"This connector supports both standard (merge) and delta updates. The default is to use standard updates. Enabling delta updates will prevent Flow from querying for documents in your BigQuery table, which can reduce latency and costs for large datasets. If you're certain that all events will have unique keys, enabling delta updates is a simple way to improve performance with no effect on the output. However, enabling delta updates is not suitable for all workflows, as the resulting table in BigQuery won't be fully reduced. You can enable delta updates on a per-binding basis:  bindings: - resource: table: ${table_name} delta_updates: true source: ${PREFIX}/${source_collection}  "},{"title":"AlloyDB","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/alloydb/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"AlloyDB","url":"reference/Connectors/materialization-connectors/alloydb/#prerequisites","content":"To use this connector, you'll need: An AlloyDB database to which to materialize, and user credentials. The connector will create new tables in the database per your specification. Tables created manually in advance are not supported.A virtual machine to connect securely to the instance via SSH tunneling. (AlloyDB doesn't support IP whitelisting.) Follow the instructions to create a virtual machine for SSH tunnelingin the same Google Cloud project as your instance. "},{"title":"Configuration​","type":1,"pageTitle":"AlloyDB","url":"reference/Connectors/materialization-connectors/alloydb/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a AlloyDB materialization, which will direct one or more of your Flow collections to your desired tables, or views, in the database. "},{"title":"Properties​","type":1,"pageTitle":"AlloyDB","url":"reference/Connectors/materialization-connectors/alloydb/#properties","content":"Endpoint​ The SSH config section is required for this connector. You'll fill in the database address with a localhost IP address, and specify your VM's IP address as the SSH address. See the table below and the sample config. Property\tTitle\tDescription\tType\tRequired/Default/database\tDatabase\tName of the logical database to materialize to.\tstring /address\tAddress\tHost and port. Set to 127.0.0.1:5432 to enable SSH tunneling.\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /schema\tDatabase Schema\tDatabase schema to use for materialized tables (unless overridden within the binding resource configuration) as well as associated materialization metadata tables\tstring\t&quot;public&quot; /user\tUser\tDatabase user to connect as.\tstring\tRequired networkTunnel\tNetwork Tunnel\tConnect to your system through an SSH server that acts as a bastion host for your network.\tObject networkTunnel/sshForwarding\tSSH Forwarding Object networkTunnel/sshForwarding/sshEndpoint\tSSH Endpoint\tEndpoint of the remote SSH server (in this case, your Google Cloud VM) that supports tunneling (in the form of ssh://user@address.\tString networkTunnel/sshForwarding/privateKey\tSSH Private Key\tPrivate key to connect to the remote SSH server.\tString\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/additional_table_create_sql\tAdditional Table Create SQL\tAdditional SQL statement(s) to be run in the same transaction that creates the table.\tstring /delta_updates\tDelta Update\tShould updates to this table be done via delta updates.\tboolean\tfalse /schema\tAlternative Schema\tAlternative schema for this table (optional). Overrides schema set in endpoint configuration.\tstring /table\tTable\tTable name to materialize to. It will be created by the connector, unless the connector has previously created it.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"AlloyDB","url":"reference/Connectors/materialization-connectors/alloydb/#sample","content":"materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: ghcr.io/estuary/materialize-alloydb:dev config: database: postgres address: 127.0.0.1:5432 password: flow user: flow networkTunnel: sshForwarding: sshEndpoint: ssh://sshUser@&lt;vm-ip-address&gt; privateKey: |2 -----BEGIN RSA PRIVATE KEY----- MIICXAIBAAKBgQCJO7G6R+kv2MMS8Suw21sk2twHg8Vog0fjimEWJEwyAfFM/Toi EJ6r5RTaSvN++/+MPWUll7sUdOOBZr6ErLKLHEt7uXxusAzOjMxFKZpEARMcjwHY v/tN1A2OYU0qay1DOwknEE0i+/Bvf8lMS7VDjHmwRaBtRed/+iAQHf128QIDAQAB AoGAGoOUBP+byAjDN8esv1DCPU6jsDf/Tf//RbEYrOR6bDb/3fYW4zn+zgtGih5t CR268+dwwWCdXohu5DNrn8qV/Awk7hWp18mlcNyO0skT84zvippe+juQMK4hDQNi ywp8mDvKQwpOuzw6wNEitcGDuACx5U/1JEGGmuIRGx2ST5kCQQDsstfWDcYqbdhr 5KemOPpu80OtBYzlgpN0iVP/6XW1e5FCRp2ofQKZYXVwu5txKIakjYRruUiiZTza QeXRPbp3AkEAlGx6wMe1l9UtAAlkgCFYbuxM+eRD4Gg5qLYFpKNsoINXTnlfDry5 +1NkuyiQDjzOSPiLZ4Abpf+a+myjOuNL1wJBAOwkdM6aCVT1J9BkW5mrCLY+PgtV GT80KTY/d6091fBMKhxL5SheJ4SsRYVFtguL2eA7S5xJSpyxkadRzR0Wj3sCQAvA bxO2fE1SRqbbF4cBnOPjd9DNXwZ0miQejWHUwrQO0inXeExNaxhYKQCcnJNUAy1J 6JfAT/AbxeSQF3iBKK8CQAt5r/LLEM1/8ekGOvBh8MAQpWBW771QzHUN84SiUd/q xR9mfItngPwYJ9d/pTO7u9ZUPHEoat8Ave4waB08DsI= -----END RSA PRIVATE KEY----- bindings: - resource: table: ${TABLE_NAME} source: ${PREFIX}/${COLLECTION_NAME}  "},{"title":"Reserved words​","type":1,"pageTitle":"AlloyDB","url":"reference/Connectors/materialization-connectors/alloydb/#reserved-words","content":"PostgreSQL has a list of reserved words that must be quoted in order to be used as an identifier. Flow considers all the reserved words that are marked as &quot;reserved&quot; in any of the columns in the official PostgreSQL documentation. These reserve words are listed in the table below. Flow automatically quotes fields that are in this list. Reserved words abs\tcurrent_transform_group_for_type\tindicator\torder\tsqlexception absolute\tcurrent_user\tinitial\tout\tsqlstate acos\tcursor\tinitially\touter\tsqlwarning action\tcycle\tinner\toutput\tsqrt add\tdatalink\tinout\tover\tstart all\tdate\tinput\toverlaps\tstatic allocate\tday\tinsensitive\toverlay\tstddev_pop alter\tdeallocate\tinsert\tpad\tstddev_samp analyse\tdec\tint\tparameter\tsubmultiset analyze\tdecfloat\tinteger\tpartial\tsubset and\tdecimal\tintersect\tpartition\tsubstring any\tdeclare\tintersection\tpattern\tsubstring_regex are\tdefault\tinterval\tper\tsucceeds array\tdeferrable\tinto\tpercent\tsum array_agg\tdeferred\tis\tpercentile_cont\tsymmetric array_max_cardinality\tdefine\tisnull\tpercentile_disc\tsystem as\tdelete\tisolation\tpercent_rank\tsystem_time asc\tdense_rank\tjoin\tperiod\tsystem_user asensitive\tderef\tjson_array\tpermute\ttable asin\tdesc\tjson_arrayagg\tplacing\ttablesample assertion\tdescribe\tjson_exists\tportion\ttan asymmetric\tdescriptor\tjson_object\tposition\ttanh at\tdeterministic\tjson_objectagg\tposition_regex\ttemporary atan\tdiagnostics\tjson_query\tpower\tthen atomic\tdisconnect\tjson_table\tprecedes\ttime authorization\tdistinct\tjson_table_primitive\tprecision\ttimestamp avg\tdlnewcopy\tjson_value\tprepare\ttimezone_hour begin\tdlpreviouscopy\tkey\tpreserve\ttimezone_minute begin_frame\tdlurlcomplete\tlag\tprimary\tto begin_partition\tdlurlcompleteonly\tlanguage\tprior\ttrailing between\tdlurlcompletewrite\tlarge\tprivileges\ttransaction bigint\tdlurlpath\tlast\tprocedure\ttranslate binary\tdlurlpathonly\tlast_value\tptf\ttranslate_regex bit\tdlurlpathwrite\tlateral\tpublic\ttranslation bit_length\tdlurlscheme\tlead\trange\ttreat blob\tdlurlserver\tleading\trank\ttrigger boolean\tdlvalue\tleft\tread\ttrim both\tdo\tlevel\treads\ttrim_array by\tdomain\tlike\treal\ttrue call\tdouble\tlike_regex\trecursive\ttruncate called\tdrop\tlimit\tref\tuescape cardinality\tdynamic\tlistagg\treferences\tunion cascade\teach\tln\treferencing\tunique cascaded\telement\tlocal\tregr_avgx\tunknown case\telse\tlocaltime\tregr_avgy\tunmatched cast\tempty\tlocaltimestamp\tregr_count\tunnest catalog\tend\tlog\tregr_intercept\tupdate ceil\tend-exec\tlog10\tregr_r2\tupper ceiling\tend_frame\tlower\tregr_slope\tusage char\tend_partition\tmatch\tregr_sxx\tuser character\tequals\tmatches\tregr_sxy\tusing character_length\tescape\tmatch_number\tregr_syy\tvalue char_length\tevery\tmatch_recognize\trelative\tvalues check\texcept\tmax\trelease\tvalue_of classifier\texception\tmeasures\trestrict\tvarbinary clob\texec\tmember\tresult\tvarchar close\texecute\tmerge\treturn\tvariadic coalesce\texists\tmethod\treturning\tvarying collate\texp\tmin\treturns\tvar_pop collation\texternal\tminute\trevoke\tvar_samp collect\textract\tmod\tright\tverbose column\tfalse\tmodifies\trollback\tversioning commit\tfetch\tmodule\trollup\tview concurrently\tfilter\tmonth\trow\twhen condition\tfirst\tmultiset\trows\twhenever connect\tfirst_value\tnames\trow_number\twhere connection\tfloat\tnational\trunning\twidth_bucket constraint\tfloor\tnatural\tsavepoint\twindow constraints\tfor\tnchar\tschema\twith contains\tforeign\tnclob\tscope\twithin continue\tfound\tnew\tscroll\twithout convert\tframe_row\tnext\tsearch\twork copy\tfree\tno\tsecond\twrite corr\tfreeze\tnone\tsection\txml corresponding\tfrom\tnormalize\tseek\txmlagg cos\tfull\tnot\tselect\txmlattributes cosh\tfunction\tnotnull\tsensitive\txmlbinary count\tfusion\tnth_value\tsession\txmlcast covar_pop\tget\tntile\tsession_user\txmlcomment covar_samp\tglobal\tnull\tset\txmlconcat create\tgo\tnullif\tshow\txmldocument cross\tgoto\tnumeric\tsimilar\txmlelement cube\tgrant\toccurrences_regex\tsin\txmlexists cume_dist\tgroup\toctet_length\tsinh\txmlforest current\tgrouping\tof\tsize\txmliterate current_catalog\tgroups\toffset\tskip\txmlnamespaces current_date\thaving\told\tsmallint\txmlparse current_default_transform_group\thold\tomit\tsome\txmlpi current_path\thour\ton\tspace\txmlquery current_role\tidentity\tone\tspecific\txmlserialize current_row\tilike\tonly\tspecifictype\txmltable current_schema\timmediate\topen\tsql\txmltext current_time\timport\toption\tsqlcode\txmlvalidate current_timestamp\tin\tor\tsqlerror\tyear "},{"title":"Elasticsearch","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Elasticsearch/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Elasticsearch","url":"reference/Connectors/materialization-connectors/Elasticsearch/#prerequisites","content":"To use this connector, you'll need: An Elastic cluster with a known endpoint If the cluster is on the Elastic Cloud, you'll need an Elastic user with a role that grants all privileges on indices you plan to materialize to within the cluster. See Elastic's documentation on defining roles andsecurity privileges. At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Configuration​","type":1,"pageTitle":"Elasticsearch","url":"reference/Connectors/materialization-connectors/Elasticsearch/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure an Elasticsearch materialization, which will direct the contents of these Flow collections into Elasticsearch indices. By default, the connector attempts to map each field in the Flow collection to the most appropriate Elasticsearch field type. However, because each JSON field type can map to multiple Elasticsearch field types, you may want to override the defaults. You can configure this by adding field_overrides to the collection's binding in the materialization specification. To do so, provide a JSON pointer to the field in the collection schema, choose the output field type, and specify additional properties, if necessary. "},{"title":"Properties​","type":1,"pageTitle":"Elasticsearch","url":"reference/Connectors/materialization-connectors/Elasticsearch/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/endpoint\tEndpoint\tEndpoint host or URL. If using Elastic Cloud, this follows the format https://CLUSTER_ID.REGION.CLOUD_PLATFORM.DOMAIN:PORT.\tstring\tRequired /password\tPassword\tPassword to connect to the endpoint.\tstring /username\tUsername\tUser to connect to the endpoint.\tstring\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/delta_updates\tDelta updates\tWhether to use standard or delta updates\tboolean\tfalse /field_overrides\tField overrides\tAssign Elastic field type to each collection field.\tarray /field_overrides/-/es_type\tElasticsearch type\tThe overriding Elasticsearch data type of the field.\tobject /field_overrides/-/es_type/date_spec\tDate specifications\tConfiguration for the date field, effective if field_type is 'date'. See Elasticsearch docs.\tobject /field_overrides/-/es_type/date_spec/format\tDate format\tFormat of the date. See Elasticsearch docs.\tstring /field_overrides/-/es_type/field_type\tField type\tThe Elasticsearch field data types. Supported types include: boolean, date, double, geo_point, geo_shape, keyword, long, null, text.\tstring /field_overrides/-/es_type/keyword_spec\tKeyword specifications\tConfiguration for the keyword field, effective if field_type is 'keyword'. See Elasticsearch docs\tobject /field_overrides/-/es_type/keyword_spec/ignore_above\tIgnore above\tStrings longer than the ignore_above setting will not be indexed or stored. See Elasticsearch docs\tinteger /field_overrides/-/es_type/text_spec\tText specifications\tConfiguration for the text field, effective if field_type is 'text'.\tobject /field_overrides/-/es_type/text_spec/dual_keyword\tDual keyword\tWhether or not to specify the field as text/keyword dual field.\tboolean /field_overrides/-/es_type/text_spec/keyword_ignore_above\tIgnore above\tEffective only if Dual Keyword is enabled. Strings longer than the ignore_above setting will not be indexed or stored. See Elasticsearch docs.\tinteger /field_overrides/-/pointer\tPointer\tA '/'-delimited json pointer to the location of the overridden field.\tstring /index\tindex\tName of the ElasticSearch index to store the materialization results.\tstring\tRequired /number_of_replicas\tNumber of replicas\tThe number of replicas in ElasticSearch index. If not set, default to be 0. For single-node clusters, make sure this field is 0, because the Elastic search needs to allocate replicas on different nodes.\tinteger\t0 /number_of_shards\tNumber of shards\tThe number of shards in ElasticSearch index. Must set to be greater than 0.\tinteger\t1 "},{"title":"Sample​","type":1,"pageTitle":"Elasticsearch","url":"reference/Connectors/materialization-connectors/Elasticsearch/#sample","content":"materializations: PREFIX/mat_name: endpoint: connector: # Path to the latest version of the connector, provided as a Docker image image: ghcr.io/estuary/materialize-elasticsearch:dev config: endpoint: https://ec47fc4d2c53414e1307e85726d4b9bb.us-east-1.aws.found.io:9243 username: flow_user password: secret # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: index: last-updated delta_updates: false field_overrides: - pointer: /updated-date es_type: field_type: date date_spec: format: yyyy-MM-dd source: PREFIX/source_collection  "},{"title":"Delta updates​","type":1,"pageTitle":"Elasticsearch","url":"reference/Connectors/materialization-connectors/Elasticsearch/#delta-updates","content":"This connector supports both standard and delta updates. You must choose an option for each binding. Learn more about delta updates and the implications of using each update type. "},{"title":"Firebolt","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Firebolt/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#prerequisites","content":"To use this connector, you'll need: A Firebolt database with at least one engine The engine must be started before creating the materialization.It's important that the engine stays up throughout the lifetime of the materialization. To ensure this is the case, select Edit Engine on your engine. In the engine settings, set Auto-stop engine after to Always On. An S3 bucket where JSON documents will be stored prior to loading The bucket must be in a supported AWS region matching your Firebolt database.The bucket may be public, or may be accessible by an IAM user. To configure your IAM user, see the steps below. At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Setup​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#setup","content":"For non-public buckets, you'll need to configure access in AWS IAM. Follow the Firebolt documentation to set up an IAM policy and role, and add it to the external table definition. Create a new IAM user. During setup: Choose Programmatic (access key) access. This ensures that an access key ID and secret access key are generated. You'll use these to configure the connector. On the Permissions page, choose Attach existing policies directly and attach the policy you created in step 1. After creating the user, download the IAM credentials file. Take note of the access key ID and secret access key and use them to configure the connector. See the Amazon docs if you lose your credentials. "},{"title":"Configuration​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Firebolt materialization, which will direct Flow data to your desired Firebolt tables via an external table. "},{"title":"Properties​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/aws_key_id\tAWS key ID\tAWS access key ID for accessing the S3 bucket.\tstring /aws_region\tAWS region\tAWS region the bucket is in.\tstring /aws_secret_key\tAWS secret access key\tAWS secret key for accessing the S3 bucket.\tstring /database\tDatabase\tName of the Firebolt database.\tstring\tRequired /engine_url\tEngine URL\tEngine URL of the Firebolt database, in the format: &lt;engine-name&gt;.&lt;organization&gt;.&lt;region&gt;.app.firebolt.io.\tstring\tRequired /password\tPassword\tFirebolt password.\tstring\tRequired /s3_bucket\tS3 bucket\tName of S3 bucket where the intermediate files for external table will be stored.\tstring\tRequired /s3_prefix\tS3 prefix\tA prefix for files stored in the bucket.\tstring /username\tUsername\tFirebolt username.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/table\tTable\tName of the Firebolt table to store materialized results in. The external table will be named after this table with an _external suffix.\tstring\tRequired /table_type\tTable type\tType of the Firebolt table to store materialized results in. See the Firebolt docs for more details.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#sample","content":"materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: database: my-db engine_url: my-db-my-engine-name.my-organization.us-east-1.app.firebolt.io password: secret # For public S3 buckets, only the bucket name is required s3_bucket: my-bucket username: firebolt-user # Path to the latest version of the connector, provided as a Docker image image: ghcr.io/estuary/materialize-firebolt:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: table: table-name table_type: fact source: ${PREFIX}/${source_collection}  "},{"title":"Delta updates​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#delta-updates","content":"Firebolt is an insert-only system; it doesn't support updates or deletes. Because of this, the Firebolt connector operates only in delta updates mode. Firebolt stores all deltas — the unmerged collection documents — directly. In some cases, this will affect how materialized views look in Firebolt compared to other systems that use standard updates. "},{"title":"Reserved words​","type":1,"pageTitle":"Firebolt","url":"reference/Connectors/materialization-connectors/Firebolt/#reserved-words","content":"Firebolt has a list of reserved words, which my not be used in identifiers. Collections with field names that include a reserved word will automatically be quoted as part of a Firebolt materialization. Reserved words all\tfalse\tor alter\tfetch\torder and\tfirst\touter array\tfloat\tover between\tfrom\tpartition bigint\tfull\tprecision bool\tgenerate\tprepare boolean\tgroup\tprimary both\thaving\tquarter case\tif\tright cast\tilike\trow char\tin\trows concat\tinner\tsample copy\tinsert\tselect create\tint\tset cross\tinteger\tshow current_date\tintersect\ttext current_timestamp\tinterval\ttime database\tis\ttimestamp date\tisnull\ttop datetime\tjoin\ttrailing decimal\tjoin_type\ttrim delete\tleading\ttrue describe\tleft\ttruncate distinct\tlike\tunion double\tlimit\tunknown_char doublecolon\tlimit_distinct\tunnest dow\tlocaltimestamp\tunterminated_string doy\tlong\tupdate drop\tnatural\tusing empty_identifier\tnext\tvarchar epoch\tnot\tweek except\tnull\twhen execute\tnumeric\twhere exists\toffset\twith explain\ton extract\tonly\t "},{"title":"Google Cloud Pub/Sub","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/google-pubsub/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"reference/Connectors/materialization-connectors/google-pubsub/#prerequisites","content":"To use this connector, you'll need: A Google Cloud project with the Google Pub/Sub API enabled.Access to the project. Different items are required to configure access using OAuth in the Flow web app (recommended), and configuring manually.At least one Flow collection to materialize. tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"OAuth authentication using the Flow web app​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"reference/Connectors/materialization-connectors/google-pubsub/#oauth-authentication-using-the-flow-web-app","content":"OAuth is the simplest authentication method, and is supported in the Flow web app. You'll need: A Google account with the role roles/pubsub.editoror equivalent for the Google Cloud project. See the Google IAM documentation to learn about granting roles. You'll supply this account's username and password to authenticate. "},{"title":"Manual authentication​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"reference/Connectors/materialization-connectors/google-pubsub/#manual-authentication","content":"Manual authentication is the only method supported when using flowctl to develop locally. You'll need: A Google service account with the role roles/pubsub.editoror equivalent for the Google Cloud project. See the Google IAM documentation to learn about granting roles. A JSON key for the service account. See the Google documentation for help creating a new service account and generating its key. "},{"title":"Configuration​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"reference/Connectors/materialization-connectors/google-pubsub/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Google Cloud Pub/Sub materialization, which will direct one or more of your Flow collections to your desired Pub/Sub topics. "},{"title":"Properties​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"reference/Connectors/materialization-connectors/google-pubsub/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/credentials\tAuthentication\tCredentials used to authenticate with Google.\tarray, boolean, null, number, object, string\tRequired /credentials/auth_type\tAuthentication type\tSet to Service for manual authentication, or use OAuth in the web app.\tstring /credentials/credentials_json\tService Account JSON\tThe JSON key of the service account to use for authorization, if configuring manually.\tstring /project_id\tGoogle Cloud Project ID\tName of the project containing the PubSub topics for this materialization.\tstring\tRequired Bindings​ caution PubSub topics need a default subscription; otherwise, delivered messages will be lost. Leave Create with Default Subscription set to the default, true, unless you have a specific reason not to do so. Property\tTitle\tDescription\tType\tRequired/Default/create_default_subscription\tCreate with Default Subscription\tCreate a default subscription when creating the topic. Will be created as &quot;&lt;topic&gt;-sub&quot;. Has no effect if the topic already exists.\tboolean\tRequired, true identifier\tResource Binding Identifier\tOptional identifier for the resource binding if creating a multiplex topic. Included as \\&quot;identifier\\&quot; attribute in published messages if specified.\tstring /topic\tTopic Name\tName of the topic to publish materialized results to.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"reference/Connectors/materialization-connectors/google-pubsub/#sample","content":"This sample reflects the manual authentication method using the CLI. materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: credentials: auth_type: Service credentials_json: {secret} project_id: my_google_cloud_project bindings: - resource: create_default_subscription: true topic: my_new_topic source: ${PREFIX}/${source_collection}  "},{"title":"Multiplex topics​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"reference/Connectors/materialization-connectors/google-pubsub/#multiplex-topics","content":"You can materialize multiple Flow collections to the same Pub/Sub topic. This is known as a multiplex topic. You do so by adding the optional identifier field to the binding configuration. When materializing to a multiplex topic, ensure that: The bindings you want to combine have the same topic name.Each binding pulls from a different Flow collectionEach binding has a unique identifier. It can be anything you'd like. The binding configuration will look similar to: bindings: - resource: identifier: one topic: multiplex-topic source: ${PREFIX}/source_collection_one - resource: identifier: two topic: multiplex-topic source: ${PREFIX}/source_collection_two  "},{"title":"Delta updates​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"reference/Connectors/materialization-connectors/google-pubsub/#delta-updates","content":"Because Google Cloud Pub/Sub is a write-only event-streaming system, this connector uses only delta updates. "},{"title":"Message ordering​","type":1,"pageTitle":"Google Cloud Pub/Sub","url":"reference/Connectors/materialization-connectors/google-pubsub/#message-ordering","content":"Google Cloud Pub/Sub manages message ordering using ordering keys. This connector sets the ordering key of published messages using the Flow collection keyof the documents being being published. Messages are published in order, on a per-key basis. "},{"title":"Apache Parquet in S3","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Parquet/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Apache Parquet in S3","url":"reference/Connectors/materialization-connectors/Parquet/#prerequisites","content":"To use this connector, you'll need: An AWS root or IAM user with access to the S3 bucket. For this user, you'll need the access key and secret access key. See the AWS blog for help finding these credentials.At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Configuration​","type":1,"pageTitle":"Apache Parquet in S3","url":"reference/Connectors/materialization-connectors/Parquet/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a materialization, which will direct the contents of these Flow collections to Parquet files in S3. "},{"title":"Properties​","type":1,"pageTitle":"Apache Parquet in S3","url":"reference/Connectors/materialization-connectors/Parquet/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/advanced Options for advanced users. You should not typically need to modify these.\tobject /advanced/endpoint\tEndpoint\tThe endpoint URI to connect to. Useful if you're connecting to a S3-compatible API that isn't provided by AWS.\tstring /awsAccessKeyId\tAccess Key ID\tAWS credential used to connect to S3.\tstring\tRequired /awsSecretAccessKey\tSecret Access Key\tAWS credential used to connect to S3.\tstring\tRequired /bucket\tBucket\tName of the S3 bucket.\tstring\tRequired /region\tRegion\tThe name of the AWS region where the S3 bucket is located.\tstring\tRequired /uploadIntervalInSeconds\tUpload Interval in Seconds\tTime interval, in seconds, at which to upload data from Flow to S3.\tinteger\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/compressionType\tCompression type\tThe method used to compress data in Parquet.\tstring /pathPrefix\tPath prefix\tThe desired Parquet file path within the bucket as determined by an S3 prefix.\tstring\tRequired The following compression types are supported: snappygziplz4zstd "},{"title":"Sample​","type":1,"pageTitle":"Apache Parquet in S3","url":"reference/Connectors/materialization-connectors/Parquet/#sample","content":"materializations: PREFIX/mat_name: endpoint: connector: config: awsAccessKeyId: AKIAIOSFODNN7EXAMPLE awsSecretAccessKey: wJalrXUtnFEMI/K7MDENG/bPxRfiCYSECRET bucket: my-bucket uploadIntervalInSeconds: 300 # Path to the latest version of the connector, provided as a Docker image image: ghcr.io/estuary/materialize-s3-parquet:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: pathPrefix: /my-prefix source: PREFIX/source_collection  "},{"title":"Delta updates​","type":1,"pageTitle":"Apache Parquet in S3","url":"reference/Connectors/materialization-connectors/Parquet/#delta-updates","content":"This connector uses only delta updates mode. Collection documents are converted to Parquet format and stored in their unmerged state. "},{"title":"Google Sheets","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Google-sheets/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/materialization-connectors/Google-sheets/#prerequisites","content":"To use this connector, you'll need: At least one Flow collection. If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. The URL of a Google spreadsheet that does not contain the output of a prior Flow materialization. caution Materializing data to a spreadsheet that already contains the output of another Flow materialization can result in an error. Use a new spreadsheet for each materialization, or completely clear the output of prior materializations from the spreadsheet before you continue. There are two ways to authenticate with Google when using this connector: signing in with Google through OAuth in the web app, and configuring manually with a Google service account key. OAuth is simpler, and is recommended when using the web app. Only manual configuration is supported using the CLI. Additional prerequisites depend on the authentication method you choose. "},{"title":"OAuth authentication using the Flow web app​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/materialization-connectors/Google-sheets/#oauth-authentication-using-the-flow-web-app","content":"You'll need: The username and password of a Google account with edit access to the destination spreadsheet. "},{"title":"Manual authentication​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/materialization-connectors/Google-sheets/#manual-authentication","content":"You'll need: Google Sheets and Google Drive APIs enabled on your Google account. A Google service account with: A JSON key generated.Edit access to the destination spreadsheet. Follow the steps below to meet these prerequisites: Enable the Google Sheets and Google Drive APIs for the Google project with which your spreadsheet is associated. (Unless you actively develop with Google Cloud, you'll likely just have one option). Create a service account and generate a JSON key. During setup, grant the account the Editor role on your project. You'll copy the contents of the downloaded key file into the Service Account JSON parameter when you configure the connector. Share your Google spreadsheet with the service account, granting edit access. "},{"title":"Configuration​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/materialization-connectors/Google-sheets/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Google Sheets materialization. "},{"title":"Properties​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/materialization-connectors/Google-sheets/#properties","content":"Endpoint​ The following properties reflect the manual authentication method. If you're working in the Flow web app, you can use OAuth, so some of these properties aren't required. Property\tTitle\tDescription\tType\tRequired/Default/credentials\tAuthentication\tCredentials used to authenticate with Google.\tarray, boolean, null, number, object, string\tRequired /credentials/auth_type\tAuthentication type\tSet to Service for manual authentication, or use OAuth in the web app.\tstring /credentials/credentials_json\tService Account JSON\tThe JSON key of the service account to use for authorization, when using the Service authentication method.\tstring\tRequired /spreadsheetURL\tSpreadsheet URL\tURL of the spreadsheet to materialize into, which is shared with the service account.\tstring\tRequired Bindings​ Configure a separate binding for each collection you want to materialize to a sheet. Note that the connector will add an addition column to the beginning of each sheet; this is to track the internal state of the data. Property\tTitle\tDescription\tType\tRequired/Default/sheet\tSheet Name\tName of the spreadsheet sheet to materialize into\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Google Sheets","url":"reference/Connectors/materialization-connectors/Google-sheets/#sample","content":"This sample reflects the manual authentication method using the CLI. materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: credentials: auth_type: Service credentials_json: &lt;secret&gt; spreadsheetURL: `https://docs.google.com/spreadsheets/d/&lt;your_spreadsheet_ID&gt;/edit image: ghcr.io/estuary/materialize-google-sheets:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: sheet: my_sheet source: ${PREFIX}/${source_collection}  "},{"title":"PostgreSQL","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/PostgreSQL/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#prerequisites","content":"To use this connector, you'll need: A Postgres database to which to materialize, and user credentials. The connector will create new tables in the database per your specification. Tables created manually in advance are not supported.At least one Flow collection "},{"title":"Configuration​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Postgres materialization, which will direct one or more of your Flow collections to your desired tables, or views, in the database. "},{"title":"Properties​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/database\tDatabase\tName of the logical database to materialize to.\tstring /address\tAddress\tHost and port of the database\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /schema\tDatabase Schema\tDatabase schema to use for materialized tables (unless overridden within the binding resource configuration) as well as associated materialization metadata tables\tstring\t&quot;public&quot; /user\tUser\tDatabase user to connect as.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/additional_table_create_sql\tAdditional Table Create SQL\tAdditional SQL statement(s) to be run in the same transaction that creates the table.\tstring /delta_updates\tDelta Update\tShould updates to this table be done via delta updates.\tboolean\tfalse /schema\tAlternative Schema\tAlternative schema for this table (optional). Overrides schema set in endpoint configuration.\tstring /table\tTable\tTable name to materialize to. It will be created by the connector, unless the connector has previously created it.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#sample","content":"materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: ghcr.io/estuary/materialize-postgres:dev config: database: flow address: localhost:5432 password: flow user: flow bindings: - resource: table: ${TABLE_NAME} source: ${PREFIX}/${COLLECTION_NAME}  "},{"title":"PostgreSQL on managed cloud platforms​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#postgresql-on-managed-cloud-platforms","content":"In addition to standard PostgreSQL, this connector supports cloud-based PostgreSQL instances. To connect securely, you must use an SSH tunnel. Google Cloud Platform, Amazon Web Service, and Microsoft Azure are currently supported. You may use other cloud platforms, but Estuary doesn't guarantee performance. "},{"title":"Setup​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#setup","content":"You must configure your database to allow connections from Estuary. The recommended method is to whitelist Estuary Flow's IP address. Amazon RDS and Amazon Aurora: Edit the VPC security group associated with your database instance, or create a new VPC security group and associate it with the database instance. Modify the instance, choosing Publicly accessible in the Connectivity settings. Refer to the steps in the Amazon documentation. Create a new inbound rule and a new outbound rule that allow all traffic from the IP address 34.121.207.128. Google Cloud SQL: Enable public IP on your database and add 34.121.207.128 as an authorized IP address. Azure Database For PostgreSQL: Create a new firewall rule that grants access to the IP address 34.121.207.128. Alternatively, you can allow secure connections via SSH tunneling. To do so: Refer to the guide to configure an SSH server on the cloud platform of your choice. Configure your connector as described in the configuration section above, with the additional of the networkTunnel stanza to enable the SSH tunnel, if using. See Connecting to endpoints on secure networksfor additional details and a sample. Configuration Tip To configure the connector, you must specify the database address in the format host:port. You can find the host and port in the following locations in each platform's console: Amazon RDS and Amazon Aurora: host as Endpoint; port as Port.Google Cloud SQL: host as Private IP Address; port is always 5432. You may need to configure private IP on your database.Azure Database: host as Server Name; port under Connection Strings (usually 5432).TimescaleDB: host as Host; port as Port. "},{"title":"Delta updates​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#delta-updates","content":"This connector supports both standard (merge) and delta updates. The default is to use standard updates. "},{"title":"Reserved words​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#reserved-words","content":"PostgreSQL has a list of reserved words that must be quoted in order to be used as an identifier. Flow considers all the reserved words that are marked as &quot;reserved&quot; in any of the columns in the official PostgreSQL documentation. These reserve words are listed in the table below. Flow automatically quotes fields that are in this list. Reserved words abs\tcurrent_transform_group_for_type\tindicator\torder\tsqlexception absolute\tcurrent_user\tinitial\tout\tsqlstate acos\tcursor\tinitially\touter\tsqlwarning action\tcycle\tinner\toutput\tsqrt add\tdatalink\tinout\tover\tstart all\tdate\tinput\toverlaps\tstatic allocate\tday\tinsensitive\toverlay\tstddev_pop alter\tdeallocate\tinsert\tpad\tstddev_samp analyse\tdec\tint\tparameter\tsubmultiset analyze\tdecfloat\tinteger\tpartial\tsubset and\tdecimal\tintersect\tpartition\tsubstring any\tdeclare\tintersection\tpattern\tsubstring_regex are\tdefault\tinterval\tper\tsucceeds array\tdeferrable\tinto\tpercent\tsum array_agg\tdeferred\tis\tpercentile_cont\tsymmetric array_max_cardinality\tdefine\tisnull\tpercentile_disc\tsystem as\tdelete\tisolation\tpercent_rank\tsystem_time asc\tdense_rank\tjoin\tperiod\tsystem_user asensitive\tderef\tjson_array\tpermute\ttable asin\tdesc\tjson_arrayagg\tplacing\ttablesample assertion\tdescribe\tjson_exists\tportion\ttan asymmetric\tdescriptor\tjson_object\tposition\ttanh at\tdeterministic\tjson_objectagg\tposition_regex\ttemporary atan\tdiagnostics\tjson_query\tpower\tthen atomic\tdisconnect\tjson_table\tprecedes\ttime authorization\tdistinct\tjson_table_primitive\tprecision\ttimestamp avg\tdlnewcopy\tjson_value\tprepare\ttimezone_hour begin\tdlpreviouscopy\tkey\tpreserve\ttimezone_minute begin_frame\tdlurlcomplete\tlag\tprimary\tto begin_partition\tdlurlcompleteonly\tlanguage\tprior\ttrailing between\tdlurlcompletewrite\tlarge\tprivileges\ttransaction bigint\tdlurlpath\tlast\tprocedure\ttranslate binary\tdlurlpathonly\tlast_value\tptf\ttranslate_regex bit\tdlurlpathwrite\tlateral\tpublic\ttranslation bit_length\tdlurlscheme\tlead\trange\ttreat blob\tdlurlserver\tleading\trank\ttrigger boolean\tdlvalue\tleft\tread\ttrim both\tdo\tlevel\treads\ttrim_array by\tdomain\tlike\treal\ttrue call\tdouble\tlike_regex\trecursive\ttruncate called\tdrop\tlimit\tref\tuescape cardinality\tdynamic\tlistagg\treferences\tunion cascade\teach\tln\treferencing\tunique cascaded\telement\tlocal\tregr_avgx\tunknown case\telse\tlocaltime\tregr_avgy\tunmatched cast\tempty\tlocaltimestamp\tregr_count\tunnest catalog\tend\tlog\tregr_intercept\tupdate ceil\tend-exec\tlog10\tregr_r2\tupper ceiling\tend_frame\tlower\tregr_slope\tusage char\tend_partition\tmatch\tregr_sxx\tuser character\tequals\tmatches\tregr_sxy\tusing character_length\tescape\tmatch_number\tregr_syy\tvalue char_length\tevery\tmatch_recognize\trelative\tvalues check\texcept\tmax\trelease\tvalue_of classifier\texception\tmeasures\trestrict\tvarbinary clob\texec\tmember\tresult\tvarchar close\texecute\tmerge\treturn\tvariadic coalesce\texists\tmethod\treturning\tvarying collate\texp\tmin\treturns\tvar_pop collation\texternal\tminute\trevoke\tvar_samp collect\textract\tmod\tright\tverbose column\tfalse\tmodifies\trollback\tversioning commit\tfetch\tmodule\trollup\tview concurrently\tfilter\tmonth\trow\twhen condition\tfirst\tmultiset\trows\twhenever connect\tfirst_value\tnames\trow_number\twhere connection\tfloat\tnational\trunning\twidth_bucket constraint\tfloor\tnatural\tsavepoint\twindow constraints\tfor\tnchar\tschema\twith contains\tforeign\tnclob\tscope\twithin continue\tfound\tnew\tscroll\twithout convert\tframe_row\tnext\tsearch\twork copy\tfree\tno\tsecond\twrite corr\tfreeze\tnone\tsection\txml corresponding\tfrom\tnormalize\tseek\txmlagg cos\tfull\tnot\tselect\txmlattributes cosh\tfunction\tnotnull\tsensitive\txmlbinary count\tfusion\tnth_value\tsession\txmlcast covar_pop\tget\tntile\tsession_user\txmlcomment covar_samp\tglobal\tnull\tset\txmlconcat create\tgo\tnullif\tshow\txmldocument cross\tgoto\tnumeric\tsimilar\txmlelement cube\tgrant\toccurrences_regex\tsin\txmlexists cume_dist\tgroup\toctet_length\tsinh\txmlforest current\tgrouping\tof\tsize\txmliterate current_catalog\tgroups\toffset\tskip\txmlnamespaces current_date\thaving\told\tsmallint\txmlparse current_default_transform_group\thold\tomit\tsome\txmlpi current_path\thour\ton\tspace\txmlquery current_role\tidentity\tone\tspecific\txmlserialize current_row\tilike\tonly\tspecifictype\txmltable current_schema\timmediate\topen\tsql\txmltext current_time\timport\toption\tsqlcode\txmlvalidate current_timestamp\tin\tor\tsqlerror\tyear "},{"title":"Changelog​","type":1,"pageTitle":"PostgreSQL","url":"reference/Connectors/materialization-connectors/PostgreSQL/#changelog","content":"The changelog includes a list of breaking changes made to this connector. Backwards-compatible changes are not listed. Proceed with caution when editing materializations created with previous versions of this connector; editing always upgrades your materialization to the latest connector version. V4: 2022-11-30​ This version includes breaking changes to materialized table columns. These provide more consistent column names and types, but tables created from previous versions of the connector may not be compatible with this version. Capitalization is now preserved when fields in Flow are converted to Postgres column names. Previously, fields containing uppercase letters were converted to lowercase. Field names and values of types date, duration, ipv4, ipv6, macaddr, macaddr8, and time are now converted into their corresponding Postgres types. Previously, only date-time was converted, and all others were materialized as strings. "},{"title":"Rockset","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Rockset/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#prerequisites","content":"To use this connector, you'll need: A Rockset API key generated The API key must have the Member or Admin role. A Rockset workspace Optional; if none exist, one will be created by the connector. A Rockset collection Optional; if none exist, one will be created by the connector. At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Configuration​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Rockset materialization, which will direct one or more of your Flow collections to your desired Rockset collections. "},{"title":"Properties​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/api_key\tRockset API Key\tThe key used to authenticate to the Rockset API. Must have role of admin or member.\tstring\tRequired /region_base_url\tRegion Base URL\tThe base URL to connect to your Rockset deployment. Example: api.usw2a1.rockset.com (do not include the protocol). See supported options and how to find yours.\tstring\tRequired Bindings​ The binding configuration includes the optional Advanced collection settings section. These settings can help optimize your output Rockset collections: Clustering fields: You can specify clustering fields for your Rockset collection's columnar index to help optimize specific query patterns. See the Rockset docs for more information.Retention period: Amount of time before data is purged, in seconds. A low value will keep the amount of data indexed in Rockset smaller. Property\tTitle\tDescription\tType\tRequired/Default/advancedCollectionSettings\tAdvanced Collection Settings object /advancedCollectionSettings/clustering_key\tClustering Key\tList of clustering fields\tarray /advancedCollectionSettings/clustering_key/-/field_name\tField Name\tThe name of a field\tstring /advancedCollectionSettings/retention_secs\tRetention Period\tNumber of seconds after which data is purged based on event time\tinteger /collection\tRockset Collection\tThe name of the Rockset collection (will be created if it does not exist)\tstring\tRequired /workspace\tWorkspace\tThe name of the Rockset workspace (will be created if it does not exist)\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#sample","content":"materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: region_base_url: api.usw2a1.rockset.com api_key: supersecret # Path to the latest version of the connector, provided as a Docker image image: ghcr.io/estuary/materialize-rockset:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: workspace: ${namespace_name} collection: ${table_name} source: ${PREFIX}/${source_collection}  "},{"title":"Delta updates and reduction strategies​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#delta-updates-and-reduction-strategies","content":"The Rockset connector operates only in delta updates mode. This means that Rockset, rather than Flow, performs the document merge. In some cases, this will affect how materialized views look in Rockset compared to other systems that use standard updates. Rockset merges documents by the key defined in the Flow collection schema, and always uses the semantics of RFC 7396 - JSON merge. This differs from how Flow would reduce documents, most notably in that Rockset will not honor any reduction strategies defined in your Flow schema. For consistent output of a given collection across Rockset and other materialization endpoints, it's important that that collection's reduction annotations in Flow mirror Rockset's semantics. To accomplish this, ensure that your collection schema has the following data reductions defined in its schema: A top-level reduction strategy of mergeA strategy of lastWriteWins for all nested values (this is the default) "},{"title":"Changelog​","type":1,"pageTitle":"Rockset","url":"reference/Connectors/materialization-connectors/Rockset/#changelog","content":"The changelog includes a list of breaking changes made to this connector. Backwards-compatible changes are not listed. Proceed with caution when editing materializations created with previous versions of this connector; editing always upgrades your materialization to the latest connector version. V2: 2022-12-06​ Region Base URL was added and is now required as part of the endpoint configuration.Event Time fields and the Insert Only option were removed from the advanced collection settings. "},{"title":"Snowflake","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/Snowflake/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#prerequisites","content":"To use this connector, you'll need: A Snowflake account that includes: A target database, to which you'll materialize dataA schema — a logical grouping of database objects — within the target databaseA virtual warehouseA user with a role assigned that grants the appropriate access levels to these resources. See the script below for details. Know your Snowflake account's host URL. This is formatted using your Snowflake account identifier, for example, orgname-accountname.snowflakecomputing.com.At least one Flow collection tip If you haven't yet captured your data from its external source, start at the beginning of the guide to create a dataflow. You'll be referred back to this connector-specific documentation at the appropriate steps. "},{"title":"Setup​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#setup","content":"To meet the prerequisites, copy and paste the following script into the Snowflake SQL editor, replacing the variable names in the first six lines. If you'd like to use an existing database, warehouse, and/or schema, be sure to setdatabase_name, warehouse_name, and estuary_schema accordingly. If you specify a new name, the script will create the item for you. You can set estuary_role, estuary_user, and estuary_password to whatever you'd like. Check the All Queries check box, and click Run. set database_name = 'ESTUARY_DB'; set warehouse_name = 'ESTUARY_WH'; set estuary_role = 'ESTUARY_ROLE'; set estuary_user = 'ESTUARY_USER'; set estuary_password = 'secret'; set estuary_schema = 'ESTUARY_SCHEMA'; -- create role and schema for Estuary create role if not exists identifier($estuary_role); grant role identifier($estuary_role) to role SYSADMIN; -- Create snowflake DB create database if not exists identifier($database_name); use database identifier($database_name); create schema if not exists identifier($estuary_schema); -- create a user for Estuary create user if not exists identifier($estuary_user) password = $estuary_password default_role = $estuary_role default_warehouse = $warehouse_name; grant role identifier($estuary_role) to user identifier($estuary_user); grant all on schema identifier($estuary_schema) to identifier($estuary_role); -- create a warehouse for estuary create warehouse if not exists identifier($warehouse_name) warehouse_size = xsmall warehouse_type = standard auto_suspend = 60 auto_resume = true initially_suspended = true; -- grant Estuary role access to warehouse grant USAGE on warehouse identifier($warehouse_name) to role identifier($estuary_role); -- grant Estuary access to database grant CREATE SCHEMA, MONITOR, USAGE on database identifier($database_name) to role identifier($estuary_role); -- change role to ACCOUNTADMIN for STORAGE INTEGRATION support to Estuary (only needed for Snowflake on GCP) use role ACCOUNTADMIN; grant CREATE INTEGRATION on account to role identifier($estuary_role); use role sysadmin; COMMIT;  "},{"title":"Configuration​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a Snowflake materialization, which will direct one or more of your Flow collections to new Snowflake tables. "},{"title":"Properties​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/account\tAccount\tThe Snowflake account identifier\tstring\tRequired /database\tDatabase\tName of the Snowflake database to which to materialize\tstring\tRequired /host\tHost URL\tThe Snowflake Host used for the connection. Example: orgname-accountname.snowflakecomputing.com (do not include the protocol).\tstring\tRequired /password\tPassword\tSnowflake user password\tstring\tRequired /role\tRole\tRole assigned to the user\tstring /schema\tSchema\tSnowflake schema within the database to which to materialize\tstring\tRequired /user\tUser\tSnowflake username\tstring\tRequired /warehouse\tWarehouse\tName of the data warehouse that contains the database\tstring\t Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/delta_updates\tDelta updates\tWhether to use standard or delta updates\tboolean /table\tTable\tTable name\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#sample","content":" materializations: ${PREFIX}/${mat_name}: endpoint: connector: config: account: acmeCo database: acmeCo_db host: orgname-accountname.snowflakecomputing.com password: secret schema: acmeCo_flow_schema user: snowflake_user warehouse: acmeCo_warehouse image: ghcr.io/estuary/materialize-snowflake:dev # If you have multiple collections you need to materialize, add a binding for each one # to ensure complete data flow-through bindings: - resource: table: ${table_name} source: ${PREFIX}/${source_collection}  "},{"title":"Delta updates​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#delta-updates","content":"This connector supports both standard (merge) and delta updates. The default is to use standard updates. Enabling delta updates will prevent Flow from querying for documents in your Snowflake table, which can reduce latency and costs for large datasets. If you're certain that all events will have unique keys, enabling delta updates is a simple way to improve performance with no effect on the output. However, enabling delta updates is not suitable for all workflows, as the resulting table in Snowflake won't be fully reduced. You can enable delta updates on a per-binding basis:  bindings: - resource: table: ${table_name} delta_updates: true source: ${PREFIX}/${source_collection}  "},{"title":"Performance considerations​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#performance-considerations","content":""},{"title":"Optimizing performance for standard updates​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#optimizing-performance-for-standard-updates","content":"When using standard updates for a large dataset, the collection key you choose can have a significant impact on materialization performance and efficiency. Snowflake uses micro partitions to physically arrange data within tables. Each micro partition includes metadata, such as the minimum and maximum values for each column. If you choose a collection key that takes advantage of this metadata to help Snowflake prune irrelevant micro partitions, you'll see dramatically better performance. For example, if you materialize a collection with a key of /user_id, it will tend to perform far worse than a materialization of /date, /user_id. This is because most materializations tend to be roughly chronological over time, and that means that data is written to Snowflake in roughly /date order. This means that updates of keys /date, /user_id will need to physically read far fewer rows as compared to a key like /user_id, because those rows will tend to live in the same micro-partitions, and Snowflake is able to cheaply prune micro-partitions that aren't relevant to the transaction. "},{"title":"Reducing active warehouse time​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#reducing-active-warehouse-time","content":"Snowflake compute is priced per second of activity, with a minimum of 60 seconds. Inactive warehouses don't incur charges. To keep costs down, you'll want to minimize your warehouse's active time. Like other Estuary connectors, this is a real-time connector that materializes documents using continuous transactions. Every time a Flow materialization commits a transaction, your warehouse becomes active. If your source data collection or collections don't change much, this shouldn't cause an issue; Flow only commits transactions when data has changed. However, if your source data is frequently updated, your materialization may have frequent transactions that result in excessive active time in the warehouse, and thus a higher bill from Snowflake. To mitigate this, we recommend a two-pronged approach: Configure your Snowflake warehouse to auto-suspend after 60 seconds. This ensures that for each transaction, you'll only be charged for one minute of compute, Snowflake's smallest granularity. Use a query like the one shown below, being sure to substitute your warehouse name: ALTER WAREHOUSE ESTUARY_WH SET auto_suspend = 60; Configure the materialization's minimum transaction duration to as long as 30 minutes. This ensures that Flow will wait at least 30 minutes between new data commits to Snowflake. If no new data appears within the 30-minute window, the interval will be longer. You can change this setting in the materialization's shard configurationas described below. For example, if you set the warehouse to auto-suspend after 60 seconds and set the materialization's minimum transaction duration to 30 minutes, you'll never incur more than 48 minutes per day of active time in the warehouse. Adding the shard configuration​ Beta UI controls for this workflow will be added to the Flow web app soon. For now, you must edit the materialization specification manually, either in the web app or using the CLI. Using the Flow web application or the flowctl CLI, create a draft materialization as you normally would. If using the web app, input the required values and click Discover Endpoint.If using the flowctl, create your materialization specification manually. Add the shards configuration to the materialization specification at the same indentation level as endpoint and bindings. Set the minTxnDuration property as high as 30m (we recommend between 15m and 30m for significant cost savings). In the web app, you do this in the Catalog Editor. shards: minTxnDuration: 30m Continue to test, save, and publish the materialization as usual. "},{"title":"Reserved words​","type":1,"pageTitle":"Snowflake","url":"reference/Connectors/materialization-connectors/Snowflake/#reserved-words","content":"Snowflake has a list of reserved words that must be quoted in order to be used as an identifier. Flow automatically quotes fields that are in the reserved words list. You can find this list in Snowflake's documentation here and in the table below. caution In Snowflake, objects created with quoted identifiers must always be referenced exactly as created, including the quotes. Otherwise, SQL statements and queries can result in errors. See the Snowflake docs. Reserved words account\tfrom\tqualify all\tfull\tregexp alter\tgrant\trevoke and\tgroup\tright any\tgscluster\trlike as\thaving\trow between\tilike\trows by\tin\tsample case\tincrement\tschema cast\tinner\tselect check\tinsert\tset column\tintersect\tsome connect\tinto\tstart connection\tis\ttable constraint\tissue\ttablesample create\tjoin\tthen cross\tlateral\tto current\tleft\ttrigger current_date\tlike\ttrue current_time\tlocaltime\ttry_cast current_timestamp\tlocaltimestamp\tunion current_user\tminus\tunique database\tnatural\tupdate delete\tnot\tusing distinct\tnull\tvalues drop\tof\tview else\ton\twhen exists\tor\twhenever false\torder\twhere following\torganization\twith for  "},{"title":"TimescaleDB","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-connectors/timescaledb/","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"TimescaleDB","url":"reference/Connectors/materialization-connectors/timescaledb/#prerequisites","content":"To use this connector, you'll need: A TimescaleDB database to which to materialize. Know your user credentials, and the host and port. If using Timescale Cloud, this information is available on your console, on the Connection info pane. At least one Flow collection. "},{"title":"Configuration​","type":1,"pageTitle":"TimescaleDB","url":"reference/Connectors/materialization-connectors/timescaledb/#configuration","content":"To use this connector, begin with data in one or more Flow collections. Use the below properties to configure a TimescaleDB materialization, which will direct one or more of your Flow collections to your desired tables, or views, in the database. The connector will create new tables in the database per your specification. Tables created manually in advance are not supported. "},{"title":"Properties​","type":1,"pageTitle":"TimescaleDB","url":"reference/Connectors/materialization-connectors/timescaledb/#properties","content":"Endpoint​ Property\tTitle\tDescription\tType\tRequired/Default/database\tDatabase\tName of the logical database to materialize to.\tstring /address\tAddress\tHost and port of the database\tstring\tRequired /password\tPassword\tPassword for the specified database user.\tstring\tRequired /schema\tDatabase Schema\tDatabase schema to use for materialized tables (unless overridden within the binding resource configuration) as well as associated materialization metadata tables\tstring\t&quot;public&quot; /user\tUser\tDatabase user to connect as.\tstring\tRequired Bindings​ Property\tTitle\tDescription\tType\tRequired/Default/additional_table_create_sql\tAdditional Table Create SQL\tAdditional SQL statement(s) to be run in the same transaction that creates the table. Useful for creating Hypertables.\tstring /delta_updates\tDelta Update\tShould updates to this table be done via delta updates.\tboolean\tfalse /schema\tAlternative Schema\tAlternative schema for this table (optional). Overrides schema set in endpoint configuration.\tstring /table\tTable\tTable name to materialize to. It will be created by the connector, unless the connector has previously created it.\tstring\tRequired "},{"title":"Sample​","type":1,"pageTitle":"TimescaleDB","url":"reference/Connectors/materialization-connectors/timescaledb/#sample","content":"materializations: ${PREFIX}/${mat_name}: endpoint: connector: image: ghcr.io/estuary/materialize-timescaledb:dev config: database: flow address: xxxxxxxxxx.xxxxxxxxxx.tsdb.cloud.timescale.com:01234 password: flow user: flow bindings: - resource: table: ${TABLE_NAME} source: ${PREFIX}/${COLLECTION_NAME}  "},{"title":"Creating TimescaleDB hypertables​","type":1,"pageTitle":"TimescaleDB","url":"reference/Connectors/materialization-connectors/timescaledb/#creating-timescaledb-hypertables","content":"Hypertables are PostgreSQL tables in TimescaleDB optimized for time-series data. They exist alongside regular PostgreSQL tables. You can add Hypertables to your materialization on a per-binding basis by adding the optional /additional_table_create_sql field to each binding configuration. Your SQL statement should take the following format: SELECT create_hypertable('table', 'timestamp_column');  Where 'table' matches the value for the field /table in that binding, and 'timestamp_column' is the name of the table column containing its time values. For example, materializing the Flow collection acmeCo/my_time_series would produce a table called 'my_time_series'. Assuming its timestamp value is in the field 'time', the binding configuration would look like: bindings: - resource: additional_table_create_sql: 'SELECT create_hypertable('my_time_series', 'time');' table: my_time_series source: acmeCo/my_time_series  "},{"title":"Delta updates​","type":1,"pageTitle":"TimescaleDB","url":"reference/Connectors/materialization-connectors/timescaledb/#delta-updates","content":"This connector supports both standard (merge) and delta updates. The default is to use standard updates. "},{"title":"Reserved words​","type":1,"pageTitle":"TimescaleDB","url":"reference/Connectors/materialization-connectors/timescaledb/#reserved-words","content":"PostgreSQL (and thus TimescaleDB) has a list of reserved words that must be quoted in order to be used as an identifier. Flow considers all the reserved words that are marked as &quot;reserved&quot; in any of the columns in the official PostgreSQL documentation. These reserve words are listed in the table below. Flow automatically quotes fields that are in this list. Reserved words abs\tcurrent_transform_group_for_type\tindicator\torder\tsqlexception absolute\tcurrent_user\tinitial\tout\tsqlstate acos\tcursor\tinitially\touter\tsqlwarning action\tcycle\tinner\toutput\tsqrt add\tdatalink\tinout\tover\tstart all\tdate\tinput\toverlaps\tstatic allocate\tday\tinsensitive\toverlay\tstddev_pop alter\tdeallocate\tinsert\tpad\tstddev_samp analyse\tdec\tint\tparameter\tsubmultiset analyze\tdecfloat\tinteger\tpartial\tsubset and\tdecimal\tintersect\tpartition\tsubstring any\tdeclare\tintersection\tpattern\tsubstring_regex are\tdefault\tinterval\tper\tsucceeds array\tdeferrable\tinto\tpercent\tsum array_agg\tdeferred\tis\tpercentile_cont\tsymmetric array_max_cardinality\tdefine\tisnull\tpercentile_disc\tsystem as\tdelete\tisolation\tpercent_rank\tsystem_time asc\tdense_rank\tjoin\tperiod\tsystem_user asensitive\tderef\tjson_array\tpermute\ttable asin\tdesc\tjson_arrayagg\tplacing\ttablesample assertion\tdescribe\tjson_exists\tportion\ttan asymmetric\tdescriptor\tjson_object\tposition\ttanh at\tdeterministic\tjson_objectagg\tposition_regex\ttemporary atan\tdiagnostics\tjson_query\tpower\tthen atomic\tdisconnect\tjson_table\tprecedes\ttime authorization\tdistinct\tjson_table_primitive\tprecision\ttimestamp avg\tdlnewcopy\tjson_value\tprepare\ttimezone_hour begin\tdlpreviouscopy\tkey\tpreserve\ttimezone_minute begin_frame\tdlurlcomplete\tlag\tprimary\tto begin_partition\tdlurlcompleteonly\tlanguage\tprior\ttrailing between\tdlurlcompletewrite\tlarge\tprivileges\ttransaction bigint\tdlurlpath\tlast\tprocedure\ttranslate binary\tdlurlpathonly\tlast_value\tptf\ttranslate_regex bit\tdlurlpathwrite\tlateral\tpublic\ttranslation bit_length\tdlurlscheme\tlead\trange\ttreat blob\tdlurlserver\tleading\trank\ttrigger boolean\tdlvalue\tleft\tread\ttrim both\tdo\tlevel\treads\ttrim_array by\tdomain\tlike\treal\ttrue call\tdouble\tlike_regex\trecursive\ttruncate called\tdrop\tlimit\tref\tuescape cardinality\tdynamic\tlistagg\treferences\tunion cascade\teach\tln\treferencing\tunique cascaded\telement\tlocal\tregr_avgx\tunknown case\telse\tlocaltime\tregr_avgy\tunmatched cast\tempty\tlocaltimestamp\tregr_count\tunnest catalog\tend\tlog\tregr_intercept\tupdate ceil\tend-exec\tlog10\tregr_r2\tupper ceiling\tend_frame\tlower\tregr_slope\tusage char\tend_partition\tmatch\tregr_sxx\tuser character\tequals\tmatches\tregr_sxy\tusing character_length\tescape\tmatch_number\tregr_syy\tvalue char_length\tevery\tmatch_recognize\trelative\tvalues check\texcept\tmax\trelease\tvalue_of classifier\texception\tmeasures\trestrict\tvarbinary clob\texec\tmember\tresult\tvarchar close\texecute\tmerge\treturn\tvariadic coalesce\texists\tmethod\treturning\tvarying collate\texp\tmin\treturns\tvar_pop collation\texternal\tminute\trevoke\tvar_samp collect\textract\tmod\tright\tverbose column\tfalse\tmodifies\trollback\tversioning commit\tfetch\tmodule\trollup\tview concurrently\tfilter\tmonth\trow\twhen condition\tfirst\tmultiset\trows\twhenever connect\tfirst_value\tnames\trow_number\twhere connection\tfloat\tnational\trunning\twidth_bucket constraint\tfloor\tnatural\tsavepoint\twindow constraints\tfor\tnchar\tschema\twith contains\tforeign\tnclob\tscope\twithin continue\tfound\tnew\tscroll\twithout convert\tframe_row\tnext\tsearch\twork copy\tfree\tno\tsecond\twrite corr\tfreeze\tnone\tsection\txml corresponding\tfrom\tnormalize\tseek\txmlagg cos\tfull\tnot\tselect\txmlattributes cosh\tfunction\tnotnull\tsensitive\txmlbinary count\tfusion\tnth_value\tsession\txmlcast covar_pop\tget\tntile\tsession_user\txmlcomment covar_samp\tglobal\tnull\tset\txmlconcat create\tgo\tnullif\tshow\txmldocument cross\tgoto\tnumeric\tsimilar\txmlelement cube\tgrant\toccurrences_regex\tsin\txmlexists cume_dist\tgroup\toctet_length\tsinh\txmlforest current\tgrouping\tof\tsize\txmliterate current_catalog\tgroups\toffset\tskip\txmlnamespaces current_date\thaving\told\tsmallint\txmlparse current_default_transform_group\thold\tomit\tsome\txmlpi current_path\thour\ton\tspace\txmlquery current_role\tidentity\tone\tspecific\txmlserialize current_row\tilike\tonly\tspecifictype\txmltable current_schema\timmediate\topen\tsql\txmltext current_time\timport\toption\tsqlcode\txmlvalidate current_timestamp\tin\tor\tsqlerror\tyear "},{"title":"Changelog​","type":1,"pageTitle":"TimescaleDB","url":"reference/Connectors/materialization-connectors/timescaledb/#changelog","content":"The changelog includes a list of breaking changes made to this connector. Backwards-compatible changes are not listed. Proceed with caution when editing materializations created with previous versions of this connector; editing always upgrades your materialization to the latest connector version. V4: 2022-11-30​ This version includes breaking changes to materialized table columns. These provide more consistent column names and types, but tables created from previous versions of the connector may not be compatible with this version. Capitalization is now preserved when fields in Flow are converted to Postgres (TimescaleDB) column names. Previously, fields containing uppercase letters were converted to lowercase. Field names and values of types date, duration, ipv4, ipv6, macaddr, macaddr8, and time are now converted into their corresponding Postgres (TimescaleDB) types. Previously, only date-time was converted, and all others were materialized as strings. "},{"title":"Materialization Protocol","type":0,"sectionRef":"#","url":"reference/Connectors/materialization-protocol/","content":"","keywords":""},{"title":"Sequence Diagram​","type":1,"pageTitle":"Materialization Protocol","url":"reference/Connectors/materialization-protocol/#sequence-diagram","content":"As a convention and to reduce ambiguity, message types from the Runtime are named in an imperative fashion (Load), while responses from the driver always have a past-tense name (Loaded): sequenceDiagram Runtime-&gt;&gt;Driver: Open{MaterializationSpec, driverCP} Note right of Driver: Connect to endpoint.&lt;br/&gt;Optionally fetch last-committed&lt;br/&gt;runtime checkpoint. Driver-&gt;&gt;Runtime: Opened{runtimeCP} Note over Runtime, Driver: One-time initialization ☝️.&lt;br/&gt; 👇 Repeats for each transaction. Note left of Runtime: Prior txn commits&lt;br/&gt;to recovery log. Note right of Driver: Prior txn commits to DB&lt;br/&gt;(where applicable). Runtime-&gt;&gt;Driver: Acknowledge Note right of Runtime: Acknowledged MAY be sent&lt;br/&gt;before Acknowledge. Note right of Driver: MAY perform an idempotent&lt;br/&gt;apply of last txn. Note left of Runtime: Runtime does NOT await&lt;br/&gt;Acknowledged before&lt;br/&gt;proceeding to send Load. Driver-&gt;&gt;Runtime: Acknowledged Note left of Runtime: Runtime may now finalize&lt;br/&gt;a pipelined transaction. Note over Runtime, Driver: End of Acknowledge phase. Runtime-&gt;&gt;Driver: Load&lt;A&gt; Note left of Runtime: Load keys may&lt;br/&gt; not exist (yet). Runtime-&gt;&gt;Driver: Load&lt;B&gt; Note right of Driver: MAY evaluate Load immediately,&lt;br/&gt;or stage for deferred retrieval. Driver-&gt;&gt;Runtime: Loaded&lt;A&gt; Runtime-&gt;&gt;Driver: Load&lt;C&gt; Runtime-&gt;&gt;Driver: Flush Driver-&gt;&gt;Runtime: Loaded&lt;C&gt; Note right of Driver: Omits Loaded for keys&lt;br/&gt;that don't exist. Driver-&gt;&gt;Runtime: Flushed Note left of Runtime: All existing keys&lt;br/&gt;have been retrieved. Note over Runtime, Driver: End of Load phase. Runtime-&gt;&gt;Driver: Store&lt;X&gt; Runtime-&gt;&gt;Driver: Store&lt;Y&gt; Runtime-&gt;&gt;Driver: Store&lt;Z&gt; Runtime-&gt;&gt;Driver: StartCommit{runtimeCP} Note right of Driver: * Completes all Store processing.&lt;br/&gt;* MAY include runtimeCP in DB txn. Note right of Driver: Commit to DB&lt;br/&gt;now underway. Driver-&gt;&gt;Runtime: StartedCommit{driverCP} Note left of Runtime: Begins commit to&lt;br/&gt; recovery log. Note over Runtime, Driver: End of Store phase. Loops around&lt;br/&gt;to Acknowledge &lt;=&gt; Acknowledged. "},{"title":"Exactly-Once Semantics​","type":1,"pageTitle":"Materialization Protocol","url":"reference/Connectors/materialization-protocol/#exactly-once-semantics","content":"The central tenant of transactional materializations is this: there is a consumption checkpoint, and there is a state of the view. As the materialization progresses, both the checkpoint and the view state will change. Updates to the checkpoint and to the view state MUST always commit together, in the exact same transaction. Flow materialization tasks have a backing transactional recovery log, which is capable of durable commits that update both the checkpoint and also a (reasonably small) driver-defined state. More on driver states later. Many interesting endpoint systems are also fully transactional in nature. When implementing a materialization driver, the first question an implementor must answer is: whose commit is authoritative? Flow's recovery log, or the materialized system? This protocol supports either. "},{"title":"Common Implementation Patterns​","type":1,"pageTitle":"Materialization Protocol","url":"reference/Connectors/materialization-protocol/#common-implementation-patterns","content":"There are a few common implementation patterns for materializations. The choice of pattern depends on the transaction capabilities of the remote endpoint. "},{"title":"Remote Store is Authoritative​","type":1,"pageTitle":"Materialization Protocol","url":"reference/Connectors/materialization-protocol/#remote-store-is-authoritative","content":"In this pattern, the remote store (for example, a database) persists view states and the Flow consumption checkpoints which those views reflect. There are many such checkpoints: one per task split, and in this pattern the Flow recovery log is effectively ignored. Typically this workflow runs in the context of a synchronous BEGIN/COMMITtransaction, which updates table states and a Flow checkpoint together. The transaction need be scoped only to the store phase of this workflow, as the materialization protocol requires only read-committed isolation semantics. Flow is a distributed system, and an important consideration is the effect of a &quot;zombie&quot; assignment of a materialization task, which can race a newly-promoted assignment of that same task. Fencing is a technique which uses the transactional capabilities of a store to &quot;fence off&quot; an older zombie assignment, such that it's prevented from committing further transactions. This avoids a failure mode where: New assignment N recovers a checkpoint at Ti.Zombie assignment Z commits another transaction at Ti+1.N beings processing from Ti, inadvertently duplicating the effects of Ti+1. When a remote store is authoritative, it must implement fencing behavior. As a sketch, the store can maintain a nonce value alongside the checkpoint of each task split. The nonce is updated on each open of this RPC, and each commit transaction then verifies that the nonce has not been changed. In the future, if another RPC opens and updates the nonce, it fences off this instance of the task split and prevents it from committing further transactions. "},{"title":"Recovery Log with Non-Transactional Store​","type":1,"pageTitle":"Materialization Protocol","url":"reference/Connectors/materialization-protocol/#recovery-log-with-non-transactional-store","content":"In this pattern, the runtime's recovery log persists the Flow checkpoint and handles fencing semantics. During the Load and Store phases, the driver directly manipulates a non-transactional store or API, such as a key/value store. Note that this pattern is at-least-once. A transaction may fail part-way through and be restarted, causing its effects to be partially or fully replayed. Care must be taken if the collection's schema has reduction annotations such assum, as those reductions may be applied more than once due to a partially completed, but ultimately failed transaction. If the collection's schema is last-write-wins, this mode still provides effectively-once behavior. Collections which aren't last-write-wins can be turned into last-write-wins through the use of derivations. "},{"title":"Recovery Log with Idempotent Apply​","type":1,"pageTitle":"Materialization Protocol","url":"reference/Connectors/materialization-protocol/#recovery-log-with-idempotent-apply","content":"In this pattern the recovery log is authoritative, but the driver uses external stable storage to stage the effects of a transaction -- rather than directly applying them to the store -- such that those effects can be idempotently applied after the transaction commits. This allows stores which feature a weaker transaction guarantee to still be used in an exactly-once way, so long as they support an idempotent apply operation. Driver checkpoints can facilitate this pattern. For example, a driver might generate a unique filename in S3 and reference it in its prepared checkpoint, which is committed to the recovery log. During the &quot;store&quot; phase, it writes to this S3 file. After the transaction commits, it tells the store of the new file to incorporate. The store must handle idempotency, by applying the effects of the unique file just once, even if told of the file multiple times. A related extension of this pattern is for the driver to embed a Flow checkpoint into its driver checkpoint. Doing so allows the driver to express an intention to restart from an older alternative checkpoint, as compared to the most recent committed checkpoint of the recovery log. As mentioned above, it's crucial that store states and checkpoints commit together. While seemingly bending that rule, this pattern is consistent with it because, on commit, the semantic contents of the store include BOTH its base state, as well as the staged idempotent update. The store just may not know it yet, but eventually it must because of the retried idempotent apply. Note the driver must therefore ensure that staged updates are fully applied before returning Loaded responses, in order to provide the correct read-committed semantics required by the Flow runtime. "},{"title":"Push-only Endpoints & Delta Updates​","type":1,"pageTitle":"Materialization Protocol","url":"reference/Connectors/materialization-protocol/#push-only-endpoints--delta-updates","content":"Some systems, such as APIs, Webhooks, and Pub/Sub, are push-only in nature. Flow materializations can run in a &quot;delta updates&quot; mode, where loads are always skipped and Flow does not attempt to store fully-reduced documents. Instead, during the store phase, the runtime sends delta updates which reflect the combined roll-up of collection documents processed only within this transaction. To illustrate the meaning of a delta update, consider documents which are simple counters, having a collection schema that uses a sum reduction strategy. Without delta updates, Flow would reduce documents -1, 3, and 2 by sum to arrive at document 4, which is stored. The next transaction, document 4 is loaded and reduced with 6, -7, and -1 to arrive at a new stored document 2. This document, 2, represents the full reduction of the collection documents materialized thus far. Compare to delta updates mode: collection documents -1, 3, and 2 are combined to store a delta-update document of 4. The next transaction starts anew, and 6, -7, and -1 combine to arrive at a delta-update document of -2. These delta updates are a windowed combine over documents seen in the current transaction only, and unlike before are not a full reduction of the document. If delta updates were written to pub/sub, note that a subscriber could further reduce over each delta update to recover the fully reduced document of 2. Note that many use cases require only lastWriteWins reduction behavior, and for these use cases delta updates does the &quot;right thing&quot; by trivially re-writing each document with its most recent version. This matches the behavior of Kafka Connect, for example. "},{"title":"Protocol Phases​","type":1,"pageTitle":"Materialization Protocol","url":"reference/Connectors/materialization-protocol/#protocol-phases","content":""},{"title":"Acknowledge​","type":1,"pageTitle":"Materialization Protocol","url":"reference/Connectors/materialization-protocol/#acknowledge","content":"Acknowledge and Acknowledged are always the first messages sent every transaction, including the very first transaction of an RPC. The Runtime sendsAcknowledge to indicate that the last transaction has committed to the recovery log. The Driver sends Acknowledged to indicate that its endpoint transaction has committed. Acknowledge and Acknowledged are not ordered. Acknowledged may be sent before Acknowledge and vice versa. The Runtime does not wait for Acknowledged before sending Load messages. In most cases the Driver should simply not read these Load messages until it has completed its own commit and sent its own Acknowledged. A Driver MAY instead process its commit and acknowledgment in the background while actively reading Load messages. It MUST NOT evaluate Loads yet, as this could otherwise be a violation of read-committed semantics, but it MAY stage them for deferred evaluation. This is recommended for Drivers that have very long commit and/or acknowledgement operations. While a background commit progresses the Flow runtime will optimistically pipeline the next transaction, processing documents and preparing for when the Driver sendsAcknowledged. Drivers following the &quot;Recovery Log with Idempotent Apply&quot; pattern must take care to properly handle the very first acknowledgement phase of an RPC. At startup, a driver cannot know if the last commit has been acknowledged. For example, a previous RPC invocation may have failed immediately after commit but prior to acknowledgement. The Driver must thus idempotent-ly apply or re-apply changes staged by a prior Driver invocation, and reply with Acknowledged only once done. Drivers with transactional semantics SHOULD send Acknowledged immediately after a previous, started commit completes. Drivers with at-least-once semantics SHOULD send Acknowledged immediately after sending StartedCommit. "},{"title":"Load​","type":1,"pageTitle":"Materialization Protocol","url":"reference/Connectors/materialization-protocol/#load","content":"Zero or more Load messages are sent by the Runtime with documents to fetch. A given document key will appear at most once in a transaction, and will not be repeated across Load messages. Drivers may immediately evaluate each Load and respond, or may queue many keys to load and defer their evaluation. The Runtime does not await any individualLoad requests. After the previous transaction has fully completed, and the driver has sentAcknowledged to the Runtime, the current transaction may begin to close. The Runtime indicates this by sending a Flush message, which is NEVER sent before Acknowledged is received. Acknowledged is thus an important signal as to when the Runtime may begin to finalize an optimistic, pipelined transaction. On reading Flush, Drivers must process all remaining Load messages, including any deferred evaluations, and send all Loaded responses prior to sending its own Flushed response. This signals to the Runtime that all documents which can be loaded have been loaded, and the transaction proceeds to the Store phase. Materialization bindings which are processing in delta-updates mode will never receive a Load message, but will receive a Flush and must still respond withFlushed. "},{"title":"Store​","type":1,"pageTitle":"Materialization Protocol","url":"reference/Connectors/materialization-protocol/#store","content":"Zero or more Store messages are sent by the Runtime to the Driver, indicating keys, documents, and extracted fields to store. No response is required of the Driver for these messages. Once all documents have been stored, the Runtime sends a StartCommit message which carries its opaque runtime checkpoint. Drivers implementing the &quot;Remote Store is Authoritative&quot; pattern must include the runtime checkpoint in its current transaction, for retrieval in a future Open of a new transactions RPC. Other driver patterns MAY ignore this checkpoint. On reading StartCommit the driver ensures that all Store messages have been processed. It begins to commit its own transaction (where applicable), and then responds with StartedCommit which contain an update to the driver's checkpoint. On the Runtime's receipt of StartedCommit, the Runtime now knows that allStore messages have been fully processed. It preserves the updated Driver checkpoint in its recovery log and begins to commit. From here, the protocol loops back around to the Acknowledge phase. "},{"title":"Editing considerations","type":0,"sectionRef":"#","url":"reference/editing/","content":"","keywords":""},{"title":"How to edit Flow entities​","type":1,"pageTitle":"Editing considerations","url":"reference/editing/#how-to-edit-flow-entities","content":"In the Flow web app, you can edit captures and materializations, and use the Schema Inference tool to edit collection schemas. Editing captures and associated collectionsEditing materializations and associated collections With flowctl, you can edit captures, materializations, collections, derivations, and tests. You do this by pulling the desired specification locally, editing, and re-publishing. Editing with flowctl "},{"title":"Endpoint configuration changes​","type":1,"pageTitle":"Editing considerations","url":"reference/editing/#endpoint-configuration-changes","content":"A common reason to edit a capture or materialization to fix a broken endpoint configuration: for example, if a database is now accessed through a different port. Changes that prevent Flow from finding the source system immediately cause the capture or materialization to fail. By contrast, certain credential changes might not cause issues unless you attempt to edit the capture or materialization. Because Flow tasks run continuously, the connector doesn't have to re-authenticate and an outdated credential won't cause failure. Editing, however, requires the task to re-start, so you'll need to provide current credentials to the endpoint configuration. Before editing, take note of any changed credentials, even if the task is still running successfully. "},{"title":"Managing connector updates​","type":1,"pageTitle":"Editing considerations","url":"reference/editing/#managing-connector-updates","content":"Connectors are updated periodically. In some cases, required fields are added or removed. When you edit a capture or materialization, you'll need to update the configuration to comply with the current connector version. You may need to change a property's formatting or add a new field. Additionally, certain updates to capture connectors can affect the way available collections are named. After editing, the connector may map a data resource to new collection with a different name. For example, say you have capture that writes to a collection called post/fruity_pebbles/nutritionFacts. You begin to edit the capture using the latest version of the connector. The connector detects the same set of nutrition facts data, but maps it to a collection called post/fruity_pebbles/nutrition-facts. If you continue to publish the edited capture, both collections will persist, but new data will be written to the new collection. Before editing, check if a connector has been updated: Go to the Admin tab and view the list of connectors. Each tile shows the date it was last updated.Check the connector's documentation. Pertinent updates, if any, are noted in the Changelog section. "},{"title":"Considerations for name changes​","type":1,"pageTitle":"Editing considerations","url":"reference/editing/#considerations-for-name-changes","content":"You're not able to change the name of a capture or materialization after you create it. You're also unable to manually change the names of collections; however, connector updates can cause collection names to change, as discussed above. It is possible to manually change the names of destination resources (tables or analogous data storage units to which collections are written) when editing a materialization. You should avoid doing so unless you want to route future data to a new location. If you do this, a new resource with that name will be created and the old resource will continue to exist. Historical data will may not be backfilled into the new resource, depending on the connector used. "},{"title":"Reduction strategies","type":0,"sectionRef":"#","url":"reference/reduction-strategies/","content":"","keywords":""},{"title":"Reduction guarantees​","type":1,"pageTitle":"Reduction strategies","url":"reference/reduction-strategies/#reduction-guarantees","content":"In Flow, documents that share the same collection key and are written to the same logical partition have a total order, meaning that one document is universally understood to have been written before the other. This isn't true of documents of the same key written to different logical partitions. These documents can be considered “mostly” ordered: Flow uses timestamps to understand the relative ordering of these documents, and while this largely produces the desired outcome, small amounts of re-ordering are possible and even likely. Flow guarantees exactly-once semantics within derived collections and materializations (so long as the target system supports transactions), and a document reduction will be applied exactly one time. Flow does not guarantee that documents are reduced in sequential order, directly into a base document. For example, documents of a single Flow capture transaction are combined together into one document per collection key at capture time – and that document may be again combined with still others, and so on until a final reduction into the base document occurs. Taken together, these total-order and exactly-once guarantees mean that reduction strategies must be associative [as in (2 + 3) + 4 = 2 + (3 + 4) ], but need not be commutative [ 2 + 3 = 3 + 2 ] or idempotent [ S u S = S ]. They expand the palette of strategies that can be implemented, and allow for more efficient implementations as compared to, for example CRDTs. In this documentation, we’ll refer to the “left-hand side” (LHS) as the preceding document and the “right-hand side” (RHS) as the following one. Keep in mind that both the LHS and RHS may themselves represent a combination of still more ordered documents because, for example, reductions are applied associatively. "},{"title":"append","type":0,"sectionRef":"#","url":"reference/reduction-strategies/append/","content":"append append works with arrays, and extends the left-hand array with items from the right-hand side. collections: - name: example/reductions/append schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Append only works with type &quot;array&quot;. # Others will throw an error at build time. type: array reduce: { strategy: append } required: [key] key: [/key] tests: &quot;Expect we can append arrays&quot;: - ingest: collection: example/reductions/append documents: - { key: &quot;key&quot;, value: [1, 2] } - { key: &quot;key&quot;, value: [3, null, &quot;abc&quot;] } - verify: collection: example/reductions/append documents: - { key: &quot;key&quot;, value: [1, 2, 3, null, &quot;abc&quot;] } The right-hand side must always be an array. The left-hand side may be null, in which case the reduction is treated as a no-op and its result remains null. This can be combined with schema conditionals to toggle whether reduction-reduction should be done or not.","keywords":""},{"title":"Organizing a Flow catalog","type":0,"sectionRef":"#","url":"reference/organizing-catalogs/","content":"","keywords":""},{"title":"import​","type":1,"pageTitle":"Organizing a Flow catalog","url":"reference/organizing-catalogs/#import","content":"Flow's import directive can help you easily handle all of these scenarios while keeping your catalogs well organized. Each catalog spec file may import any number of other files, and each import may refer to either relative or an absolute URL. When you use import in a catalog spec, you're conceptually bringing the entirety of another catalog — as well as the schemas and typescript files it uses — into your catalog. Imports are also transitive, so when you import another catalog, you're also importing everything that other catalog has imported. This allows you to keep your catalogs organized, and is flexible enough to support collaboration between separate teams and organizations. Perhaps the best way of explaining this is with some examples. Example: Organizing collections​ Let's look at a relatively simple case in which you want to organize your collections into multiple catalog files. Say you work for Acme Corp on the team that's introducing Flow. You might start with the collections and directory structure below: acme/customers/customerInfo acme/products/info/manufacturers acme/products/info/skus acme/products/inventory acme/sales/pending acme/sales/complete  acme ├── flow.yaml ├── customers │ ├── flow.ts │ ├── flow.yaml │ └── schemas.yaml ├── products │ ├── flow.yaml │ ├── info │ │ ├── flow.ts │ │ ├── flow.yaml │ │ └── schemas.yaml │ └── inventory │ ├── flow.ts │ ├── flow.yaml │ └── schemas.yaml schemas.yaml └── sales ├── flow.ts ├── flow.yaml └── schemas.yaml  It's immediately clear where each of the given collections is defined, since the directory names match the path segments in the collection names. This is not required by theflowctl CLI, but is strongly recommended, since it makes your catalogs more readable and maintainable. Each directory contains a catalog spec (flow.yaml), which will import all of the catalogs from child directories. So, the top-level catalog spec, acme/flow.yaml, might look something like this: import: - customers/flow.yaml - products/flow.yaml - sales/flow.yaml  This type of layout has a number of other advantages. During development, you can easily work with a subset of collections using, for example, flowctl test --source acme/products/flow.yaml to run only the tests for product-related collections. It also allows other imports to be more granular. For example, you might want a derivation under sales to read from acme/products/info. Since info has a separate catalog spec, acme/sales/flow.yaml can import acme/products/info/flow.yaml without creating a dependency on the inventory collection. Example: Separate environments​ It's common to use separate environments for tiers like development, staging, and production. Flow catalog specs often necessarily include endpoint configuration for external systems that will hold materialized views. Let's say you want your production environment to materialize views to Snowflake, but you want to develop locally on SQLite. We might modify the Acme example slightly to account for this. acme ├── dev.flow.yaml ├── prod.flow.yaml ... the remainder is the same as above  Each of the top-level catalog specs might import all of the collections and define an endpoint called ourMaterializationEndpoint that points to the desired system. The import block might be the same for each system, but each file may use a different configuration for the endpoint, which is used by any materializations that reference it. Our configuration for our development environment will look like: dev.flow.yaml import: - customers/flow.yaml - products/flow.yaml - sales/flow.yaml ourMaterializationEndpoint: # dev.flow.yaml sqlite: path: dev-materializations.db  While production will look like: prod.flow.yaml import: - customers/flow.yaml - products/flow.yaml - sales/flow.yaml endpoints: snowflake: account: acme_production role: admin schema: snowflake.com/acmeProd user: importantAdmin password: abc123 warehouse: acme_production  When we test the draft locally, we'll work with dev.flow.yaml, but we'll publish prod.flow.yaml. Everything will continue to work because in our development environment we'll be binding collections to our local SQLite DB and in production we'll use Snowflake. Example: Cross-team collaboration​ When working across teams, it's common for one team to provide a data product for another to reference and use. Flow is designed for cross-team collaboration, allowing teams and users to reference each other's full catalog or schema.  Again using the Acme example, let's imagine we have two teams. Team Web is responsible for Acme's website, and Team User is responsible for providing a view of Acme customers that's always up to date. Since Acme wants a responsive site that provides a good customer experience, Team Web needs to pull the most up-to-date information from Team User at any point. Let's look at Team User's collections: teamUser.flow.yaml import: - userProfile.flow.yaml  Which references: userProfile.flow.yaml collection: userProfile: schema: -&quot;/userProfile/schema&quot; key: [/id]  Team User references files in their directory, which they actively manage in both their import and schema sections. If Team Web wants to access user data (and they have access), they can use a relative path or a URL-based path given that Team User publishes their data to a URL for access: teamWeb.flow.yaml import: -http://www.acme.com/teamUser#userProfile.flow.yaml -webStuff.flow.yaml  Now Team Web has direct access to collections (referenced by their name) to build derived collections on top of. They can also directly import schemas: webStuff.flow.yaml collection: webStuff: schema: -http://acme.com/teamUser#userProfile/#schema key: [/id]  "},{"title":"Global namespace​","type":1,"pageTitle":"Organizing a Flow catalog","url":"reference/organizing-catalogs/#global-namespace","content":"Every Flow collection has a name, and that name must be unique within a running Flow system. Flow collections should be thought of as existing within a global namespace. Keeping names globally unique makes it easy to import catalogs from other teams, or even other organizations, without having naming conflicts or ambiguities. For example, imagine your catalog for the inside sales team has a collection just named customers. If you later try to import a catalog from the outside sales team that also contains a customers collection, 💥 there's a collision. A better collection name would be acme/inside-sales/customers. This allows a catalog to include customer data from separate teams, and also separate organizations. Learn more about the Flow namespace. "},{"title":"Composing with conditionals","type":0,"sectionRef":"#","url":"reference/reduction-strategies/composing-with-conditionals/","content":"Composing with conditionals Reduction strategies are JSON Schema annotations. As such, their applicability at a given document location can be controlled through the use of conditional keywords within the schema, like oneOf or if/then/else. This means Flow’s built-in strategies can be combined with schema conditionals to construct a wider variety of custom reduction behaviors. For example, here’s a reset-able counter: collections: - name: example/reductions/sum-reset schema: type: object properties: key: { type: string } value: { type: number } required: [key] # Use oneOf to express a tagged union over &quot;action&quot;. oneOf: # When action = reset, reduce by taking this document. - properties: { action: { const: reset } } reduce: { strategy: lastWriteWins } # When action = sum, reduce by summing &quot;value&quot;. Keep the LHS &quot;action&quot;, # preserving a LHS &quot;reset&quot;, so that resets are properly associative. - properties: action: const: sum reduce: { strategy: firstWriteWins } value: { reduce: { strategy: sum } } reduce: { strategy: merge } key: [/key] tests: &quot;Expect we can sum or reset numbers&quot;: - ingest: collection: example/reductions/sum-reset documents: - { key: &quot;key&quot;, action: sum, value: 5 } - { key: &quot;key&quot;, action: sum, value: -1.2 } - verify: collection: example/reductions/sum-reset documents: - { key: &quot;key&quot;, value: 3.8 } - ingest: collection: example/reductions/sum-reset documents: - { key: &quot;key&quot;, action: reset, value: 0 } - { key: &quot;key&quot;, action: sum, value: 1.3 } - verify: collection: example/reductions/sum-reset documents: - { key: &quot;key&quot;, value: 1.3 } ","keywords":""},{"title":"firstWriteWins and lastWriteWins","type":0,"sectionRef":"#","url":"reference/reduction-strategies/firstwritewins-and-lastwritewins/","content":"firstWriteWins and lastWriteWins firstWriteWins always takes the first value seen at the annotated location. Likewise, lastWriteWins always takes the last. Schemas that don’t have an explicit reduce annotation default to lastWriteWins behavior. collections: - name: example/reductions/fww-lww schema: type: object reduce: { strategy: merge } properties: key: { type: string } fww: { reduce: { strategy: firstWriteWins } } lww: { reduce: { strategy: lastWriteWins } } required: [key] key: [/key] tests: &quot;Expect we can track first- and list-written values&quot;: - ingest: collection: example/reductions/fww-lww documents: - { key: &quot;key&quot;, fww: &quot;one&quot;, lww: &quot;one&quot; } - { key: &quot;key&quot;, fww: &quot;two&quot;, lww: &quot;two&quot; } - verify: collection: example/reductions/fww-lww documents: - { key: &quot;key&quot;, fww: &quot;one&quot;, lww: &quot;two&quot; } ","keywords":""},{"title":"merge","type":0,"sectionRef":"#","url":"reference/reduction-strategies/merge/","content":"merge merge reduces the left-hand side and right-hand side by recursively reducing shared document locations. The LHS and RHS must either both be objects, or both be arrays. If both sides are objects, merge performs a deep merge of each property. If LHS and RHS are both arrays, items at each index of both sides are merged together, extending the shorter of the two sides by taking items off the longer: collections: - name: example/reductions/merge schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Merge only works with types &quot;array&quot; or &quot;object&quot;. # Others will throw an error at build time. type: [array, object] reduce: { strategy: merge } # Deeply merge sub-locations (items or properties) by summing them. items: type: number reduce: { strategy: sum } additionalProperties: type: number reduce: { strategy: sum } required: [key] key: [/key] tests: &quot;Expect we can merge arrays by index&quot;: - ingest: collection: example/reductions/merge documents: - { key: &quot;key&quot;, value: [1, 1] } - { key: &quot;key&quot;, value: [2, 2, 2] } - verify: collection: example/reductions/merge documents: - { key: &quot;key&quot;, value: [3, 3, 2] } &quot;Expect we can merge objects by property&quot;: - ingest: collection: example/reductions/merge documents: - { key: &quot;key&quot;, value: { &quot;a&quot;: 1, &quot;b&quot;: 1 } } - { key: &quot;key&quot;, value: { &quot;a&quot;: 1, &quot;c&quot;: 1 } } - verify: collection: example/reductions/merge documents: - { key: &quot;key&quot;, value: { &quot;a&quot;: 2, &quot;b&quot;: 1, &quot;c&quot;: 1 } } Merge may also take a key, which is one or more JSON pointers that are relative to the reduced location. If both sides are arrays and a merge key is present, then a deep sorted merge of the respective items is done, as ordered by the key. Arrays must be pre-sorted and de-duplicated by the key, and merge itself always maintains this invariant. Note that you can use a key of [“”] for natural item ordering, such as merging sorted arrays of scalars. collections: - name: example/reductions/merge-key schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: type: array reduce: strategy: merge key: [/k] items: { reduce: { strategy: firstWriteWins } } required: [key] key: [/key] tests: &quot;Expect we can merge sorted arrays&quot;: - ingest: collection: example/reductions/merge-key documents: - { key: &quot;key&quot;, value: [{ k: &quot;a&quot;, v: 1 }, { k: &quot;b&quot;, v: 1 }] } - { key: &quot;key&quot;, value: [{ k: &quot;a&quot;, v: 2 }, { k: &quot;c&quot;, v: 2 }] } - verify: collection: example/reductions/merge-key documents: - { key: &quot;key&quot;, value: [{ k: &quot;a&quot;, v: 1 }, { k: &quot;b&quot;, v: 1 }, { k: &quot;c&quot;, v: 2 }], } As with append, the LHS of merge may be null, in which case the reduction is treated as a no-op and its result remains null.","keywords":""},{"title":"minimize and maximize","type":0,"sectionRef":"#","url":"reference/reduction-strategies/minimize-and-maximize/","content":"minimize and maximize minimize and maximize reduce by taking the smallest or largest seen value, respectively. collections: - name: example/reductions/min-max schema: type: object reduce: { strategy: merge } properties: key: { type: string } min: { reduce: { strategy: minimize } } max: { reduce: { strategy: maximize } } required: [key] key: [/key] tests: &quot;Expect we can min/max values&quot;: - ingest: collection: example/reductions/min-max documents: - { key: &quot;key&quot;, min: 32, max: &quot;abc&quot; } - { key: &quot;key&quot;, min: 42, max: &quot;def&quot; } - verify: collection: example/reductions/min-max documents: - { key: &quot;key&quot;, min: 32, max: &quot;def&quot; } minimize and maximize can also take a key, which is one or more JSON pointers that are relative to the reduced location. Keys make it possible to minimize and maximize over complex types by ordering over an extracted composite key. In the event that a right-hand side document key equals the current left-hand side minimum or maximum, the documents are deeply merged. This can be used to, for example, track not just the minimum value but also the number of times it’s been seen: collections: - name: example/reductions/min-max-key schema: type: object reduce: { strategy: merge } properties: key: { type: string } min: $anchor: min-max-value type: array items: - type: string - type: number reduce: { strategy: sum } reduce: strategy: minimize key: [/0] max: $ref: &quot;#min-max-value&quot; reduce: strategy: maximize key: [/0] required: [key] key: [/key] tests: &quot;Expect we can min/max values using a key extractor&quot;: - ingest: collection: example/reductions/min-max-key documents: - { key: &quot;key&quot;, min: [&quot;a&quot;, 1], max: [&quot;a&quot;, 1] } - { key: &quot;key&quot;, min: [&quot;c&quot;, 2], max: [&quot;c&quot;, 2] } - { key: &quot;key&quot;, min: [&quot;b&quot;, 3], max: [&quot;b&quot;, 3] } - { key: &quot;key&quot;, min: [&quot;a&quot;, 4], max: [&quot;a&quot;, 4] } - verify: collection: example/reductions/min-max-key documents: # Min of equal keys [&quot;a&quot;, 1] and [&quot;a&quot;, 4] =&gt; [&quot;a&quot;, 5]. - { key: &quot;key&quot;, min: [&quot;a&quot;, 5], max: [&quot;c&quot;, 2] } ","keywords":""},{"title":"sum","type":0,"sectionRef":"#","url":"reference/reduction-strategies/sum/","content":"sum sum reduces two numbers or integers by adding their values. collections: - name: example/reductions/sum schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Sum only works with types &quot;number&quot; or &quot;integer&quot;. # Others will throw an error at build time. type: number reduce: { strategy: sum } required: [key] key: [/key] tests: &quot;Expect we can sum two numbers&quot;: - ingest: collection: example/reductions/sum documents: - { key: &quot;key&quot;, value: 5 } - { key: &quot;key&quot;, value: -1.2 } - verify: collection: example/reductions/sum documents: - { key: &quot;key&quot;, value: 3.8 } ","keywords":""},{"title":"Working with logs and statistics","type":0,"sectionRef":"#","url":"reference/working-logs-stats/","content":"","keywords":""},{"title":"Accessing logs and statistics​","type":1,"pageTitle":"Working with logs and statistics","url":"reference/working-logs-stats/#accessing-logs-and-statistics","content":"You can access logs and statistics in the Flow web app, by materializing them to an external endpoint, or from the command line. "},{"title":"Logs and statistics in the Flow web app​","type":1,"pageTitle":"Working with logs and statistics","url":"reference/working-logs-stats/#logs-and-statistics-in-the-flow-web-app","content":"You can view a subset of logs and statistics for individual tasks in the Flow web app. Logs​ After you publish a new capture or materialization, a pop-up window appears that displays the task's logs. Once you close the window, you can't regain access to the full logs in the web app. For a complete view of logs, use flowctl or materialize the logs collection to an outside system. However, if a task fails, you can view the logs associated with the error(s) that caused the failure. In the Details view of the published capture or materialization, click the name of its shard to display the logs. Statistics​ Two statistics are shown for each capture, collection, and materialization: Bytes Written or Read. This corresponds to the bytesTotal property of the stats collection.Docs Written or Read. This corresponds to the docsTotal property of the stats collection. These fields have slightly different meanings for each Flow entity type: For captures, Bytes Written and Docs Written represent the total data written across all of the capture's associated collections.For collections, Bytes Written and Docs Written represent the data written to the collection from its associated capture or derivation.For materializations, Bytes Read and Docs Read represent the total data read from all of the materialization's associated collections. "},{"title":"Accessing logs and statistics from the command line​","type":1,"pageTitle":"Working with logs and statistics","url":"reference/working-logs-stats/#accessing-logs-and-statistics-from-the-command-line","content":"The flowctl logs and flowctl stats subcommands allow you to print logs and stats, respectively, from the command line. This method allows more flexibility and is ideal for debugging. You can retrieve logs and stats for any published Flow task. For example: flowctl logs --task acmeCo/anvils/capture-one flowctl stats --task acmeCo/anvils/capture-one --uncommitted  Beta The --uncommitted flag is currently required for flowctl stats. This means that all statistics are read, regardless of whether they are about a successfully committed transaction, or a transaction that was rolled back or uncommitted. In the future, committed reads will be the default. Printing logs or stats since a specific time​ To limit output, you can retrieve logs are stats starting at a specific time in the past. For example: flowctl stats --task acmeCo/anvils/materialization-one --since 1h  ...will retrieve stats from approximately the last hour. The actual start time will always be at the previous fragment boundary, so it can be significantly before the requested time period. Additional options for flowctl logs and flowctl stats can be accessed through command-line help. "},{"title":"Accessing logs or stats by materialization​","type":1,"pageTitle":"Working with logs and statistics","url":"reference/working-logs-stats/#accessing-logs-or-stats-by-materialization","content":"You can materialize your logs or stats collections to an external system. This is typically the preferred method if you’d like to continuously work with or monitor logs or statistics. You can materialize the logs or statistics for all tasks, or select a subset of tasks using a partition selector (the logs and stats collections are partitioned on tasks). caution Be sure to add a partition selector to exclude the logs and statistics of the materialization itself. Otherwise, you could trigger an infinite loop in which the connector materializes its own logs and statistics, collects logs and statistic on that event, and so on. acmeCo/anvils/logs: endpoint: connector: image: ghcr.io/estuary/materialize-webhook:dev config: address: my.webhook.com bindings: - resource: relativePath: /log/wordcount source: ops/acmeCo/logs # Exclude the logs of this materialization to avoid an infinite loop. partitions: exclude: name: ['acmeCo/anvils/logs']  "},{"title":"Available statistics​","type":1,"pageTitle":"Working with logs and statistics","url":"reference/working-logs-stats/#available-statistics","content":"Available statistics include information about the amount of data in inputs and outputs of each transaction. They also include temporal information about the transaction. Statistics vary by task type (capture, materialization, or derivation). A thorough knowledge of Flow's advanced concepts is necessary to effectively leverage these statistics. stats collection documents include the following properties. "},{"title":"Shard information​","type":1,"pageTitle":"Working with logs and statistics","url":"reference/working-logs-stats/#shard-information","content":"A stats document begins with data about the shard processing the transaction. Each processing shard is uniquely identified by the combination of its name, keyBegin, and rClockBegin. This information is important for tasks with multiple shards: it allows you to determine whether data throughput is evenly distributed amongst those shards. Property\tDescription\tData Type\tApplicable Task Type/shard\tFlow shard information\tobject\tAll /shard/kind\tThe type of catalog task. One of &quot;capture&quot;, &quot;derivation&quot;, or &quot;materialization&quot;\tstring\tAll /shard/name\tThe name of the catalog task (without the task type prefix)\tstring\tAll /shard/keyBegin\tWith rClockBegin, this comprises the shard ID. The inclusive beginning of the shard's assigned key range.\tstring\tAll /shard/rClockBegin\tWith keyBegin, this comprises the shard ID. The inclusive beginning of the shard's assigned rClock range.\tstring\tAll "},{"title":"Transaction information​","type":1,"pageTitle":"Working with logs and statistics","url":"reference/working-logs-stats/#transaction-information","content":"stats documents include information about a transaction: its inputs and outputs, the amount of data processed, and the time taken. You can use this information to ensure that your Flow tasks are running efficiently, and that the amount of data processed matches your expectations. Property\tDescription\tData Type\tApplicable Task Type/ts\tTimestamp corresponding to the start of the transaction, rounded to the nearest minute\tstring\tAll /openSecondsTotal\tTotal time that the transaction was open before starting to commit\tnumber\tAll /txnCount\tTotal number of transactions represented by this stats document. Used for reduction.\tinteger\tAll /capture\tCapture stats, organized by collection\tobject\tCapture /materialize\tMaterialization stats, organized by collection\tobject\tMaterialization /derive\tDerivation statistics\tobject\tDerivation /&lt;task-type&gt;/&lt;collection-name&gt;/right/\tInput documents from a the task's source\tobject\tCapture, materialization /&lt;task-type&gt;/&lt;collection-name&gt;/left/\tInput documents from an external destination; used for reduced updates in materializations\tobject\tMaterialization /&lt;task-type&gt;/&lt;collection-name&gt;/out/\tOutput documents from the transaction\tobject\tAll /&lt;task-type&gt;/{}/docsTotal\tTotal number of documents\tinteger\tAll /&lt;task-type&gt;/{}/bytesTotal\tTotal number of bytes representing the JSON encoded documents\tinteger\tAll /derivations/transforms/transformStats\tStats for a specific transform of a derivation, which will have an update, publish, or both\tobject\tDerivation /derivations/transforms/transformStats/input\tThe input documents that were fed into this transform\tobject\tDerivation /derivations/transforms/transformStats/update\tThe outputs from update lambda invocations, which were combined into registers\tobject\tDerivation /derivations/transforms/transformStats/publish\tThe outputs from publish lambda invocations.\tobject\tDerivation /derivations/registers/createdTotal\tThe total number of new register keys that were created\tinteger\tDerivation "},{"title":"set","type":0,"sectionRef":"#","url":"reference/reduction-strategies/set/","content":"set set interprets the document location as an update to a set. The location must be an object having only “add&quot;, “intersect&quot;, and “remove” properties. Any single “add&quot;, “intersect&quot;, or “remove” is always allowed. A document with “intersect” and “add” is allowed, and is interpreted as applying the intersection to the left-hand side set, followed by a union with the additions. A document with “remove” and “add” is also allowed, and is interpreted as applying the removals to the base set, followed by a union with the additions. “remove” and “intersect” within the same document are prohibited. Set additions are deeply merged. This makes sets behave like associative maps, where the “value” of a set member can be updated by adding it to the set again, with a reducible update. Sets may be objects, in which case the object property serves as the set item key: collections: - name: example/reductions/set schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Sets are always represented as an object. type: object reduce: { strategy: set } # Schema for &quot;add&quot;, &quot;intersect&quot;, and &quot;remove&quot; properties # (each a map of keys and their associated sums): additionalProperties: type: object additionalProperties: type: number reduce: { strategy: sum } # Flow requires that all parents of locations with a reduce # annotation also have one themselves. # This strategy therefore must (currently) be here, but is ignored. reduce: { strategy: lastWriteWins } required: [key] key: [/key] tests: &quot;Expect we can apply set operations to incrementally build associative maps&quot;: - ingest: collection: example/reductions/set documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 1, &quot;b&quot;: 1, &quot;c&quot;: 1 } } } - { key: &quot;key&quot;, value: { &quot;remove&quot;: { &quot;b&quot;: 0 } } } - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 1, &quot;d&quot;: 1 } } } - verify: collection: example/reductions/set documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 2, &quot;c&quot;: 1, &quot;d&quot;: 1 } } } - ingest: collection: example/reductions/set documents: - { key: &quot;key&quot;, value: { &quot;intersect&quot;: { &quot;a&quot;: 0, &quot;d&quot;: 0 } } } - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 1, &quot;e&quot;: 1 } } } - verify: collection: example/reductions/set documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: { &quot;a&quot;: 3, &quot;d&quot;: 1, &quot;e&quot;: 1 } } } Sets can also be sorted arrays, which are ordered using a provide key extractor. Keys are given as one or more JSON pointers, each relative to the item. As with merge, arrays must be pre-sorted and de-duplicated by the key, and set reductions always maintain this invariant. Use a key extractor of [“”] to apply the natural ordering of scalar values. Whether array or object types are used, the type must always be consistent across the “add” / “intersect” / “remove” terms of both sides of the reduction. collections: - name: example/reductions/set-array schema: type: object reduce: { strategy: merge } properties: key: { type: string } value: # Sets are always represented as an object. type: object reduce: strategy: set key: [/0] # Schema for &quot;add&quot;, &quot;intersect&quot;, &amp; &quot;remove&quot; properties # (each a sorted array of [key, sum] 2-tuples): additionalProperties: type: array # Flow requires that all parents of locations with a reduce # annotation also have one themselves. # This strategy therefore must (currently) be here, but is ignored. reduce: { strategy: lastWriteWins } # Schema for contained [key, sum] 2-tuples: items: type: array items: - type: string - type: number reduce: { strategy: sum } reduce: { strategy: merge } required: [key] key: [/key] tests: ? &quot;Expect we can apply operations of sorted-array sets to incrementally build associative maps&quot; : - ingest: collection: example/reductions/set-array documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 1], [&quot;b&quot;, 1], [&quot;c&quot;, 1]] } } - { key: &quot;key&quot;, value: { &quot;remove&quot;: [[&quot;b&quot;, 0]] } } - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 1], [&quot;d&quot;, 1]] } } - verify: collection: example/reductions/set-array documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 2], [&quot;c&quot;, 1], [&quot;d&quot;, 1]] } } - ingest: collection: example/reductions/set-array documents: - { key: &quot;key&quot;, value: { &quot;intersect&quot;: [[&quot;a&quot;, 0], [&quot;d&quot;, 0]] } } - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 1], [&quot;e&quot;, 1]] } } - verify: collection: example/reductions/set-array documents: - { key: &quot;key&quot;, value: { &quot;add&quot;: [[&quot;a&quot;, 3], [&quot;d&quot;, 1], [&quot;e&quot;, 1]] } } ","keywords":""}]