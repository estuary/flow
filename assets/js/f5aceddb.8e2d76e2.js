"use strict";(self.webpackChunksite=self.webpackChunksite||[]).push([[4185],{28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>l});var t=i(96540);const s={},a=t.createContext(s);function r(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(a.Provider,{value:n},e.children)}},55153:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"reference/backfilling-data","title":"Backfilling Data","description":"When new captures are created, you often have the option of backfilling data. This captures data in its current state and then switches to capturing change events on an ongoing basis.","source":"@site/docs/reference/backfilling-data.md","sourceDirName":"reference","slug":"/reference/backfilling-data","permalink":"/reference/backfilling-data","draft":false,"unlisted":false,"editUrl":"https://github.com/estuary/flow/edit/master/site/docs/reference/backfilling-data.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Allowlisting IP Addresses for Estuary Flow","permalink":"/reference/allow-ip-addresses"},"next":{"title":"Handling Deletions in Estuary Flow","permalink":"/reference/deletions"}}');var s=i(74848),a=i(28453);const r={},l="Backfilling Data",o={},c=[{value:"Backfill types",id:"backfill-types",level:2},{value:"Incremental backfill",id:"incremental-backfill",level:3},{value:"Materialization backfill",id:"materialization-backfill",level:3},{value:"Dataflow reset",id:"dataflow-reset",level:3},{value:"Backfill Selection",id:"backfill-selection",level:3},{value:"Preventing backfills",id:"preventing-backfills",level:2},{value:"Preventing backfills during database upgrades",id:"preventing-backfills-during-database-upgrades",level:3},{value:"Resource configuration backfill modes",id:"resource-configuration-backfill-modes",level:2},{value:"Advanced backfill configuration in specific systems",id:"advanced-backfill-configuration-in-specific-systems",level:2},{value:"PostgreSQL Capture",id:"postgresql-capture",level:3}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"backfilling-data",children:"Backfilling Data"})}),"\n",(0,s.jsx)(n.p,{children:"When new captures are created, you often have the option of backfilling data. This captures data in its current state and then switches to capturing change events on an ongoing basis."}),"\n",(0,s.jsx)(n.p,{children:"This is desirable in most cases, as it ensures that a complete view of your data is captured into Flow."}),"\n",(0,s.jsx)(n.p,{children:"You also have options to backfill data in either the source, destination, or entire dataflow after initial connector setup. This can be useful to refresh data or recover in the event of data corruption."}),"\n",(0,s.jsxs)(n.p,{children:["Also see how ",(0,s.jsx)(n.a,{href:"https://docs.estuary.dev/concepts/advanced/evolutions/#what-do-schema-evolutions-do",children:"schema evolution"})," can help with backfills when a source's schema becomes incompatible with the existing schema."]}),"\n",(0,s.jsx)(n.h2,{id:"backfill-types",children:"Backfill types"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Backfill options",src:i(90914).A+"",width:"1000",height:"939"})}),"\n",(0,s.jsx)(n.h3,{id:"incremental-backfill",children:"Incremental backfill"}),"\n",(0,s.jsx)(n.p,{children:"An incremental backfill on a capture refreshes data from your source system into Estuary collections without dropping your destination tables.\nThis is useful when you need to update your collections with the latest source data or recover from data loss in your collections."}),"\n",(0,s.jsx)(n.p,{children:"When you perform an incremental backfill:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Data is reread from the source system"}),"\n",(0,s.jsx)(n.li,{children:"The data is stored in Estuary collections"}),"\n",(0,s.jsx)(n.li,{children:"Existing destination tables remain intact"}),"\n",(0,s.jsx)(n.li,{children:"New or updated data will be materialized to destinations as part of the normal process"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"To perform an incremental backfill:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Navigate to the Sources tab in the Flow web UI"}),"\n",(0,s.jsxs)(n.li,{children:["Start editing your capture and click the ",(0,s.jsx)(n.strong,{children:"Backfill"})," button"]}),"\n",(0,s.jsxs)(n.li,{children:["In the ",(0,s.jsx)(n.strong,{children:"Backfill mode"})," dropdown, select the ",(0,s.jsx)(n.strong,{children:"Incremental backfill (advanced)"})," option"]}),"\n",(0,s.jsxs)(n.li,{children:["(Optional) Choose a specific ",(0,s.jsx)(n.a,{href:"#resource-configuration-backfill-modes",children:(0,s.jsx)(n.strong,{children:"Resource configuration backfill mode"})})," for the collection for advanced use cases"]}),"\n",(0,s.jsx)(n.li,{children:"Save and publish your changes"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This option is ideal when you want to ensure your collections have the most up-to-date data without\ndisrupting your destination systems."}),"\n",(0,s.jsxs)(n.p,{children:["This includes ",(0,s.jsx)(n.a,{href:"/reference/Connectors/materialization-connectors/Dekaf/",children:(0,s.jsx)(n.strong,{children:"Dekaf"})})," materializations:\nbecause Dekaf simply provides a Kafka interface, the destination schema is managed via the connected service, such as ClickHouse or Tinybird, rather than Estuary.\nTo ensure you don't disrupt your setup with these systems, you can use an incremental backfill.\nUsing a dataflow reset may require you to manually update your schemas in the 3rd party system as on initial creation for these connectors."]}),"\n",(0,s.jsx)(n.p,{children:"When you perform an incremental backfill, all data is pulled into collections again, and\nmaterializations that use those collections will read and process this data. How the data is handled\nat the destination depends on your materialization settings:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Standard (merge) updates:"})," The system will query the destination for existing records\nwith matching keys and merge the incoming data with existing records. This prevents\nduplication in your destination tables."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Delta updates:"})," New records are inserted without checking for existing data, which can\nresult in duplicate records in your destination."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"materialization-backfill",children:"Materialization backfill"}),"\n",(0,s.jsx)(n.p,{children:"A materialization backfill drops and recreates your destination tables using the data currently stored\nin your Estuary collections. This is useful when you need to refresh destination tables without\nrereading from the source."}),"\n",(0,s.jsx)(n.p,{children:"When you perform a materialization backfill:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Destination tables are dropped and recreated"}),"\n",(0,s.jsx)(n.li,{children:"Data is read from existing Estuary collections"}),"\n",(0,s.jsx)(n.li,{children:"No new data is pulled from the source"}),"\n",(0,s.jsx)(n.li,{children:"Tables are repopulated with data from collections"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"To perform a materialization backfill:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Navigate to the Destinations tab in the Flow web UI"}),"\n",(0,s.jsx)(n.li,{children:"Find your materialization and start editing it"}),"\n",(0,s.jsxs)(n.li,{children:["Under the ",(0,s.jsx)(n.strong,{children:"Source Collections"})," section, expand ",(0,s.jsx)(n.strong,{children:"Advanced options"})]}),"\n",(0,s.jsxs)(n.li,{children:["Select the ",(0,s.jsx)(n.strong,{children:"Backfill"})," button"]}),"\n",(0,s.jsx)(n.li,{children:"Save and publish your changes"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Or you can select individual collections to backfill:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Navigate to the Destinations tab in the Flow web UI"}),"\n",(0,s.jsx)(n.li,{children:"Find your materialization and start editing it"}),"\n",(0,s.jsx)(n.li,{children:"Select a collection you would like to backfill"}),"\n",(0,s.jsxs)(n.li,{children:["In the collection's ",(0,s.jsx)(n.strong,{children:"Resource configuration"}),", expand the ",(0,s.jsx)(n.strong,{children:"Advanced options"})," section"]}),"\n",(0,s.jsxs)(n.li,{children:["Select the ",(0,s.jsx)(n.strong,{children:"Backfill"})," button"]}),"\n",(0,s.jsx)(n.li,{children:"Repeat steps 3-5 for all collections you'd like to backfill"}),"\n",(0,s.jsx)(n.li,{children:"Save and publish your changes"}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{type:"tip",children:(0,s.jsx)(n.p,{children:"Dropping destination tables may create downtime for your destination table consumers while data is backfilled."})}),"\n",(0,s.jsx)(n.p,{children:"This option is best when you need to refresh destination tables but are confident your collections\ncontain all the necessary historical data."}),"\n",(0,s.jsx)(n.h3,{id:"dataflow-reset",children:"Dataflow reset"}),"\n",(0,s.jsx)(n.p,{children:"A dataflow reset is the most comprehensive option, refreshing the entire pipeline from source to\ndestination. This option drops destination tables, rereads data from the source into collections, and\nthen materializes that data to the destination."}),"\n",(0,s.jsx)(n.p,{children:"When you perform a dataflow reset:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Data is reread from the source system"}),"\n",(0,s.jsx)(n.li,{children:"Inferred schemas are reset"}),"\n",(0,s.jsx)(n.li,{children:"Destination tables are dropped and recreated"}),"\n",(0,s.jsx)(n.li,{children:"Collections are replaced with fresh source data"}),"\n",(0,s.jsx)(n.li,{children:"Derivations are dropped and recreated"}),"\n",(0,s.jsx)(n.li,{children:"Destination tables are repopulated with the refreshed data"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"To perform a dataflow reset:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Navigate to the Sources tab in the Flow web UI"}),"\n",(0,s.jsxs)(n.li,{children:["Start editing your capture and click the ",(0,s.jsx)(n.strong,{children:"Backfill"})," button"]}),"\n",(0,s.jsxs)(n.li,{children:["In the ",(0,s.jsx)(n.strong,{children:"Backfill mode"})," dropdown, select the ",(0,s.jsx)(n.strong,{children:"Dataflow reset"})," option"]}),"\n",(0,s.jsxs)(n.li,{children:["(Optional) Choose a specific ",(0,s.jsx)(n.a,{href:"#resource-configuration-backfill-modes",children:(0,s.jsx)(n.strong,{children:"Resource configuration backfill mode"})})," for the collection for advanced use cases"]}),"\n",(0,s.jsx)(n.li,{children:"Save and publish your changes"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This option is ideal when you need a complete refresh of your entire data pipeline, especially when\nyou suspect data inconsistencies between source, collections, and destinations."}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsxs)(n.p,{children:["If your source is connected to a ",(0,s.jsx)(n.a,{href:"/reference/Connectors/materialization-connectors/Dekaf/",children:(0,s.jsx)(n.strong,{children:"Dekaf"})})," materialization (including ClickHouse, Tinybird, StarTree, and more), consider using an ",(0,s.jsx)(n.strong,{children:"incremental backfill"})," instead of a dataflow reset to avoid schema mismatches."]})}),"\n",(0,s.jsx)(n.h3,{id:"backfill-selection",children:"Backfill Selection"}),"\n",(0,s.jsx)(n.p,{children:"Backfills can be a powerful tool to recover data, but can also result in unnecessary data costs if used incorrectly. This section will help you choose when to backfill and which backfill option to use."}),"\n",(0,s.jsx)(n.p,{children:"When deciding which backfill type to use, consider:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data retention:"})," If using Estuary's trial buckets, data expires after approximately 20\ndays. For full historical data, ",(0,s.jsx)(n.a,{href:"/getting-started/installation",children:"configure your own storage bucket"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Table size:"})," For very large tables (TBs of data), consider the impact (time, data, cost) of\ndropping and recreating tables."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Downtime tolerance:"})," Materialization and dataflow resets involve dropping destination\ntables, which creates downtime."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Update strategy:"})," Consider whether your materializations use standard (merge) or delta\nupdates, as this affects how backfilled data is handled at the destination. Using incremental\nbackfills (not dropping the destination tables) when you have materializations that use\ndelta updates may result in duplicate rows."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"By understanding these backfill types, you can choose the appropriate method to maintain data\nconsistency across your Estuary Flow pipelines while minimizing disruption to your data consumers."}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"If I want to..."})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Then I should..."})})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Refresh my collections with source data, without dropping destination tables"}),(0,s.jsxs)(n.td,{children:["Use an ",(0,s.jsx)(n.strong,{children:"Incremental Backfill"})," to pull all source data into Estuary collections"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Rebuild destination tables using existing collection data"}),(0,s.jsxs)(n.td,{children:["Use a ",(0,s.jsx)(n.strong,{children:"Materialization Backfill"})," to drop and recreate destination tables from collections"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Completely refresh my entire data pipeline from source to destination"}),(0,s.jsxs)(n.td,{children:["Use a ",(0,s.jsx)(n.strong,{children:"Dataflow Reset"})," to drop destination tables and backfill from source"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Recover from a replication slot failure in PostgreSQL"}),(0,s.jsxs)(n.td,{children:["Use an ",(0,s.jsx)(n.strong,{children:"Incremental Backfill"})," to re-establish consistency"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Add a new table to my existing data flow"}),(0,s.jsxs)(n.td,{children:["Use an ",(0,s.jsx)(n.strong,{children:"Incremental Backfill"})," for just the new binding"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Ensure my own storage bucket contains complete historical data"}),(0,s.jsxs)(n.td,{children:["Use an ",(0,s.jsx)(n.strong,{children:"Incremental Backfill"})," after setting up the new storage mapping"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Recreate a destination table from scratch when using trial buckets with limited retention"}),(0,s.jsxs)(n.td,{children:["Use a ",(0,s.jsx)(n.strong,{children:"Dataflow Reset"})," to ensure all historical source data is included"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Update my destination with schema changes from the source"}),(0,s.jsxs)(n.td,{children:["Use a ",(0,s.jsx)(n.strong,{children:"Dataflow Reset"})," to ensure schema changes are properly propagated"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Recover from data corruption in both collections and destinations"}),(0,s.jsxs)(n.td,{children:["Use a ",(0,s.jsx)(n.strong,{children:"Dataflow Reset"})," for a complete refresh of the entire pipeline"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Recover from data corruption in both collections and destinations for very large (TBs of data) datasets"}),(0,s.jsxs)(n.td,{children:["Use a ",(0,s.jsx)(n.strong,{children:"Dataflow Reset"})," for a refresh of the pipeline with capture configurations to start the backfill from a particular timestamp or transaction id"]})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"preventing-backfills",children:"Preventing backfills"}),"\n",(0,s.jsx)(n.p,{children:"Preventing backfills when possible can help save costs and computational resources. You may find it appropriate to skip the backfill, especially for extremely large datasets or tables."}),"\n",(0,s.jsx)(n.p,{children:"In this case, many connectors allow you to turn off backfilling on a per-stream or per-table basis. See each individual connector's properties for details."}),"\n",(0,s.jsx)(n.h3,{id:"preventing-backfills-during-database-upgrades",children:"Preventing backfills during database upgrades"}),"\n",(0,s.jsx)(n.p,{children:"During an upgrade, some databases invalidate a replication slot, binlog position, CDC tables, or similar. As Flow relies on these methods to keep its place, upgrades will disrupt the Flow pipeline in these cases."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["If a database upgrade ",(0,s.jsx)(n.strong,{children:"will not"})," affect these resources, the Flow connector should simply resume when the upgrade completes and no action is required."]}),"\n",(0,s.jsxs)(n.li,{children:["If a database upgrade ",(0,s.jsx)(n.strong,{children:"will"})," affect these or similar resources, you may need to trigger a backfill after the upgrade completes."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The easiest and most bulletproof solution when this happens is to backfill all bindings of the impacted capture(s) after performing the upgrade. This will permit the captures to recreate entities as necessary, establish a new CDC position, and then backfill all table contents to ensure that any changes which might have occurred in the meantime are correctly captured."}),"\n",(0,s.jsx)(n.p,{children:"However, it is common to want to avoid a full backfill when performing this sort of database maintenance, as these backfills may take some time and require a significant amount of extra data movement even if nothing has actually changed. Some connectors provide features which may be used to accomplish this, however they typically require some amount of extra setup or user knowledge to guarantee certain invariants (put simply: if there were a more efficient way to re-establish consistency in the general case, that's what we would already be doing when asked to backfill the data again)."}),"\n",(0,s.jsx)(n.p,{children:"For example, Postgres currently deletes or requires users to drop logical replication slots during a major version upgrade. To prevent a full backfill during the upgrade, follow these steps:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Pause database writes so no further changes can occur."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Monitor the current capture to ensure captures are fully up-to-date."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"These two steps ensure the connector won't miss any changes."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Perform the database upgrade."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Backfill all bindings of the capture using the ",(0,s.jsx)(n.a,{href:"#resource-configuration-backfill-modes",children:'"Only Changes" backfill mode'}),' and make sure to select "Incremental Backfill (Advanced)" from the drop down.']}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'This will not cause a full backfill. "Backfilling" all bindings at once resets the WAL (Write-Ahead Log) position for the capture, essentially allowing it to "jump ahead" to the current end of the WAL. The "Only Changes" mode will skip re-reading existing table content.  Incremental backfill will append new data to your current collection.'}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Resume database writes."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"resource-configuration-backfill-modes",children:"Resource configuration backfill modes"}),"\n",(0,s.jsxs)(n.p,{children:['The connectors that use CDC (Change Data Capture) allow fine-grained control of backfills for individual tables. These bindings include a "Backfill Mode" dropdown in their resource configuration. This setting then translates to a ',(0,s.jsx)(n.code,{children:"mode"})," field for that resource in the specification. For example:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'"bindings": [\n    {\n      "resource": {\n        "namespace": "public",\n        "stream": "tableName",\n        "mode": "Only Changes"\n      },\n      "target": "Artificial-Industries/postgres/public/tableName"\n    }\n  ]\n'})}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsxs)(n.p,{children:["In general, you should not change this setting. Make sure you understand your use case, such as ",(0,s.jsx)(n.a,{href:"#preventing-backfills-during-database-upgrades",children:"preventing backfills"}),"."]})}),"\n",(0,s.jsx)(n.p,{children:"The following modes are available:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Normal:"})," backfills chunks of the table and emits all replication events regardless of whether they occur within the backfilled portion of the table or not."]}),"\n",(0,s.jsx)(n.p,{children:"In Normal mode, the connector fetches key-ordered chunks of the table for the backfill while performing reads of the WAL.\nAll WAL changes are emitted immediately, whether or not they relate to an unread portion of the table. Therefore, if a change is made, it shows up quickly even if its table is still backfilling."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Precise:"})," backfills chunks of the table and filters replication events in portions of the table which haven't yet been reached."]}),"\n",(0,s.jsx)(n.p,{children:"In Precise mode, the connector fetches key-ordered chunks of the table for the backfill while performing reads of the WAL.\nAny WAL changes for portions of the table that have already been backfilled are emitted. In contrast to Normal mode, however, WAL changes are suppressed if they relate to a part of the table that hasn't been backfilled yet."}),"\n",(0,s.jsx)(n.p,{children:"WAL changes and backfill chunks get stitched together to produce a fully consistent logical sequence of changes for each key. For example, you are guaranteed to see an insert before an update or delete."}),"\n",(0,s.jsx)(n.p,{children:"Note that Precise backfill is not possible in some cases due to equality comparison challenges when using varying character encodings."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Only Changes:"})," skips backfilling the table entirely and jumps directly to replication streaming for the entire dataset."]}),"\n",(0,s.jsx)(n.p,{children:"No backfill of the table content is performed at all. Only WAL changes are emitted."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Without Primary Key:"})," can be used to capture tables without any form of unique primary key."]}),"\n",(0,s.jsxs)(n.p,{children:["The connector uses an alternative physical row identifier (such as a Postgres ",(0,s.jsx)(n.code,{children:"ctid"}),") to scan backfill chunks, rather than walking the table in key order."]}),"\n",(0,s.jsx)(n.p,{children:"This mode lacks the exact correctness properties of the Normal backfill mode."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"If you do not choose a specific backfill mode, Flow will default to an automatic mode."}),"\n",(0,s.jsx)(n.h2,{id:"advanced-backfill-configuration-in-specific-systems",children:"Advanced backfill configuration in specific systems"}),"\n",(0,s.jsx)(n.h3,{id:"postgresql-capture",children:"PostgreSQL Capture"}),"\n",(0,s.jsxs)(n.p,{children:["PostgreSQL's ",(0,s.jsx)(n.code,{children:"xmin"})," system column can be used as a cursor to keep track of the current location in a table. If you need to re-backfill a Postgres table, you can reduce the affected data volume by specifying a minimum or maximum backfill ",(0,s.jsx)(n.code,{children:"XID"}),". Estuary will only backfill rows greater than or less than the specified ",(0,s.jsx)(n.code,{children:"XID"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["This can be especially useful in cases where you do not want to re-backfill a full table, but cannot complete the steps in ",(0,s.jsx)(n.a,{href:"#preventing-backfills",children:"Preventing backfills"})," above, such as if you cannot pause database writes during an upgrade."]}),"\n",(0,s.jsx)(n.p,{children:"To configure this option:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Determine the ",(0,s.jsx)(n.code,{children:"xmin"})," value you want to use."]}),"\n",(0,s.jsxs)(n.p,{children:["You can run a query to find a suitable ",(0,s.jsx)(n.code,{children:"XID"}),", such as:\n",(0,s.jsx)(n.code,{children:"SELECT xmin FROM {your_table_name} WHERE created_at < {desired_timestamp} and created_at > {desired_timestamp};"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"In the Estuary dashboard, edit your PostgreSQL Capture."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Under Endpoint Config, expand ",(0,s.jsx)(n.strong,{children:"Advanced Options"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:['Fill out the "Minimum Backfill XID" or "Maximum Backfill XID" field with the ',(0,s.jsx)(n.code,{children:"xmin"})," value you retrieved."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:'Click "Backfill".'}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Save and publish your changes."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["In rare cases, this method may not work as expected, as in situations where a database has already filled up its entire ",(0,s.jsx)(n.code,{children:"xmin"})," space. In such cases of ",(0,s.jsx)(n.code,{children:"xmin"})," wrapping, using both Minimum and Maximum Backfill XID fields can help narrow down a specific range to backfill."]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},90914:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/backfill-options-b0cce7e2021ec49c333a678370d6fe15.png"}}]);