"use strict";(self.webpackChunksite=self.webpackChunksite||[]).push([[8152],{14121:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>r,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"getting-started/tutorials/postgresql_cdc_to_snowflake","title":"PostgreSQL CDC streaming to Snowflake","description":"Introduction","source":"@site/docs/getting-started/tutorials/postgresql_cdc_to_snowflake.md","sourceDirName":"getting-started/tutorials","slug":"/getting-started/tutorials/postgresql_cdc_to_snowflake","permalink":"/getting-started/tutorials/postgresql_cdc_to_snowflake","draft":false,"unlisted":false,"editUrl":"https://github.com/estuary/flow/edit/master/site/docs/getting-started/tutorials/postgresql_cdc_to_snowflake.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"id":"postgresql_cdc_to_snowflake","title":"PostgreSQL CDC streaming to Snowflake","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Flow tutorials","permalink":"/getting-started/tutorials/"},"next":{"title":"Real-time CDC with MongoDB","permalink":"/getting-started/tutorials/real_time_cdc_with_mongodb"}}');var s=a(74848),o=a(28453);const r={id:"postgresql_cdc_to_snowflake",title:"PostgreSQL CDC streaming to Snowflake",sidebar_position:1},i="PostgreSQL CDC streaming to Snowflake",l={},c=[{value:"Introduction<a></a>",id:"introduction",level:2},{value:"What is CDC?<a></a>",id:"what-is-cdc",level:2},{value:"Prerequisites<a></a>",id:"prerequisites",level:2},{value:"Step 1. Set up source database<a></a>",id:"step-1-set-up-source-database",level:2},{value:"PostgreSQL setup<a></a>",id:"postgresql-setup",level:3},{value:"Configuring PostgreSQL for CDC<a></a>",id:"configuring-postgresql-for-cdc",level:3},{value:"Expose the database to the internet via ngrok<a></a>",id:"expose-the-database-to-the-internet-via-ngrok",level:3},{value:"Step 2. Set up a Capture<a></a>",id:"step-2-set-up-a-capture",level:2},{value:"Step 3. Set up a Materialization<a></a>",id:"step-3-set-up-a-materialization",level:2},{value:"Party time!<a></a>",id:"party-time",level:2},{value:"Clean up<a></a>",id:"clean-up",level:2},{value:"Next Steps<a></a>",id:"next-steps",level:2}];function d(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components},{Head:a}=t;return a||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Head",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(a,{children:(0,s.jsx)("meta",{property:"og:image",content:"https://storage.googleapis.com/estuary-marketing-strapi-uploads/uploads//architecture_6bbaf2c5a6/architecture_6bbaf2c5a6.png"})}),"\n",(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"postgresql-cdc-streaming-to-snowflake",children:"PostgreSQL CDC streaming to Snowflake"})}),"\n",(0,s.jsxs)(t.h2,{id:"introduction",children:["Introduction",(0,s.jsx)("a",{id:"introduction"})]}),"\n",(0,s.jsx)(t.p,{children:"In this tutorial, we'll set up a streaming CDC pipeline from PostgreSQL to Snowflake using Estuary Flow. By the end, you\u2019ll have learned everything you need to know about building a pipeline on your own."}),"\n",(0,s.jsx)(t.p,{children:"You'll use Flow's PostgreSQL capture connector and Snowflake materialization connector to set up an end-to-end CDC pipeline in three steps:"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"First, you\u2019ll ingest change event data from a PostgreSQL database, using a table filled with generated realistic product data."}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"Then, you\u2019ll learn how to configure Flow to persist data as collections while maintaining data integrity."}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"And finally, you will see how you can materialize these collections in Snowflake to make them ready for analytics!"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"By the end of this tutorial, you'll have established a robust and efficient data pipeline with near real-time replication of data from PostgreSQL to Snowflake."}),"\n",(0,s.jsx)(t.p,{children:"Before you get started, make sure you do two things."}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["Sign up for Estuary Flow ",(0,s.jsx)(t.a,{href:"https://dashboard.estuary.dev/register",children:"here"}),". It\u2019s simple, fast and free."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["Make sure you also join the ",(0,s.jsx)(t.a,{href:"https://estuary-dev.slack.com/ssb/redirect#/shared-invite/email",children:"Estuary Slack Community"}),". Don\u2019t struggle. Just ask a question."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(t.h2,{id:"what-is-cdc",children:["What is CDC?",(0,s.jsx)("a",{id:"what-is-cdc"})]}),"\n",(0,s.jsx)(t.p,{children:"CDC, or Change Data Capture, is a method used to track and capture changes made to data in a database. It enables the real-time capture of insertions, updates, and deletions, providing a continuous stream of changes."}),"\n",(0,s.jsx)(t.p,{children:"This stream of change data is invaluable for keeping downstream systems synchronized and up-to-date with the source database, facilitating real-time analytics, replication, and data integration. In essence, CDC allows organizations to capture and react to data changes as they occur, ensuring data accuracy and timeliness across their systems. CDC provides a lower-latency, lower-load way to extract data. It\u2019s also often the only way to capture every change as well as deletes, which are harder to track with batch-based extraction."}),"\n",(0,s.jsxs)(t.p,{children:["If you are interested in the intricacies of change data capture, head over to ",(0,s.jsx)(t.a,{href:"https://estuary.dev/cdc-done-correctly/",children:"this"})," article, where we explain the theory behind it - this is not a requirement for this tutorial, so if you want to dive in head first, keep on reading!"]}),"\n",(0,s.jsxs)(t.h2,{id:"prerequisites",children:["Prerequisites",(0,s.jsx)("a",{id:"prerequisites"})]}),"\n",(0,s.jsx)(t.p,{children:"This tutorial will assume you have access to the following things:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"Docker: for convenience, we are providing a docker compose definition which will allow you to spin up a database and a fake data generator service in about 5 seconds!"}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsxs)(t.p,{children:["ngrok: Flow is a fully managed service. Because the database used in this tutorial will be running on your machine, you\u2019ll need something to expose it to the internet. ",(0,s.jsx)(t.a,{href:"https://ngrok.com/",children:"ngrok"})," is a lightweight tool that does just that."]}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:"Snowflake account: The target data warehouse for our flow is Snowflake. In order to follow along with the tutorial, a trial account is perfectly fine."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(t.h2,{id:"step-1-set-up-source-database",children:["Step 1. Set up source database",(0,s.jsx)("a",{id:"step-1-set-up-source-database"})]}),"\n",(0,s.jsxs)(t.h3,{id:"postgresql-setup",children:["PostgreSQL setup",(0,s.jsx)("a",{id:"postgresql-setup"})]}),"\n",(0,s.jsx)(t.p,{children:"As this tutorial is focused on CDC replication from PostgreSQL, we\u2019ll need a database. We recommend you create this database first, so you can learn Flow more easily. Then try these steps on your own database. Let\u2019s take a look at what we are working with!"}),"\n",(0,s.jsxs)(t.p,{children:["Save the below ",(0,s.jsx)(t.code,{children:"yaml"})," snippet as a file called ",(0,s.jsx)(t.code,{children:"docker-compose.yml"}),". This ",(0,s.jsx)(t.code,{children:"docker-compose.yml"})," file contains the service definitions for the PostgreSQL database and the mock data generator service."]}),"\n",(0,s.jsx)(t.admonition,{type:"tip",children:(0,s.jsx)(t.p,{children:"Since V2, compose is integrated into your base Docker package, there\u2019s no need to download any separate tooling!"})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-yaml",metastring:'title="docker-compose.yml"',children:'services:\n\xa0\xa0postgres:\n\xa0\xa0\xa0\xa0image: postgres:latest\n\xa0\xa0\xa0\xa0container_name: postgres_cdc\n\xa0\xa0\xa0\xa0hostname: postgres_cdc\n\xa0\xa0\xa0\xa0restart: unless-stopped\n\xa0\xa0\xa0\xa0user: postgres\n\xa0\xa0\xa0\xa0environment:\n\xa0\xa0\xa0\xa0\xa0\xa0POSTGRES_USER: postgres\n\xa0\xa0\xa0\xa0\xa0\xa0POSTGRES_DB: postgres\n\xa0\xa0\xa0\xa0\xa0\xa0POSTGRES_PASSWORD: postgres\n\xa0\xa0\xa0\xa0command:\n\xa0\xa0\xa0\xa0\xa0\xa0- "postgres"\n\xa0\xa0\xa0\xa0\xa0\xa0- "-c"\n\xa0\xa0\xa0\xa0\xa0\xa0- "wal_level=logical"\n\xa0\xa0\xa0\xa0healthcheck:\n\xa0\xa0\xa0\xa0\xa0\xa0test: ["CMD-SHELL", "sh -c \'pg_isready -U flow_capture -d postgres\'"]\n\xa0\xa0\xa0\xa0\xa0\xa0interval: 5s\n\xa0\xa0\xa0\xa0\xa0\xa0timeout: 10s\n\xa0\xa0\xa0\xa0\xa0\xa0retries: 120\xa0\xa0\xa0\n\xa0\xa0\xa0\xa0volumes:\n\xa0\xa0\xa0\xa0\xa0\xa0- ./init.sql:/docker-entrypoint-initdb.d/init.sql\xa0\n\xa0\xa0\xa0\xa0ports:\n\xa0\xa0\xa0\xa0\xa0\xa0- "5432:5432"\n\n\xa0\xa0datagen:\n\xa0\xa0\xa0\xa0image: materialize/datagen\n\xa0\xa0\xa0\xa0container_name: datagen\n\xa0\xa0\xa0\xa0restart: unless-stopped\n\xa0\xa0\xa0\xa0environment:\n\xa0\xa0\xa0\xa0\xa0\xa0POSTGRES_HOST: postgres_cdc\n\xa0\xa0\xa0\xa0\xa0\xa0POSTGRES_PORT: 5432\n\xa0\xa0\xa0\xa0\xa0\xa0POSTGRES_DB: postgres\n\xa0\xa0\xa0\xa0\xa0\xa0POSTGRES_USER: postgres\n\xa0\xa0\xa0\xa0\xa0\xa0POSTGRES_PASSWORD: postgres\n\xa0\xa0\xa0\xa0entrypoint:\n\xa0\xa0\xa0\xa0\xa0\xa0"datagen -s /app/schemas/products.sql -n 10000 -f postgres -w 1000"\n\xa0\xa0\xa0\xa0depends_on:\n\xa0\xa0\xa0\xa0\xa0\xa0postgres:\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0condition: service_healthy\n\n\xa0\xa0\xa0\xa0volumes:\n    \xa0\xa0\xa0\xa0\xa0\xa0- ./schemas/products.sql:/app/schemas/products.sql\n'})}),"\n",(0,s.jsx)(t.p,{children:"Don\u2019t be alarmed by all these Docker configurations, they are made to be reproducible on any machine, so you don\u2019t have to worry about modifying anything in them! Before you spin up the database, let\u2019s take a quick look at what exactly you can expect to happen."}),"\n",(0,s.jsxs)(t.p,{children:["Next up, create a folder called ",(0,s.jsx)(t.code,{children:"schemas"})," and paste the below SQL DDL into a file called ",(0,s.jsx)(t.code,{children:"products.sql"}),". This file contains the schema of the demo data."]}),"\n",(0,s.jsx)(t.admonition,{type:"note",children:(0,s.jsxs)(t.p,{children:["This file defines the schema via a create table statement, but the actual table creation happens in the ",(0,s.jsx)(t.code,{children:"init.sql"})," file, this is just a quirk of the ",(0,s.jsx)(t.a,{href:"https://github.com/MaterializeInc/datagen",children:"Datagen"})," data generator tool."]})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",metastring:'title="products.sql"',children:'CREATE TABLE "public"."products" (\n\xa0\xa0"id" int PRIMARY KEY,\n\xa0\xa0"name" varchar COMMENT \'faker.internet.userName()\',\n\xa0\xa0"merchant_id" int NOT NULL COMMENT \'faker.datatype.number()\',\n\xa0\xa0"price" int COMMENT \'faker.datatype.number()\',\n\xa0\xa0"status" varchar COMMENT \'faker.datatype.boolean()\',\n\xa0\xa0"created_at" timestamp DEFAULT (now())\n);\n'})}),"\n",(0,s.jsxs)(t.p,{children:["If you take a closer look at the schema definition, you can see that in the ",(0,s.jsx)(t.code,{children:"COMMENT"})," attribute we define Python snippets which actually tell Datagen how to generate fake data for those fields!"]}),"\n",(0,s.jsxs)(t.p,{children:["Finally, create the ",(0,s.jsx)(t.code,{children:"init.sql"})," file, which contains the database-level requirements to enable Flow to stream CDC data."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",metastring:'title="init.sql"',children:'CREATE USER flow_capture WITH PASSWORD \'secret\' REPLICATION;\n\nGRANT pg_read_all_data TO flow_capture;\n\nCREATE TABLE products (\n\xa0\xa0"id" int PRIMARY KEY,\n\xa0\xa0"name" varchar COMMENT \'faker.internet.userName()\',\n\xa0\xa0"merchant_id" int NOT NULL COMMENT \'faker.datatype.number()\',\n\xa0\xa0"price" int COMMENT \'faker.datatype.number()\',\n\xa0\xa0"status" varchar COMMENT \'faker.datatype.boolean()\',\n\xa0\xa0"created_at" timestamp DEFAULT (now())\n);\n\nCREATE TABLE IF NOT EXISTS public.flow_watermarks (slot TEXT PRIMARY KEY, watermark TEXT);\nGRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture;\nCREATE PUBLICATION flow_publication;\nALTER PUBLICATION flow_publication SET (publish_via_partition_root = true);\nALTER PUBLICATION flow_publication ADD TABLE public.flow_watermarks, public.products;\n'})}),"\n",(0,s.jsxs)(t.p,{children:["In the ",(0,s.jsx)(t.code,{children:"init.sql"})," file, you create the products table and all the database objects Flow requires for real-time CDC streaming."]}),"\n",(0,s.jsxs)(t.h3,{id:"configuring-postgresql-for-cdc",children:["Configuring PostgreSQL for CDC",(0,s.jsx)("a",{id:"configuring-postgresql-for-cdc"})]}),"\n",(0,s.jsx)(t.p,{children:"To enable CDC replication in PostgreSQL, several database objects need to be created and configured. These objects facilitate the capture and propagation of data changes to downstream systems. Let's examine each object and its significance in the context of CDC replication:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"CREATE USER flow_capture WITH PASSWORD 'secret' REPLICATION;\n"})}),"\n",(0,s.jsx)(t.p,{children:"This user is dedicated to the CDC replication process. It is granted the necessary permissions to read all data from the database, allowing it to capture changes across tables efficiently. In a production environment, make sure you use a more secure password than what is in the example."}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"GRANT pg_read_all_data TO flow_capture;\n"})}),"\n",(0,s.jsxs)(t.p,{children:["Granting the ",(0,s.jsx)(t.code,{children:"pg_read_all_data"})," privilege to the ",(0,s.jsx)(t.code,{children:"flow_capture"})," user ensures that it can access and read data from all tables in the database, essential for capturing changes."]}),"\n",(0,s.jsx)(t.admonition,{type:"note",children:(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.code,{children:"pg_read_all_data"})," is used for convenience, but is not a hard requirement, since it is possible to grant a more granular set of permissions. For more details check out the ",(0,s.jsx)(t.a,{href:"https://docs.estuary.dev/reference/Connectors/capture-connectors/PostgreSQL/#self-hosted-postgresql",children:"connector docs"}),"."]})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"CREATE TABLE products (...)\n"})}),"\n",(0,s.jsxs)(t.p,{children:["The source tables, such as the ",(0,s.jsx)(t.code,{children:"products"})," table in this example, contain the data whose changes we want to capture and replicate. It is recommended for tables to have a primary key defined, although not a hard requirement for CDC."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"CREATE TABLE IF NOT EXISTS public.flow_watermarks (...)\n"})}),"\n",(0,s.jsxs)(t.p,{children:["The ",(0,s.jsx)(t.code,{children:"flow_watermarks"})," table is a small \u201cscratch space\u201d to which the connector occasionally writes a small amount of data to ensure accuracy when backfilling preexisting table contents."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"GRANT ALL PRIVILEGES ON TABLE public.flow_watermarks TO flow_capture;\n"})}),"\n",(0,s.jsxs)(t.p,{children:["The ",(0,s.jsx)(t.code,{children:"flow_capture"})," user needs full privileges on the ",(0,s.jsx)(t.code,{children:"flow_watermarks"})," table to insert, update, and query metadata related to the replication process."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"CREATE PUBLICATION flow_publication;\nALTER PUBLICATION flow_publication SET (publish_via_partition_root = true);\nALTER PUBLICATION flow_publication ADD TABLE public.flow_watermarks, public.products;\n"})}),"\n",(0,s.jsxs)(t.p,{children:["A publication defines a set of tables whose changes will be replicated. In this case, the ",(0,s.jsx)(t.code,{children:"flow_publication"})," publication includes the ",(0,s.jsx)(t.code,{children:"public.flow_watermarks"})," and ",(0,s.jsx)(t.code,{children:"public.products"})," tables."]}),"\n",(0,s.jsxs)(t.p,{children:["These commands configure the ",(0,s.jsx)(t.code,{children:"flow_publication"})," publication to publish changes via partition root and add the specified tables to the publication. By setting ",(0,s.jsx)(t.code,{children:"publish_via_partition_root"})," to true, the publication ensures that updates to partitioned tables are correctly captured and replicated."]}),"\n",(0,s.jsx)(t.admonition,{type:"note",children:(0,s.jsxs)(t.p,{children:["The table in this tutorial is not partitioned, but we recommend always setting ",(0,s.jsx)(t.code,{children:"publish_via_partition_root"})," when creating a publication."]})}),"\n",(0,s.jsx)(t.p,{children:"These objects form the backbone of a robust CDC replication setup, ensuring data consistency and integrity across systems. After the initial setup, you will not have to touch these objects in the future, unless you wish to start ingesting change events from a new table."}),"\n",(0,s.jsxs)(t.p,{children:["With that out of the way, you\u2019re ready to start the source database. In order to initialize Postgres and the fake data generator service, all you have to do is execute the following (to free up your current terminal, use the ",(0,s.jsx)(t.code,{children:"-d"})," flag so the containers run in a daemonized background process):"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sh",children:"docker compose up\n"})}),"\n",(0,s.jsxs)(t.p,{children:["After a few seconds, you should see that both services are up and running. The ",(0,s.jsx)(t.code,{children:"postgres_cdc"})," service should print the following on the terminal:"]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sh",children:"postgres_cdc\xa0 | LOG:\xa0 database system is ready to accept connections\n"})}),"\n",(0,s.jsxs)(t.p,{children:["While the ",(0,s.jsx)(t.code,{children:"datagen"})," service will be a little bit more spammy, as it prints every record it generates, but don\u2019t be alarmed, this is enough for us to verify that both are up and running. Let\u2019s see how we can expose the database so Flow can connect to it."]}),"\n",(0,s.jsxs)(t.h3,{id:"expose-the-database-to-the-internet-via-ngrok",children:["Expose the database to the internet via ngrok",(0,s.jsx)("a",{id:"expose-the-database-to-the-internet-via-ngrok"})]}),"\n",(0,s.jsx)(t.p,{children:"As mentioned above, the next step is to make the database available for other services. To do this in one quick command, we can use ngrok, a free CLI tool that enables tunneling of services. In our case we only want to expose the port 5432 and only the tcp protocol."}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sh",children:"ngrok tcp 5432\n"})}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://storage.googleapis.com/estuary-marketing-strapi-uploads/uploads//ngrok_87b04412a3/ngrok_87b04412a3.png",alt:"ngrok"})}),"\n",(0,s.jsxs)(t.p,{children:["You should immediately be greeted with a screen that contains the public URL for the tunnel we just started! In the example above, the public URL ",(0,s.jsx)(t.code,{children:"5.tcp.eu.ngrok.io:14407"})," is mapped to ",(0,s.jsx)(t.code,{children:"localhost:5432"}),", which is the address of the Postgres database."]}),"\n",(0,s.jsx)(t.admonition,{type:"note",children:(0,s.jsx)(t.p,{children:"Don\u2019t close this window while working on the tutorial as this is required to keep the connections between Flow and the database alive."})}),"\n",(0,s.jsx)(t.p,{children:"Before we jump into setting up the replication, you can quickly verify the data being properly generated by connecting to the database and peeking into the products table, as shown below:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sh",children:'~ psql -h 5.tcp.eu.ngrok.io -p 14407 -U postgres -d postgres\nPassword for user postgres:\npsql (16.2)\nType "help" for help.\n\npostgres=# \\d\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0List of relations\n\xa0Schema |\xa0 \xa0 \xa0 Name \xa0 \xa0 \xa0 | Type\xa0 |\xa0 Owner\n--------+-----------------+-------+----------\n\xa0public | flow_watermarks | table | postgres\n\xa0public | products\xa0 \xa0 \xa0 \xa0 | table | postgres\n(2 rows)\n\npostgres=# select count(*) from products;\n\xa0count\n-------\n\xa0\xa02637\n(1 row)\n\npostgres=# select count(*) from products;\n\xa0count\n-------\n\xa0\xa02642\n(1 row)\n'})}),"\n",(0,s.jsxs)(t.p,{children:["By executing a ",(0,s.jsx)(t.code,{children:"count(*)"})," statement a few seconds apart you are able to verify that data is in fact being written into the table."]}),"\n",(0,s.jsxs)(t.h2,{id:"step-2-set-up-a-capture",children:["Step 2. Set up a Capture",(0,s.jsx)("a",{id:"step-2-set-up-a-capture"})]}),"\n",(0,s.jsxs)(t.p,{children:["Good news, the hard part is over! Smooth sailing from here on out. Head over to your Flow dashboard (if you haven\u2019t registered yet, you can do so ",(0,s.jsx)(t.a,{href:"https://dashboard.estuary.dev/register",children:"here"}),".) and create a new ",(0,s.jsx)(t.strong,{children:"Capture."})," A capture is how Flow ingests data from an external source. Every Data Flow starts with a Capture."]}),"\n",(0,s.jsxs)(t.p,{children:["Go to the sources page by clicking on the ",(0,s.jsx)(t.strong,{children:"Sources"})," on the left hand side of your screen, then click on ",(0,s.jsx)(t.strong,{children:"+ New Capture"})]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://storage.googleapis.com/estuary-marketing-strapi-uploads/uploads//new_capture_4583a8a120/new_capture_4583a8a120.png",alt:"Add new Capture"})}),"\n",(0,s.jsxs)(t.p,{children:["Configure the connection to the database based on the information we gathered in the previous step and press ",(0,s.jsx)(t.strong,{children:"Next."})]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://storage.googleapis.com/estuary-marketing-strapi-uploads/uploads//capture_configuration_89e2133f83/capture_configuration_89e2133f83.png",alt:"Configure Capture"})}),"\n",(0,s.jsx)(t.p,{children:"On the following page, we can configure how our incoming data should be represented in Flow as collections. As a quick refresher, let\u2019s recap how Flow represents data on a high level."}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Documents"})}),"\n",(0,s.jsxs)(t.p,{children:["The documents of your flows are stored in collections: real-time data lakes of JSON documents in cloud storage. Documents being backed by an object storage mean that once you start capturing data, you won\u2019t have to worry about it not being available to replay \u2013 object stores such as S3 can be configured to cheaply store data forever. See ",(0,s.jsx)(t.a,{href:"https://docs.estuary.dev/concepts/collections/#documents",children:"docs page"})," for more information."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Schemas"})}),"\n",(0,s.jsx)(t.p,{children:"Flow documents and collections always have an associated schema that defines the structure, representation, and constraints of your documents. In most cases, Flow generates a functioning schema on your behalf during the discovery phase of capture, which has already automatically happened - that\u2019s why you\u2019re able to take a peek into the structure of the incoming data!"}),"\n",(0,s.jsx)(t.p,{children:"To see how Flow parsed the incoming records, click on the Collection tab and verify the inferred schema looks correct."}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://storage.googleapis.com/estuary-marketing-strapi-uploads/uploads//collections_configuration_34e53025c7/collections_configuration_34e53025c7.png",alt:"Configure Collections"})}),"\n",(0,s.jsx)(t.p,{children:"Before you advance to the next step, let\u2019s take a look at the other configuration options we have here. You\u2019ll see three toggles, all turned on by default:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Automatically keep schemas up to date"})}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Automatically add new source collections"})}),"\n"]}),"\n",(0,s.jsxs)(t.li,{children:["\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Breaking changes re-version collections"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"All of these settings relate to how Flow handles schema evolution, so let\u2019s take a quick detour to explain them from a high-level perspective."}),"\n",(0,s.jsx)(t.p,{children:"Estuary Flow's schema evolution feature seamlessly handles updates to dataset structures within a Data Flow, ensuring uninterrupted operation. Collection specifications define each dataset, including key, schema, and partitions. When specs change, schema evolution automatically updates associated components to maintain compatibility."}),"\n",(0,s.jsx)(t.p,{children:"It addresses breaking changes by updating materializations or recreating collections with new names, preventing disruptions. Common causes of breaking changes include modifications to collection schemas, which require updates to materializations."}),"\n",(0,s.jsx)(t.p,{children:"Overall, schema evolution streamlines adaptation to structural changes, maintaining smooth data flow within the system."}),"\n",(0,s.jsxs)(t.p,{children:["For more information, check out the dedicated documentation ",(0,s.jsx)(t.a,{href:"https://docs.estuary.dev/guides/schema-evolution/",children:"page"})," for schema evolution."]}),"\n",(0,s.jsxs)(t.p,{children:["For the sake of this tutorial, feel free to leave everything at its default setting and press ",(0,s.jsx)(t.strong,{children:"Next"})," again, then ",(0,s.jsx)(t.strong,{children:"Save and Publish"})," to deploy the connector and kick off a backfill."]}),"\n",(0,s.jsxs)(t.h2,{id:"step-3-set-up-a-materialization",children:["Step 3. Set up a Materialization",(0,s.jsx)("a",{id:"step-3-set-up-a-materialization"})]}),"\n",(0,s.jsx)(t.p,{children:"Similarly to the source side, we\u2019ll need to set up some initial configuration in Snowflake to allow Flow to materialize collections into a table."}),"\n",(0,s.jsx)(t.p,{children:"Preparing Snowflake for use with Estuary Flow involves the following steps:"}),"\n",(0,s.jsx)(t.p,{children:"1. Keep the Flow web app open and open a new tab or window to access your Snowflake console."}),"\n",(0,s.jsx)(t.p,{children:"2. Create a new SQL worksheet. This provides a platform to execute queries."}),"\n",(0,s.jsxs)(t.p,{children:["3. Paste the provided script into the SQL console, adjusting the value for ",(0,s.jsx)(t.code,{children:"estuary_password"})," to a strong password."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"set database_name = 'ESTUARY_DB';\nset warehouse_name = 'ESTUARY_WH';\nset estuary_role = 'ESTUARY_ROLE';\nset estuary_user = 'ESTUARY_USER';\nset estuary_password = 'secret';\nset estuary_schema = 'ESTUARY_SCHEMA';\n-- create role and schema for Estuary\ncreate role if not exists identifier($estuary_role);\ngrant role identifier($estuary_role) to role SYSADMIN;\n-- Create snowflake DB\ncreate database if not exists identifier($database_name);\nuse database identifier($database_name);\ncreate schema if not exists identifier($estuary_schema);\n-- create a user for Estuary\ncreate user if not exists identifier($estuary_user)\npassword = $estuary_password\ndefault_role = $estuary_role\ndefault_warehouse = $warehouse_name;\ngrant role identifier($estuary_role) to user identifier($estuary_user);\ngrant all on schema identifier($estuary_schema) to identifier($estuary_role);\n-- create a warehouse for estuary\ncreate warehouse if not exists identifier($warehouse_name)\nwarehouse_size = xsmall\nwarehouse_type = standard\nauto_suspend = 60\nauto_resume = true\ninitially_suspended = true;\n-- grant Estuary role access to warehouse\ngrant USAGE\non warehouse identifier($warehouse_name)\nto role identifier($estuary_role);\n-- grant Estuary access to database\ngrant CREATE SCHEMA, MONITOR, USAGE on database identifier($database_name) to role identifier($estuary_role);\n-- change role to ACCOUNTADMIN for STORAGE INTEGRATION support to Estuary (only needed for Snowflake on GCP)\nuse role ACCOUNTADMIN;\ngrant CREATE INTEGRATION on account to role identifier($estuary_role);\nuse role sysadmin;\nCOMMIT;\n"})}),"\n",(0,s.jsx)(t.p,{children:'4. Execute all the queries by clicking the drop-down arrow next to the Run button and selecting "Run All."'}),"\n",(0,s.jsx)(t.p,{children:"5. Snowflake will process the queries, setting up the necessary roles, databases, schemas, users, and warehouses for Estuary Flow."}),"\n",(0,s.jsx)(t.p,{children:"6. Once the setup is complete, return to the Flow web application to continue with the integration process."}),"\n",(0,s.jsxs)(t.p,{children:["Back in Flow, head over to the ",(0,s.jsx)(t.strong,{children:"Destinations"})," page, where you can ",(0,s.jsx)(t.a,{href:"https://dashboard.estuary.dev/materializations/create",children:"create a new Materialization"}),"."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://storage.googleapis.com/estuary-marketing-strapi-uploads/uploads//new_materialization_31df04d81f/new_materialization_31df04d81f.png",alt:"Add new Materialization"})}),"\n",(0,s.jsx)(t.p,{children:"Choose Snowflake and start filling out the connection details based on the values inside the script you executed in the previous step. If you haven\u2019t changed anything, this is how the connector configuration should look like:"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://storage.googleapis.com/estuary-marketing-strapi-uploads/uploads//materialization_endpoint_configuration_0d540a12b5/materialization_endpoint_configuration_0d540a12b5.png",alt:"Configure Materialization endpoint"})}),"\n",(0,s.jsx)(t.p,{children:"You can grab your Snowflake host URL and account identifier by navigating to these two little buttons on the Snowflake UI."}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://storage.googleapis.com/estuary-marketing-strapi-uploads/uploads//snowflake_account_id_af1cc78df8/snowflake_account_id_af1cc78df8.png",alt:"Grab your Snowflake account id"})}),"\n",(0,s.jsx)(t.p,{children:'If you scroll down to the Advanced Options section, you will be able to configure the "Update Delay" parameter. If you leave this parameter unset, the default value of 30 minutes will be used.'}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://storage.googleapis.com/estuary-marketing-strapi-uploads/uploads//snowflake_update_delay_dark_f26179d3fc/snowflake_update_delay_dark_f26179d3fc.png",alt:"Update Delay"})}),"\n",(0,s.jsx)(t.p,{children:"The Update Delay parameter in Estuary materializations offers a flexible approach to data ingestion scheduling. It represents the amount of time the system will wait before it begins materializing the latest data."}),"\n",(0,s.jsx)(t.p,{children:"For example, if an update delay is set to 2 hours, the materialization task will pause for 2 hours before processing the latest available data. This delay ensures that data is not pulled in immediately after it becomes available, allowing your Snowflake warehouse to go idle and be suspended in between updates, which can significantly reduce the number of credits consumed."}),"\n",(0,s.jsx)(t.p,{children:"After the connection details are in place, the next step is to link the capture we just created to Flow is able to see collections we are loading data into from Postgres."}),"\n",(0,s.jsx)(t.p,{children:"You can achieve this by clicking on the \u201cSource from Capture\u201d button, and selecting the name of the capture from the table."}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://storage.googleapis.com/estuary-marketing-strapi-uploads/uploads//link_source_to_capture_b0d37a738f/link_source_to_capture_b0d37a738f.png",alt:"Link Capture"})}),"\n",(0,s.jsxs)(t.p,{children:["After pressing continue, you are met with a few configuration options, but for now, feel free to press ",(0,s.jsx)(t.strong,{children:"Next,"})," then ",(0,s.jsx)(t.strong,{children:"Save and Publish"})," in the top right corner, the defaults will work perfectly fine for this tutorial."]}),"\n",(0,s.jsx)(t.p,{children:"A successful deployment will look something like this:"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://storage.googleapis.com/estuary-marketing-strapi-uploads/uploads//publish_successful_4e18642288/publish_successful_4e18642288.png",alt:"Successful Deployment screen"})}),"\n",(0,s.jsx)(t.p,{children:"And that\u2019s pretty much it, you\u2019ve successfully published a real-time CDC pipeline. Let\u2019s check out Snowflake to see how the data looks."}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://storage.googleapis.com/estuary-marketing-strapi-uploads/uploads//snowflake_verification_2eb047efec/snowflake_verification_2eb047efec.png",alt:"Results in Snowflake"})}),"\n",(0,s.jsx)(t.p,{children:"Looks like the data is arriving as expected, and the schema of the table is properly configured by the connector based on the types of the original table in Postgres."}),"\n",(0,s.jsx)(t.p,{children:"To get a feel for how the data flow works, head over to the collection details page on the Flow web UI to see your changes immediately. On the Snowflake end, they will be materialized after the next update."}),"\n",(0,s.jsx)(t.admonition,{type:"note",children:(0,s.jsx)(t.p,{children:'Based on your configuration of the "Update Delay" parameter when setting up the Snowflake Materialization, you might have to wait until the configured amount of time passes for your changes to make it to the destination.'})}),"\n",(0,s.jsxs)(t.h2,{id:"party-time",children:["Party time!",(0,s.jsx)("a",{id:"party-time"})]}),"\n",(0,s.jsx)(t.p,{children:"Congratulations! \ud83c\udf89 You've successfully set up a CDC pipeline from PostgreSQL to Snowflake using Estuary Flow. In just a few minutes, you've learned how to configure log-based CDC replication, handle schema evolution, and deploy a robust data integration solution."}),"\n",(0,s.jsx)(t.p,{children:"Take a moment to celebrate your achievement! You've not only gained valuable technical knowledge but also demonstrated the agility and efficiency of modern data engineering practices. With your newfound skills, you're well-equipped to tackle complex data challenges and drive innovation in your organization."}),"\n",(0,s.jsxs)(t.h2,{id:"clean-up",children:["Clean up",(0,s.jsx)("a",{id:"clean-up"})]}),"\n",(0,s.jsx)(t.p,{children:"After every party, it\u2019s customary to clean up after ourselves. After you are done exploring the flow, make sure to remove any resources which you won\u2019t use anymore!"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Postgres"})}),"\n",(0,s.jsx)(t.p,{children:"Simply stop the Docker containers & terminate the ngrok process. This will terminate both the database process and the tunnel which exposed it to the internet."}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Snowflake"})}),"\n",(0,s.jsx)(t.p,{children:"To clean up resources in Snowflake use the following SQL script."}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",metastring:'title="snowflake_cleanup.sql"',children:"-- Revoke all privileges from Estuary role\nrevoke all privileges on schema ESTUARY_SCHEMA from role ESTUARY_ROLE;\nrevoke all privileges on database ESTUARY_DB from role ESTUARY_ROLE;\n\n-- Drop the warehouse\ndrop warehouse if exists ESTUARY_WH;\n\n-- Drop the role and user\ndrop user if exists ESTUARY_USER;\ndrop role if exists ESTUARY_ROLE;\n\n-- Drop the schema and database\ndrop schema if exists ESTUARY_SCHEMA;\ndrop database if exists ESTUARY_DB;\n"})}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.strong,{children:"Flow"})}),"\n",(0,s.jsx)(t.p,{children:"In the UI, disable or delete any resources you don\u2019t wish to keep."}),"\n",(0,s.jsxs)(t.h2,{id:"next-steps",children:["Next Steps",(0,s.jsx)("a",{id:"next-steps"})]}),"\n",(0,s.jsx)(t.p,{children:"That\u2019s it! You should have everything you need to know to create your own data pipeline for loading data into Snowflake!\xa0"}),"\n",(0,s.jsx)(t.p,{children:"Now try it out on your own PostgreSQL database or other sources."}),"\n",(0,s.jsxs)(t.p,{children:["If you want to learn more, make sure you read through the ",(0,s.jsx)(t.a,{href:"https://docs.estuary.dev/",children:"Estuary documentation"}),"."]}),"\n",(0,s.jsxs)(t.p,{children:["You\u2019ll find instructions on how to use other connectors ",(0,s.jsx)(t.a,{href:"https://docs.estuary.dev/",children:"here"}),". There are more tutorials ",(0,s.jsx)(t.a,{href:"https://docs.estuary.dev/guides/",children:"here"}),".\xa0"]}),"\n",(0,s.jsxs)(t.p,{children:["Also, don\u2019t forget to join the ",(0,s.jsx)(t.a,{href:"https://estuary-dev.slack.com/ssb/redirect#/shared-invite/email",children:"Estuary Slack Community"}),"!"]})]})}function h(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453:(e,t,a)=>{a.d(t,{R:()=>r,x:()=>i});var n=a(96540);const s={},o=n.createContext(s);function r(e){const t=n.useContext(o);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),n.createElement(o.Provider,{value:t},e.children)}}}]);