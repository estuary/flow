"use strict";(self.webpackChunksite=self.webpackChunksite||[]).push([[1557],{28453:(e,t,o)=>{o.d(t,{R:()=>c,x:()=>a});var n=o(96540);const r={},s=n.createContext(r);function c(e){const t=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:c(e.components),n.createElement(s.Provider,{value:t},e.children)}},44741:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>i,contentTitle:()=>a,default:()=>d,frontMatter:()=>c,metadata:()=>n,toc:()=>l});const n=JSON.parse('{"id":"reference/Connectors/capture-connectors/PostgreSQL/postgres-batch","title":"PostgreSQL Batch Query Connector","description":"This connector captures data from Postgres into Flow collections by periodically","source":"@site/docs/reference/Connectors/capture-connectors/PostgreSQL/postgres-batch.md","sourceDirName":"reference/Connectors/capture-connectors/PostgreSQL","slug":"/reference/Connectors/capture-connectors/PostgreSQL/postgres-batch","permalink":"/reference/Connectors/capture-connectors/PostgreSQL/postgres-batch","draft":false,"unlisted":false,"editUrl":"https://github.com/estuary/flow/edit/master/site/docs/reference/Connectors/capture-connectors/PostgreSQL/postgres-batch.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Neon PostgreSQL","permalink":"/reference/Connectors/capture-connectors/PostgreSQL/neon-postgres"},"next":{"title":"Qualtrics","permalink":"/reference/Connectors/capture-connectors/qualtrics"}}');var r=o(74848),s=o(28453);const c={},a="PostgreSQL Batch Query Connector",i={},l=[];function u(e){const t={a:"a",code:"code",h1:"h1",header:"header",p:"p",strong:"strong",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.header,{children:(0,r.jsx)(t.h1,{id:"postgresql-batch-query-connector",children:"PostgreSQL Batch Query Connector"})}),"\n",(0,r.jsx)(t.p,{children:"This connector captures data from Postgres into Flow collections by periodically\nexecuting queries and translating the results into JSON documents."}),"\n",(0,r.jsxs)(t.p,{children:["For local development or open-source workflows, ",(0,r.jsx)(t.a,{href:"https://ghcr.io/estuary/source-postgres-batch:dev",children:(0,r.jsx)(t.code,{children:"ghcr.io/estuary/source-postgres-batch:dev"})})," provides the latest version of the connector as a Docker image. You can also follow the link in your browser to see past image versions."]}),"\n",(0,r.jsxs)(t.p,{children:["We recommend using our ",(0,r.jsx)(t.a,{href:"http://go.estuary.dev/source-postgres",children:"PostgreSQL CDC Connector"})," instead\nif possible. Using CDC provides lower latency data capture, delete and update events, and usually\nhas a smaller impact on the source database."]}),"\n",(0,r.jsx)(t.p,{children:"However there are some circumstances where this might not be feasible. Perhaps you need\nto capture from a managed PostgreSQL instance which doesn't support logical replication.\nOr perhaps you need to capture the contents of a view or the result of an ad-hoc query.\nThat's the sort of situation this connector is intended for."}),"\n",(0,r.jsxs)(t.p,{children:["The number one caveat you need to be aware of when using this connector is that ",(0,r.jsx)(t.strong,{children:"it will\nperiodically execute its update query over and over"}),". At the default polling interval of\n5 minutes, a naive ",(0,r.jsx)(t.code,{children:"SELECT * FROM foo"})," query against a 100 MiB view will produce 30 GiB/day\nof ingested data, most of it duplicated."]}),"\n",(0,r.jsxs)(t.p,{children:["This is why the connector's autodiscovery logic only returns ordinary tables of data, because\nin that particular case we can use the ",(0,r.jsx)(t.code,{children:"xmin"})," system column as a cursor and ask the database\nto ",(0,r.jsx)(t.code,{children:"SELECT xmin, * FROM foo WHERE xmin::text::bigint > $1;"}),"."]}),"\n",(0,r.jsxs)(t.p,{children:['If you start editing these queries or manually adding capture bindings for views or to run\nad-hoc queries, you need to either have some way of restricting the query to "just the new\nrows since last time" or else have your polling interval set high enough that the data rate\n',(0,r.jsx)(t.code,{children:"<DatasetSize> / <PollingInterval>"})," is an amount of data you're willing to deal with."]})]})}function d(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}}}]);