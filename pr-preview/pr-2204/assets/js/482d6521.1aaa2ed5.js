"use strict";(self.webpackChunksite=self.webpackChunksite||[]).push([[2505],{28453:(e,n,t)=>{t.d(n,{R:()=>c,x:()=>l});var i=t(96540);const r={},s=i.createContext(r);function c(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:c(e.components),i.createElement(s.Provider,{value:n},e.children)}},66453:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>c,metadata:()=>i,toc:()=>o});const i=JSON.parse('{"id":"reference/Connectors/materialization-connectors/amazon-s3-csv","title":"CSV Files in Amazon S3","description":"This connector materializes delta updates of Flow collections into files in an S3 bucket per the CSV format described in RFC-4180.","source":"@site/docs/reference/Connectors/materialization-connectors/amazon-s3-csv.md","sourceDirName":"reference/Connectors/materialization-connectors","slug":"/reference/Connectors/materialization-connectors/amazon-s3-csv","permalink":"/pr-preview/pr-2204/reference/Connectors/materialization-connectors/amazon-s3-csv","draft":false,"unlisted":false,"editUrl":"https://github.com/estuary/flow/edit/master/site/docs/reference/Connectors/materialization-connectors/amazon-s3-csv.md","tags":[],"version":"current","frontMatter":{"description":"This connector materializes delta updates of Flow collections into files in an S3 bucket per the CSV format described in RFC-4180."},"sidebar":"tutorialSidebar","previous":{"title":"Amazon Redshift","permalink":"/pr-preview/pr-2204/reference/Connectors/materialization-connectors/amazon-redshift"},"next":{"title":"Apache Iceberg Tables in Amazon S3  (delta updates)","permalink":"/pr-preview/pr-2204/reference/Connectors/materialization-connectors/amazon-s3-iceberg"}}');var r=t(74848),s=t(28453);const c={description:"This connector materializes delta updates of Flow collections into files in an S3 bucket per the CSV format described in RFC-4180."},l="CSV Files in Amazon S3",d={},o=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Configuration",id:"configuration",level:2},{value:"Properties",id:"properties",level:3},{value:"Endpoint",id:"endpoint",level:4},{value:"Bindings",id:"bindings",level:4},{value:"Sample",id:"sample",level:3},{value:"File Names",id:"file-names",level:2},{value:"Eventual Consistency",id:"eventual-consistency",level:2}];function a(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"csv-files-in-amazon-s3",children:"CSV Files in Amazon S3"})}),"\n",(0,r.jsxs)(n.p,{children:["This connector materializes ",(0,r.jsx)(n.a,{href:"/pr-preview/pr-2204/concepts/materialization#delta-updates",children:"delta updates"})," of\nFlow collections into files in an S3 bucket per the CSV format described in\n",(0,r.jsx)(n.a,{href:"https://www.rfc-editor.org/rfc/rfc4180.html",children:"RFC-4180"}),". The CSV files are compressed using Gzip\ncompression."]}),"\n",(0,r.jsx)(n.p,{children:"The delta updates are batched within Flow, converted to CSV files, and then pushed to the S3 bucket\nat a time interval that you set. Files are limited to a configurable maximum size. Each materialized\nFlow collection will produce many separate files."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://ghcr.io/estuary/materialize-s3-csv:dev",children:(0,r.jsx)(n.code,{children:"ghcr.io/estuary/materialize-s3-csv:dev"})})," provides\nthe latest connector image. You can also follow the link in your browser to see past image versions."]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(n.p,{children:"To use this connector, you'll need:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["An S3 bucket to write files to. See ",(0,r.jsx)(n.a,{href:"https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html",children:"this\nguide"})," for\ninstructions on setting up a new S3 bucket."]}),"\n",(0,r.jsxs)(n.li,{children:["An AWS root or IAM user with the\n",(0,r.jsx)(n.a,{href:"https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html",children:(0,r.jsx)(n.code,{children:"s3:PutObject"})})," permission\nfor the S3 bucket. For this user, you'll need the ",(0,r.jsx)(n.strong,{children:"access key"})," and ",(0,r.jsx)(n.strong,{children:"secret access key"}),". See\nthe ",(0,r.jsx)(n.a,{href:"https://aws.amazon.com/blogs/security/wheres-my-secret-access-key/",children:"AWS blog"})," for help\nfinding these credentials."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"configuration",children:"Configuration"}),"\n",(0,r.jsx)(n.p,{children:"Use the below properties to configure the materialization, which will direct one or more of your\nFlow collections to your bucket."}),"\n",(0,r.jsx)(n.h3,{id:"properties",children:"Properties"}),"\n",(0,r.jsx)(n.h4,{id:"endpoint",children:"Endpoint"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Property"}),(0,r.jsx)(n.th,{children:"Title"}),(0,r.jsx)(n.th,{children:"Description"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Required/Default"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"/bucket"})})}),(0,r.jsx)(n.td,{children:"Bucket"}),(0,r.jsx)(n.td,{children:"Bucket to store materialized objects."}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{children:"Required"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"/awsAccessKeyId"})})}),(0,r.jsx)(n.td,{children:"AWS Access Key ID"}),(0,r.jsx)(n.td,{children:"Access Key ID for writing data to the bucket."}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{children:"Required"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"/awsSecretAccessKey"})})}),(0,r.jsx)(n.td,{children:"AWS Secret Access key"}),(0,r.jsx)(n.td,{children:"Secret Access Key for writing data to the bucket."}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{children:"Required"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"/region"})})}),(0,r.jsx)(n.td,{children:"Region"}),(0,r.jsx)(n.td,{children:"Region of the bucket to write to."}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{children:"Required"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"/uploadInterval"})})}),(0,r.jsx)(n.td,{children:"Upload Interval"}),(0,r.jsx)(n.td,{children:"Frequency at which files will be uploaded."}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{children:"5m"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/prefix"})}),(0,r.jsx)(n.td,{children:"Prefix"}),(0,r.jsx)(n.td,{children:"Optional prefix that will be used to store objects."}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/fileSizeLimit"})}),(0,r.jsx)(n.td,{children:"File Size Limit"}),(0,r.jsx)(n.td,{children:"Approximate maximum size of materialized files in bytes. Defaults to 10737418240 (10 GiB) if blank."}),(0,r.jsx)(n.td,{children:"integer"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/endpoint"})}),(0,r.jsx)(n.td,{children:"Custom S3 Endpoint"}),(0,r.jsx)(n.td,{children:"The S3 endpoint URI to connect to. Use if you're materializing to a compatible API that isn't provided by AWS. Should normally be left blank."}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"/csvConfig/skipHeaders"})}),(0,r.jsx)(n.td,{children:"Skip Headers"}),(0,r.jsx)(n.td,{children:"Do not write headers to files."}),(0,r.jsx)(n.td,{children:"integer"}),(0,r.jsx)(n.td,{})]})]})]}),"\n",(0,r.jsx)(n.h4,{id:"bindings",children:"Bindings"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Property"}),(0,r.jsx)(n.th,{children:"Title"}),(0,r.jsx)(n.th,{children:"Description"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Required/Default"})]})}),(0,r.jsx)(n.tbody,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"/path"})})}),(0,r.jsx)(n.td,{children:"Path"}),(0,r.jsx)(n.td,{children:"The path that objects will be materialized to."}),(0,r.jsx)(n.td,{children:"string"}),(0,r.jsx)(n.td,{children:"Required"})]})})]}),"\n",(0,r.jsx)(n.h3,{id:"sample",children:"Sample"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'materializations:\n  ${PREFIX}/${mat_name}:\n    endpoint:\n      connector:\n        image: "ghcr.io/estuary/materialize-s3-csv:dev"\n        config:\n          bucket: bucket\n          awsAccessKeyId: <access_key_id>\n          awsSecretAccessKey: <secret_access_key>\n          region: us-east-2\n          uploadInterval: 5m\n    bindings:\n      - resource:\n          path: ${COLLECTION_NAME}\n        source: ${PREFIX}/${COLLECTION_NAME}\n'})}),"\n",(0,r.jsx)(n.h2,{id:"file-names",children:"File Names"}),"\n",(0,r.jsx)(n.p,{children:"Materialized files are named with monotonically increasing integer values, padded with leading 0's\nso they remain lexically sortable. For example, a set of files may be materialized like this for a\ngiven collection:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"bucket/prefix/path/v0000000000/00000000000000000000.csv\nbucket/prefix/path/v0000000000/00000000000000000001.csv\nbucket/prefix/path/v0000000000/00000000000000000002.csv\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Here the values for ",(0,r.jsx)(n.strong,{children:"bucket"})," and ",(0,r.jsx)(n.strong,{children:"prefix"})," are from your endpoint configuration. The ",(0,r.jsx)(n.strong,{children:"path"})," is\nspecific to the binding configuration. ",(0,r.jsx)(n.strong,{children:"v0000000000"})," represents the current ",(0,r.jsx)(n.strong,{children:"backfill counter"}),"\nfor binding and will be increased if the binding is re-backfilled, along with the file names\nstarting back over from 0."]}),"\n",(0,r.jsx)(n.h2,{id:"eventual-consistency",children:"Eventual Consistency"}),"\n",(0,r.jsx)(n.p,{children:"In rare circumstances, recently materialized files may be re-written by files with the same name if\nthe materialization shard is interrupted in the middle of processing a Flow transaction and the\ntransaction must be re-started. Files that were committed as part of a completed transaction will\nnever be re-written. In this way, eventually all collection data will be written to files\neffectively-once, although inconsistencies are possible when accessing the most recently written\ndata."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}}}]);