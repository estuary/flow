"use strict";(self.webpackChunksite=self.webpackChunksite||[]).push([[8424],{22634:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"getting-started/comparisons","title":"Comparisons","description":"High level explanations of Flow in terms of the systems you already know","source":"@site/docs/getting-started/comparisons.md","sourceDirName":"getting-started","slug":"/getting-started/comparisons","permalink":"/pr-preview/pr-2450/getting-started/comparisons","draft":false,"unlisted":false,"editUrl":"https://github.com/estuary/flow/edit/master/site/docs/getting-started/comparisons.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"description":"High level explanations of Flow in terms of the systems you already know"},"sidebar":"docsSidebar","previous":{"title":"Deployment Options","permalink":"/pr-preview/pr-2450/getting-started/deployment-options"},"next":{"title":"Configure Cloud Storage","permalink":"/pr-preview/pr-2450/getting-started/installation"}}');var s=a(74848),n=a(28453);const i={sidebar_position:4,description:"High level explanations of Flow in terms of the systems you already know"},r="Comparisons",l={},c=[{value:"Comparison Overview",id:"comparison-overview",level:2},{value:"Batch ELT (Fivetran)",id:"batch-elt-fivetran",level:3},{value:"Streaming Solutions (Confluent, Debezium)",id:"streaming-solutions-confluent-debezium",level:3},{value:"Open-Source Platforms (Airbyte, Meltano)",id:"open-source-platforms-airbyte-meltano",level:3},{value:"Technical Comparisons",id:"technical-comparisons",level:2},{value:"Apache Beam and Google Cloud Dataflow",id:"apache-beam-and-google-cloud-dataflow",level:3},{value:"Kafka",id:"kafka",level:3},{value:"Spark",id:"spark",level:3},{value:"Hadoop, HDFS, and Hive",id:"hadoop-hdfs-and-hive",level:3},{value:"Fivetran, Airbyte, and other ELT solutions",id:"fivetran-airbyte-and-other-elt-solutions",level:3},{value:"dbt",id:"dbt",level:3},{value:"Materialize, Rockset, ksqlDB, and other real-time databases",id:"materialize-rockset-ksqldb-and-other-real-time-databases",level:3},{value:"Snowflake, BigQuery, and other OLAP databases",id:"snowflake-bigquery-and-other-olap-databases",level:3}];function d(e){const t={a:"a",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,n.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"comparisons",children:"Comparisons"})}),"\n",(0,s.jsx)(t.p,{children:"Estuary Flow is a hosted data pipeline platform. To understand what makes Estuary unique, it's useful to compare across other similar offerings in the ETL and ELT space."}),"\n",(0,s.jsxs)(t.p,{children:["This document will explore main comparison points with other solutions. For detailed feature, cost, and pro/con breakdowns, evaluate solutions in our ",(0,s.jsx)(t.a,{href:"https://estuary.dev/etl-tools/",children:"comparison center"}),"."]}),"\n",(0,s.jsxs)(t.p,{children:["You can also find more ",(0,s.jsx)(t.a,{href:"#technical-comparisons",children:"technical comparison information below"})," to understand how Flow's underlying technology and assumptions compare to other data frameworks and systems, like Kafka, Apache Beam, and Hadoop."]}),"\n",(0,s.jsx)(t.h2,{id:"comparison-overview",children:"Comparison Overview"}),"\n",(0,s.jsx)(t.p,{children:"Estuary distinguishes itself from other platforms in a number of ways. Some of Estuary's main features include:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Supports both real-time and batch data"}),"\n",(0,s.jsxs)(t.li,{children:["Hundreds of in-house, purpose-built ",(0,s.jsx)(t.a,{href:"/reference/Connectors",children:"connectors"})]}),"\n",(0,s.jsx)(t.li,{children:"Efficient pipelines with CDC and techniques to reduce compute costs"}),"\n",(0,s.jsxs)(t.li,{children:["Flexible ",(0,s.jsx)(t.a,{href:"/getting-started/deployment-options",children:"deployment options"})," for sensitive data"]}),"\n",(0,s.jsxs)(t.li,{children:["Automatic ",(0,s.jsx)(t.a,{href:"/concepts/schemas/#continuous-schema-inference",children:"schema inference"})," and ",(0,s.jsx)(t.a,{href:"/guides/schema-evolution",children:"evolution"})]}),"\n",(0,s.jsx)(t.li,{children:"Intuitive, low pricing"}),"\n"]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.a,{href:"/getting-started/who-should-use-flow",children:"Learn more about who should use Flow"}),"."]}),"\n",(0,s.jsx)(t.h3,{id:"batch-elt-fivetran",children:"Batch ELT (Fivetran)"}),"\n",(0,s.jsx)(t.p,{children:"A number of data pipeline solutions are batch-only or batch-first.\nWhile useful for analytics reports and other scheduled jobs, this inherently limits the use cases a tool can support.\nUsers must either leave real-time analytical capabilities out of their final product or fragment their data architecture to implement real-time pipelines with other tools."}),"\n",(0,s.jsx)(t.p,{children:"Estuary supports batch use cases with configurable sync schedules while also supporting real-time CDC and streaming use cases.\nBecause of Estuary's unique technical capabilities, a single connector can often handle both real-time streams and batch jobs of specific data collections going to the same destination."}),"\n",(0,s.jsxs)(t.p,{children:["Many batch-based systems are also ELT by default, saving the transformation step until after data is loaded into the destination system.\nEstuary provides a ",(0,s.jsx)(t.a,{href:"/guides/dbt-integration",children:"dbt Cloud"})," integration option for ELT workflows and also allows users to create SQL or TypeScript ",(0,s.jsx)(t.a,{href:"/guides/flowctl/create-derivation",children:"transformations"})," that get applied before data reaches the destination.\nThis can increase efficiency when materializing transformed data to multiple different destinations, as well as saving on compute and storage costs with destination systems."]}),"\n",(0,s.jsx)(t.h3,{id:"streaming-solutions-confluent-debezium",children:"Streaming Solutions (Confluent, Debezium)"}),"\n",(0,s.jsx)(t.p,{children:"On the other end of the scale from batch solutions are the streaming-centric solutions.\nA number of these solutions are based on Kafka or other services that require extensive setup and maintenance, giving real-time data a reputation for complexity."}),"\n",(0,s.jsx)(t.p,{children:"With optimized no-code CDC connectors, Estuary is built to simplify real-time data.\nFlow's underlying technology, Gazette, solves for complex real-time problems like exactly-once delivery and highly available reliability."}),"\n",(0,s.jsx)(t.p,{children:"Real-time data can also have a reputation for expense, especially if teams forego batch data entirely.\nMany destinations calculate compute pricing on active per-time-unit usage.\nSince real-time solutions are often transferring data constantly, this can lead to steep destination costs."}),"\n",(0,s.jsx)(t.p,{children:"Since Estuary supports both real-time and batch pipelines, users can save on unnecessary expense by sticking with batch schedules for use cases that don't require real-time data.\nIt's also easy to switch to real-time later if the situation changes."}),"\n",(0,s.jsx)(t.h3,{id:"open-source-platforms-airbyte-meltano",children:"Open-Source Platforms (Airbyte, Meltano)"}),"\n",(0,s.jsx)(t.p,{children:"Open-source data pipeline platforms often boast a high connector count by relying on contributions from the community.\nThis can be misleading, as many of the connectors can be inefficient, unfit for a user's specific use case, vary in quality, or simply be unmaintained.\nThis leads to hidden costs in engineering hours for setup and troubleshooting."}),"\n",(0,s.jsx)(t.p,{children:"Estuary creates in-house, purpose-built connectors which are continually maintained and improved.\nThis ongoing optimization means connectors move data faster and require fewer compute resources in the destination system."}),"\n",(0,s.jsxs)(t.p,{children:["If your solution requires a new connector that Estuary doesn't currently support, or new resources on an existing connector, ",(0,s.jsx)(t.a,{href:"https://go.estuary.dev/request-connector",children:"you can always request new features"}),"."]}),"\n",(0,s.jsxs)(t.p,{children:["There are still many benefits in open-source, especially in terms of transparency.\nTo that end, Estuary is ",(0,s.jsx)(t.strong,{children:"open-core"})," software, with ",(0,s.jsx)(t.a,{href:"https://github.com/estuary/flow",children:"platform"})," and ",(0,s.jsx)(t.a,{href:"https://github.com/estuary/connectors",children:"connector code"})," available for anyone to view and evaluate on GitHub."]}),"\n",(0,s.jsx)(t.h2,{id:"technical-comparisons",children:"Technical Comparisons"}),"\n",(0,s.jsx)(t.p,{children:"Because Flow combines many functionalities, it's related to many types of data systems. Choose a familiar system from the list below to jump to an explanation of how it compares with Flow (or how you can use the two together)."}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"/pr-preview/pr-2450/getting-started/comparisons#apache-beam-and-google-cloud-dataflow",children:"Apache Beam and Google Cloud Dataflow"})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"/pr-preview/pr-2450/getting-started/comparisons#kafka",children:"Kafka"})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"/pr-preview/pr-2450/getting-started/comparisons#spark",children:"Spark"})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"/pr-preview/pr-2450/getting-started/comparisons#hadoop-hdfs-and-hive",children:"Hadoop, HDFS, and Hive"})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"/pr-preview/pr-2450/getting-started/comparisons#fivetran-airbyte-and-other-elt-solutions",children:"Fivetran, Airbyte, and other ELT solutions"})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"/pr-preview/pr-2450/getting-started/comparisons#dbt",children:"dbt"})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"/pr-preview/pr-2450/getting-started/comparisons#materialize-rockset-ksqldb-and-other-real-time-databases",children:"Materialize, Rockset, ksqlDB, and other realtime databases"})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"/pr-preview/pr-2450/getting-started/comparisons#snowflake-bigquery-and-other-olap-databases",children:"Snowflake, BigQuery, and other OLAP databases"})}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"apache-beam-and-google-cloud-dataflow",children:"Apache Beam and Google Cloud Dataflow"}),"\n",(0,s.jsx)(t.p,{children:"Flow\u2019s most apt comparison is to Apache Beam. You may use a variety of runners (processing engines) for your Beam deployment. One of the most popular, Google Cloud Dataflow, is a more robust redistribution under an additional SDK. Regardless of how you use Beam, there\u2019s a lot of conceptual overlap with Flow. This makes Beam and Flow alternatives rather than complementary technologies, but there are key differences."}),"\n",(0,s.jsxs)(t.p,{children:["Like Beam, Flow\u2019s primary primitive is a ",(0,s.jsx)(t.strong,{children:"collection"}),". You build a processing graph (called a ",(0,s.jsx)(t.strong,{children:"pipeline"})," in Beam and a ",(0,s.jsx)(t.strong,{children:"Data Flow"})," in Flow) by relating multiple collections together through procedural transformations, or lambdas. As with Beam, Flow\u2019s runtime performs automatic data shuffles and is designed to allow fully automatic scaling. Also like Beam, collections have associated schemas."]}),"\n",(0,s.jsx)(t.p,{children:"Unlike Beam, Flow doesn\u2019t distinguish between batch and streaming contexts. Flow unifies these paradigms under a single collection concept, allowing you to seamlessly work with both data types."}),"\n",(0,s.jsx)(t.p,{children:"Also, while Beam allows you the option to define combine operators, Flow\u2019s runtime always applies combine operators. These are built using the declared semantics of the document\u2019s schema, which makes it much more efficient and cost-effective to work with streaming data."}),"\n",(0,s.jsxs)(t.p,{children:["Finally, Flow allows stateful stream-to-stream joins without the windowing semantics imposed by Beam. Notably, Flow\u2019s modeling of state \u2013 via its per-key ",(0,s.jsx)(t.strong,{children:"register"})," concept \u2013 is substantially more powerful than Beam's per-key-and-window model. For example, registers can trivially model the cumulative lifetime value of a customer."]}),"\n",(0,s.jsx)(t.h3,{id:"kafka",children:"Kafka"}),"\n",(0,s.jsx)(t.p,{children:"Flow inhabits a different space than Kafka does by itself. Kafka is an infrastructure that supports streaming applications running elsewhere. Flow is an opinionated framework for working with real-time data. You might think of Flow as an analog to an opinionated bundling of several important features from the broader Kafka ecosystem."}),"\n",(0,s.jsxs)(t.p,{children:["Flow is built on ",(0,s.jsx)(t.a,{href:"https://gazette.readthedocs.io/en/latest/",children:"Gazette"}),", a highly-scalable streaming broker similar to log-oriented pub/sub systems. Thus, Kafka is more directly comparable to Gazette. Flow also uses Gazette\u2019s consumer framework, which has similarities to Kafka ",(0,s.jsx)(t.strong,{children:"consumers"}),". Both manage scale-out execution contexts for consumer tasks, offer durable local task stores, and provide exactly-once semantics."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.a,{href:"/pr-preview/pr-2450/concepts/advanced/journals",children:"Journals"})," in Gazette and Flow are roughly analogous to Kafka ",(0,s.jsx)(t.strong,{children:"partitions"}),". Each journal is a single append-only log. Gazette has no native notion of a ",(0,s.jsx)(t.strong,{children:"topic"}),", but instead supports label-based selection of subsets of journals, which tends to be more flexible. Gazette journals store data in contiguous chunks called ",(0,s.jsx)(t.strong,{children:"fragments"}),", which typically live in cloud storage. Each journal can have its own separate storage configuration, which Flow leverages to allow users to bring their own cloud storage buckets. Another unique feature of Gazette is its ability to serve reads of historical data by providing clients with pre-signed cloud storage URLs, which enables it to serve many readers very efficiently."]}),"\n",(0,s.jsxs)(t.p,{children:["Generally, Flow users don't need to know or care much about Gazette and its architecture, since Flow provides a higher-level interface over groups of journals, called ",(0,s.jsx)(t.strong,{children:"collections"}),"."]}),"\n",(0,s.jsxs)(t.p,{children:["Flow ",(0,s.jsx)(t.a,{href:"/pr-preview/pr-2450/concepts/collections",children:"collections"})," are somewhat similar to Kafka ",(0,s.jsx)(t.strong,{children:"streams"}),", but with some important differences. Collections always store JSON and must have an associated JSON schema. Collections also support automatic logical and physical partitioning. Each collection is backed by one or more journals, depending on the partitioning."]}),"\n",(0,s.jsxs)(t.p,{children:["Flow ",(0,s.jsx)(t.a,{href:"/pr-preview/pr-2450/concepts/#tasks",children:"tasks"})," are most similar to Kafka ",(0,s.jsx)(t.strong,{children:"stream processors"}),", but are more opinionated. Tasks fall into one of three categories: captures, derivations, and materializations. Tasks may also have more than one process, which Flow calls ",(0,s.jsx)(t.a,{href:"/pr-preview/pr-2450/concepts/advanced/shards",children:(0,s.jsx)(t.strong,{children:"shards"})}),", to allow for parallel processing. Tasks and shards are fully managed by Flow. This includes transactional state management and zero-downtime splitting of shards, which enables turnkey scaling."]}),"\n",(0,s.jsx)(t.p,{children:"See how Flow compares to popular stream processing platforms that use Kafka:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"https://estuary.dev/vs-confluent/",children:"Flow vs Confluent feature and pricing breakdown"})}),"\n",(0,s.jsx)(t.li,{children:(0,s.jsx)(t.a,{href:"https://estuary.dev/vs-debezium/",children:"Flow vs Debezium feature and pricing breakdown"})}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"spark",children:"Spark"}),"\n",(0,s.jsx)(t.p,{children:"Spark can be described as a batch engine with stream processing add-ons, where Flow is fundamentally a streaming system that is able to easily integrate with batch systems."}),"\n",(0,s.jsxs)(t.p,{children:["You can think of a Flow ",(0,s.jsx)(t.strong,{children:"collection"})," as a set of RDDs with common associated metadata. In Spark, you can save an RDD to a variety of external systems, like cloud storage or a database. Likewise, you can load from a variety of external systems to create an RDD. Finally, you can transform one RDD into another. You use Flow collections in a similar manner. They represent a logical dataset, which you can ",(0,s.jsx)(t.strong,{children:"materialize"})," to push the data into some external system like cloud storage or a database. You can also create a collection that is ",(0,s.jsx)(t.strong,{children:"derived"})," by applying stateful transformations to one or more source collections."]}),"\n",(0,s.jsx)(t.p,{children:"Unlike Spark RDDs, Flow collections are backed by one or more unbounded append-only logs. Therefore, you don't create a new collection each time data arrives; you simply append to the existing one. Collections can be partitioned and can support extremely large volumes of data."}),"\n",(0,s.jsxs)(t.p,{children:["Spark's processing primitives, ",(0,s.jsx)(t.strong,{children:"applications"}),", ",(0,s.jsx)(t.strong,{children:"jobs"}),", and ",(0,s.jsx)(t.strong,{children:"tasks"}),", don't translate perfectly to Flow, but we can make some useful analogies. This is partly because Spark is not very opinionated about what an application does. Your Spark application could read data from cloud storage, then transform it, then write the results out to a database. The closest analog to a Spark application in Flow is the ",(0,s.jsx)(t.strong,{children:"Data Flow"}),". A Data Flow is a composition of Flow tasks, which are quite different from tasks in Spark."]}),"\n",(0,s.jsxs)(t.p,{children:["In Flow, a task is a logical unit of work that does ",(0,s.jsx)(t.em,{children:"one"})," of capture (ingest), derive (transform), or materialize (write results to an external system). What Spark calls a task is actually closer to a Flow ",(0,s.jsx)(t.strong,{children:"shard"}),". In Flow, a task is a logical unit of work, and ",(0,s.jsx)(t.a,{href:"/pr-preview/pr-2450/concepts/advanced/shards",children:"shards"})," represent the potentially numerous processes that actually carry out that work. Shards are the unit of parallelism in Flow, and you can easily split them for turnkey scaling."]}),"\n",(0,s.jsx)(t.p,{children:"Composing Flow tasks is also a little different than composing Spark jobs. Flow tasks always produce and/or consume data in collections, instead of piping data directly from one shard to another. This is because every task in Flow is transactional and, to the greatest degree possible, fault-tolerant. This design also affords painless backfills of historical data when you want to add new transformations or materializations."}),"\n",(0,s.jsx)(t.h3,{id:"hadoop-hdfs-and-hive",children:"Hadoop, HDFS, and Hive"}),"\n",(0,s.jsx)(t.p,{children:"There are many different ways to use Hadoop, HDFS, and the ecosystem of related projects, several of which are useful comparisons to Flow."}),"\n",(0,s.jsxs)(t.p,{children:["To gain an understanding of Flow's processing model for derivations, see ",(0,s.jsx)(t.a,{href:"https://www.estuary.dev/why-mapreduce-is-making-a-comeback/",children:"this blog post about MapReduce in Flow"}),"."]}),"\n",(0,s.jsx)(t.p,{children:"HDFS is sometimes used as a system of record for analytics data, typically paired with an orchestration system for analytics jobs. If you do this, you likely export datasets from your source systems into HDFS. Then, you use some other tool to coordinate running various MapReduce jobs, often indirectly through systems like Hive."}),"\n",(0,s.jsxs)(t.p,{children:["For this use case, the best way of describing Flow is that it completely changes the paradigm. In Flow, you always append data to existing ",(0,s.jsx)(t.strong,{children:"collections"}),", rather than creating a new one each time a job is run. In fact, Flow has no notion of a ",(0,s.jsx)(t.strong,{children:"job"})," like there is in Hadoop. Flow tasks run continuously and everything stays up to date in real time, so there's never a need for outside orchestration or coordination. Put simply, Flow collections are log-like, and files in HDFS typically store table-like data. ",(0,s.jsx)(t.a,{href:"https://www.estuary.dev/the-power-and-implications-of-data-materialization/",children:"This blog post"})," explores those differences in greater depth."]}),"\n",(0,s.jsx)(t.p,{children:"To make this more concrete, imagine a hypothetical example of a workflow in the Hadoop world where you export data from a source system, perform some transformations, and then run some Hive queries."}),"\n",(0,s.jsxs)(t.p,{children:["In Flow, you instead define a ",(0,s.jsx)(t.strong,{children:"capture"})," of data from the source, which runs continuously and keeps a collection up to date with the latest data from the source. Then you transform the data with Flow ",(0,s.jsx)(t.strong,{children:"derivations"}),", which again apply the transformations incrementally and in real time. While you ",(0,s.jsx)(t.em,{children:"could"})," actually use tools like Hive to directly query data from Flow collections \u2014 the layout of collection data in cloud storage is intentionally compatible with this \u2014 you could also ",(0,s.jsx)(t.strong,{children:"materialize"})," a view of your transformation results to any database, which is also kept up to date in real time."]}),"\n",(0,s.jsx)(t.h3,{id:"fivetran-airbyte-and-other-elt-solutions",children:"Fivetran, Airbyte, and other ELT solutions"}),"\n",(0,s.jsxs)(t.p,{children:["Tools like Fivetran and Airbyte are purpose-built to move data from one place to another. These ELT tools typically model sources and destinations, and run regularly scheduled jobs to export from the source directly to the destination. Flow models things differently. Instead of modeling the world in terms of independent scheduled jobs that copy data from source to destination, Data Flows model a directed graph of\n",(0,s.jsx)(t.a,{href:"../../concepts/captures",children:(0,s.jsx)(t.strong,{children:"captures"})})," (reads from sources),\n",(0,s.jsx)(t.a,{href:"../../concepts/derivations",children:(0,s.jsx)(t.strong,{children:"derivations"})})," (transforms), and\n",(0,s.jsx)(t.a,{href:"/concepts/materialization",children:(0,s.jsx)(t.strong,{children:"materializations"})})," (writes to destinations).\nCollectively, these are called ",(0,s.jsx)(t.em,{children:"tasks"}),"."]}),"\n",(0,s.jsxs)(t.p,{children:["Tasks in Flow are only indirectly linked. Captures read data from a source and output to ",(0,s.jsx)(t.strong,{children:"collections"}),". Flow collections store all the data in cloud storage, with configurable retention for historical data. You can then materialize each collection to any number of destination systems. Each one will be kept up to date in real time, and new materializations can automatically backfill all your historical data. Collections in Flow always have an associated JSON schema, and they use that to ensure the validity of all collection data. Tasks are also transactional and generally guarantee end-to-end exactly-once processing (so long as the endpoint system can accommodate them)."]}),"\n",(0,s.jsxs)(t.p,{children:["Like Airbyte, Flow uses ",(0,s.jsx)(t.a,{href:"/pr-preview/pr-2450/concepts/connectors",children:"connectors"})," for interacting with external systems in captures and materializations. For captures,\nFlow integrates the Airbyte specification,\nso all Airbyte source connectors can be used with Flow.\nFor materializations, Flow uses its own protocol which is not compatible with the Airbyte spec.\nIn either case, the usage of connectors is pretty similar."]}),"\n",(0,s.jsx)(t.p,{children:"In terms of technical capabilities, Flow can do everything that these tools can and more.\nBoth Fivetran and Airbyte both currently have graphical interfaces that make them much easier for\nnon-technical users to configure. Flow, too, is focused on empowering non-technical users through its web application.\nAt the same time, Flow offers declarative YAML for configuration, which works excellently in a GitOps workflow."}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://estuary.dev/vs-fivetran/",children:"Flow vs Fivetran feature and pricing breakdown."})}),"\n",(0,s.jsx)(t.h3,{id:"dbt",children:"dbt"}),"\n",(0,s.jsx)(t.p,{children:"dbt is a tool that enables data analysts and engineers to transform data in their warehouses more effectively."}),"\n",(0,s.jsx)(t.p,{children:"In addition to \u2013 and perhaps more important than \u2013 its transform capability, dbt brought an entirely new workflow for working with data:\none that prioritizes version control, testing, local development, documentation, composition, and re-use."}),"\n",(0,s.jsx)(t.p,{children:"Like dbt, Flow uses a declarative model and tooling, but the similarities end there. dbt is a tool for defining transformations, which are executed within your analytics warehouse.\nFlow is a tool for delivering data to that warehouse, as well as continuous operational transforms that are applied everywhere else."}),"\n",(0,s.jsx)(t.p,{children:"These two tools can make lots of sense to use together. First, Flow brings timely, accurate data to the warehouse.\nWithin the warehouse, analysts can use tools like dbt to explore the data. The Flow pipeline is then ideally suited to\nproductionize important insights as materialized views or by pushing to another destination."}),"\n",(0,s.jsx)(t.p,{children:"Put another way, Flow is a complete ELT platform, but you might choose to perform and manage more complex transformations in\na separate, dedicated tool like dbt. While Flow and dbt don\u2019t interact directly, both offer easy integration through your data warehouse."}),"\n",(0,s.jsx)(t.h3,{id:"materialize-rockset-ksqldb-and-other-real-time-databases",children:"Materialize, Rockset, ksqlDB, and other real-time databases"}),"\n",(0,s.jsx)(t.p,{children:"Modern real-time databases like Materialize, Rockset, and ksqlDB consume streams of data, oftentimes from Kafka brokers,\nand can keep SQL views up to date in real time."}),"\n",(0,s.jsx)(t.p,{children:"These real-time databases have a lot of conceptual overlap with Flow. The biggest difference is that Flow can materialize this same type of incrementally updated view into any database, regardless of whether that database has real-time capabilities or not. "}),"\n",(0,s.jsxs)(t.p,{children:["However, this doesn't mean that Flow should  ",(0,s.jsx)(t.em,{children:"replace"})," these systems in your stack. In fact, it can be optimal to use Flow to feed data into them.\nFlow adds real-time data capture and materialization options that many real-time databases don't support.\nOnce data has arrived in the database, you have access to real-time SQL analysis and other analytical tools not native to Flow.\nFor further explanation, read the section below on OLAP databases."]}),"\n",(0,s.jsx)(t.h3,{id:"snowflake-bigquery-and-other-olap-databases",children:"Snowflake, BigQuery, and other OLAP databases"}),"\n",(0,s.jsxs)(t.p,{children:["Flow differs from OLAP databases mainly in that it's not a database. Flow has no query interface, and no plans to add one. Instead, Flow allows you to use the query interfaces of any database by ",(0,s.jsx)(t.strong,{children:"materializing"})," views into it."]}),"\n",(0,s.jsxs)(t.p,{children:["Flow is similar to OLAP databases in that it can be the source of truth for all analytics data (though it's also capable enough to handle operational workloads). Instead of schemas and tables, Flow defines ",(0,s.jsx)(t.strong,{children:"collections"}),". These collections are conceptually similar to database tables in the sense that they are containers for data with an associated (primary) key. Under the hood, Flow collections are each backed by append-only logs, where each document in the log represents a delta update for a given key."]}),"\n",(0,s.jsx)(t.p,{children:"Collections can be easily materialized into a variety of external systems, such as Snowflake or BigQuery. This creates a table in your OLAP database that is continuously kept up to date with the collection. With Flow, there's no need to schedule exports to these systems, and thus no need to orchestrate the timing of those exports. You can also materialize a given collection into multiple destination systems, so you can always use whichever system is best for the type of queries you want to run."}),"\n",(0,s.jsx)(t.p,{children:"Like Snowflake, Flow uses inexpensive cloud storage for all collection data. It even lets you bring your own storage bucket, so you're always in control. Unlike data warehouses, Flow is able to directly capture data from source systems, and continuously and incrementally keep everything up to date."}),"\n",(0,s.jsx)(t.p,{children:"A common pattern is to use Flow to capture data from multiple different sources and materialize it into a data warehouse. Flow can also help you avoid expenses associated with queries you frequently pull from a data warehouse by keeping an up-to-date view of them where you want it. Because of Flow\u2019s exactly-once processing guarantees, these materialized views are always correct, consistent, and fault-tolerant."})]})}function h(e={}){const{wrapper:t}={...(0,n.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453:(e,t,a)=>{a.d(t,{R:()=>i,x:()=>r});var o=a(96540);const s={},n=o.createContext(s);function i(e){const t=o.useContext(n);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),o.createElement(n.Provider,{value:t},e.children)}}}]);