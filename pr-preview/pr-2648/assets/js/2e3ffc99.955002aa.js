"use strict";(self.webpackChunksite=self.webpackChunksite||[]).push([[8036],{10399:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/evolution-re-create-ui-d8ae2ab634980751f8c87a0483f96d39.png"},28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var s=i(96540);const t={},a=s.createContext(t);function r(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(a.Provider,{value:n},e.children)}},38993:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"guides/schema-evolution","title":"Schema Evolution","description":"When collection specifications and schemas change, you must make corresponding changes in other parts of your Data Flow to avoid errors. In this guide, you\'ll learn how to respond to different types of collection changes.","source":"@site/docs/guides/schema-evolution.md","sourceDirName":"guides","slug":"/guides/schema-evolution","permalink":"/pr-preview/pr-2648/guides/schema-evolution","draft":false,"unlisted":false,"editUrl":"https://github.com/estuary/flow/edit/master/site/docs/guides/schema-evolution.md","tags":[],"version":"current","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"How to Generate an Estuary Refresh Token","permalink":"/pr-preview/pr-2648/guides/how_to_generate_refresh_token"},"next":{"title":"dbt Cloud Integration","permalink":"/pr-preview/pr-2648/guides/dbt-integration/"}}');var t=i(74848),a=i(28453);const r={},o="Schema Evolution",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Schema evolution scenarios",id:"schema-evolution-scenarios",level:2},{value:"Re-creating a collection",id:"re-creating-a-collection",level:3},{value:"A new field is added",id:"a-new-field-is-added",level:3},{value:"A field&#39;s data type has changed",id:"a-fields-data-type-has-changed",level:3},{value:"A field was removed",id:"a-field-was-removed",level:3},{value:"How schema evolution works",id:"how-schema-evolution-works",level:2},{value:"Write schemas vs. read schemas",id:"write-schemas-vs-read-schemas",level:3},{value:"Auto-inference only widens",id:"auto-inference-only-widens",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Why are my new columns showing NULL values?",id:"why-are-my-new-columns-showing-null-values",level:3},{value:"My data stopped flowing after a schema change",id:"my-data-stopped-flowing-after-a-schema-change",level:3},{value:"Transient validation errors during schema changes",id:"transient-validation-errors-during-schema-changes",level:3},{value:"Configuring automatic schema change handling",id:"configuring-automatic-schema-change-handling",level:3},{value:"Type inference issues with NoSQL sources",id:"type-inference-issues-with-nosql-sources",level:3}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"schema-evolution",children:"Schema Evolution"})}),"\n",(0,t.jsx)(n.p,{children:"When collection specifications and schemas change, you must make corresponding changes in other parts of your Data Flow to avoid errors. In this guide, you'll learn how to respond to different types of collection changes."}),"\n",(0,t.jsxs)(n.p,{children:["Manual methods (using flowctl) as well as features available in the Estuary web app are covered here.\nFor an in-depth overview of the automatic schema evolution feature in the web app and how it works, see ",(0,t.jsx)(n.a,{href:"/pr-preview/pr-2648/concepts/advanced/evolutions",children:"this article"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsxs)(n.p,{children:["Estuary ",(0,t.jsx)(n.a,{href:"/pr-preview/pr-2648/concepts/collections",children:"collections"})," serve not only as your real-time data storage, but also as a contract between tasks that produce and consume their data. ",(0,t.jsx)(n.strong,{children:"Captures"})," are producers, ",(0,t.jsx)(n.strong,{children:"materializations"})," are consumers, and ",(0,t.jsx)(n.strong,{children:"derivations"})," can act as either."]}),"\n",(0,t.jsx)(n.p,{children:"This contract helps prevent data loss and error in your Data Flows, and is defined in terms of the collection specification, or spec, which includes:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The JSON schema"}),"\n",(0,t.jsxs)(n.li,{children:["The collection ",(0,t.jsx)(n.code,{children:"key"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/pr-preview/pr-2648/concepts/advanced/projections",children:"Projections"}),", if any"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"There are many reasons a collection spec might change. Often, it's due to a change in the source data. Regardless, you'll need to make changes to downstream tasks \u2014 most often, materializations \u2014\xa0to avoid errors."}),"\n",(0,t.jsx)(n.h2,{id:"schema-evolution-scenarios",children:"Schema evolution scenarios"}),"\n",(0,t.jsx)(n.p,{children:"This guide is broken down into sections for different common scenarios, depending on which properties of the collection spec have changed."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsxs)(n.a,{href:"#re-creating-a-collection",children:["The ",(0,t.jsx)(n.code,{children:"key"})," pointers have changed"]})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#re-creating-a-collection",children:"The logical partitioning configuration has changed"})}),"\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"schema"})," (or ",(0,t.jsx)(n.code,{children:"readSchema"})," if defined separately) has changed","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#a-new-field-is-added",children:"A new field is added"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#a-fields-data-type-has-changed",children:"A field's data type has changed"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#a-field-was-removed",children:"A field was removed"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["There are a variety of reasons why these properties may change, and also different mechanisms for detecting changes in source data. In general, it doesn't matter why the collection spec has changed, only ",(0,t.jsx)(n.em,{children:"what"})," has changed. However, ",(0,t.jsx)(n.a,{href:"/pr-preview/pr-2648/concepts/captures#automatically-update-captures",children:"AutoDiscovers"})," are able to handle some of these scenarios automatically. Where applicable, AutoDiscover behavior will be called out under each section."]})}),"\n",(0,t.jsx)(n.h3,{id:"re-creating-a-collection",children:"Re-creating a collection"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.em,{children:["Scenario: the ",(0,t.jsx)(n.code,{children:"key"})," pointer or logical partitioning configurations have changed."]})}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"key"})," of an Estuary collection cannot be changed after the collection is created. The same is true of the logical partitioning, which also cannot be changed after the collection is created."]}),"\n",(0,t.jsx)(n.p,{children:"If you need to change either of those parts of a collection spec, you'll need to create a new collection and update the bindings of any captures or materializations that reference the old collection."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Web app workflow"})}),"\n",(0,t.jsx)(n.p,{children:"If you're working in the Estuary web app, you'll see an error message and an option to re-create the collection as shown in the example below."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:i(10399).A+"",width:"2318",height:"1215"})}),"\n",(0,t.jsxs)(n.p,{children:["Click ",(0,t.jsx)(n.strong,{children:"Apply"})," to re-create the collection and update any tasks that reference the old collection with the new name."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"flowctl workflow:"})}),"\n",(0,t.jsxs)(n.p,{children:["If you're working with flowctl, you'll need to re-create the collection manually in your ",(0,t.jsx)(n.code,{children:"flow.yaml"})," file. You must also update any captures or materializations that reference it. For example, say you have a data flow defined by the following specs:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"captures:\n  acmeCo/inventory/source-postgres:\n    endpoint:\n      connector:\n        image: ghcr.io/estuary/source-postgres:v1\n        config: encrypted-pg-config.sops.yaml\n    bindings:\n      - resource:\n          namespace: public\n          stream: anvils\n          mode: Normal\n        target: acmeCo/inventory/anvils\n\ncollections:\n  acmeCo/inventory/anvils:\n    key: [/sku]\n    schema:\n      type: object\n      properties:\n        sku: { type: string }\n        warehouse_id: { type: string }\n        quantity: { type: integer }\n      required: [sku, warehouse_id, quantity]\n\nmaterializations:\n  acmeCo/data-warehouse/materialize-snowflake:\n    endpoint:\n      connector:\n        image: ghcr.io/estuary/materialize-snowflake:v1\n        config: encrypted-snowflake-config.sops.yaml\n    bindings:\n      - source: acmeCo/inventory/anvils\n        resource:\n          table: anvils\n          schema: inventory\n"})}),"\n",(0,t.jsxs)(n.p,{children:["To change the collection key, you would update the YAML like so. Note the capture ",(0,t.jsx)(n.code,{children:"target"}),", collection name, and materialization ",(0,t.jsx)(n.code,{children:"source"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"captures:\n  acmeCo/inventory/source-postgres:\n    endpoint:\n      connector:\n        image: ghcr.io/estuary/source-postgres:v1\n        config: encrypted-pg-config.sops.yaml\n    bindings:\n      - resource:\n          namespace: public\n          stream: anvils\n          mode: Normal\n        backfill: 1\n        target: acmeCo/inventory/anvils_v2\n\ncollections:\n  acmeCo/inventory/anvils_v2:\n    key: [/sku]\n    schema:\n      type: object\n      properties:\n        sku: { type: string }\n        warehouse_id: { type: string }\n        quantity: { type: integer }\n      required: [sku, warehouse_id, quantity]\n\nmaterializations:\n  acmeCo/data-warehouse/materialize-snowflake:\n    endpoint:\n      connector:\n        image: ghcr.io/estuary/materialize-snowflake:v1\n        config: encrypted-snowflake-config.sops.yaml\n    bindings:\n      - source: acmeCo/inventory/anvils_v2\n        backfill: 1\n        resource:\n          table: anvils\n          schema: inventory\n"})}),"\n",(0,t.jsxs)(n.p,{children:["The existing ",(0,t.jsx)(n.code,{children:"acmeCo/inventory/anvils"})," collection will not be modified and will remain in place, but won't update because no captures are writing to it."]}),"\n",(0,t.jsxs)(n.p,{children:["Also note the addition of the ",(0,t.jsx)(n.code,{children:"backfill"})," property. If the ",(0,t.jsx)(n.code,{children:"backfill"})," property already exists, just increment its value. For the materialization, this will ensure that the destination table in Snowflake gets dropped and re-created, and that the materialization will backfill it from the beginning. In the capture, it similarly causes it to start over from the beginning, writing the captured data into the new collection."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Auto-Discovers:"})}),"\n",(0,t.jsxs)(n.p,{children:["If you enabled the option to ",(0,t.jsxs)(n.a,{href:"/pr-preview/pr-2648/concepts/captures#automatically-update-captures",children:[(0,t.jsx)(n.strong,{children:"Automatically keep schemas up to date"})," (",(0,t.jsx)(n.code,{children:"autoDiscover"}),")"]})," and selected ",(0,t.jsx)(n.strong,{children:"Breaking change re-versions collections"})," (",(0,t.jsx)(n.code,{children:"evolveIncompatibleCollections"}),") for the capture, this evolution would be performed automatically."]}),"\n",(0,t.jsx)(n.h3,{id:"a-new-field-is-added",children:"A new field is added"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.em,{children:"Scenario: this is one way in which the schema can change."})}),"\n",(0,t.jsxs)(n.p,{children:["When a new field appears in the collection schema, it ",(0,t.jsx)(n.em,{children:"may"})," automatically be added to any materializations that use ",(0,t.jsx)(n.code,{children:"recommended"})," fields. Recommended fields are enabled by default in each binding. See ",(0,t.jsx)(n.a,{href:"/concepts/materialization/#projected-fields",children:"the materialization docs"})," for more info about how to enable or disable ",(0,t.jsx)(n.code,{children:"recommended"})," fields."]}),"\n",(0,t.jsx)(n.p,{children:'When recommended fields are enabled, new fields are added automatically if they meet the criteria for the particular materialization connector. For example, scalar fields (strings, numbers, and booleans) are considered "recommended" fields when materializing to database tables.'}),"\n",(0,t.jsxs)(n.p,{children:["If your materialization binding is set to ",(0,t.jsx)(n.code,{children:"recommended: false"}),", or if the new field is not recommended, you can manually add it to the materialization."]}),"\n",(0,t.jsx)(n.p,{children:"To manually add a field:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"In the web app,"})," ",(0,t.jsx)(n.a,{href:"/guides/edit-data-flows/#edit-a-materialization",children:"edit the materialization"}),", find the affected binding, and click ",(0,t.jsx)(n.strong,{children:"Show Fields"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Using flowctl,"})," add the field to ",(0,t.jsx)(n.code,{children:"fields.include"})," in the materialization specification as shown ",(0,t.jsx)(n.a,{href:"/concepts/materialization/#projected-fields",children:"here"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsxs)(n.p,{children:["Newly added fields will not be set for rows that have already been materialized. If you want to ensure that all rows have the new field, just increment the ",(0,t.jsx)(n.code,{children:"backfill"})," counter in the affected binding to have it re-start from the beginning."]})}),"\n",(0,t.jsx)(n.h3,{id:"a-fields-data-type-has-changed",children:"A field's data type has changed"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.em,{children:"Scenario: this is one way in which the schema can change."})}),"\n",(0,t.jsx)(n.p,{children:"When a field's data type has changed, the effect on your materialization depends on the specific connector you're using."}),"\n",(0,t.jsxs)(n.admonition,{type:"warning",children:[(0,t.jsxs)(n.p,{children:["Note that these restrictions only apply to fields that are actively being materialized. If a field is ",(0,t.jsx)(n.a,{href:"/concepts/materialization/#projected-fields",children:"excluded from your materialization"}),", either explicitly or because it's not recommended, then the data types may change in any way."]}),(0,t.jsxs)(n.p,{children:["Regardless of whether the field is materialized or not, it must still pass schema validation tests. Therefore, you must still make sure existing data remains valid against the new schema. For example, if you changed ",(0,t.jsx)(n.code,{children:"excluded_field: { type: string }"})," to ",(0,t.jsx)(n.code,{children:"type: integer"})," while there was existing data with string values, your materialization would fail due to a schema validation error."]})]}),"\n",(0,t.jsxs)(n.p,{children:["Database and data warehouse materializations tend to be somewhat restrictive about changing column types. They typically only allow dropping ",(0,t.jsx)(n.code,{children:"NOT NULL"})," constraints. This means that you can safely change a schema to make a required field optional, or to add ",(0,t.jsx)(n.code,{children:"null"})," as a possible type, and the materialization will continue to work normally.  Most other types of changes will require materializing into a new table."]}),"\n",(0,t.jsx)(n.p,{children:"The best way to find out whether a change is acceptable to a given connector is to run a test or attempt to re-publish. Failed attempts to publish won't affect any tasks that are already running."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Web app workflow"})}),"\n",(0,t.jsxs)(n.p,{children:["If you're working in Estuary's web app, and attempt to publish a change that's unacceptable to the connector, you'll see an error message and an offer to increment the necessary ",(0,t.jsx)(n.code,{children:"backfill"})," counters, or, in rare cases, to re-create the collection."]}),"\n",(0,t.jsxs)(n.p,{children:["Click ",(0,t.jsx)(n.strong,{children:"Apply"})," to to accept this solution and continue to publish."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"flowctl workflow"})}),"\n",(0,t.jsx)(n.p,{children:"If you test or attempt to publish a change that's unacceptable to the connector, you'll see an error message pointing to the field that's changed. In most cases, you can work around the issue by manually updating the materialization to materialize into a new table."}),"\n",(0,t.jsx)(n.p,{children:"For example, say you have a data flow defined by the following specs:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"collections:\n  acmeCo/inventory/anvils:\n    key: [/sku]\n    schema:\n      type: object\n      properties:\n        sku: { type: string }\n        quantity: { type: integer }\n        description: { type: string }\n      required: [sku, quantity]\n\nmaterializations:\n  acmeCo/data-warehouse/materialize-snowflake:\n    endpoint:\n      connector:\n        image: ghcr.io/estuary/materialize-snowflake:v1\n        config: encrypted-snowflake-config.sops.yaml\n    bindings:\n      - source: acmeCo/inventory/anvils\n        backfill: 3\n        resource:\n          table: anvils\n          schema: inventory\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Let's say the type of ",(0,t.jsx)(n.code,{children:"description"})," was broadened to allow ",(0,t.jsx)(n.code,{children:"object"})," values in addition to ",(0,t.jsx)(n.code,{children:"string"}),". You'd update your specs as follows:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"collections:\n  acmeCo/inventory/anvils:\n    key: [/sku]\n    schema:\n      type: object\n      properties:\n        sku: { type: string }\n        quantity: { type: integer }\n        description: { type: [string, object] }\n      required: [sku, quantity]\n\nmaterializations:\n  acmeCo/data-warehouse/materialize-snowflake:\n    endpoint:\n      connector:\n        image: ghcr.io/estuary/materialize-snowflake:v1\n        config: encrypted-snowflake-config.sops.yaml\n    bindings:\n      - source: acmeCo/inventory/anvils\n        backfill: 4\n        resource:\n          table: anvils\n          schema: inventory\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Note that the only change was to increment the ",(0,t.jsx)(n.code,{children:"backfill"})," counter. If the previous binding spec did not specify ",(0,t.jsx)(n.code,{children:"backfill"}),", then just add ",(0,t.jsx)(n.code,{children:"backfill: 1"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["This works because the type is broadened, so existing values will still validate against the new schema. If this were not the case, then you'd likely need to ",(0,t.jsx)(n.a,{href:"#re-creating-a-collection",children:"re-create the whole collection"}),"."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Auto-Discovers:"})}),"\n",(0,t.jsxs)(n.p,{children:["If you enabled the option to ",(0,t.jsxs)(n.a,{href:"/pr-preview/pr-2648/concepts/captures#automatically-update-captures",children:[(0,t.jsx)(n.strong,{children:"Automatically keep schemas up to date"})," (",(0,t.jsx)(n.code,{children:"autoDiscover"}),")"]})," and selected ",(0,t.jsx)(n.strong,{children:"Breaking change re-versions collections"})," (",(0,t.jsx)(n.code,{children:"evolveIncompatibleCollections"}),") for the capture, this evolution would be performed automatically."]}),"\n",(0,t.jsx)(n.h3,{id:"a-field-was-removed",children:"A field was removed"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.em,{children:"Scenario: this is one way in which the schema can change."})}),"\n",(0,t.jsxs)(n.p,{children:["Removing fields is generally allowed by all connectors, and does not require new tables or collections. Note that for database materializations, the existing column will ",(0,t.jsx)(n.em,{children:"not"})," be dropped, and will just be ignored by the materialization going forward. A ",(0,t.jsx)(n.code,{children:"NOT NULL"})," constraint would be removed from that column, but it will otherwise be left in place."]}),"\n",(0,t.jsx)(n.h2,{id:"how-schema-evolution-works",children:"How schema evolution works"}),"\n",(0,t.jsx)(n.p,{children:"Understanding how Estuary handles schemas helps you troubleshoot issues and make informed decisions about backfills."}),"\n",(0,t.jsx)(n.h3,{id:"write-schemas-vs-read-schemas",children:"Write schemas vs. read schemas"}),"\n",(0,t.jsx)(n.p,{children:"Collections maintain two separate schemas:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Write schema"}),": Constrains new documents coming from captures and derivations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Read schema"}),": Validates existing documents when they're consumed by materializations"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"These operate independently. When a capture discovers a schema change, it updates the write schema for new documents. Existing documents in the collection were written under older schemas and remain valid under the read schema."}),"\n",(0,t.jsxs)(n.p,{children:["This separation is why new columns show ",(0,t.jsx)(n.code,{children:"NULL"})," for historical data\u2014those documents were captured before the column existed."]}),"\n",(0,t.jsx)(n.h3,{id:"auto-inference-only-widens",children:"Auto-inference only widens"}),"\n",(0,t.jsxs)(n.p,{children:['Estuary\'s schema inference creates "shrink-wrap" schemas that accommodate all document variations seen so far. Importantly, ',(0,t.jsx)(n.strong,{children:"inference only widens schemas, never narrows them"}),"."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Compatible changes (collection level):"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Adding a new field"}),"\n",(0,t.jsxs)(n.li,{children:["Changing ",(0,t.jsx)(n.code,{children:"required"})," to optional"]}),"\n",(0,t.jsxs)(n.li,{children:["Widening types (e.g., ",(0,t.jsx)(n.code,{children:"integer"})," to ",(0,t.jsx)(n.code,{children:"number"}),")"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Incompatible changes (require backfill):"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Narrowing types (e.g., ",(0,t.jsx)(n.code,{children:"string"})," to ",(0,t.jsx)(n.code,{children:"integer"}),")"]}),"\n",(0,t.jsxs)(n.li,{children:["Adding a ",(0,t.jsx)(n.code,{children:"required"})," constraint"]}),"\n",(0,t.jsx)(n.li,{children:"Changing key pointers or partitioning"}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsxs)(n.p,{children:["Even when a change is compatible at the collection level, your destination connector may require a backfill to alter the table schema. For example, widening ",(0,t.jsx)(n.code,{children:"integer"})," to ",(0,t.jsx)(n.code,{children:"bigint"})," is compatible in the collection, but some databases need a backfill to apply the column type change."]})}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["For a deep dive into the architecture of schema evolution, see ",(0,t.jsx)(n.a,{href:"https://github.com/estuary/flow/discussions/1988",children:"Discussion #1988: Collection Evolution and Source-Defined Schema"}),"."]})}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,t.jsx)(n.h3,{id:"why-are-my-new-columns-showing-null-values",children:"Why are my new columns showing NULL values?"}),"\n",(0,t.jsxs)(n.p,{children:["When you add a new column to a source table, existing documents in Estuary collections were captured ",(0,t.jsx)(n.em,{children:"before"})," that column existed. The new column will only have values for documents captured ",(0,t.jsx)(n.em,{children:"after"})," the schema change."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Example:"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["You add column ",(0,t.jsx)(n.code,{children:"is_active"})," to your source table"]}),"\n",(0,t.jsx)(n.li,{children:"Estuary detects the change and updates the collection schema"}),"\n",(0,t.jsxs)(n.li,{children:["New documents include ",(0,t.jsx)(n.code,{children:"is_active"}),", but historical documents don't have it"]}),"\n",(0,t.jsxs)(n.li,{children:["When materialized, historical rows show ",(0,t.jsx)(n.code,{children:"NULL"})," for ",(0,t.jsx)(n.code,{children:"is_active"})]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Solution:"})," Trigger a backfill to recapture historical data with the new schema."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"UI"}),": Go to your capture \u2192 Click ",(0,t.jsx)(n.strong,{children:"Backfill"})," on the affected binding"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"flowctl"}),": Increment the ",(0,t.jsx)(n.code,{children:"backfill"})," counter in your capture spec and republish"]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["For very large tables, consider whether you need historical values for the new column, or if ",(0,t.jsx)(n.code,{children:"NULL"})," for older records is acceptable."]})}),"\n",(0,t.jsx)(n.h3,{id:"my-data-stopped-flowing-after-a-schema-change",children:"My data stopped flowing after a schema change"}),"\n",(0,t.jsx)(n.p,{children:"If data stops flowing after a schema change at your source, work through these checks:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Is the capture running?"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Check the hourly data graph and logs in the UI"}),"\n",(0,t.jsx)(n.li,{children:"Most schema changes (like adding columns) are handled automatically\u2014the capture briefly restarts to pick up the new schema"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Is the materialization running?"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Check the hourly data graph and logs in the UI"}),"\n",(0,t.jsx)(n.li,{children:"If stopped, look for schema validation errors in the logs"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:'Did you see a "Changes rejected" prompt?'})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["When schema changes would break downstream tasks, the UI shows: ",(0,t.jsx)(n.em,{children:'"Changes rejected due to incompatible collection updates"'})]}),"\n",(0,t.jsxs)(n.li,{children:["Click ",(0,t.jsx)(n.strong,{children:"Apply"})," to automatically update downstream tasks (this may trigger backfills)"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Common causes and solutions:"})}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Symptom"}),(0,t.jsx)(n.th,{children:"Likely Cause"}),(0,t.jsx)(n.th,{children:"Solution"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'Transient "document failed validation" error'}),(0,t.jsx)(n.td,{children:"Schema inference racing with new records"}),(0,t.jsxs)(n.td,{children:["See ",(0,t.jsx)(n.a,{href:"#transient-validation-errors-during-schema-changes",children:"Transient validation errors"})," section below"]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"Changes rejected due to incompatible collection updates"'}),(0,t.jsx)(n.td,{children:"Incompatible schema change detected"}),(0,t.jsxs)(n.td,{children:["Click ",(0,t.jsx)(n.strong,{children:"Apply"})," to update downstream tasks"]})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:'"Unsupported operation DROP TABLE"'}),(0,t.jsx)(n.td,{children:"Destructive DDL change"}),(0,t.jsx)(n.td,{children:"Disable capture, remove binding, re-enable"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Data flows but destination unchanged"}),(0,t.jsx)(n.td,{children:"Processing delay or materialization paused"}),(0,t.jsx)(n.td,{children:"Check task status in UI"})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"transient-validation-errors-during-schema-changes",children:"Transient validation errors during schema changes"}),"\n",(0,t.jsx)(n.p,{children:"When your source data changes shape, you may see errors like:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"document failed validation against its collection JSON Schema\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Why this happens:"})}),"\n",(0,t.jsx)(n.p,{children:"Schema updates and new data records are processed through separate paths. When a source schema changes:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"The capture detects the change and publishes an updated collection schema"}),"\n",(0,t.jsx)(n.li,{children:"New records (with the new shape) start flowing immediately"}),"\n",(0,t.jsx)(n.li,{children:"The materialization receives these new records"}),"\n",(0,t.jsxs)(n.li,{children:["If a record arrives ",(0,t.jsx)(n.em,{children:"before"})," the schema update is applied, validation fails"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This is a side effect of a low-latency system\u2014we don't artificially delay records, so the schema change can arrive slightly after the first records with the new shape. The system is designed to recover automatically."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"How to identify transient vs. persistent failures:"})}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Indicator"}),(0,t.jsx)(n.th,{children:"Transient (wait)"}),(0,t.jsx)(n.th,{children:"Persistent (action needed)"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Timing"}),(0,t.jsx)(n.td,{children:"Started during/after a source schema change"}),(0,t.jsx)(n.td,{children:"No recent schema changes"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Duration"}),(0,t.jsx)(n.td,{children:"Resolves within 5-10 minutes"}),(0,t.jsx)(n.td,{children:"Persists beyond 30 minutes"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Error pattern"}),(0,t.jsx)(n.td,{children:"Intermittent, then stops"}),(0,t.jsx)(n.td,{children:"Continuous, every record fails"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Materialization status"}),(0,t.jsx)(n.td,{children:"Restarts automatically"}),(0,t.jsx)(n.td,{children:"Stays failed or loops"})]})]})]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"What happens during recovery:"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"The materialization encounters validation errors and restarts"}),"\n",(0,t.jsx)(n.li,{children:"On restart, it picks up the updated collection schema"}),"\n",(0,t.jsx)(n.li,{children:"Processing resumes normally with the new schema"}),"\n",(0,t.jsx)(n.li,{children:"No data is lost\u2014failed records are retried after the schema update"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"When to take action:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Wait 15-30 minutes"})," before investigating further"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Check the logs"})," for the specific validation error\u2014if it references a field that was just added/changed, it's likely transient"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"If errors persist"}),", verify your source schema change was compatible (see ",(0,t.jsx)(n.a,{href:"#auto-inference-only-widens",children:"Auto-inference only widens"})," above)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"If you made an incompatible change"}),", you may need to trigger a backfill or adjust your schema"]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"High-volume sources with frequent schema changes may see these transient errors more often. If this becomes disruptive, consider batching schema changes during low-traffic periods."})}),"\n",(0,t.jsx)(n.h3,{id:"configuring-automatic-schema-change-handling",children:"Configuring automatic schema change handling"}),"\n",(0,t.jsxs)(n.p,{children:["You can configure how materializations respond to incompatible schema changes using the ",(0,t.jsx)(n.code,{children:"onIncompatibleSchemaChange"})," setting."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"In the Flow web app:"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Edit your materialization"}),"\n",(0,t.jsxs)(n.li,{children:["Go to ",(0,t.jsx)(n.strong,{children:"Collections"})]}),"\n",(0,t.jsxs)(n.li,{children:["Click ",(0,t.jsx)(n.strong,{children:"Config"})," on a collection"]}),"\n",(0,t.jsxs)(n.li,{children:["Expand ",(0,t.jsx)(n.strong,{children:"Advanced Options"})]}),"\n",(0,t.jsxs)(n.li,{children:["Set ",(0,t.jsx)(n.strong,{children:"Incompatible Schema Change"})," to your preferred action"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Available options:"})}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Option"}),(0,t.jsx)(n.th,{children:"Behavior"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsxs)(n.td,{children:[(0,t.jsx)(n.strong,{children:"backfill"})," ",(0,t.jsx)(n.em,{children:"(default)"})]}),(0,t.jsx)(n.td,{children:"Automatically backfill and re-materialize the affected binding"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"abort"})}),(0,t.jsx)(n.td,{children:"Fail the publication, preventing incompatible changes"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"disableBinding"})}),(0,t.jsx)(n.td,{children:"Disable only the affected binding until manually re-enabled"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"disableTask"})}),(0,t.jsx)(n.td,{children:"Disable the entire materialization until manually re-enabled"})]})]})]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Using flowctl:"})}),"\n",(0,t.jsx)(n.p,{children:"Set the field at the top level (applies to all bindings) or per-binding:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"materializations:\n  myPrefix/my-materialization:\n    onIncompatibleSchemaChange: disableTask  # or: backfill (default), abort, disableBinding\n    bindings:\n      - source: myPrefix/my-collection\n        onIncompatibleSchemaChange: abort  # Override for this binding\n        resource:\n          table: my_table\n"})}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsx)(n.p,{children:"These behaviors only trigger during automated actions like AutoDiscover. Manual changes via the UI will prompt you to choose an action."})}),"\n",(0,t.jsx)(n.h3,{id:"type-inference-issues-with-nosql-sources",children:"Type inference issues with NoSQL sources"}),"\n",(0,t.jsx)(n.p,{children:"NoSQL databases like MongoDB allow flexible schemas where the same field can have different types across documents. Estuary infers a schema based on the data it observes, which can lead to unexpected type widening."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Common issue:"}),' "My date field is being stored as a string"']}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Why this happens:"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["The first documents captured had the field as a string value (e.g., ",(0,t.jsx)(n.code,{children:'"2024-01-15"'}),")"]}),"\n",(0,t.jsxs)(n.li,{children:["Estuary inferred the type as ",(0,t.jsx)(n.code,{children:"string"})]}),"\n",(0,t.jsx)(n.li,{children:"Later documents with proper Date types are converted to strings to match the inferred schema"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Remember: auto-inference only widens, never narrows. Once a field is inferred as ",(0,t.jsx)(n.code,{children:"string"}),", it won't narrow to ",(0,t.jsx)(n.code,{children:"date"}),"\u2014even if all subsequent documents use the correct type."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Options:"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Accept the widened type"}),": In many cases, the widened type works fine downstream. String dates can still be parsed by your destination or BI tools. Evaluate whether this actually causes problems before investing effort in a fix."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Fix at the source, then backfill"}),": The cleanest solution when the type matters:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Correct the inconsistent data types in your source database"}),"\n",(0,t.jsxs)(n.li,{children:["Trigger a ",(0,t.jsx)(n.a,{href:"/reference/backfilling-data/#dataflow-reset",children:"dataflow reset"})," to re-infer the schema from scratch"]}),"\n",(0,t.jsx)(n.li,{children:"The backfill recaptures all data with consistent types, and the schema is re-inferred correctly"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Use a derivation"})," ",(0,t.jsx)(n.em,{children:"(last resort)"}),": If you can't fix the source data, create a derivation to transform the field:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"SELECT\n  _id,\n  PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%S', created_at) as created_at\nFROM source_collection\n"})}),"\n",(0,t.jsx)(n.p,{children:"This adds complexity to your pipeline, so only use this when options 1 and 2 aren't viable."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["When setting up captures from schema-flexible sources, review the inferred schema before creating materializations. If types look incorrect, fix the source data and backfill ",(0,t.jsx)(n.em,{children:"before"})," proceeding\u2014it's much easier to get the schema right from the start."]})})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);