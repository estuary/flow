syntax = "proto3";

package flow;

import "broker/protocol/protocol.proto";
import "consumer/protocol/protocol.proto";
import "consumer/recoverylog/recorded_op.proto";
import "ptypes/empty/empty.proto";
import "gogoproto/gogo.proto";

option go_package = "protocol";

option (gogoproto.marshaler_all) = true;
option (gogoproto.protosizer_all) = true;
option (gogoproto.unmarshaler_all) = true;

// ContentType is an encoding used for document content.
enum ContentType {
  INVALID = 0;
  // JSON is the usual text encoding, with a trailing newline.
  JSON = 1;

  // Future ContentTypes may include MessagePack or tape-based representations.
};

// Slice represents a contiguous slice of bytes within an associated Arena.
message Slice {
  option (gogoproto.goproto_unrecognized) = false;
  option (gogoproto.goproto_unkeyed) = false;
  option (gogoproto.goproto_sizecache) = false;

  uint32 begin = 1;
  uint32 end = 2;
}

// UUIDParts is a deconstructed, RFC 4122 v1 variant Universally Unique
// Identifier as used by Gazette.
message UUIDParts {
  option (gogoproto.equal) = true;
  option (gogoproto.goproto_unrecognized) = false;
  option (gogoproto.goproto_unkeyed) = false;
  option (gogoproto.goproto_sizecache) = false;

  // Producer is the unique node identifier portion of a v1 UUID, as the high
  // 48 bits of |producer_and_flags|. The MSB must be 1 to mark this producer
  // as "multicast" and not an actual MAC address (as per RFC 4122).
  //
  // Bits 49-54 must be zero.
  //
  // The low 10 bits are the 10 least-significant bits of the v1 UUID clock
  // sequence, used by Gazette to represent flags over message transaction
  // semantics.
  fixed64 producer_and_flags = 1;
  // Clock is a v1 UUID 60-bit timestamp (60 MSBs), followed by 4 bits of
  // sequence counter.
  fixed64 clock = 2
      [ (gogoproto.casttype) = "go.gazette.dev/core/message.Clock" ];
}

// Shuffle of documents, mapping each document to member indicies within a
// Ring.
message Shuffle {
  option (gogoproto.equal) = true;

  // Transform for which this Shuffle is being applied.
  string transform = 1 [ (gogoproto.casttype) = "Transform" ];
  // Composite key over which shuffling occurs, specified as one or more
  // JSON-Pointers indicating a message location to extract.
  repeated string shuffle_key_ptr = 2;
  // uses_source_key is true if shuffle_key_ptr is the source's native key,
  // and false if it's some other key. When shuffling using the source's key,
  // we can minimize data movement by assigning a shard coordinator for each
  // journal such that the shard's key range overlap that of the journal.
  bool uses_source_key = 3;
  // filter_r_clocks is true if the shuffle coordinator should filter documents
  // sent to each subscriber based on its covered r-clock ranges and the
  // individual document clocks. If false, the subscriber's r-clock range is
  // ignored and all documents which match the key range are sent.
  //
  // filter_r_clocks is set 'true' when reading on behalf of transforms having
  // a "publish" but not an "update" lambda, as such documents have no
  // side-effects on the reader's state store, and would not be published anyway
  // for falling outside of the reader's r-clock range.
  bool filter_r_clocks = 6;
  // Optional hash applied to extracted, packed shuffle keys. Hashes can:
  // * Mitigate shard skew which might otherwise occur due to key locality
  //   (many co-occurring updates to "nearby" keys).
  // * Give predictable storage sizes for keys which are otherwise unbounded.
  // * Allow for joins over sensitive fields, which should not be stored
  //   in-the-clear at rest where possible.
  // Either cryptographic or non-cryptographic functions may be appropriate
  // depending on thse use case.
  enum Hash {
    // None performs no hash, returning the original key.
    NONE = 0;
    // MD5 returns the MD5 digest of the original key. It is not a safe
    // cryptographic hash, but is well-known and fast, with good distribution
    // properties.
    MD5 = 1;
  };
  Hash hash = 4;
  // Number of seconds for which documents of this collection are delayed
  // while reading, relative to other documents (when back-filling) and the
  // present wall-clock time (when tailing).
  uint32 read_delay_seconds = 5;
}

// JournalShuffle is a Shuffle of a Journal by a Coordinator shard.
message JournalShuffle {
  option (gogoproto.equal) = true;

  // Journal to be shuffled.
  string journal = 1
      [ (gogoproto.casttype) = "go.gazette.dev/core/broker/protocol.Journal" ];
  // Coordinator is the Shard ID which is responsible for reads of this journal.
  string coordinator = 2
      [ (gogoproto.casttype) =
            "go.gazette.dev/core/consumer/protocol.ShardID" ];
  // Shuffle of this JournalShuffle.
  Shuffle shuffle = 3
      [ (gogoproto.nullable) = false, (gogoproto.embed) = true ];
}

// Transform describes a specific transform of a derived collection.
message TransformSpec {
  // Source collection read by this transform.
  message Source {
    // Name of the collection.
    string name = 1 [ (gogoproto.casttype) = "Collection" ];
    // Selector of partitions of the collection which this transform reads.
    protocol.LabelSelector partitions = 2 [ (gogoproto.nullable) = false ];
  }
  Source source = 2 [ (gogoproto.nullable) = false ];
  // Shuffle applied to source documents for this transform.
  // Note that the Shuffle embeds the Transform name.
  Shuffle shuffle = 3 [ (gogoproto.nullable) = false ];
  // Derived collection produced by this transform.
  message Derivation {
    string name = 1 [ (gogoproto.casttype) = "Collection" ];
  }
  Derivation derivation = 5 [ (gogoproto.nullable) = false ];
}

// Field holds a column of values extracted from a document location.
message Field {
  // Value is the extracted representation of the field value.
  message Value {
    enum Kind {
      INVALID = 0;
      NULL = 1;
      TRUE = 2;
      FALSE = 3;
      STRING = 4;
      UNSIGNED = 5;
      SIGNED = 6;
      DOUBLE = 7;
      OBJECT = 8;
      ARRAY = 9;
    }
    Kind kind = 1;
    uint64 unsigned = 2;
    sint64 signed = 3;
    double double = 4;
    Slice bytes = 5 [ (gogoproto.nullable) = false ];
  };
  repeated Value values = 1 [ (gogoproto.nullable) = false ];
}

// RangeSpec describes the ranges of shuffle keys and r-clocks which a reader
// is responsible for.
message RangeSpec {
  // Byte [begin, end) exclusive range of keys to be shuffled to this reader.
  bytes key_begin = 2;
  bytes key_end = 3;
  // Rotated [begin, end) exclusive ranges of Clocks to be shuffled to this
  // reader.
  uint64 r_clock_begin = 4;
  uint64 r_clock_end = 5;
}

// ShuffleRequest is the request message of a Shuffle RPC.
message ShuffleRequest {
  JournalShuffle shuffle = 1 [ (gogoproto.nullable) = false ];
  RangeSpec range = 2 [ (gogoproto.nullable) = false ];
  // Offset to begin reading the journal from.
  int64 offset = 3
      [ (gogoproto.casttype) = "go.gazette.dev/core/broker/protocol.Offset" ];
  // Offset to stop reading the journal at, or zero if unbounded.
  int64 end_offset = 4
      [ (gogoproto.casttype) = "go.gazette.dev/core/broker/protocol.Offset" ];
  // Resolution header of the |config.coordinator_index| shard.
  protocol.Header resolution = 5;
}

// ShuffleResponse is the streamed response message of a Shuffle RPC.
message ShuffleResponse {
  // Status of the Shuffle RPC.
  consumer.Status status = 1;
  // Header of the response.
  protocol.Header header = 2;
  // Terminal error encountered while serving this ShuffleRequest. A terminal
  // error is only sent if a future ShuffleRequest of this same configuration
  // and offset will fail in the exact same way, and operator intervention is
  // required to properly recover. Such errors are returned so that the caller
  // can also abort with a useful, contextual error message.
  //
  // Examples of terminal errors include the requested journal not existing,
  // or data corruption. Errors *not* returned as |terminal_error| include
  // network errors, process failures, and other conditions which can be
  // retried.
  string terminal_error = 3;
  // Offset which was read through to produce this ShuffleResponse.
  int64 read_through = 4
      [ (gogoproto.casttype) = "go.gazette.dev/core/broker/protocol.Offset" ];
  // WriteHead of the journal as reported by the broker, as of the creation of
  // this ShuffleResponse.
  int64 write_head = 5
      [ (gogoproto.casttype) = "go.gazette.dev/core/broker/protocol.Offset" ];
  // Memory arena of this message.
  bytes arena = 6 [ (gogoproto.casttype) = "Arena" ];
  // Transform name, passed through from the ShuffleRequest.
  string transform = 7 [ (gogoproto.casttype) = "Transform" ];
  // ContentType of documents in this ShuffleResponse.
  ContentType content_type = 8;
  // Content of documents included in this ShuffleResponse.
  repeated Slice content = 9 [ (gogoproto.nullable) = false ];
  // The begin offset of each document within the requested journal.
  repeated int64 begin = 10
      [ (gogoproto.casttype) = "go.gazette.dev/core/broker/protocol.Offset" ];
  // The end offset of each document within the journal.
  repeated int64 end = 11
      [ (gogoproto.casttype) = "go.gazette.dev/core/broker/protocol.Offset" ];
  // UUIDParts of each document.
  repeated UUIDParts uuid_parts = 12 [ (gogoproto.nullable) = false ];
  // Packed, embedded encoding of the shuffle key into a byte string.
  // If the Shuffle specified a Hash to use, it's applied as well.
  repeated Slice packed_key = 13 [ (gogoproto.nullable) = false ];
  // Extracted shuffle key of each document, with one Field for each
  // component of the composite shuffle key.
  repeated Field shuffle_key = 14 [ (gogoproto.nullable) = false ];
}

service Shuffler {
  rpc Shuffle(ShuffleRequest) returns (stream ShuffleResponse);
}

message ExtractRequest {
  // Memory arena of this message.
  bytes arena = 1 [ (gogoproto.casttype) = "Arena" ];
  // ContentType of documents in this ExtractRequest.
  ContentType content_type = 2;
  // Content of documents included in this ExtractRequest.
  repeated Slice content = 3 [ (gogoproto.nullable) = false ];
  // JSON pointer of document UUID to extract.
  // If empty, UUIDParts are not extracted.
  string uuid_ptr = 4;
  // Composite of JSON pointers to extract from documents and hash.
  // If empty, hashes are not extracted.
  repeated string hash_ptrs = 5;
  // Field JSON pointers to extract from documents and return.
  // If empty, no fields are extracted.
  repeated string field_ptrs = 6;
};

message ExtractResponse {
  // Memory arena of this message.
  bytes arena = 1 [ (gogoproto.casttype) = "Arena" ];
  // UUIDParts extracted from request Documents.
  repeated UUIDParts uuid_parts = 2 [ (gogoproto.nullable) = false ];
  // Hashes extracted from request Documents (low 64-bits).
  // If the request |hash_ptrs| was empty, so are these.
  repeated fixed64 hashes_low = 3;
  // Hashes extracted from request Documents (high 64-bits).
  repeated fixed64 hashes_high = 4;
  // Fields extracted from request Documents, one column per request pointer.
  repeated Field fields = 5 [ (gogoproto.nullable) = false ];
};

service Extract { rpc Extract(ExtractRequest) returns (ExtractResponse); }

message CombineRequest {
  // Memory arena of this message.
  bytes arena = 1 [ (gogoproto.casttype) = "Arena" ];
  // ContentType of documents in this CombineRequest.
  ContentType content_type = 2;
  // Content of documents included in this CombineRequest.
  repeated Slice content = 3 [ (gogoproto.nullable) = false ];
  // ContentType of documents in the returned CombineResponse.
  ContentType accept = 4;
  // Schema against which documents are to be validated,
  // and which provides reduction annotations.
  string schema_uri = 5;
  // Composite key used to group documents to be combined, specified as one or
  // more JSON-Pointers indicating a message location to extract.
  repeated string key_ptr = 6;
  // Field JSON pointers to be extracted from combined documents and returned.
  // If empty, no fields are extracted.
  repeated string field_ptrs = 7;
}

message CombineResponse {
  // Memory arena of this message.
  bytes arena = 1 [ (gogoproto.casttype) = "Arena" ];
  // Content of documents included in this CombineResponse.
  // ContentType is that of the CombineRequest's |accept| field.
  repeated Slice content = 3 [ (gogoproto.nullable) = false ];
  // Fields extracted from request Documents, one column per request pointer.
  repeated Field fields = 4 [ (gogoproto.nullable) = false ];
}

service Combine { rpc Combine(CombineRequest) returns (CombineResponse); }

// DeriveRequest is the streamed union type message of a Derive RPC.
message DeriveRequest {
  // OPEN is sent (only) as the first message of a Derive RPC,
  // and opens the derive transaction.
  message Open {
    // Collection to be derived.
    string collection = 1 [ (gogoproto.casttype) = "Collection" ];
    // ContentType of documents in the returned DeriveResponse.
    ContentType accept = 2;
  };
  // EXTEND extends the derive transaction with additional
  // source collection documents.
  //
  // * The flow consumer sends any number of EXTEND DeriveRequests,
  //   containing source collection documents.
  // * Concurrently, the derive worker responds with any number of
  //   EXTEND DeriveResponses, each having documents to be added to
  //   the collection being derived.
  // * The flow consumer is responsible for publishing each derived
  //   document to the appropriate collection & partition.
  // * Note that DeriveRequest and DeriveResponse EXTEND messages are _not_ 1:1.
  //
  // EXTEND transitions to EXTEND or FLUSH.
  message Extend {
    // Transform to which documents are applied.
    string transform = 1 [ (gogoproto.casttype) = "Transform" ];
    // Memory arena of this message.
    bytes arena = 2 [ (gogoproto.casttype) = "Arena" ];
    // ContentType of documents.
    ContentType content_type = 3;
    // Content of documents.
    repeated Slice content = 4 [ (gogoproto.nullable) = false ];
  };
  // FLUSH indicates the transacton pipeline is to flush.
  //
  // * The flow consumer issues FLUSH when its consumer transaction begins to
  //   close.
  // * The derive worker responds with FLUSH to indicate that all source
  //   documents have been processed and all derived documents emitted.
  // * The flow consumer awaits the response FLUSH, while continuing to begin
  //   publish operations for all derived documents seen in the meantime.
  // * On seeing FLUSH, the flow consumer is assured it's sequenced and started
  //   publishing all derived documents of the transaction, and can now build
  //   the consumer.Checkpoint which will be committed to the store.
  //
  // FLUSH transitions to PREPARE.
  message Flush {};
  // PREPARE begins a commit of the transaction.
  //
  // * The flow consumer sends PREPARE with its consumer.Checkpoint.
  // * On receipt, the derive worker queues an atomic recoverylog.Recorder
  //   block that's conditioned on an (unresolved) "commit" future. Within
  //   this recording block, underlying store commits (SQLite COMMIT and writing
  //   a RocksDB WriteBatch) are issued to persist all state changes of the
  //   transaction, along with the consumer.Checkpoint.
  // * The derive worker responds with PREPARE once all local commits have
  //   completed, and recoverylog writes have been queued (but not started,
  //   awaiting COMMIT).
  // * On receipt, the flow consumer arranges to invoke COMMIT on the completion
  //   of all outstanding journal writes -- this the OpFuture passed to the
  //   Store.StartCommit interface. It returns a future which will resolve only
  //   after reading COMMIT from this transaction -- the OpFuture returned by
  //   that interface.
  //
  // It's an error if a prior transaction is still running at the onset of
  // PREPARE. However at the completion of PREPARE, a new & concurrent
  // Transaction may begin, though it itself cannot PREPARE until this
  // Transaction fully completes.
  //
  // PREPARE transitions to COMMIT.
  message Prepare {
    // Checkpoint to commit.
    consumer.Checkpoint checkpoint = 1 [ (gogoproto.nullable) = false ];
  };
  // COMMIT commits the transaction by resolving the "commit" future created
  // during PREPARE, allowing the atomic commit block created in PREPARE
  // to flush to the recovery log. The derive worker responds with COMMIT
  // when the commit barrier has fully resolved.
  //
  // COMMIT transitions to stream close.
  message Commit {};

  oneof kind {
    Open open = 1;
    Extend extend = 2;
    Flush flush = 3;
    Prepare prepare = 4;
    Commit commit = 5;
  };
}

// DeriveResponse is the streamed response message of a Derive RPC.
message DeriveResponse {
  // EXTEND extends the derive transaction with additional derived collection
  // documents.
  message Extend {
    // Memory arena of this message.
    bytes arena = 1 [ (gogoproto.casttype) = "Arena" ];
    // Content of documents. ContentType is as specified by
    // DeriveRequest.Open.accept.
    repeated Slice content = 2 [ (gogoproto.nullable) = false ];
    // Logical partitions extracted from |documents|.
    repeated Field partitions = 3 [ (gogoproto.nullable) = false ];
  };
  // FLUSH is sent in response to a DeriveRequest.Flush, only after all
  // request documents have been processed and response Extend messages sent.
  message Flush {};
  // PREPARE is sent in response to a DeriveRequest.Prepare, only after local
  // store updates for commit (including the provided checkpoint) have been
  // staged behind a created, unresolved commit barrier.
  message Prepare {};
  // COMMIT is sent in response to a DeriveRequest.Commit, when the
  // commit barrier has resolved (meaning the transaction is committed).
  message Commit {};

  oneof kind {
    Extend extend = 2;
    Flush flush = 3;
    Prepare prepare = 4;
    Commit commit = 5;
  };
};

service Derive {
  // RestoreCheckpoint recovers the most recent Checkpoint previously committed
  // to the Store. It is called just once, at Shard start-up. If an external
  // system is used, it should install a transactional "write fence" to ensure
  // that an older Store instance of another process cannot successfully
  // StartCommit after this RestoreCheckpoint returns.
  rpc RestoreCheckpoint(google.protobuf.Empty) returns (consumer.Checkpoint);

  // Derive begins a pipelined derive transaction, following the
  // state machine detailed in DeriveState.
  rpc Derive(stream DeriveRequest) returns (stream DeriveResponse);

  // BuildHints returns FSMHints which may be played back to fully reconstruct
  // the local filesystem state produced by this derive worker. It may block
  // while pending operations sync to the recovery log.
  rpc BuildHints(google.protobuf.Empty) returns (recoverylog.FSMHints);
}