syntax = "proto3";

package flow;
option go_package = "github.com/estuary/flow/go/protocols/flow";

import "broker/protocol/protocol.proto";
import "consumer/protocol/protocol.proto";
import "consumer/recoverylog/recorded_op.proto";
import "ptypes/empty/empty.proto";
import "gogoproto/gogo.proto";

option (gogoproto.marshaler_all) = true;
option (gogoproto.protosizer_all) = true;
option (gogoproto.unmarshaler_all) = true;
option (gogoproto.goproto_getters_all) = false;

// Slice represents a contiguous slice of bytes within an associated Arena.
message Slice {
  option (gogoproto.goproto_unrecognized) = false;
  option (gogoproto.goproto_unkeyed) = false;
  option (gogoproto.goproto_sizecache) = false;

  uint32 begin = 1;
  uint32 end = 2;
}

// UUIDParts is a deconstructed, RFC 4122 v1 variant Universally Unique
// Identifier as used by Gazette.
message UUIDParts {
  option (gogoproto.equal) = true;
  option (gogoproto.goproto_unrecognized) = false;
  option (gogoproto.goproto_unkeyed) = false;
  option (gogoproto.goproto_sizecache) = false;

  // Producer is the unique node identifier portion of a v1 UUID, as the high
  // 48 bits of |producer_and_flags|. The MSB must be 1 to mark this producer
  // as "multicast" and not an actual MAC address (as per RFC 4122).
  //
  // Bits 49-54 must be zero.
  //
  // The low 10 bits are the 10 least-significant bits of the v1 UUID clock
  // sequence, used by Gazette to represent flags over message transaction
  // semantics.
  fixed64 producer_and_flags = 1;
  // Clock is a v1 UUID 60-bit timestamp (60 MSBs), followed by 4 bits of
  // sequence counter.
  fixed64 clock = 2
      [ (gogoproto.casttype) = "go.gazette.dev/core/message.Clock" ];
}

// Shuffle is a description of a document shuffle, where each document
// is mapped into:
//  * An extracted, packed composite key (a "shuffle key").
//  * A rotated Clock value (an "r-clock").
// The packed key and r-clock can then be compared to a reader RangeSpec.
message Shuffle {
  option (gogoproto.equal) = true;

  // Composite key over which shuffling occurs, specified as one or more
  // JSON-Pointers indicating a message location to extract.
  repeated string shuffle_key_ptr = 1;
  // uses_source_key is true if shuffle_key_ptr is the source's native key,
  // and false if it's some other key. When shuffling using the source's key,
  // we can minimize data movement by assigning a shard coordinator for each
  // journal such that the shard's key range overlap that of the journal.
  bool uses_source_key = 2;
  // filter_r_clocks is true if the shuffle coordinator should filter documents
  // sent to each subscriber based on its covered r-clock ranges and the
  // individual document clocks. If false, the subscriber's r-clock range is
  // ignored and all documents which match the key range are sent.
  //
  // filter_r_clocks is set 'true' when reading on behalf of transforms having
  // a "publish" but not an "update" lambda, as such documents have no
  // side-effects on the reader's state store, and would not be published anyway
  // for falling outside of the reader's r-clock range.
  bool filter_r_clocks = 3;
  // Optional hash applied to extracted, packed shuffle keys. Hashes can:
  // * Mitigate shard skew which might otherwise occur due to key locality
  //   (many co-occurring updates to "nearby" keys).
  // * Give predictable storage sizes for keys which are otherwise unbounded.
  // * Allow for joins over sensitive fields, which should not be stored
  //   in-the-clear at rest where possible.
  // Either cryptographic or non-cryptographic functions may be appropriate
  // depending on thse use case.
  enum Hash {
    // None performs no hash, returning the original key.
    NONE = 0;
    // MD5 returns the MD5 digest of the original key. It is not a safe
    // cryptographic hash, but is well-known and fast, with good distribution
    // properties.
    MD5 = 1;
  };
  Hash hash = 4;
  // Number of seconds for which documents of this collection are delayed
  // while reading, relative to other documents (when back-filling) and the
  // present wall-clock time (when tailing).
  uint32 read_delay_seconds = 5;
}

// JournalShuffle is a Shuffle of a Journal by a Coordinator shard.
message JournalShuffle {
  option (gogoproto.equal) = true;

  // Journal to be shuffled.
  string journal = 1
      [ (gogoproto.casttype) = "go.gazette.dev/core/broker/protocol.Journal" ];
  // Coordinator is the Shard ID which is responsible for reads of this journal.
  string coordinator = 2
      [ (gogoproto.casttype) =
            "go.gazette.dev/core/consumer/protocol.ShardID" ];
  // Shuffle of this JournalShuffle.
  Shuffle shuffle = 3
      [ (gogoproto.nullable) = false, (gogoproto.embed) = true ];
  // Is this a reply of the journal's content?
  // We separate ongoing vs replayed reads of a journal's content into
  // distinct rings, so that ongoing reads cannot deadlock a replay read.
  //
  // If we didn't do this, a shard might issue a replay read while
  // *also* having a full recv queue of its ongoing read. Then, the
  // the server would on sending yet another ongoing read, such that
  // it's unable to service the replay read that would ultimately
  // unblock the shard / allow it to drain new ongoing reads.
  bool replay = 4;
}

// Projection is a mapping between a document location, specified as a
// JSON-Pointer, and a corresponding field string in a flattened
// (i.e. tabular or SQL) namespace which aliases it.
message Projection {
  // Document location of this projection, as a JSON-Pointer.
  string ptr = 1;
  // Field is the flattened, tabular alias of this projection.
  string field = 2;
  // Was this projection user provided ?
  bool user_provided = 3;
  // Does this projection constitute a logical partitioning of the collection?
  bool is_partition_key = 4;
  // Does this location form (part of) the collection key?
  bool is_primary_key = 5;
  // Inference of this projection.
  Inference inference = 6;
}

// Inference details type information which is statically known
// about a given document location.
message Inference {
  // The possible types for this location.
  // Subset of ["null", "boolean", "object", "array", "integer", "numeric", "string"].
  repeated string types = 1;
  // Whether the projection must always exist (either as a location within)
  // the source document, or as a null-able column in the database.
  bool must_exist = 2;
  // String type-specific inferences.
  message String {
    // Annotated Content-Type when the projection is of "string" type.
    string content_type = 3;
    // Annotated format when the projection is of "string" type.
    string format = 4;
    // Whether the value is base64-encoded when the projection is of "string" type.
    bool is_base64 = 5;
    // Maximum length when the projection is of "string" type. Zero for no limit.
    uint32 max_length = 6;
  }
  String string = 3;
  // The title from the schema, if provided
  string title = 4;
  // The description from the schema, if provided
  string description = 5;
}

message CollectionSpec {
  // Name of this collection.
  string name = 1 [ (gogoproto.casttype) = "Collection" ];
  // Schema against which collection documents are validated,
  // and which provides reduction annotations.
  string schema_uri = 2;
  // Composite key of the collection, as JSON-Pointers.
  repeated string key_ptrs = 3;
  // JSON pointer locating the UUID of each collection document.
  string uuid_ptr = 4;
  // Logical partition fields of this collection.
  repeated string partition_fields = 5;
  // Logical projections of this collection
  repeated Projection projections = 6;
  // JournalSpec used for dynamically-created journals of this collection.
  protocol.JournalSpec journal_spec = 7 [ (gogoproto.nullable) = false ];
  // JSON-encoded document template for creating Gazette consumer
  // transaction acknowledgements of writes into this collection.
  bytes ack_json_template = 8;
}

// Transform describes a specific transform of a derived collection.
message TransformSpec {
  // Stable name of this transform, scoped to it's Derivation.
  string name = 1 [ (gogoproto.casttype) = "Transform" ];
  // ID of this transform within the present catalog DB.
  // This ID is *not* stable /across/ different catalog DBs.
  // Instead, use |name| for equality testing.
  int32 catalog_db_id = 2;
  // Source collection read by this transform.
  message Source {
    // Name of the collection.
    string name = 1 [ (gogoproto.casttype) = "Collection" ];
    // Selector of partitions of the collection which this transform reads.
    protocol.LabelSelector partitions = 2 [ (gogoproto.nullable) = false ];
  }
  Source source = 3 [ (gogoproto.nullable) = false ];
  // Shuffle applied to source documents for this transform.
  // Note that the Shuffle embeds the Transform name.
  Shuffle shuffle = 4 [ (gogoproto.nullable) = false ];
  // Derived collection produced by this transform.
  message Derivation {
    string name = 1 [ (gogoproto.casttype) = "Collection" ];
  }
  Derivation derivation = 5 [ (gogoproto.nullable) = false ];
}

// Field holds a column of values extracted from a document location.
message Field {
  // Value is the extracted representation of the field value.
  message Value {
    option (gogoproto.goproto_unrecognized) = false;
    option (gogoproto.goproto_unkeyed) = false;
    option (gogoproto.goproto_sizecache) = false;

    enum Kind {
      INVALID = 0;
      NULL = 1;
      TRUE = 2;
      FALSE = 3;
      STRING = 4;
      UNSIGNED = 5;
      SIGNED = 6;
      DOUBLE = 7;
      OBJECT = 8;
      ARRAY = 9;
    }
    Kind kind = 1;
    uint64 unsigned = 2;
    sint64 signed = 3;
    double double = 4;
    Slice bytes = 5 [ (gogoproto.nullable) = false ];
  };
  repeated Value values = 1 [ (gogoproto.nullable) = false ];
}

// RangeSpec describes the ranges of shuffle keys and r-clocks which a reader
// is responsible for.
message RangeSpec {
  // Byte [begin, end) exclusive range of keys to be shuffled to this reader.
  bytes key_begin = 2;
  bytes key_end = 3;
  // Rotated [begin, end) exclusive ranges of Clocks to be shuffled to this
  // reader.
  uint64 r_clock_begin = 4;
  uint64 r_clock_end = 5;
}

// ShuffleRequest is the request message of a Shuffle RPC.
message ShuffleRequest {
  JournalShuffle shuffle = 1 [ (gogoproto.nullable) = false ];
  RangeSpec range = 2 [ (gogoproto.nullable) = false ];
  // Offset to begin reading the journal from.
  int64 offset = 3
      [ (gogoproto.casttype) = "go.gazette.dev/core/broker/protocol.Offset" ];
  // Offset to stop reading the journal at, or zero if unbounded.
  int64 end_offset = 4
      [ (gogoproto.casttype) = "go.gazette.dev/core/broker/protocol.Offset" ];
  // Resolution header of the |config.coordinator_index| shard.
  protocol.Header resolution = 5;
}

// ShuffleResponse is the streamed response message of a Shuffle RPC.
message ShuffleResponse {
  // Status of the Shuffle RPC.
  consumer.Status status = 1;
  // Header of the response.
  protocol.Header header = 2;
  // Terminal error encountered while serving this ShuffleRequest. A terminal
  // error is only sent if a future ShuffleRequest of this same configuration
  // and offset will fail in the exact same way, and operator intervention is
  // required to properly recover. Such errors are returned so that the caller
  // can also abort with a useful, contextual error message.
  //
  // Examples of terminal errors include the requested journal not existing,
  // or data corruption. Errors *not* returned as |terminal_error| include
  // network errors, process failures, and other conditions which can be
  // retried.
  string terminal_error = 3;
  // Offset which was read through to produce this ShuffleResponse.
  int64 read_through = 4
      [ (gogoproto.casttype) = "go.gazette.dev/core/broker/protocol.Offset" ];
  // WriteHead of the journal as reported by the broker, as of the creation of
  // this ShuffleResponse.
  int64 write_head = 5
      [ (gogoproto.casttype) = "go.gazette.dev/core/broker/protocol.Offset" ];
  // Memory arena of this message.
  bytes arena = 6 [ (gogoproto.casttype) = "Arena" ];
  // Shuffled documents, each encoded in the 'application/json'
  // media-type.
  repeated Slice docs_json = 7 [ (gogoproto.nullable) = false ];
  // The begin offset of each document within the requested journal.
  repeated int64 begin = 8
      [ (gogoproto.casttype) = "go.gazette.dev/core/broker/protocol.Offset" ];
  // The end offset of each document within the journal.
  repeated int64 end = 9
      [ (gogoproto.casttype) = "go.gazette.dev/core/broker/protocol.Offset" ];
  // UUIDParts of each document.
  repeated UUIDParts uuid_parts = 10 [ (gogoproto.nullable) = false ];
  // Packed, embedded encoding of the shuffle key into a byte string.
  // If the Shuffle specified a Hash to use, it's applied as well.
  repeated Slice packed_key = 11 [ (gogoproto.nullable) = false ];
  // Extracted shuffle key of each document, with one Field for each
  // component of the composite shuffle key.
  repeated Field shuffle_key = 12 [ (gogoproto.nullable) = false ];
}

service Shuffler {
  rpc Shuffle(ShuffleRequest) returns (stream ShuffleResponse);
}

message ExtractRequest {
  // Memory arena of this message.
  bytes arena = 1 [ (gogoproto.casttype) = "Arena" ];
  // Documents to extract, each encoded in the 'application/json'
  // media-type.
  repeated Slice docs_json = 2 [ (gogoproto.nullable) = false ];
  // JSON pointer of the document UUID to extract.
  // If empty, UUIDParts are not extracted.
  string uuid_ptr = 3;
  // Field JSON pointers to extract from documents and return.
  // If empty, no fields are extracted.
  repeated string field_ptrs = 4;
};

message ExtractResponse {
  // Memory arena of this message.
  bytes arena = 1 [ (gogoproto.casttype) = "Arena" ];
  // UUIDParts extracted from request Documents.
  repeated UUIDParts uuid_parts = 2 [ (gogoproto.nullable) = false ];
  // Fields extracted from request Documents, one column per request pointer.
  repeated Field fields = 3 [ (gogoproto.nullable) = false ];
};

service Extract { rpc Extract(ExtractRequest) returns (ExtractResponse); }

message CombineRequest {
  // Open a CombineRequest.
  message Open {
    // Schema against which documents are to be validated,
    // and which provides reduction annotations.
    string schema_uri = 1;
    // Composite key used to group documents to be combined, specified as one or
    // more JSON-Pointers indicating a message location to extract.
    // If empty, all request documents are combined into a single response
    // document.
    repeated string key_ptr = 2;
    // Field JSON pointers to be extracted from combined documents and returned.
    // If empty, no fields are extracted.
    repeated string field_ptrs = 3;
    // JSON-Pointer at which a placeholder UUID should be inserted into
    // returned documents. If empty, no placeholder is inserted.
    string uuid_placeholder_ptr = 4;
    // Prune is true if this CombineRequest includes the root-most
    // (equivalently, left-most) document of each key. Depending on the
    // reduction strategy, additional pruning can be done in this case
    // (i.e., removing tombstones) that isn't possible in a partial
    // non-root reduction.
    bool prune = 5;
  };

  // Continue an opened CombineRequest.
  message Continue {
    // Memory arena of this message.
    bytes arena = 1 [ (gogoproto.casttype) = "Arena" ];
    // Request documents to combine, each encoded in the 'application/json'
    // media-type.
    repeated Slice docs_json = 2 [ (gogoproto.nullable) = false ];
  };

  oneof kind {
    Open open = 1;
    Continue continue = 2;
  };
}

message CombineResponse {
  // Memory arena of this message.
  bytes arena = 1 [ (gogoproto.casttype) = "Arena" ];
  // Combined response documents, each encoded in the 'application/json'
  // media-type.
  repeated Slice docs_json = 2 [ (gogoproto.nullable) = false ];
  // Fields extracted from request Documents, one column per request field
  // pointer.
  repeated Field fields = 3 [ (gogoproto.nullable) = false ];
}

service Combine {
  rpc Combine(stream CombineRequest) returns (stream CombineResponse);
}

// DeriveRequest is the streamed union type message of a Derive RPC.
message DeriveRequest {
  // Open is sent (only) as the first message of a Derive RPC,
  // and opens the derive transaction.
  message Open {
    // Collection to be derived.
    string collection = 1 [ (gogoproto.casttype) = "Collection" ];
  };
  // Continue extends the derive transaction with additional
  // source collection documents.
  //
  // * The flow consumer sends any number of EXTEND DeriveRequests,
  //   containing source collection documents.
  // * Concurrently, the derive worker responds with any number of
  //   EXTEND DeriveResponses, each having documents to be added to
  //   the collection being derived.
  // * The flow consumer is responsible for publishing each derived
  //   document to the appropriate collection & partition.
  // * Note that DeriveRequest and DeriveResponse Continue messages are _not_
  // 1:1.
  //
  // Continue transitions to continue or flush.
  message Continue {
    // Memory arena of this message.
    bytes arena = 1 [ (gogoproto.casttype) = "Arena" ];
    // Source documents to derive from, each encoded in the 'application/json'
    // media-type.
    repeated Slice docs_json = 2 [ (gogoproto.nullable) = false ];
    // UUIDParts extracted from documents.
    repeated UUIDParts uuid_parts = 3 [ (gogoproto.nullable) = false ];
    // Packed, embedded encoding of the shuffle key into a byte string.
    // If the Shuffle specified a Hash to use, it's applied as well.
    repeated Slice packed_key = 4 [ (gogoproto.nullable) = false ];
    // Catalog transform ID to which each document is applied.
    repeated int32 transform_id = 5;
  };
  // FLUSH indicates the transacton pipeline is to flush.
  //
  // * The flow consumer issues FLUSH when its consumer transaction begins to
  //   close.
  // * The derive worker responds with FLUSH to indicate that all source
  //   documents have been processed and all derived documents emitted.
  // * The flow consumer awaits the response FLUSH, while continuing to begin
  //   publish operations for all derived documents seen in the meantime.
  // * On seeing FLUSH, the flow consumer is assured it's sequenced and started
  //   publishing all derived documents of the transaction, and can now build
  //   the consumer.Checkpoint which will be committed to the store.
  //
  // FLUSH transitions to PREPARE.
  message Flush {
    // JSON-Pointer of the UUID placeholder in returned documents.
    string uuid_placeholder_ptr = 1;
    // Field JSON pointers to be extracted from combined documents and returned.
    // If empty, no fields are extracted.
    repeated string field_ptrs = 2;
  };
  // PREPARE begins a commit of the transaction.
  //
  // * The flow consumer sends PREPARE with its consumer.Checkpoint.
  // * On receipt, the derive worker queues an atomic recoverylog.Recorder
  //   block that's conditioned on an (unresolved) "commit" future. Within
  //   this recording block, underlying store commits (SQLite COMMIT and writing
  //   a RocksDB WriteBatch) are issued to persist all state changes of the
  //   transaction, along with the consumer.Checkpoint.
  // * The derive worker responds with PREPARE once all local commits have
  //   completed, and recoverylog writes have been queued (but not started,
  //   awaiting COMMIT).
  // * On receipt, the flow consumer arranges to invoke COMMIT on the completion
  //   of all outstanding journal writes -- this the OpFuture passed to the
  //   Store.StartCommit interface. It returns a future which will resolve only
  //   after reading COMMIT from this transaction -- the OpFuture returned by
  //   that interface.
  //
  // It's an error if a prior transaction is still running at the onset of
  // PREPARE. However at the completion of PREPARE, a new & concurrent
  // Transaction may begin, though it itself cannot PREPARE until this
  // Transaction fully completes.
  //
  // PREPARE transitions to COMMIT.
  message Prepare {
    // Checkpoint to commit.
    consumer.Checkpoint checkpoint = 1 [ (gogoproto.nullable) = false ];
  };

  oneof kind {
    Open open = 1;
    Continue continue = 2;
    Flush flush = 3;
    Prepare prepare = 4;
  };
}

// DeriveResponse is the streamed response message of a Derive RPC.
message DeriveResponse {

  message Continue {};

  oneof kind {
    // Continue extends the derive transaction with additional derived
    // and combined collection documents.
    Continue continue = 1;
    // Flushed CombineResponses are sent in response to a request flush.
    // An empty CombineResponse signals that no further responses remain
    // to be sent, and the server is ready to prepare to commit.
    CombineResponse flush = 2;
  };
};

service Derive {
  // RestoreCheckpoint recovers the most recent Checkpoint previously committed
  // to the Store. It is called just once, at Shard start-up. If an external
  // system is used, it should install a transactional "write fence" to ensure
  // that an older Store instance of another process cannot successfully
  // StartCommit after this RestoreCheckpoint returns.
  rpc RestoreCheckpoint(google.protobuf.Empty) returns (consumer.Checkpoint);

  // Derive begins a pipelined derive transaction, following the
  // state machine detailed in DeriveState.
  rpc Derive(stream DeriveRequest) returns (stream DeriveResponse);

  // BuildHints returns FSMHints which may be played back to fully reconstruct
  // the local filesystem state produced by this derive worker. It may block
  // while pending operations sync to the recovery log.
  rpc BuildHints(google.protobuf.Empty) returns (recoverylog.FSMHints);

  // ClearRegisters is a testing-only API which resets registers in between test cases.
  rpc ClearRegisters(google.protobuf.Empty) returns (google.protobuf.Empty);
}

// IngestRequest describes documents to ingest into collections.
message IngestRequest {
  // Collection describes an ingest into a collection.
  message Collection {
    // Name of the collection into which to ingest.
    string name = 1 [ (gogoproto.casttype) = "Collection" ];
    // Newline-separated JSON documents to ingest.
    bytes docs_json_lines = 2;
  };
  repeated Collection collections = 1 [ (gogoproto.nullable) = false ];
}

// IngestResponse is the result of an Ingest RPC.
message IngestResponse {
  // Journals appended to by this ingestion, and their maximum offset on commit.
  map<string, int64> journal_write_heads = 1 [
    (gogoproto.castkey) = "go.gazette.dev/core/broker/protocol.Journal",
    (gogoproto.castvalue) = "go.gazette.dev/core/broker/protocol.Offset"
  ];
  // Etcd header which describes current journal partitions.
  protocol.Header.Etcd journal_etcd = 2 [ (gogoproto.nullable) = false ];
}

// Ingester offers transactional ingest of documents into collections.
service Ingester {
  rpc Ingest(IngestRequest) returns (IngestResponse);
  // rpc IngestMany(stream IngestRequest) returns (IngestResponse);
  // rpc IngestStream(stream IngestRequest) returns (stream IngestResponse);
}

// AdvanceTimeRequest is a testing-only request to modify effective test time.
message AdvanceTimeRequest {
  // Number of seconds to add to the current clock delta.
  uint64 add_clock_delta_seconds = 1;
}

// AdvanceTimeRequest is a testing-only response to modify effective test time.
message AdvanceTimeResponse {
  // Current effective delta from wall-clock time, in seconds.
  uint64 clock_delta_seconds = 1;
}

// ClearRegistersRequest is a testing-only request to remove all registers of a shard.
message ClearRegistersRequest {
  protocol.Header header = 1;
  string shard_id = 2
      [ (gogoproto.casttype) =
            "go.gazette.dev/core/consumer/protocol.ShardID" ];
}

// ClearRegistersResponse is a testing-only response to remove all registers of a shard.
message ClearRegistersResponse {
  consumer.Status status = 1;
  protocol.Header header = 2 [ (gogoproto.nullable) = false ];
}

service Testing {
  rpc AdvanceTime(AdvanceTimeRequest) returns (AdvanceTimeResponse);
  rpc ClearRegisters(ClearRegistersRequest) returns (ClearRegistersResponse);
}
