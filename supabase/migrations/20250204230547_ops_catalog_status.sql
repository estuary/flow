begin;

create table public.connector_status (
    catalog_name public.catalog_name not null primary key,
    flow_document json not null
);
alter table public.connector_status owner to stats_loader;

comment on table public.connector_status is
'Holds the most recent status of the connector for each task, which is materialized
by the ops catalog.';
comment on column public.connector_status.catalog_name is
'The name of the task that the connector status is for. Tasks having multiple
shards will have a single connector status row, with the status itself being a
reduction of the statuses from all shards';
comment on column public.connector_status.flow_document is
'The contents of the `flow_document` column are serialized instances of the
`ConnectorStatus` struct defined in `crates/models/src/status/connector.rs`.';


create table public.shard_failures (
    catalog_name public.catalog_name not null,
    build public.flowid not null,
    ts timestamptz not null,
    flow_document json not null
);
alter table public.shard_failures owner to stats_loader;
-- Index to support controllers deleting old shard failures by name and build.
create index on public.shard_failures (catalog_name, build);
-- Index to support the cleanup cron job, and maybe also controllers deleting
-- shard failures by time.
create index on public.shard_failures (ts);

comment on table public.shard_failures is
'Records of task shard failures, which are materialized by the ops catalog using delta updates.
This table is effectively append-only. An insert into this table will trigger a run of the
corresponding live spec controller, which will respond to the failure and cleanup unneeded rows.';
comment on column public.shard_failures.catalog_name is
'The name of the task that the shard failure is for.';
comment on column public.shard_failures.build is
'The id of the build that the shard was running when it failed.';
comment on column public.shard_failures.ts is
'The timestamp of when the failure event was generated by the Flow runtime';
comment on column public.shard_failures.flow_document is
'The contents of the `flow_document` column are serialized instances of the
`ShardFailure` struct defined in `crates/models/src/status/activation.rs`.';


create function internal.on_shard_failure() returns trigger
LANGUAGE plpgsql SECURITY DEFINER
AS $$
declare
    controller_task_id public.flowid;
begin

    select ls.controller_task_id into controller_task_id
    from live_specs ls
    where ls.catalog_name = new.catalog_name;

    -- It is possible for shard failures to be observed after the task has been deleted.
    -- In that case, we just ignore the failure.
    if controller_task_id is not null then
        perform internal.send_to_task(
            controller_task_id,
            '00:00:00:00:00:00:00:00'::flowid,
            '{"type":"shard_failed"}'
            );
    end if;
return null;
end;
$$;

create function internal.delete_old_shard_failures() returns integer
LANGUAGE sql SECURITY DEFINER
as $$
    with deleted as (
        delete from public.shard_failures
        where ts < (now() - '48h'::interval)
        returning ts
    )
    select count(*) from deleted;
$$;

-- Run every weekday at 5pm. Controllers are supposed to clean up these failures
-- after at most 25 hours. But controllers cannot do so for failures that are
-- materialized after the live spec (along with the controller) has been
-- deleted, or if the controller itself is failing. So this is a catch-all to
-- clean up any shard failures don't get deleted by the normal process. Failures are
-- deleted after 48 hours, to give us time to investigate any potential issues that
-- may have prevented their deletion.
select cron.schedule('delete_old_shard_failures', '0 17 * * 1-5', 'internal.delete_old_shard_failures');

create trigger on_shard_failure_insert after insert on public.shard_failures
for each row
execute function internal.on_shard_failure();


alter table public.data_planes add column ops_l1_events_name public.catalog_name;
alter table public.data_planes add column ops_l2_events_transform text;

comment on column public.data_planes.ops_l1_events_name is
'The name of the L1 events derivation in the ops catalog';
comment on column public.data_planes.ops_l2_events_transform is
'The name of the events transform in the L2 events rollup of the ops catalog';

-- Populate these new columns. Note that the legacy data plane needs some
-- special handling because it doesn't follow the normal naming convention.
-- The substr call is to strip off the 'ops/dp/' prefix.
update public.data_planes
set
    ops_l1_events_name = case when data_plane_name = 'ops/dp/public/gcp-us-central1-c1'
        then 'ops/rollups/L1/public/gcp-us-central1-c1/events'
        else concat('ops/rollups/L1/', substr(data_plane_name, 8), '/events')
        end,
    ops_l2_events_transform = concat('from.', data_plane_fqdn, '.events')
where
    ops_l1_events_name is null and ops_l2_events_transform is null;

alter table public.data_planes alter column ops_l1_events_name set not null;
alter table public.data_planes alter column ops_l2_events_transform set not null;

-- We need to update the `catalog_stats*` tables in order to allow a trailing
-- slash in the `catalog_name`. This is a gross and annoying process because
-- there's views that depend on that column, which all need to be dropped and
-- re-created. We also need to drop and re-create the rls policy.
drop policy "Users must be authorized to the catalog name" on public.catalog_stats;
drop VIEW internal.new_free_trial_tenants;
drop view public.alert_all;
drop VIEW internal.alert_data_movement_stalled;

-- Next we alter the table, which requires a detaching and re-attaching the table partitions;
alter table public.catalog_stats detach partition public.catalog_stats_hourly;
alter table public.catalog_stats detach partition public.catalog_stats_daily;
alter table public.catalog_stats detach partition public.catalog_stats_monthly;

alter table public.catalog_stats_hourly alter column catalog_name type text using catalog_name::text;
alter table public.catalog_stats_daily alter column catalog_name type text using catalog_name::text;
alter table public.catalog_stats_monthly alter column catalog_name type text using catalog_name::text;
alter table public.catalog_stats alter column catalog_name type text using catalog_name::text;

ALTER TABLE ONLY public.catalog_stats ATTACH PARTITION public.catalog_stats_daily FOR VALUES IN ('daily');
ALTER TABLE ONLY public.catalog_stats ATTACH PARTITION public.catalog_stats_hourly FOR VALUES IN ('hourly');
ALTER TABLE ONLY public.catalog_stats ATTACH PARTITION public.catalog_stats_monthly FOR VALUES IN ('monthly');

-- Now we can re-create the RLS policy and views.
CREATE POLICY "Users must be authorized to the catalog name" ON public.catalog_stats
FOR SELECT USING (
    ( EXISTS (
        SELECT 1
        FROM public.auth_roles('read'::public.grant_capability) r(role_prefix, capability)
        WHERE (catalog_stats.catalog_name ^@ (r.role_prefix)::text)
    ))
);

-- This view differs from its original definition, since it can now take
-- advantage of the pre-aggregated catalog stats instead of aggregating them
-- using `group by`. Also, the `today_captures` and `today_materializations`
-- columns are removed because they were unused.
CREATE VIEW internal.new_free_trial_tenants AS WITH hours_by_day AS (
  SELECT
    t.tenant,
    cs.ts,
    (
      cs.usage_seconds :: numeric / 3600.0
    ) as daily_usage_hours
  FROM
    public.tenants as t
    join public.catalog_stats_daily cs on t.tenant :: text = cs.catalog_name
  WHERE
    t.trial_start IS NULL -- Where the tenant has used more than 52.8 hours of task time in a given day.
    and (
      cs.usage_seconds :: numeric / 3600.0
    ) > 52.8
),
hours_by_month AS (
  SELECT
    t.tenant,
    cs.ts,
    cs.usage_seconds :: numeric / 3600.0 as monthly_usage_hours
  FROM
    public.tenants t
    join public.catalog_stats_monthly cs on t.tenant :: text = cs.catalog_name
  WHERE
    t.trial_start IS NULL
    and (
      cs.usage_seconds :: numeric / 3600.0
    ) > (24 * 31 * 2):: numeric * 1.1
),
gbs_by_month AS (
  SELECT
    t.tenant,
    cs.ts,
    ceil(
      (
        cs.bytes_written_by_me + cs.bytes_read_by_me
      ):: numeric / (10.0 ^ 9.0)
    ) AS monthly_usage_gbs
  FROM
    public.tenants t
    join public.catalog_stats_monthly cs on t.tenant = cs.catalog_name
  WHERE
    t.trial_start IS NULL
    and ceil(
      (
        cs.bytes_written_by_me + cs.bytes_read_by_me
      ):: numeric / (10.0 ^ 9.0)
    ) > 10.0
)
SELECT
  t.tenant,
  max(hours_by_day.daily_usage_hours) AS max_daily_usage_hours,
  max(
    hours_by_month.monthly_usage_hours
  ) AS max_monthly_usage_hours,
  max(gbs_by_month.monthly_usage_gbs) AS max_monthly_gb
FROM
  public.tenants t
  left join hours_by_day on t.tenant = hours_by_day.tenant
  left join hours_by_month on t.tenant = hours_by_month.tenant
  left join gbs_by_month on t.tenant = gbs_by_month.tenant
group by
  t.tenant;

ALTER VIEW internal.new_free_trial_tenants OWNER TO postgres;

-- This only changes to this are to cast `alert_data_processing.catalog_name` to `text`.
create view internal.alert_data_movement_stalled as
select
  'data_movement_stalled' :: public.alert_type as alert_type,
  alert_data_processing.catalog_name as catalog_name,
  json_build_object(
    'bytes_processed',
    coalesce(
      sum(
        catalog_stats_hourly.bytes_written_by_me + catalog_stats_hourly.bytes_written_to_me + catalog_stats_hourly.bytes_read_by_me
      ),
      0
    ):: bigint,
    'recipients',
    array_agg(
      distinct jsonb_build_object(
        'email', alert_subscriptions.email,
        'full_name', auth.users.raw_user_meta_data ->> 'full_name'
      )
    ),
    'evaluation_interval',
    alert_data_processing.evaluation_interval,
    'spec_type',
    live_specs.spec_type
  ) as arguments,
  true as firing
from
  public.alert_data_processing
  left join public.live_specs on alert_data_processing.catalog_name = live_specs.catalog_name
  and live_specs.spec is not null
  and (
    live_specs.spec -> 'shards' ->> 'disable'
  ):: boolean is not true
  left join public.catalog_stats_hourly on alert_data_processing.catalog_name::text = catalog_stats_hourly.catalog_name
  and catalog_stats_hourly.ts >= date_trunc(
    'hour', now() - alert_data_processing.evaluation_interval
  )
  left join public.alert_subscriptions on alert_data_processing.catalog_name ^@ alert_subscriptions.catalog_prefix
  and email is not null
  left join auth.users on auth.users.email = alert_subscriptions.email
  and auth.users.is_sso_user is false
where
  live_specs.created_at <= date_trunc(
    'hour', now() - alert_data_processing.evaluation_interval
  )
group by
  alert_data_processing.catalog_name,
  alert_data_processing.evaluation_interval,
  live_specs.spec_type
having
  coalesce(
    sum(
      catalog_stats_hourly.bytes_written_by_me + catalog_stats_hourly.bytes_written_to_me + catalog_stats_hourly.bytes_read_by_me
    ),
    0
  ):: bigint = 0;

CREATE VIEW public.alert_all AS
SELECT
  alert_free_trial.alert_type,
  alert_free_trial.catalog_name,
  alert_free_trial.arguments,
  alert_free_trial.firing
FROM
  internal.alert_free_trial
UNION ALL
SELECT
  alert_free_trial_ending.alert_type,
  alert_free_trial_ending.catalog_name,
  alert_free_trial_ending.arguments,
  alert_free_trial_ending.firing
FROM
  internal.alert_free_trial_ending
UNION ALL
SELECT
  alert_free_trial_stalled.alert_type,
  alert_free_trial_stalled.catalog_name,
  alert_free_trial_stalled.arguments,
  alert_free_trial_stalled.firing
FROM
  internal.alert_free_trial_stalled
UNION ALL
SELECT
  alert_missing_payment_method.alert_type,
  alert_missing_payment_method.catalog_name,
  alert_missing_payment_method.arguments,
  alert_missing_payment_method.firing
FROM
  internal.alert_missing_payment_method
UNION ALL
SELECT
  alert_data_movement_stalled.alert_type,
  alert_data_movement_stalled.catalog_name,
  alert_data_movement_stalled.arguments,
  alert_data_movement_stalled.firing
FROM
  internal.alert_data_movement_stalled;

ALTER VIEW public.alert_all OWNER TO postgres;

-- Fix up the billing function so that it doesn't count prefix stats in addition
-- to the tasks themselves. That is the only change here. We might one day want
-- to have this function just directly query the tenant-level rollups instead of
-- aggregating all of the individual tasks, but we can't do that yet because the
-- aggregates will be incomplete for the current month.
CREATE or replace FUNCTION internal.billing_report_202308(billed_prefix public.catalog_prefix, billed_month timestamp with time zone) RETURNS jsonb
    LANGUAGE plpgsql SECURITY DEFINER
    AS $$
declare
  -- Output variables.
  o_daily_usage       jsonb;
  o_data_gb           numeric;
  o_line_items        jsonb = '[]';
  o_recurring_fee     integer;
  o_subtotal          integer;
  o_task_hours        numeric;
  o_trial_credit      integer;
  o_free_tier_credit  integer;
  o_trial_start       date;
  o_trial_range       daterange;
  o_free_tier_range   daterange;
  o_billed_range      daterange;
begin

  -- Ensure `billed_month` is the truncated start of the billed month.
  billed_month = date_trunc('month', billed_month);

  with vars as (
    select
      t.data_tiers,
      t.trial_start,
      t.usage_tiers,
      tstzrange(billed_month, billed_month  + '1 month', '[)') as billed_range,
      case when t.trial_start is not null
        then daterange(t.trial_start::date, ((t.trial_start::date) + interval '1 month')::date, '[)')
        else 'empty' end as trial_range,
      -- In order to smoothly transition between free tier credit and free trial credit,
      -- the free tier covers all usage up to, but _not including_ the trial start date.
      -- On the trial start date, the free trial credit takes over.
      daterange(NULL, t.trial_start::date, '[)') as free_tier_range,
      -- Reveal contract costs only when computing whole-tenant billing.
      case when t.tenant = billed_prefix then t.recurring_usd_cents else 0 end as recurring_fee
      from tenants t
      where billed_prefix ^@ t.tenant -- Prefix starts with tenant.
  ),
  -- Roll up each day's incremental usage.
  daily_stat_deltas as (
    select
      ts,
      sum(bytes_written_by_me + bytes_read_by_me) / (10.0^9.0) as data_gb,
      sum(usage_seconds) / (60.0 * 60) as task_hours
    from catalog_stats, vars
      where catalog_name ^@ billed_prefix -- Name starts with prefix.
      and catalog_name not like '%/' -- filter out pre-aggregated prefix stats
      and grain = 'daily'
      and billed_range @> ts
      group by ts
  ),
  -- Map to cumulative daily usage.
  -- Note sum(...) over (order by ts) yields the running sum of its aggregate.
  daily_stats as (
    select
      ts,
      sum(data_gb) over w as data_gb,
      sum(task_hours) over w as task_hours
    from daily_stat_deltas
    window w as (order by ts)
  ),
  -- Extend with line items for each category for the period ending with the given day.
  daily_line_items as (
    select
      daily_stats.*,
      internal.tier_line_items(ceil(data_gb)::integer, data_tiers, 'Data processing', 'GB') as data_line_items,
      internal.tier_line_items(ceil(task_hours)::integer, usage_tiers, 'Task usage', 'hour') as task_line_items
    from daily_stats, vars
  ),
  -- Extend with per-category subtotals for the period ending with the given day.
  daily_totals as (
    select
      daily_line_items.*,
      data_subtotal,
      task_subtotal
    from daily_line_items,
      lateral (select sum((li->>'subtotal')::numeric) as data_subtotal from jsonb_array_elements(data_line_items) li) l1,
      lateral (select sum((li->>'subtotal')::numeric) as task_subtotal from jsonb_array_elements(task_line_items) li) l2
  ),
  -- Map cumulative totals to per-day deltas.
  daily_deltas as (
    select
      ts,
      data_gb       - (coalesce(lag(data_gb,         1) over w, 0)) as data_gb,
      data_subtotal - (coalesce(lag(data_subtotal,   1) over w, 0)) as data_subtotal,
      task_hours    - (coalesce(lag(task_hours,      1) over w, 0)) as task_hours,
      task_subtotal - (coalesce(lag(task_subtotal,   1) over w, 0)) as task_subtotal
      from daily_totals
      window w as (order by ts)
  ),
  -- 1) Group daily_deltas into a JSON array
  -- 2) Sum a trial credit from daily deltas that overlap with the trial period.
  daily_array_and_trial_credits as (
    select
    jsonb_agg(jsonb_build_object(
      'ts', ts,
      'data_gb', data_gb,
      'data_subtotal', data_subtotal,
      'task_hours', task_hours,
      'task_subtotal', task_subtotal
    )) as daily_usage,
    coalesce(sum(data_subtotal + task_subtotal) filter (where trial_range @> (ts::date)),0 ) as trial_credit,
    coalesce(sum(data_subtotal + task_subtotal) filter (where free_tier_range @> (ts::date)),0 ) as free_tier_credit
    from daily_deltas, vars
  ),
  -- The last day captures the cumulative billed period.
  last_day as (
    select * from daily_line_items
    order by ts desc limit 1
  ),
  -- If we're reporting for the whole tenant then gather billing adjustment line-items.
  adjustments as (
    select coalesce(jsonb_agg(
      jsonb_build_object(
        'description', detail,
        'count', 1,
        'rate', usd_cents,
        'subtotal', usd_cents
      )
    ), '[]') as adjustment_line_items
    from internal.billing_adjustments a
    where a.tenant = billed_prefix and a.billed_month = billing_report_202308.billed_month
  )
  select into
    -- Block of variables being selected into.
    o_daily_usage,
    o_data_gb,
    o_line_items,
    o_recurring_fee,
    o_task_hours,
    o_trial_credit,
    o_trial_start,
    o_trial_range,
    o_billed_range,
    o_free_tier_credit,
    o_free_tier_range
    -- The actual selected columns.
    daily_usage,
    data_gb,
    data_line_items || task_line_items || adjustment_line_items,
    recurring_fee,
    task_hours,
    trial_credit,
    trial_start,
    trial_range,
    billed_range,
    free_tier_credit,
    free_tier_range
  from daily_array_and_trial_credits, last_day, adjustments, vars;

  -- Add line items for recurring service fee & free trial credit.
  if o_recurring_fee != 0 then
    o_line_items = jsonb_build_object(
      'description', 'Recurring service charge',
      'count', 1,
      'rate', o_recurring_fee,
      'subtotal', o_recurring_fee
    ) || o_line_items;
  end if;

  -- Display a (possibly zero) free trial credit if the trial range overlaps the billed range
  if o_trial_range && o_billed_range then
    o_line_items = o_line_items || jsonb_build_object(
      'description', format('Free trial credit (%s - %s)', lower(o_trial_range), (upper(o_trial_range) - interval '1 day')::date),
      'count', 1,
      'rate', -o_trial_credit,
      'subtotal', -o_trial_credit
    );
  end if;

  -- Display the free tier credit if the free tier range overlaps the billed range
  if o_free_tier_range && o_billed_range then
    o_line_items = o_line_items || jsonb_build_object(
      'description', case when upper(o_free_tier_range) is not null
        then format('Free tier credit ending %s', (upper(o_free_tier_range) - interval '1 day')::date)
        else 'Free tier credit'
      end,
      'count', 1,
      'rate', -o_free_tier_credit,
      'subtotal', -o_free_tier_credit
    );
  end if;

  -- Roll up the final subtotal.
  select into o_subtotal sum((l->>'subtotal')::numeric)
    from jsonb_array_elements(o_line_items) l;

  return jsonb_build_object(
    'billed_month', billed_month,
    'billed_prefix', billed_prefix,
    'daily_usage', o_daily_usage,
    'line_items', o_line_items,
    'processed_data_gb', o_data_gb,
    'recurring_fee', o_recurring_fee,
    'subtotal', o_subtotal,
    'task_usage_hours', o_task_hours,
    'trial_credit', coalesce(o_trial_credit, 0),
    'free_tier_credit', coalesce(o_free_tier_credit, 0),
    'trial_start', o_trial_start
  );

end
$$;


ALTER FUNCTION internal.billing_report_202308(billed_prefix public.catalog_prefix, billed_month timestamp with time zone) OWNER TO postgres;

commit;
