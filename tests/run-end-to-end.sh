#!/bin/bash
#
# This script runs the examples catalog to completion using a temp-data-plane in --poll mode,
# and outputs selected materializations.

# -e causes the script to exit on encountering an error
# -m turns on job management, required for our use of `fg` below.
set -em


ROOTDIR="$(realpath $(git rev-parse --show-toplevel))"
cd "${ROOTDIR}"

function bail() {
    echo "$@" 1>&2
    exit 1
}

# The test to run, a folder name relative to `tests` directory
TEST=$1
# Root of the running test
TEST_ROOT="${ROOTDIR}/tests/${TEST}"

# Temporary test directory into which we'll build our test database,
# and stage temporary data plane files.
TESTDIR="$(mktemp -d -t flow-end-to-end-XXXXXXXXXX)"

echo "temporary test directory: $TESTDIR"

# Move sshd configs to the temp dir, which will be removed after execution.
cp -r "${ROOTDIR}/tests/sshforwarding/sshd-configs" "${TESTDIR}"

# Docker compose file for starting / stopping the testing SSH server and PSQL DB.
SSH_PSQL_DOCKER_COMPOSE="${TESTDIR}/sshd-configs/docker-compose.yaml"
function startTestInfra() {
  docker-compose --file ${SSH_PSQL_DOCKER_COMPOSE} up --detach
  # Allow postgres to be prepared.
  sleep 10
}
function stopTestInfra() {
  docker-compose --file ${SSH_PSQL_DOCKER_COMPOSE} down
}

function cleanupDataIfPassed() {
    if [[ -z "$TESTS_PASSED" ]]; then
        echo "Tests failed, retaining data dir: $TESTDIR"
    else
        echo "Tests passed, deleting data dir: $TESTDIR"
	# Need sudo and force deletion to delete some read-only temp files owned by root,
	# which are generated by the test infra of openssh server during testing.
        sudo rm -rf "$TESTDIR"
    fi
}

# Start local ssh server and postgres database.
startTestInfra

# `flowctl` commands which interact with the data plane look for *_ADDRESS
# variables, which are used by the temp-data-plane we're about to start.
export BROKER_ADDRESS=unix://localhost${TESTDIR}/gazette.sock
export CONSUMER_ADDRESS=unix://localhost${TESTDIR}/consumer.sock

# Start an empty local data plane within our TESTDIR as a background job.
# --poll so that connectors are polled rather than continuously tailed.
# --sigterm to verify we cleanly tear down the test catalog (otherwise it hangs).
# --tempdir to use our known TESTDIR rather than creating a new temporary directory.
# --unix-sockets to create UDS socket files in TESTDIR in well-known locations.
flowctl temp-data-plane \
    --poll \
    --sigterm \
    --tempdir ${TESTDIR} \
    --unix-sockets \
    &
DATA_PLANE_PID=$!

# `flowctl temp-data-plane` always uses ./builds/ of --tempdir as its --flow.builds-root.
# See cmd-temp-data-plane.go.
export BUILDS_ROOT=${TESTDIR}/builds

# Arrange to stop the data plane on exit and remove the temporary directory.
trap "kill -s SIGTERM ${DATA_PLANE_PID} && wait ${DATA_PLANE_PID} && stopTestInfra && cleanupDataIfPassed" EXIT

BUILD_ID=run-end-to-end-${TEST}

# Build the catalog. Arrange for it to be removed on exit.
flowctl api build \
    --directory ${TESTDIR}/catalog-build \
    --build-id ${BUILD_ID} \
    --source ${TEST_ROOT}/flow.yaml \
    --ts-package \
    || bail "Catalog build failed."
# Move the built database to the data plane's builds root.
mv ${TESTDIR}/catalog-build/${BUILD_ID} ${BUILDS_ROOT}/
# Activate the catalog.
flowctl api activate --build-id ${BUILD_ID} --all || bail "Activate failed."
# Wait for polling pass to finish.
flowctl api await --build-id ${BUILD_ID} || bail "Await failed."

function ssh_psql_exec() {
    docker-compose --file ${SSH_PSQL_DOCKER_COMPOSE} exec -T -e PGPASSWORD=flow postgres psql -w -U flow -d flow "$@"
}

shopt -s nullglob
# Data saved in tunneled postgres
for table_expected in ${TEST_ROOT}/*.tunnel.rows; do
    table_id=$(basename $table_expected .tunnel.rows)
    actual=${TESTDIR}/${table_id}.rows

    columns=$(head -n 1 $table_expected | sed 's/,/","/g')

    ssh_psql_exec -c "SELECT \"$columns\" FROM $table_id;" --csv -P pager=off >> $actual
    diff --suppress-common-lines $actual $table_expected || bail "Test failed"
done

# Data saved in local postgres
for table_expected in ${TEST_ROOT}/*.local.rows; do
    table_id=$(basename $table_expected .local.rows)
    actual=${TESTDIR}/${table_id}.rows

    columns=$(head -n 1 $table_expected | sed 's/,/","/g')

    psql -h localhost -w -U flow -d flow -c "SELECT \"$columns\" FROM $table_id;" --csv -P pager=off >> $actual
    diff --suppress-common-lines $actual $table_expected || bail "Test failed"
done

# Logs from connector. In this case we don't do a full diff between all lines, we just check
# that the expected logs exist among all the logs from the connector.
# Logs from the runtime and connector can be volatile, but there are certain logs that are important signals for us
for table_expected in ${TEST_ROOT}/logs; do
    table_id="flow_logs"
    actual=${TESTDIR}/${table_id}

    columns=$(head -n 1 $table_expected | sed 's/,/","/g')

    psql -h localhost -w -U flow -d flow -c "SELECT \"$columns\" FROM $table_id;" --csv -P pager=off >> $actual
    # content of $table_expected must be found in $actual
    cat $actual | grep $(tail +2 $table_expected)
done

# TODO: support checking exit status code of connector

## Clean up the activated catalog.
flowctl api delete --build-id ${BUILD_ID} --all || bail "Delete failed."

# Setting this to true will cause TESTDIR to be cleaned up on exit
TESTS_PASSED=true
